{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xml.etree.ElementTree.ElementTree at 0x7f074228a160>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import argparse, os, csv       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A.1:  Okay. /  {F Uh, } first, {F um, } I need to know, {F uh, } \n",
      "\n",
      "how do you feel [ about, + {F uh, } about ]\n",
      "\n",
      "sending, {F uh, } an elderly, {F uh, } family member to a nursing home? /\n",
      "\n",
      "\n",
      "\n",
      "B.2:  {D Well, } of course, [ it's, + {D you know, } it's ]\n",
      "\n",
      " one of the last few things in the\n",
      "\n",
      "world you'd ever want to do, {D you know. }  \n",
      "\n",
      "Unless it's just, {D you know, } really,\n",
      "\n",
      "{D you know, }  and, {F uh, } [ for their, + {F uh, } {D you know, } \n",
      "\n",
      "for their ] own good. /\n",
      "\n",
      "\n",
      "\n",
      "A.3:  Yes. /  Yeah. /\n",
      "\n",
      "\n",
      "\n",
      "B.4:  I'd be very very careful [ and, + ] {F uh, } {D you know, } \n",
      "\n",
      "checking them out. / {F Uh, } our, -/ \n",
      "\n",
      "had t-, place my mother in a nursing home. /\n",
      "\n",
      "  She had a rather massive stroke\n",
      "\n",
      "[ about, + {F uh, } about ] --\n",
      "\n",
      "\n",
      "\n",
      "A.5:  Uh-huh. /\n",
      "\n",
      "\n",
      "\n",
      "B.6:  -- {F uh, } eight months ago I guess. / {C And, }  {F uh, } \n",
      "\n",
      "[ we were, + I was ] fortunate in that /\n",
      "\n",
      "I was personally acquainted with the, {F uh, } \n",
      "\n",
      "people who, {F uh, } ran the nursing home\n",
      "\n",
      "in our little hometown. /\n",
      "\n",
      "\n",
      "\n",
      "A.7:  Yeah. /\n",
      "\n",
      "\n",
      "\n",
      "B.8:  {C So, } I was very comfortable, {D you know, } \n",
      "\n",
      "in doing it when it got to the point\n",
      "\n",
      "that we had to do it. /  {C But } there's, -/ {D well, } \n",
      "\n",
      "I had an occasion for my\n",
      "\n",
      "mother-in-law who had fell and [ needed to be, + ] {D you know, } \n",
      "\n",
      "could not take care of\n",
      "\n",
      "herself anymore, was confined to a nursing home for a while / that was really\n",
      "\n",
      "not a very good experience. /  {F Uh, } it had to be done \n",
      "\n",
      "in a hurry. /  {E I mean, } we\n",
      "\n",
      "didn't have, {D you know, } {D like } six months to check \n",
      "\n",
      "all of these places out. /  {C And }\n",
      "\n",
      "it was really [ not, + not ] very good, {F uh, } deal. /\n",
      "\n",
      "\n",
      "\n",
      "A.9:  Yeah. /\n",
      "\n",
      "\n",
      "\n",
      "B.10:  We were not really happy with,\n",
      "\n",
      "\n",
      "\n",
      "A.11:  Yeah. /\n",
      "\n",
      "\n",
      "\n",
      "B.12:  nursing home that we finally had. /  Fortunately, she only had to stay\n",
      "\n",
      "<noise> <<faint in background>> a few weeks /\n",
      "\n",
      " {C and } she was able [ to, + to ] return to\n",
      "\n",
      "her apartment again. / {C But } it's really a [ big, + {F uh, } \n",
      "\n",
      " big, ] {F uh, }\n",
      "\n",
      "\n",
      "\n",
      "A.13:  Yeah. /\n",
      "\n",
      "\n",
      "\n",
      "B.14: decision [ as to, +  {D you know, } when to ] do it. /  {D You know, } \n",
      "\n",
      "is there something\n",
      "\n",
      "else we could have done, {D you know, } in checking out all \n",
      "\n",
      "the places that, {F uh, }\n",
      "\n",
      "might be available. / Of course, {D you know, } there's not\n",
      "\n",
      " one on every corner,\n",
      "\n",
      "\n",
      "\n",
      "A.15:  Yeah. /\n",
      "\n",
      "\n",
      "\n",
      "B.16:  especially, {D you know, } smaller areas,\n",
      "\n",
      "\n",
      "\n",
      "A.17:  Uh-huh. /\n",
      "\n",
      "\n",
      "\n",
      "B.18:  smaller towns. /\n",
      "\n",
      "\n",
      "\n",
      "A.19:  Uh-huh. /  Yeah. / Probably the hardest thing [ in, + in ]\n",
      "\n",
      " my family, {F uh, } my\n",
      "\n",
      "grandmother, she had to be put in a nursing home / {C and, }  {F um, } \n",
      "\n",
      "she had used the\n",
      "\n",
      "walker [ for, + for ] quite some time, probably about six to \n",
      "\n",
      "nine months. /  {C And, }  {F um, }\n",
      "\n",
      "she had a fall / {C and, }  {F uh, } finally, {F uh, } she\n",
      "\n",
      " had Parkinson's disease, /\n",
      "\n",
      "\n",
      "\n",
      "B.20:  Oh. /\n",
      "\n",
      "\n",
      "\n",
      "A.21:  {C and } it got so much that she could not take care of\n",
      "\n",
      " her house. /\n",
      "\n",
      "\n",
      "\n",
      "B.22:  Right. /\n",
      "\n",
      "\n",
      "\n",
      "A.23:  Then she lived in an apartment / {C and, }  {F uh, } \n",
      "\n",
      "that was even harder --\n",
      "\n",
      "\n",
      "\n",
      "B.24:  Uh-huh. /\n",
      "\n",
      "\n",
      "\n",
      "A.25:  -- actually. /\n",
      "\n",
      "\n",
      "\n",
      "B.26:  Uh-huh. /\n",
      "\n",
      "\n",
      "\n",
      "A.27:  {C Because } [ it was, + {D you know, } it was ]\n",
      "\n",
      " just a [ change of, + change of ] location / {C and }\n",
      "\n",
      "it was very disturbing for her because she had been so used to traveling. /\n",
      "\n",
      "\n",
      "\n",
      "B.28:  Yes. /\n",
      "\n",
      "\n",
      "\n",
      "A.29:  {E I mean, } [ [ she tr-, + she had, ] + she had ]\n",
      "\n",
      " children all across the United States -- /\n",
      "\n",
      "\n",
      "\n",
      "B.30:  Uh-huh /\n",
      "\n",
      "\n",
      "\n",
      "A.31:  -- {C and, } {D you know, } she spent nine months out of \n",
      "\n",
      "the year just visiting her\n",
      "\n",
      "children. /\n",
      "\n",
      "\n",
      "\n",
      "B.32:  Right. /  Uh-huh. /\n",
      "\n",
      "\n",
      "\n",
      "A.33:  {C And, }  {F um, } that was pretty heart-rending for her. /\n",
      "\n",
      "\n",
      "\n",
      "B.34:  I can imagine. /\n",
      "\n",
      "\n",
      "\n",
      "A.35:  I think when she finally came to the realization \n",
      "\n",
      "that, {D you know, } no, [ I\n",
      "\n",
      "cannot, + I cannot ] take care of myself. /\n",
      "\n",
      "\n",
      "\n",
      "B.36:  Uh-huh. /  That's tough. / That's tough. /\n",
      "\n",
      "\n",
      "\n",
      "A.37:  Yeah. /  {E I mean, } for somebody who [ is, + ] {D you know, } \n",
      "\n",
      "for most of their life [ has, +\n",
      "\n",
      "has, ] {F uh, } not just merely had a farm but had ten children --\n",
      "\n",
      "\n",
      "\n",
      "B.38:  Uh-huh. /\n",
      "\n",
      "\n",
      "\n",
      "A.39:  -- had a farm, ran everything because her husband \n",
      "\n",
      "was away in the coal\n",
      "\n",
      "mines. /\n",
      "\n",
      "\n",
      "\n",
      "B.40:  Uh-huh. /\n",
      "\n",
      "\n",
      "\n",
      "A.41:  {C And, } {D you know, } facing that situation, [ it's, + it's ] \n",
      "\n",
      "quite a dilemma. /\n",
      "\n",
      "\n",
      "\n",
      "B.42:  Yes. /\n",
      "\n",
      "\n",
      "\n",
      "A.43:  I think, -/ \n",
      "\n",
      "\n",
      "\n",
      "B.44:  my mother,  {E excuse me. } -/ Go ahead. /\n",
      "\n",
      "\n",
      "\n",
      "A.45:  Yeah. / {D Well, } [ my, + {F uh, } my, ] {F uh, } -/ \n",
      "\n",
      "probably one of the biggest decisions I think\n",
      "\n",
      "that was very strengthening for our family was rather than have one child make\n",
      "\n",
      "that decision --\n",
      "\n",
      "\n",
      "\n",
      "B.46:  Uh-huh. /\n",
      "\n",
      "\n",
      "\n",
      "A.47:  -- than just delegate it. /  I think that [ they, + they ]\n",
      "\n",
      " had a great deal of, {F um, } -/ \n",
      "\n",
      "all the brothers and sisters got together / {C and } they actually \n",
      "\n",
      "had a conference. /\n",
      "\n",
      "\n",
      "\n",
      "B.48:  That's great. /\n",
      "\n",
      "\n",
      "\n",
      "A.49:  {C And, }  {E I mean, } [ it was just, + it was probably ]\n",
      "\n",
      " one of the most strengthening\n",
      "\n",
      "things for our family, getting down together and doing that. /\n",
      "\n",
      "\n",
      "\n",
      "B.50:  That's right. /\n",
      "\n",
      "\n",
      "\n",
      "A.51:  [ {C And, } + {C and } ] just the children were \n",
      "\n",
      "involved in the decision, because it\n",
      "\n",
      "involved just them.\n",
      "\n",
      "\n",
      "\n",
      "B.52:  Right. / Yeah. /\n",
      "\n",
      "\n",
      "\n",
      "A.53:  [ And, + ] {D you know, } making that decision \n",
      "\n",
      "and then finding a place / {C and } everybody\n",
      "\n",
      "had duties --\n",
      "\n",
      "\n",
      "\n",
      "B.54:  Uh-huh. /\n",
      "\n",
      "\n",
      "\n",
      "A.55:  -- to perform. / {D You know, } whether it was \n",
      "\n",
      "just, {D you know, } giving money or \n",
      "\n",
      "whether it was actually taking part in a lot,\n",
      "\n",
      "\n",
      "\n",
      "B.56:  Uh-huh. /\n",
      "\n",
      "\n",
      "\n",
      "A.57:  of the decision making, {D you know, }\n",
      "\n",
      "\n",
      "\n",
      "B.58:  Yep. /\n",
      "\n",
      "\n",
      "\n",
      "A.59:  like finding a proper nursing home. /\n",
      "\n",
      "\n",
      "\n",
      "B.60:  You were very,\n",
      "\n",
      "\n",
      "\n",
      "A.61:  {C And } they, -/ \n",
      "\n",
      "\n",
      "\n",
      "B.62:  fortunate. /\n",
      "\n",
      "\n",
      "\n",
      "A.63:  I know. /  [ They, + {C and, }  {D well, } \n",
      "\n",
      "[ [ they had, + {D well, } they had, ] + they had ] ] seen it\n",
      "\n",
      "coming. /\n",
      "\n",
      "\n",
      "\n",
      "B.64:  Uh-huh. /\n",
      "\n",
      "\n",
      "\n",
      "A.65:  [ {C So, } + {C so, } ] -/ {E I mean, } it, -/ {E I mean, }  \n",
      "\n",
      " [ [ [ [ [ I, + I, ] + I, ] + I har-, ] + I, ] + I ] truly wish that if\n",
      "\n",
      "something like that were to happen that my children would \n",
      "\n",
      "do something like, \n",
      "\n",
      "\n",
      "\n",
      "B.66:  Uh-huh. /\n",
      "\n",
      "\n",
      "\n",
      "A.67:  that for me. /\n",
      "\n",
      "\n",
      "\n",
      "B.68:  Uh-huh. / Absolutely. / Unfortunately, a lot of times \n",
      "\n",
      "[ it, +  responsibilities ]\n",
      "\n",
      "like that seem to fall to, {D you know, } maybe one child \n",
      "\n",
      "in the whole family, {D you know. } /\n",
      "\n",
      "\n",
      "\n",
      "A.69:  Yeah. / Yeah. / {D Well, } [ we, + we, ] -/ \n",
      "\n",
      "\n",
      "\n",
      "B.70:  {C And, }  {F uh, } it's usually not a very [ smooth, +\n",
      "\n",
      "\n",
      "\n",
      "A.71:  Yeah. /\n",
      "\n",
      "\n",
      "\n",
      "B.72:  smooth ]  thing. /  [ We were, +  I was ] lucky too that \n",
      "\n",
      "I only have one brother. /\n",
      "\n",
      "\n",
      "\n",
      "A.73:  Uh-huh. /\n",
      "\n",
      "\n",
      "\n",
      "B.74:  {C And, }  {F uh, } fortunately, we agreed, {D you know, } \n",
      "\n",
      "on exactly, {D you know, } what we\n",
      "\n",
      "thought should be done. /  My mother also was [ very +  very ] \n",
      "\n",
      "independent. /  She [ had\n",
      "\n",
      "her own, + still had her own ] little house and still driving \n",
      "\n",
      "her own car,\n",
      "\n",
      "\n",
      "\n",
      "A.75:  Yeah. /\n",
      "\n",
      "\n",
      "\n",
      "B.76:  at age eighty-three. /  We were lucky [ in, that +  in one ] respect \n",
      "\n",
      "in that after\n",
      "\n",
      "she had her stroke she wasn't [ really, + {D you know, } really ]\n",
      "\n",
      " much aware of what was\n",
      "\n",
      "going on. /\n",
      "\n",
      "\n",
      "\n",
      "A.77:  Uh-huh. /\n",
      "\n",
      "\n",
      "\n",
      "B.78:  That nursing home life would not have been, {D you know, } \n",
      "\n",
      "anything of her\n",
      "\n",
      "choosing, / of course, [ she would, + she would ]\n",
      "\n",
      " not have been happy there at all. /\n",
      "\n",
      "\n",
      "\n",
      "A.79:  {F Um. } /\n",
      "\n",
      "\n",
      "\n",
      "B.80:  {C But } as it turned out the stroke took care\n",
      "\n",
      " of that concern for us. /\n",
      "\n",
      "\n",
      "\n",
      "A.81:  Yeah. / Yeah. / {D Well, } [ with my, + with my ]\n",
      "\n",
      " grandmother I think [ it was, + it was ]\n",
      "\n",
      "such [ that, + {F uh, } that ] she did not have the problem \n",
      "\n",
      "with, -/ she was very well aware /\n",
      "\n",
      "{C and } her daughter came and visited her -- /\n",
      "\n",
      "\n",
      "\n",
      "B.82:  Uh-huh. /\n",
      "\n",
      "\n",
      "\n",
      "A.83:  -- at least her daughter,\n",
      "\n",
      "\n",
      "\n",
      "B.84:  Uh-huh. /\n",
      "\n",
      "\n",
      "\n",
      "A.85:  came and visited her and also her several grandchildren,\n",
      "\n",
      "\n",
      "\n",
      "B.86:  Uh-huh. /\n",
      "\n",
      "\n",
      "\n",
      "A.87:  came and visited her every day. /\n",
      "\n",
      "\n",
      "\n",
      "B.88:  That's great. /\n",
      "\n",
      "\n",
      "\n",
      "A.89:  {C And } I think that when she passed away it was \n",
      "\n",
      "probably one of the greatest, -/ \n",
      "\n",
      "<lipsmack> {F um, } [ [ I, + I, ] + I ] think it would be, -/ \n",
      "\n",
      "it was more of a relief --\n",
      "\n",
      "\n",
      "\n",
      "B.90:  Uh-huh. /\n",
      "\n",
      "\n",
      "\n",
      "A.91:  -- for her. /\n",
      "\n",
      "\n",
      "\n",
      "B.92:  Sure. /\n",
      "\n",
      "\n",
      "\n",
      "A.93:  {C And, }  {F um, } -/ {E I mean, } {C but } [ she was truly, +\n",
      "\n",
      "she was truly ] aware. /  {E I mean, } [ [ [ I, + I, ] + I \n",
      "\n",
      "di-, ] + I ] don't know [ how I would, + how I would ]\n",
      "\n",
      " deal if one of my parents came\n",
      "\n",
      "[ with, + with ] Alzheimer's. \n",
      "\n",
      "\n",
      "\n",
      "B.94:  Uh-huh. /\n",
      "\n",
      "\n",
      "\n",
      "A.95:  or something like, \n",
      "\n",
      "\n",
      "\n",
      "B.96:  That would be tough. /\n",
      "\n",
      "\n",
      "\n",
      "A.97:  that [ which is, + which is ] far more devastating. /\n",
      "\n",
      "\n",
      "\n",
      "B.98:  Yes. /  Absolutely. /\n",
      "\n",
      "\n",
      "\n",
      "A.99:  {C And, }  {F um, } [ [ I, + I, ] + I ] think that what \n",
      "\n",
      "one thing that they were concerned\n",
      "\n",
      "probably was the fact it wasn't necessarily, {D you know, } \n",
      "\n",
      "like the quantity of\n",
      "\n",
      "care -- \n",
      "\n",
      "\n",
      "\n",
      "B.100:  Uh-huh. /\n",
      "\n",
      "\n",
      "\n",
      "A.101:  -- but the quality of,\n",
      "\n",
      "\n",
      "\n",
      "B.102:  Yes. /\n",
      "\n",
      "\n",
      "\n",
      "A.103:  care. /  That the people that worked there --\n",
      "\n",
      "\n",
      "\n",
      "B.104:  Right. /\n",
      "\n",
      "\n",
      "\n",
      "A.105:  -- [ were very + --\n",
      "\n",
      "\n",
      "\n",
      "B.106:  Uh-huh. /\n",
      "\n",
      "\n",
      "\n",
      "A.107:  -- were very ] interested that, to make it as close a \n",
      "\n",
      "home environment,\n",
      "\n",
      "\n",
      "\n",
      "B.108:  Uh-huh. /\n",
      "\n",
      "\n",
      "\n",
      "A.109:  as possible. /\n",
      "\n",
      "\n",
      "\n",
      " B.110:  Yes. / It would, -/ \n",
      "\n",
      "\n",
      "\n",
      "A.111:  [ [ I think, + I think, ] + I think, ] {D you know } \n",
      "\n",
      "for myself [ I, + I ] see that as probably\n",
      "\n",
      "[ the, + the, ] what everything would hinge upon. /\n",
      "\n",
      "\n",
      "\n",
      "B.112:  {F Oh, } /\n",
      "\n",
      "\n",
      "\n",
      "A.113:  Is it, -/ how close is it to a home environment. /\n",
      "\n",
      "\n",
      "\n",
      "B.114:  Yes. /  That's right. /\n",
      "\n",
      "\n",
      "\n",
      "A.115:  [ That's the, + that's probably the ] major question. /\n",
      "\n",
      "\n",
      "\n",
      "B.116:  I think that great strides are being made \n",
      "\n",
      "nowadays [ in, + in ] caring for the\n",
      "\n",
      "elderly, {D you know, } [ [ in several, + in a, ] +\n",
      "\n",
      "\n",
      "\n",
      "A.117:  Yeah. /\n",
      "\n",
      "\n",
      "\n",
      "B.118:  in a ] whole lot of areas. /\n",
      "\n",
      "\n",
      "\n",
      "A.119:  Yeah. /\n",
      "\n",
      "\n",
      "\n",
      "B.120:  Just people are, of course,  populations getting older. /\n",
      "\n",
      "\n",
      "\n",
      "A.121:  Yeah. / {D You know } [ it's, + it's ]\n",
      "\n",
      " interesting [ that, + that ] [ a lot, + ] the population of\n",
      "\n",
      "the United States is changing because, {D you know, } {F uh, }\n",
      "\n",
      " now that so many more\n",
      "\n",
      "minorities, where they have had extended families for such a long time. /\n",
      "\n",
      "\n",
      "\n",
      "B.122:  Uh-huh. /\n",
      "\n",
      "\n",
      "\n",
      "A.123:  {F Um, } matter of fact in the United \n",
      "\n",
      "States we used to have extended families. /\n",
      "\n",
      "\n",
      "\n",
      "B.124:  Uh-huh, /\n",
      "\n",
      "\n",
      "\n",
      "A.125:  It wasn't, /\n",
      "\n",
      "\n",
      "\n",
      "B.126:  true. /\n",
      "\n",
      "\n",
      "\n",
      "A.127:  {C But, }  I guess as we become more \n",
      "\n",
      "industrialized and more, {D you know, } less in a\n",
      "\n",
      "rural situation --\n",
      "\n",
      "\n",
      "\n",
      "B.128:  Yes, {F um. } /\n",
      "\n",
      "\n",
      "\n",
      "A.129:  -- [ [ [ [ we, + we  don't, ] +\n",
      "\n",
      " we, ] + we, ] + we ] choose not to deal with the,\n",
      "\n",
      "\n",
      "\n",
      "B.130:  Uh-huh. /\n",
      "\n",
      "\n",
      "\n",
      "A.131:  extended family --\n",
      "\n",
      "\n",
      "\n",
      "B.132:  Uh-huh. /\n",
      "\n",
      "\n",
      "\n",
      "A.133:  -- because we feel it's kind of cumbersome,\n",
      "\n",
      "\n",
      "\n",
      "B.134:  That's right. /\n",
      "\n",
      "\n",
      "\n",
      "A.135:  when in reality it makes things much much easier. /\n",
      "\n",
      "\n",
      "\n",
      "B.136:  Sure. / Absolutely. /  {C And, }\n",
      "\n",
      "\n",
      "\n",
      "A.137:  {F Uh. }\n",
      "\n",
      "\n",
      "\n",
      "B.138:  [ people, + things ] are scattered so much nowadays. /\n",
      "\n",
      "\n",
      "\n",
      "A.139:  Yeah. /  Yeah. / [ [ I, + I, ] + I ] think that \n",
      "\n",
      "[ perhaps, + perhaps ] the extended family, {D you know, } \n",
      "\n",
      "that it maybe one of the solutions to a lot of things even child care. /\n",
      "\n",
      "\n",
      "\n",
      "B.140:  Yes. /\n",
      "\n",
      "\n",
      "\n",
      "A.141:  {D You know } {E I mean, } of course [ there, + there ]\n",
      "\n",
      " comes other issues, {D you know, }\n",
      "\n",
      "whether or not any of the grandparents <laughter> \n",
      "\n",
      "whether  (( we feel like ))  [ are\n",
      "\n",
      "going to be a good, + <laughter> they're going to be a good ] caretaker for our\n",
      "\n",
      "children. /\n",
      "\n",
      "\n",
      "\n",
      "B.142:  <Laughter> Yes, /\n",
      "\n",
      "\n",
      "\n",
      "A.143:  {C But, } -/ \n",
      "\n",
      "\n",
      "\n",
      "B.144:  just because they're grandparents. /\n",
      "\n",
      "\n",
      "\n",
      "A.145:  {E I mean } they raised us after all. /\n",
      "\n",
      "\n",
      "\n",
      "B.146:  Yeah. /  Just because they're grandparents that \n",
      "\n",
      "doesn't automatically make\n",
      "\n",
      "them a good child carer. /\n",
      "\n",
      "\n",
      "\n",
      "A.147:  Yeah.  / {C But, }  {F uh, } [ [ I, + I, ] + I ]\n",
      "\n",
      " think that, {D you know, } [ we always, + ] {F uh, } {E I mean, } [ I've, +\n",
      "\n",
      "I've ]  had a lot of good experiences [ with, + {F uh, } with ]\n",
      "\n",
      "  [ many +  many ] people especially\n",
      "\n",
      "where they've had, {F uh, } extended family. / \n",
      "\n",
      "[ {C And } I, + {C and } [ I, +  I ] ] kind of see [ that, +\n",
      "\n",
      "that, ] {D you know, } perhaps, {D you know, } we may need \n",
      "\n",
      "to {D like } get close to the family\n",
      "\n",
      "environment [ and + --\n",
      "\n",
      "\n",
      "\n",
      "B.148:  Uh-huh. /\n",
      "\n",
      "\n",
      "\n",
      "A.149:  -- and ] get down to the values of, {D you know, } -/ \n",
      "\n",
      "{E I mean, } {F uh, } it's, -/ money seems\n",
      "\n",
      "to be too big of an issue. \n",
      "\n",
      "\n",
      "\n",
      "B.150:  {F Oh, } yeah. /\n",
      "\n",
      "\n",
      "\n",
      "A.151:  [ [ [ With, + \n",
      "\n",
      "\n",
      "\n",
      "B.152:  sure. /\n",
      "\n",
      "\n",
      "\n",
      "A.153:  with, ] + with, ] + \n",
      "\n",
      "\n",
      "\n",
      "B.154:  realistically it is. /\n",
      "\n",
      "\n",
      "\n",
      "A.155:  with ] what's going on today / {C and } [ [ I, + I ]\n",
      "\n",
      " think, + I think ] that [ we may not, + that\n",
      "\n",
      "may be, ] {D you know, } perhaps if we put money on the back burner --\n",
      "\n",
      "\n",
      "\n",
      "B.156:  Uh-huh. /\n",
      "\n",
      "\n",
      "\n",
      "A.157:  -- [ that may, + that may ]\n",
      "\n",
      " choose to alleviate a lot of the problem. /\n",
      "\n",
      "\n",
      "\n",
      "B.158:  That would,\n",
      "\n",
      "\n",
      "\n",
      "A.159:  {E I mean, }\n",
      "\n",
      "\n",
      "\n",
      "B.160:  certainly help. /\n",
      "\n",
      "\n",
      "\n",
      "A.161:  {E I mean } [ we may not, + we may not ]\n",
      "\n",
      " have as high a standard of living -- /\n",
      "\n",
      "\n",
      "\n",
      "B.162:  Uh-huh. /\n",
      "\n",
      "\n",
      "\n",
      "A.163:  --  [ but the qua-, +  but actually \n",
      "\n",
      "have a truer standard ] of living. /\n",
      "\n",
      "\n",
      "\n",
      "B.164:  Right. /\n",
      "\n",
      " That's just a matter of defining priorities, I guess --\n",
      "\n",
      "\n",
      "\n",
      "A.165:  Yeah. / Yeah. /\n",
      "\n",
      "\n",
      "\n",
      "B.166:  -- or some priorities anyway. /\n",
      "\n",
      "\n",
      "\n",
      "A.167:  Yeah. /\n",
      "\n",
      "\n",
      "\n",
      "B.168:  I think your right. /\n",
      "\n",
      "\n",
      "\n",
      "@@A.169:  Okay. /  {D Well, } I guess that was it <laughter>. /\n",
      "\n",
      "\n",
      "\n",
      "B.170:  Okay. / It was good talking to you. /\n",
      "\n",
      "\n",
      "\n",
      "A.171:  Okay. /\n",
      "\n",
      "\n",
      "\n",
      "B.172:  Yeah. /\n",
      "\n",
      "\n",
      "\n",
      "A.173:  All right. /\n",
      "\n",
      "\n",
      "\n",
      "B.174:  Take care. /\n",
      "\n",
      "\n",
      "\n",
      "A.175:  Hey. /\n",
      "\n",
      "\n",
      "\n",
      "B.176:  # Bye-bye. # /\n",
      "\n",
      "\n",
      "\n",
      "A.177:  # Bye-bye. # /\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "parentheses=r'\\([^)(]+[^)( ] *\\)'\n",
    "parenthesestokeep=r'\\([^)(]+[^)(.!?—\\-, ] *\\)'\n",
    "speakertag=r'((?<=[^\\w\\d \\\",])|^) *(?![?\\.,!:\\-\\—\\[\\]\\(\\)])(?:[A-Z\\d][^\\s.?!\\[\\]\\(\\)]*\\s?)*:(?=[^\\w]*[A-Z])'#lookahead keeps semicolon in false cases.\n",
    "parenthesestoremove=r'\\(([^\\(\\)]+[\\w ]+)\\):?'\n",
    "parenthesesaroundsentence=r'\\(([^\\w]*[^\\(\\)]+[_^\\W]+)\\):?'\n",
    "squarebracketsaroundsentence=r'\\[([^\\[\\]]+)\\]' #generic since it seems like the square brackets just denote unclear speech.\n",
    "\n",
    "\n",
    "def to_emdash(s):\n",
    "    return re.sub('--','—',s)\n",
    "\n",
    "def strip_accents(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFKD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def removespeakertags(text):\n",
    "    return re.sub(speakertag,' ',text)\n",
    "\n",
    "def removenametags(text):\n",
    "    # return re.sub(r\"(?<=[a-z][.?!;]) *[ A-z.,\\-']{1,25}:\",' ',text)\n",
    "    return re.sub(r\"(?<=[a-z][.?!;])([\\(\\[]* *)[ A-Za-z.,\\-']{1,25}:\", \"\\g<1>\",text)\n",
    "\n",
    "def removeparentheses(text):\n",
    "    return re.sub(parenthesestoremove, ' ',text)\n",
    "\n",
    "def removeparenthesesaroundsentence(text):\n",
    "    return re.sub(parenthesesaroundsentence,r'\\g<1>',text)\n",
    "\n",
    "def removedashafterpunct(text):\n",
    "    return re.sub(r\"([^A-Za-zÀ-ÖØ-öø-ÿ0-9 ]+ *)-+( *[^- ])\",r\"\\g<1> \\g<2>\",text)\n",
    "\n",
    "def removesquarebrackets(text):\n",
    "    return re.sub(squarebracketsaroundsentence, r'\\g<1>',text)\n",
    "\n",
    "def removemusic(text):\n",
    "    text = re.sub(r'♫( *[^♫ ])+ *♫', ' ',text)\n",
    "    return re.sub(r'♪( *[^♫ ])+ *♪', ' ',text)\n",
    "\n",
    "def reducewhitespaces(text):\n",
    "    text=re.sub(r'(?<=[.?!,;:\\—\\-]) *(?=[.?!,;:\\—\\-])','',text)\n",
    "    return re.sub(r'\\s+', ' ',text)\n",
    "\n",
    "def removeemptyquotes(text):\n",
    "    text= re.sub(r\"'[_^\\W]*'\",' ',text)\n",
    "    text= re.sub(r\"\\([_^\\W]*\\)\",' ',text)\n",
    "    text= re.sub(r\"\\[[_^\\W]*\\]\",' ',text)\n",
    "    return re.sub(r'\"[_^\\W]*\"',' ',text)\n",
    "\n",
    "def ellipsistounicode(text):\n",
    "    text = re.sub(r'\\.{3,}(?= )','…',text) #ellipsis without trailing punctuation\n",
    "    return re.sub(r'\\.{3,}([^\\w\\s])','…\\g<1>',text) #ellipsis with trailing punctuation\n",
    "\n",
    "def removenonsentencepunct(text):\n",
    "    return re.sub(r'[^A-Za-z\\d\\s$%&+=€²£¢¥…,.!?;:\\-\\–\\—\\']',' ',text)\n",
    "\n",
    "def combinerepeatedpunct(text):\n",
    "    newtext=[text,re.sub(r'([_^\\W]+) *\\1+','\\g<1> ',text)]\n",
    "    i=1\n",
    "    while (newtext[0]!=newtext[1]):\n",
    "        i+=1\n",
    "        newtext[i%2]=re.sub(r'([_^\\W]+) *\\1+','\\g<1> ',newtext[(1+i)%2])\n",
    "    return newtext[i%2]\n",
    "\n",
    "def endashtohyphen(text):\n",
    "    return re.sub('–','-',text)\n",
    "\n",
    "def removedashafterpunct(text):\n",
    "    return re.sub(r\"([^A-Za-z0-9 ]+ *)-+( *[^- ])\",r\"\\g<1> \\g<2>\",text)\n",
    "\n",
    "def pronouncesymbol(text):\n",
    "    text=re.sub(\"\\$ *([\\d](\\.[\\d])?+)\", \"\\g<1> dollars \",text)\n",
    "    text=re.sub('\\£ *([\\d](\\.[\\d])?+)', \" pounds \",text)\n",
    "    text=re.sub(\"\\$\", \" dollars \",text)\n",
    "    text=re.sub(\"\\£\", \" pounds \",text)\n",
    "    text=re.sub('€', \" euro \",text)\n",
    "    text=re.sub('¥', \" yen \",text)\n",
    "    text=re.sub(\"¢\",\" cents \",text)\n",
    "    text=re.sub('(?<=\\d)\\.(?=\\d)',' point ',text)\n",
    "    text=re.sub('\\+',' plus ',text)\n",
    "    text=re.sub('%',' percent ',text)\n",
    "    text=re.sub('²',' squared ',text)\n",
    "    text=re.sub('&', ' and ',text)\n",
    "    return text\n",
    "\n",
    "def stripleadingpunctuation(text):\n",
    "    return re.sub(r'^[^A-Z]*','',text)\n",
    "\n",
    "def striptrailingtext(text):\n",
    "    return re.sub(r'[^!.?…;]*$','',text)\n",
    "\n",
    "def preprocess(tedtalks):\n",
    "    print('stripping accents')\n",
    "    tedtalks=tedtalks.apply(strip_accents)\n",
    "    print('removing speaker tags')\n",
    "    tedtalks=tedtalks.apply(removespeakertags)\n",
    "    print('removing name tags')\n",
    "    tedtalks=tedtalks.apply(removenametags) # Remove *Mr Brown: *Hi!\n",
    "    print('removing non-sentence parenthesis')\n",
    "    tedtalks=tedtalks.apply(removeparentheses) # Remove (Whispers) without punct\n",
    "    print('removing parenthesis')\n",
    "    tedtalks=tedtalks.apply(removeparenthesesaroundsentence) #Remove -> (<- Hi Everyone! ->)<-\n",
    "    print('removing square brackets')\n",
    "    tedtalks=tedtalks.apply(removesquarebrackets) #Remove entire [unclear text]\n",
    "    print('removing music lyrics')\n",
    "    tedtalks=tedtalks.apply(removemusic)\n",
    "    print('removing empty tags')\n",
    "    tedtalks=tedtalks.apply(removeemptyquotes)\n",
    "    print('removing non-sentence punctuation')\n",
    "    tedtalks=tedtalks.apply(removenonsentencepunct)\n",
    "    print('change to unicode ellipsis')\n",
    "    tedtalks=tedtalks.apply(ellipsistounicode)\n",
    "    print('2 hyphen to emdash')\n",
    "    tedtalks=tedtalks.apply(to_emdash)\n",
    "    print('endash to hyphen')\n",
    "    tedtalks=tedtalks.apply(endashtohyphen)\n",
    "    print('remove hyphen after punct')\n",
    "    tedtalks=tedtalks.apply(removedashafterpunct)\n",
    "    print('combine repeated punctuation')\n",
    "    tedtalks=tedtalks.apply(combinerepeatedpunct)\n",
    "    print('pronounce symbol')\n",
    "    tedtalks=tedtalks.apply(pronouncesymbol)\n",
    "    print('strip leading')\n",
    "    tedtalks=tedtalks.apply(stripleadingpunctuation)\n",
    "    print('strip trailing')\n",
    "    tedtalks=tedtalks.apply(striptrailingtext)\n",
    "    print('reduce whitespaces')\n",
    "    tedtalks=tedtalks.apply(reducewhitespaces)\n",
    "    print('--done--')\n",
    "    return tedtalks\n",
    "\n",
    "def text2csv(source:str,target:str):\n",
    "    rows=dict()\n",
    "    talkid=-1\n",
    "    with open(source,'r') as f:\n",
    "        for line in f:\n",
    "            if line[:8]=='<talkid>':\n",
    "                talkid=int(re.search(\"(?<=<talkid>)[0-9]+\",line)[0])\n",
    "                print(talkid)\n",
    "                continue\n",
    "            if line[0]!='<':\n",
    "                line=re.sub('\\n',' ',line)\n",
    "                if not talkid in rows.keys():\n",
    "                    rows[talkid]=''\n",
    "                rows[talkid]+=line\n",
    "\n",
    "    tedtalks=pd.DataFrame.from_dict({'id':rows.keys(),'transcript':rows.values()})\n",
    "\n",
    "    tedtalks.loc[:,'transcript'] = preprocess(tedtalks.transcript.astype(str))\n",
    "    tedtalks=tedtalks.loc[tedtalks.transcript.map(lambda x:len(x.split())>=1)]\n",
    "    tedtalks.to_csv(target,index=None)\n",
    "    \n",
    "def xml2csv(source:str,target:str):\n",
    "    tree=ET.parse(source)\n",
    "    tree.getroot()[0]\n",
    "    rows={}\n",
    "    for child in tree.getroot()[0]:\n",
    "        talkid=int(child[3].text)\n",
    "        if not talkid in rows.keys():\n",
    "            rows[talkid]=''\n",
    "        for i in child.findall('seg'):\n",
    "            rows[talkid]+=re.sub('\\n',' ',i.text)\n",
    "    tedtalks=pd.DataFrame.from_dict({'id':rows.keys(),'transcript':rows.values()})\n",
    "\n",
    "    tedtalks.loc[:,'transcript'] = preprocess(tedtalks.transcript.astype(str))\n",
    "    tedtalks=tedtalks.loc[tedtalks.transcript.map(lambda x:len(x.split())>=1)]\n",
    "    tedtalks.to_csv(target,index=None)        \n",
    "\n",
    "f=open(\"/home/nxingyu/data/LDC99T42/treebank_3/dysfl/dff/swbd/2/sw2005.dff\")\n",
    "started=False\n",
    "for line in f:\n",
    "    if started==False and re.match('^=+',line):\n",
    "        started=True\n",
    "        continue\n",
    "    if started:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stripping accents\n",
      "removing speaker tags\n",
      "removing name tags\n",
      "removing non-sentence parenthesis\n",
      "removing parenthesis\n",
      "removing square brackets\n",
      "removing music lyrics\n",
      "removing empty tags\n",
      "removing non-sentence punctuation\n",
      "change to unicode ellipsis\n",
      "2 hyphen to emdash\n",
      "endash to hyphen\n",
      "remove hyphen after punct\n",
      "combine repeated punctuation\n",
      "pronounce symbol\n",
      "strip leading\n",
      "strip trailing\n",
      "reduce whitespaces\n",
      "--done--\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "parentheses=r'\\([^)(]+[^)( ] *\\)'\n",
    "parenthesestokeep=r'\\([^)(]+[^)(.!?—\\-, ] *\\)'\n",
    "speakertag=r'((?<=[^\\w\\d \\\",])|^) *(?![?\\.,!:\\-\\—\\[\\]\\(\\)])(?:[A-Z\\d][^\\s.?!\\[\\]\\(\\)]*\\s?)*:(?=[^\\w]*[A-Z])'#lookahead keeps semicolon in false cases.\n",
    "parenthesestoremove=r'\\(([^\\(\\)]+[\\w ]+)\\):?'\n",
    "parenthesesaroundsentence=r'\\(([^\\w]*[^\\(\\)]+[_^\\W]+)\\):?'\n",
    "squarebracketsaroundsentence=r'\\[([^\\[\\]]+)\\]' #generic since it seems like the square brackets just denote unclear speech.\n",
    "\n",
    "\n",
    "def to_emdash(s):\n",
    "    return re.sub('--','—',s)\n",
    "\n",
    "def strip_accents(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFKD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def removespeakertags(text):\n",
    "    return re.sub(speakertag,' ',text)\n",
    "\n",
    "def removenametags(text):\n",
    "    # return re.sub(r\"(?<=[a-z][.?!;]) *[ A-z.,\\-']{1,25}:\",' ',text)\n",
    "    return re.sub(r\"(?<=[a-z][.?!;])([\\(\\[]* *)[ A-Za-z.,\\-']{1,25}:\", \"\\g<1>\",text)\n",
    "\n",
    "def removeparentheses(text):\n",
    "    return re.sub(parenthesestoremove, ' ',text)\n",
    "\n",
    "def removeparenthesesaroundsentence(text):\n",
    "    return re.sub(parenthesesaroundsentence,r'\\g<1>',text)\n",
    "\n",
    "def removedashafterpunct(text):\n",
    "    return re.sub(r\"([^A-Za-zÀ-ÖØ-öø-ÿ0-9 ]+ *)-+( *[^- ])\",r\"\\g<1> \\g<2>\",text)\n",
    "\n",
    "def removesquarebrackets(text):\n",
    "    return re.sub(squarebracketsaroundsentence, r'\\g<1>',text)\n",
    "\n",
    "def removemusic(text):\n",
    "    text = re.sub(r'♫( *[^♫ ])+ *♫', ' ',text)\n",
    "    return re.sub(r'♪( *[^♫ ])+ *♪', ' ',text)\n",
    "\n",
    "def reducewhitespaces(text):\n",
    "    text=re.sub(r'(?<=[.?!,;:\\—\\-]) *(?=[.?!,;:\\—\\-])','',text)\n",
    "    return re.sub(r'\\s+', ' ',text)\n",
    "\n",
    "def removeemptyquotes(text):\n",
    "    text= re.sub(r\"'[_^\\W]*'\",' ',text)\n",
    "    text= re.sub(r\"\\([_^\\W]*\\)\",' ',text)\n",
    "    text= re.sub(r\"\\[[_^\\W]*\\]\",' ',text)\n",
    "    return re.sub(r'\"[_^\\W]*\"',' ',text)\n",
    "\n",
    "def ellipsistounicode(text):\n",
    "    text = re.sub(r'\\.{3,}(?= )','…',text) #ellipsis without trailing punctuation\n",
    "    return re.sub(r'\\.{3,}([^\\w\\s])','…\\g<1>',text) #ellipsis with trailing punctuation\n",
    "\n",
    "def removenonsentencepunct(text):\n",
    "    return re.sub(r'[^A-Za-z\\d\\s$%&+=€²£¢¥…,.!?;:\\-\\–\\—\\']',' ',text)\n",
    "\n",
    "def combinerepeatedpunct(text):\n",
    "    newtext=[text,re.sub(r'([_^\\W]+) *\\1+','\\g<1> ',text)]\n",
    "    i=1\n",
    "    while (newtext[0]!=newtext[1]):\n",
    "        i+=1\n",
    "        newtext[i%2]=re.sub(r'([_^\\W]+) *\\1+','\\g<1> ',newtext[(1+i)%2])\n",
    "    return newtext[i%2]\n",
    "\n",
    "def endashtohyphen(text):\n",
    "    return re.sub('–','-',text)\n",
    "\n",
    "def removedashafterpunct(text):\n",
    "    return re.sub(r\"([^A-Za-z0-9 ]+ *)-+( *[^- ])\",r\"\\g<1> \\g<2>\",text)\n",
    "\n",
    "def pronouncesymbol(text):\n",
    "    text=re.sub(\"\\$ *([\\d](\\.[\\d])?+)\", \"\\g<1> dollars \",text)\n",
    "    text=re.sub('\\£ *([\\d](\\.[\\d])?+)', \" pounds \",text)\n",
    "    text=re.sub(\"\\$\", \" dollars \",text)\n",
    "    text=re.sub(\"\\£\", \" pounds \",text)\n",
    "    text=re.sub('€', \" euro \",text)\n",
    "    text=re.sub('¥', \" yen \",text)\n",
    "    text=re.sub(\"¢\",\" cents \",text)\n",
    "    text=re.sub('(?<=\\d)\\.(?=\\d)',' point ',text)\n",
    "    text=re.sub('\\+',' plus ',text)\n",
    "    text=re.sub('%',' percent ',text)\n",
    "    text=re.sub('²',' squared ',text)\n",
    "    text=re.sub('&', ' and ',text)\n",
    "    return text\n",
    "\n",
    "def stripleadingpunctuation(text):\n",
    "    return re.sub(r'^[^A-Z]*','',text)\n",
    "\n",
    "def striptrailingtext(text):\n",
    "    return re.sub(r'[^!.?…;]*$','',text)\n",
    "\n",
    "def preprocess(tedtalks):\n",
    "    print('stripping accents')\n",
    "    tedtalks=tedtalks.apply(strip_accents)\n",
    "    print('removing speaker tags')\n",
    "    tedtalks=tedtalks.apply(removespeakertags)\n",
    "    print('removing name tags')\n",
    "    tedtalks=tedtalks.apply(removenametags) # Remove *Mr Brown: *Hi!\n",
    "    print('removing non-sentence parenthesis')\n",
    "    tedtalks=tedtalks.apply(removeparentheses) # Remove (Whispers) without punct\n",
    "    print('removing parenthesis')\n",
    "    tedtalks=tedtalks.apply(removeparenthesesaroundsentence) #Remove -> (<- Hi Everyone! ->)<-\n",
    "    print('removing square brackets')\n",
    "    tedtalks=tedtalks.apply(removesquarebrackets) #Remove entire [unclear text]\n",
    "    print('removing music lyrics')\n",
    "    tedtalks=tedtalks.apply(removemusic)\n",
    "    print('removing empty tags')\n",
    "    tedtalks=tedtalks.apply(removeemptyquotes)\n",
    "    print('removing non-sentence punctuation')\n",
    "    tedtalks=tedtalks.apply(removenonsentencepunct)\n",
    "    print('change to unicode ellipsis')\n",
    "    tedtalks=tedtalks.apply(ellipsistounicode)\n",
    "    print('2 hyphen to emdash')\n",
    "    tedtalks=tedtalks.apply(to_emdash)\n",
    "    print('endash to hyphen')\n",
    "    tedtalks=tedtalks.apply(endashtohyphen)\n",
    "    print('remove hyphen after punct')\n",
    "    tedtalks=tedtalks.apply(removedashafterpunct)\n",
    "    print('combine repeated punctuation')\n",
    "    tedtalks=tedtalks.apply(combinerepeatedpunct)\n",
    "    print('pronounce symbol')\n",
    "    tedtalks=tedtalks.apply(pronouncesymbol)\n",
    "    print('strip leading')\n",
    "    tedtalks=tedtalks.apply(stripleadingpunctuation)\n",
    "    print('strip trailing')\n",
    "    tedtalks=tedtalks.apply(striptrailingtext)\n",
    "    print('reduce whitespaces')\n",
    "    tedtalks=tedtalks.apply(reducewhitespaces)\n",
    "    print('--done--')\n",
    "    return tedtalks\n",
    "\n",
    "def text2csv(source:str,target:str):\n",
    "    rows=dict()\n",
    "    talkid=-1\n",
    "    with open(source,'r') as f:\n",
    "        for line in f:\n",
    "            if line[:8]=='<talkid>':\n",
    "                talkid=int(re.search(\"(?<=<talkid>)[0-9]+\",line)[0])\n",
    "                print(talkid)\n",
    "                continue\n",
    "            if line[0]!='<':\n",
    "                line=re.sub('\\n',' ',line)\n",
    "                if not talkid in rows.keys():\n",
    "                    rows[talkid]=''\n",
    "                rows[talkid]+=line\n",
    "\n",
    "    tedtalks=pd.DataFrame.from_dict({'id':rows.keys(),'transcript':rows.values()})\n",
    "\n",
    "    tedtalks.loc[:,'transcript'] = preprocess(tedtalks.transcript.astype(str))\n",
    "    tedtalks=tedtalks.loc[tedtalks.transcript.map(lambda x:len(x.split())>=1)]\n",
    "    tedtalks.to_csv(target,index=None)\n",
    "    \n",
    "def xml2csv(source:str,target:str):\n",
    "    tree=ET.parse(source)\n",
    "    tree.getroot()[0]\n",
    "    rows={}\n",
    "    for child in tree.getroot()[0]:\n",
    "        talkid=int(child[3].text)\n",
    "        if not talkid in rows.keys():\n",
    "            rows[talkid]=''\n",
    "        for i in child.findall('seg'):\n",
    "            rows[talkid]+=re.sub('\\n',' ',i.text)\n",
    "    tedtalks=pd.DataFrame.from_dict({'id':rows.keys(),'transcript':rows.values()})\n",
    "\n",
    "    tedtalks.loc[:,'transcript'] = preprocess(tedtalks.transcript.astype(str))\n",
    "    tedtalks=tedtalks.loc[tedtalks.transcript.map(lambda x:len(x.split())>=1)]\n",
    "    tedtalks.to_csv(target,index=None)        \n",
    "            \n",
    "xml2csv(\"/home/nxingyu2/data/2012-03/texts/en/fr/en-fr/IWSLT12.TALK.tst2010.en-fr.en.xml\",        \n",
    "        '/home/nxingyu2/data/ted2010.test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree=ET.parse(\"/home/nxingyu2/data/2012-03/texts/en/fr/en-fr/IWSLT12.TALK.tst2010.en-fr.en.xml\")\n",
    "tree.getroot()[0]\n",
    "rows={}\n",
    "# print(tre)\n",
    "# for child in tree.getroot()[0]:\n",
    "#     talkid=int(child[3].text)\n",
    "#     if not talkid in rows.keys():\n",
    "#         rows[talkid]=''\n",
    "#     print(child.findall('seg'))\n",
    "#     for i in child.findall('seg'):\n",
    "#         rows[talkid]+=re.sub('\\n',' ',i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9gBCXwk06oSO",
    "outputId": "9bff0f11-bac9-4b11-e506-f59d29f9abf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/root/data/’: File exists\n",
      "/root/data\n",
      "Downloading ted-ultimate-dataset.zip to /root/data\n",
      " 89% 177M/199M [00:02<00:00, 80.1MB/s]\n",
      "100% 199M/199M [00:02<00:00, 95.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "# !pip install kaggle==1.5.6\n",
    "import os\n",
    "os.environ['KAGGLE_USERNAME'] = \"ngxingyu\"\n",
    "os.environ['KAGGLE_KEY'] = \"1bbb182c67c54f035f76bf34bac11751\"\n",
    "%mkdir ~/data/\n",
    "%cd ~/data/\n",
    "!kaggle datasets download -d miguelcorraljr/ted-ultimate-dataset\n",
    "!unzip ~/data/ted-ultimate-dataset.zip\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "POH_Rou53509"
   },
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ryZ8thS22yk",
    "outputId": "29f3c805-a18c-47b5-e09c-0d81114605c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'!': 1,\n",
       "  ',': 2,\n",
       "  '-': 3,\n",
       "  '.': 4,\n",
       "  ':': 5,\n",
       "  ';': 6,\n",
       "  '?': 7,\n",
       "  '—': 8,\n",
       "  '…': 9,\n",
       "  ' ': 0,\n",
       "  '': -100},\n",
       " {1: '!',\n",
       "  2: ',',\n",
       "  3: '-',\n",
       "  4: '.',\n",
       "  5: ':',\n",
       "  6: ';',\n",
       "  7: '?',\n",
       "  8: '—',\n",
       "  9: '…',\n",
       "  0: ' ',\n",
       "  -100: ''})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "tags=sorted(list('.?!,;:-—…'))\n",
    "tag2id = {tag: id+1 for id, tag in enumerate(tags)}\n",
    "tag2id[' ']=0\n",
    "tag2id['']=-100\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}\n",
    "tag2id,id2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "g01xetZb4HTU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import LongTensor,FloatTensor\n",
    "from typing import Optional\n",
    "class PunctuationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_ids:LongTensor, attention_mask:FloatTensor, labels:Optional[LongTensor] = None) -> None:\n",
    "        \"\"\"\n",
    "        :param input_ids: tokenids\n",
    "        :param attention_mask: attention_mask, null->0\n",
    "        :param labels: true labels, optional\n",
    "        :return None\n",
    "        \"\"\"\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\":param idx: implement index\"\"\"\n",
    "        item = {'input_ids': torch.as_tensor(self.input_ids[idx],dtype=torch.long),\n",
    "        'attention_mask': torch.as_tensor(self.attention_mask[idx],dtype=torch.float32),\n",
    "        'labels': torch.as_tensor(self.labels[idx],dtype=torch.long)}\n",
    "        return item\n",
    "    def view(self,idx:int)->str:\n",
    "        return ' '.join([''.join(x) for x in list(zip(tokenizer.convert_ids_to_tokens(self.input_ids[idx]),[id2tag[x] for x in self.labels[idx].tolist()]))])\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h_OhfV3922yh",
    "outputId": "94b1ddef-70cb-4f80-e379-fdf4e2bbbe67"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default-5020f345641810cf (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/nxingyu/.cache/huggingface/datasets/csv/default-5020f345641810cf/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/nxingyu/.cache/huggingface/datasets/csv/default-5020f345641810cf/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# ted=load_dataset('csv',data_files={'train':'/content/gdrive/MyDrive/ASR/ted_talks_processed.train.csv',\n",
    "#                                          'dev':'/content/gdrive/MyDrive/ASR/ted_talks_processed.dev.csv',\n",
    "#                                          'test':'/content/gdrive/MyDrive/ASR/ted_talks_processed.test.csv'})\n",
    "ted=load_dataset('csv',data_files={'train':'/home/nxingyu/project/data/ted_talks_processed.train.csv',\n",
    "                                         'dev':'/home/nxingyu/project/data/ted_talks_processed.dev.csv',\n",
    "                                         'test':'/home/nxingyu/project/data/ted_talks_processed.test.csv'})\n",
    "# subtitles=load_dataset('csv',data_files={'train':'/home/nxingyu/project/data/open_subtitles_processed.train.csv',\n",
    "#                                          'dev':'/home/nxingyu/project/data/open_subtitles_processed.dev.csv',\n",
    "#                                          'test':'/home/nxingyu/project/data/open_subtitles_processed.test.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ToHCATk22yi",
    "outputId": "7a51f2f9-96b8-47b3-f46a-af0e7a43b90c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". 40835 0.3979476484690198\n",
      "? 3679 0.035852807609098175\n",
      "! 291 0.002835870349075175\n",
      ", 48727 0.4748572319566531\n",
      "; 586 0.005710721733876469\n",
      ": 1243 0.012113356851891554\n",
      "- 3874 0.03775313310074649\n",
      "— 3168 0.03087298029508644\n",
      "… 211 0.00205624963455279\n",
      "734841\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def punct_proportion(df):\n",
    "    l=[]\n",
    "    st=pd.Series(df['transcript']).str\n",
    "    for c in \".?!,;:-—…\":\n",
    "        l.append(sum(st.count(\"\\\\\"+c)))\n",
    "    [print(i[0],i[1],i[1]/sum(l)) for i in zip(list(\".?!,;:-—…\"),l)]\n",
    "    print(st.count('\\w+').sum())\n",
    "for split in ted:\n",
    "    punct_proportion(ted[split])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Qy62-kY622yj",
    "outputId": "cc7e014a-0405-4b4a-8223-3b4ca84b18ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". 47443035 0.44392753421469083\n",
      "? 13250829 0.12398886041482206\n",
      "! 6519047 0.0609991426589736\n",
      ", 24551760 0.22973239965425646\n",
      "; 57285 0.0005360194346227758\n",
      ": 374276 0.003502124638437183\n",
      "- 10479724 0.09805945244798349\n",
      "— 9564 8.949096399986432e-05\n",
      "… 4185605 0.03916497557221373\n",
      "419201886\n",
      "\n",
      ". 5921757 0.4430048481920479\n",
      "? 1660096 0.12419127911939411\n",
      "! 813038 0.06082312660995144\n",
      ", 3062385 0.22909609462708524\n",
      "; 8073 0.0006039386856729181\n",
      ": 57019 0.004265574125899185\n",
      "- 1321067 0.09882862227992877\n",
      "— 591 4.421253105818092e-05\n",
      "… 523225 0.03914230382896229\n",
      "52422785\n",
      "\n",
      ". 5921304 0.4422626972770535\n",
      "? 1670946 0.12480309826421737\n",
      "! 834767 0.06234881793231256\n",
      ", 3063118 0.2287845428570959\n",
      "; 7344 0.0005485239820152251\n",
      ": 44933 0.0033560495756930976\n",
      "- 1324557 0.09893127451608667\n",
      "— 1210 9.037500248344532e-05\n",
      "… 520479 0.03887462059304226\n",
      "52341671\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for split in subtitles:\n",
    "    punct_proportion(subtitles[split])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cfd4ced77714a9dbbad9a7b99126bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "degree=0\n",
    "max_length=128\n",
    "data=ted['dev'].map(chunk_examples_with_degree(degree), batched=True, batch_size=128,remove_columns=ted['dev'].column_names)\n",
    "# tokenizer.tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.convert_tokens_to_ids(data['texts'][0])\n",
    "# subwords = list([list(map(tokenizer.tokenize, x)) for x in data['texts']])\n",
    "\n",
    "max_len=128\n",
    "def flatten(list_of_lists):\n",
    "    for list in list_of_lists:\n",
    "        for item in list:\n",
    "            yield item\n",
    "\n",
    "def subword_tokenize(tokens):\n",
    "    subwords = list(map(tokenizer.tokenize, tokens))\n",
    "    subword_lengths = list(map(len, subwords))\n",
    "    subwords = list(flatten(subwords))\n",
    "    token_end_idxs = np.cumsum([0]+subword_lengths[:-1])+np.array(subword_lengths)-1\n",
    "    return subwords, token_end_idxs\n",
    "\n",
    "def position_to_mask(max_len,indices):\n",
    "    o=np.zeros(max_len,dtype=np.int)\n",
    "    o[indices%(max_len-2)+1]=1\n",
    "    return o\n",
    "\n",
    "def pad_ids_to_len(max_len,ids):\n",
    "    o=np.zeros(max_len, dtype=np.int)\n",
    "    o[:len(ids)]=np.array(ids)\n",
    "    return o\n",
    "\n",
    "def chunk_to_len(max_len,tokens,labels):\n",
    "    subwords,token_end_idxs = subword_tokenize(tokens)\n",
    "    teim=token_end_idxs%(max_length-2)\n",
    "    split_token_end_idxs=np.array_split(token_end_idxs,(np.argwhere((teim[1:])<teim[:-1]).flatten()+1).tolist())\n",
    "    split_subwords=np.array_split(subwords,token_end_idxs[np.argwhere((teim[1:])<teim[:-1]).flatten()+1].tolist())\n",
    "    split_labels=np.array_split(labels[1:],(np.argwhere((teim[1:])<teim[:-1]).flatten()+1).tolist())\n",
    "    ids=torch.tensor([pad_ids_to_len(max_len,tokenizer.convert_tokens_to_ids(['[CLS]']+list(_)[:max_len-2]+['[SEP]'])) for _ in split_subwords])\n",
    "    masks=torch.tensor([position_to_mask(max_len,_) for _ in split_token_end_idxs])\n",
    "    padded_labels=torch.tensor([pad_ids_to_len(max_len,[0]+list(_)[:max_len-2]+[0]) for _ in split_labels])\n",
    "    return ids,masks,padded_labels\n",
    "\n",
    "def chunk_to_len_batch(max_len,tokens,labels):\n",
    "    batch_ids=[]\n",
    "    batch_masks=[]\n",
    "    batch_labels=[]\n",
    "    for i,_ in enumerate(zip(tokens,labels)):\n",
    "        a,b,c=chunk_to_len(max_len,*_)\n",
    "        batch_ids.append(a)\n",
    "        batch_masks.append(b)\n",
    "        batch_labels.append(c)\n",
    "    return PunctuationDataset(torch.cat(batch_ids), torch.cat(batch_masks), torch.cat(batch_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=ted['dev'].map(chunk_examples_with_degree(degree), batched=True, batch_size=128,remove_columns=ted['dev'].column_names)\n",
    "dev_data=chunk_to_len_batch(max_len,data['texts'],data['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  1045,  2064,  1005,  1056,  2393,  2021,  2023,  4299,  2000,\n",
       "          2228,  2055,  2043,  2017,  1005,  2128,  1037,  2210,  4845,  1998,\n",
       "          2035,  2115,  2814,  3198,  2017,  2065,  1037, 22519,  2071,  2507,\n",
       "          2017,  2028,  4299,  1999,  1996,  2088,  2054,  2052,  2009,  2022,\n",
       "          1998,  1045,  2467,  4660,  2092,  1045,  1005,  1040,  2215,  1996,\n",
       "          4299,  2000,  2031,  1996,  9866,  2000,  2113,  3599,  2054,  2000,\n",
       "          4299,  2005,  2092,  2059,  2017,  1005,  1040,  2022, 14180,  2138,\n",
       "          2017,  1005,  1040,  2113,  2054,  2000,  4299,  2005,  1998,  2017,\n",
       "          1005,  1040,  2224,  2039,  2115,  4299,  1998,  2085,  2144,  2057,\n",
       "          2069,  2031,  2028,  4299,  4406,  2197,  2095,  2027,  2018,  2093,\n",
       "          8996,  1045,  1005,  1049,  2025,  2183,  2000,  4299,  2005,  2008,\n",
       "          2061,  2292,  1005,  1055,  2131,  2000,  2054,  1045,  2052,  2066,\n",
       "          2029,  2003,  2088,  3521,  1998,  1045,  2113,   102]),\n",
       " 'attention_mask': tensor([0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0.]),\n",
       " 'labels': tensor([0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 7, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 4, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
       "         0, 2, 0, 2, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 4,\n",
       "         0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3834, 3835)"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subwords,token_end_idxs = subword_tokenize(tokens)\n",
    "# token_end_idxs\n",
    "len(tokens),len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(110, 110),\n",
       " (107, 107),\n",
       " (116, 116),\n",
       " (112, 112),\n",
       " (109, 109),\n",
       " (122, 122),\n",
       " (123, 123),\n",
       " (124, 124),\n",
       " (124, 124),\n",
       " (122, 122),\n",
       " (119, 119),\n",
       " (112, 112),\n",
       " (122, 122),\n",
       " (116, 116),\n",
       " (111, 111),\n",
       " (116, 116),\n",
       " (117, 117),\n",
       " (115, 115),\n",
       " (116, 116),\n",
       " (109, 109),\n",
       " (112, 112),\n",
       " (113, 113),\n",
       " (116, 116),\n",
       " (118, 118),\n",
       " (114, 114),\n",
       " (112, 112),\n",
       " (108, 108),\n",
       " (124, 124),\n",
       " (116, 116),\n",
       " (120, 120),\n",
       " (120, 120),\n",
       " (114, 114),\n",
       " (112, 112),\n",
       " (13, 13)]"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens=data['texts'][0]\n",
    "labels=data['tags'][0]\n",
    "subwords,token_end_idxs = subword_tokenize(tokens)\n",
    "teim=token_end_idxs%(max_length-2)\n",
    "split_token_end_idxs=np.array_split(token_end_idxs,(np.argwhere((teim[1:])<teim[:-1]).flatten()+1).tolist())\n",
    "split_subwords=np.array_split(subwords,token_end_idxs[np.argwhere((teim[1:])<teim[:-1]).flatten()+1].tolist())\n",
    "split_labels=np.array_split(labels[1:],(np.argwhere((teim[1:])<teim[:-1]).flatten()+1).tolist())\n",
    "list(zip([len(_) for _ in split_token_end_idxs],[len(_) for _ in split_labels]))\n",
    "# ids=torch.tensor([pad_ids_to_len(max_len,tokenizer.convert_tokens_to_ids(['[CLS]']+list(_)+['[SEP]'])) for _ in split_subwords])\n",
    "# masks=torch.tensor([position_to_mask(max_len,_) for _ in split_token_end_idxs])\n",
    "# labels=torch.tensor([pad_ids_to_len(max_len,[0]+list(_)+[0]) for _ in split_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103,
     "referenced_widgets": [
      "4126b507acb94b68919a3106f811d262",
      "77aae155cbe44998af7f5c6e7d09941a",
      "cb3a63cecafe4b998bbe7c54313f9443",
      "8706c3fb380844378b5e5f75d2428643",
      "652d1164fb10493fa23cdaec9f216458",
      "8becc36787ec4b1f8a01bf5f6079b0a3",
      "7a7307ac70c8406199c64930c80b5c09",
      "93b92df558d84a03897722ceeccdbab1"
     ]
    },
    "id": "zJjbQ5qq22yk",
    "outputId": "726ed714-7550-40a9-9a64-eae26a27e311"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc9bfb4591c4cbea3ddf974451f5621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "..............................................................................................................................................................................................................................................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import regex as re\n",
    "import numpy as np\n",
    "\n",
    "def text2masks(n):\n",
    "    def text2masks(text):\n",
    "        '''Converts single paragraph of text into a list of words and corresponding punctuation based on the degree requested.'''\n",
    "        if n==0: \n",
    "            refilter=\"(?<=[.?!,;:\\-—… ])(?=[^.?!,;:\\-—… ])|$\" \n",
    "        else:\n",
    "            refilter=\"[.?!,;:\\-—…]{1,%d}(?= *[^.?!,;:\\-—…]+|$)|(?<=[^.?!,;:\\-—…]) +(?=[^.?!,;:\\-—…])\"%(n)\n",
    "        word=re.split(refilter,text, flags=re.V1)\n",
    "        punct=re.findall(refilter,text, flags=re.V1)\n",
    "        wordlist,punctlist=([] for _ in range(2))\n",
    "        for i in zip(word,punct+['']):\n",
    "            w,p=i[0].strip(),i[1].strip()\n",
    "            if w!='':\n",
    "                wordlist.append(re.sub(r'[.?!,;:\\-—… ]','',w))\n",
    "                punctlist.append(0 if not w[-1] in '.?!,;:-—…' else tag2id[w[-1]])\n",
    "            if p!='':\n",
    "                wordlist.append(p)\n",
    "                punctlist.append(0)\n",
    "        return(wordlist,punctlist)\n",
    "    return text2masks\n",
    "assert(text2masks(0)('\"Hello!!')==(['\"Hello'], [1]))\n",
    "assert(text2masks(1)('\"Hello!!')==(['\"Hello', '!'], [1, 0]))\n",
    "assert(text2masks(0)('\"Hello!!, I am human.')==(['\"Hello','I','am','human'], [2,0,0,4]))\n",
    "assert(text2masks(2)('\"Hello!!, I am human.')==(['\"Hello', '!,','I','am','human','.'], [1,0,0,0,0,0]))\n",
    "def chunk_examples_with_degree(n):\n",
    "    '''Ensure batched=True if using dataset.map or ensure the examples are wrapped in lists.'''\n",
    "    def chunk_examples(examples):\n",
    "        output={}\n",
    "        output['texts']=[]\n",
    "        output['tags']=[]\n",
    "        for sentence in examples['transcript']:\n",
    "            text,tag=text2masks(n)(sentence)\n",
    "            output['texts'].append(text)\n",
    "            output['tags'].append([0]+tag if text[0]!='' else tag) # [0]+tag so that in all case, the first tag refers to [CLS]\n",
    "        return output\n",
    "    return chunk_examples\n",
    "assert(chunk_examples_with_degree(0)({'transcript':['Hello!Bye…']})=={'texts': [['Hello', 'Bye']], 'tags': [[0, 1, 9]]})\n",
    "\n",
    "def encode_tags(encodings, docs, max_length, overlap):\n",
    "    encoded_labels = []\n",
    "    doc_id=0\n",
    "    label_offset=0\n",
    "    for doc_offset,current_doc_id in zip(encodings.offset_mapping,encodings['overflow_to_sample_mapping']):\n",
    "        if current_doc_id>doc_id:\n",
    "            doc_id+=1\n",
    "            label_offset=0\n",
    "            print('.', end='')\n",
    "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * 0 #-100\n",
    "        arr_offset = np.array(doc_offset)\n",
    "        if arr_offset[1,0]!=0: #Resolution if first token belongs to previous word.\n",
    "            label_offset+=1\n",
    "        # Gives the labels that should be assigned punctuation \n",
    "        # (after the tok before word prefixes: arr_offset :(0,i) where i>0)\n",
    "        arr_mask = ((arr_offset[:,0] == 0) & (arr_offset[:,1] != 0))[1:].tolist()+[False] \n",
    "        #Get index of last non-sep/unk/pad word\n",
    "        sep_idx=np.argwhere(arr_offset.sum(1)>0)[-1,0]\n",
    "        arr_mask[sep_idx]=True\n",
    "        doc_enc_labels[arr_mask] = docs[doc_id][label_offset:label_offset+sum(arr_mask)]\n",
    "        encoded_labels.append(doc_enc_labels)\n",
    "        label_offset+=sum(arr_mask[:max_length-overlap-1])-1 #-1 Assuming the last token is standalone word\n",
    "    return encoded_labels\n",
    "\n",
    "def process_dataset(dataset, split, max_length=128, overlap=63, degree=0):\n",
    "    data=dataset[split].map(chunk_examples_with_degree(degree), batched=True, batch_size=max_length,remove_columns=dataset[split].column_names)\n",
    "    encodings=tokenizer(data['texts'], is_split_into_words=True, return_offsets_mapping=True,\n",
    "              return_overflowing_tokens=True, padding=True, truncation=True, max_length=max_length, stride=overlap)\n",
    "    labels=encode_tags(encodings, data['tags'], max_length, overlap)\n",
    "    encodings.pop(\"offset_mapping\")\n",
    "    encodings.pop(\"overflow_to_sample_mapping\")\n",
    "    return PunctuationDataset(torch.tensor(encodings['input_ids'],dtype=torch.long),\n",
    "        torch.tensor(encodings['attention_mask'],dtype=torch.long),\n",
    "        torch.tensor(labels,dtype=torch.long))\n",
    "    \n",
    "test_dataset=process_dataset(ted,'test',degree=1)#,10,3)\n",
    "# dev_dataset=process_dataset(ted,'dev')\n",
    "# train_dataset=process_dataset(ted,'train')\n",
    "\n",
    "# for name,dataset in {'test':test_dataset ,'train':train_dataset, 'dev':dev_dataset}.items():#\n",
    "#     torch.save(dataset, '/content/gdrive/MyDrive/ASR/ted-'+name+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wm_RAYx4OxFe"
   },
   "outputs": [],
   "source": [
    "encodings=tokenizer(['Helloooo','I','am','a','human','meat','meat','meat'], is_split_into_words=True, return_offsets_mapping=True,\n",
    "              return_overflowing_tokens=True, padding=True, truncation=True,max_length=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6JAhezZeFni5",
    "outputId": "adc01270-7ad1-49bc-c5a3-27b123f15b11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 103,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_offset=np.array(encodings['offset_mapping'][1])\n",
    "arr_mask = ((arr_offset[:,0] == 0) & (arr_offset[:,1] != 0))[1:].tolist()+[False] # Gives the labels that should be assigned punctuation\n",
    "# arr_mask[encodings['input_ids'][1].index(102)-1]=True\n",
    "sep_idx=np.argwhere(arr_offset.sum(1)>0)[-1,0]\n",
    "# arr_mask[sep_idx]=True\n",
    "# arr_mask,encodings\n",
    "arr_offset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-571-b0ee4aa28e3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "torch.hstack([torch.zeros(5),torch.zeros(5)]).astype(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# !rm 'sample.csv'\n",
    "# # f=open('sample','ab')\n",
    "# with open(\"sample.csv\",\"ab\") as f:\n",
    "#     np.savetxt(f,torch.hstack([torch.ones(1,10),torch.zeros(1,10),torch.zeros(1,10)]),)\n",
    "#     np.savetxt(f,torch.hstack([torch.zeros(1,10),torch.ones(1,10),torch.zeros(1,10)]),)\n",
    "#     np.savetxt(f,torch.hstack([torch.zeros(1,10),torch.zeros(1,10),torch.ones(1,10)]),)\n",
    "# f.close()\n",
    "# np.loadtxt(\"./data/ted_talks_processed.test-batched.csv\").shape\n",
    "import torch.utils.data as data\n",
    "\n",
    "class CSVDataset(data.Dataset):\n",
    "    def __init__(self, path, chunksize, nb_samples):\n",
    "        self.path = path\n",
    "        self.chunksize = chunksize\n",
    "        self.len = int(nb_samples / self.chunksize)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        print(index)\n",
    "        x = next(\n",
    "            pd.read_csv(\n",
    "                self.path,\n",
    "                skiprows=(index%self.len)*self.chunksize,\n",
    "                chunksize=self.chunksize,\n",
    "                header=None,\n",
    "                delimiter=' '))\n",
    "        x = torch.from_numpy(x.values).reshape(-1,3,128)\n",
    "        return x[:,0,:],x[:,1,:],x[:,2,:]\n",
    "#         return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "ted_train_batched=CSVDataset('./data/ted_talks_processed.train-batched.csv',1000,57676)\n",
    "ted_dev_batched=CSVDataset('./data/ted_talks_processed.dev-batched.csv',1000,7439)\n",
    "ted_test_batched=CSVDataset('./data/ted_talks_processed.test-batched.csv',1000,7236)\n",
    "open_test_batched=CSVDataset('./data/open_subtitles_processed.test-batched.csv',1000,568113)\n",
    "# test_batched[0].shape\n",
    "# pd.read_csv('./data/ted_talks_processed.dev-batched.csv',header=None,delimiter=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>2054</td>\n",
       "      <td>1996</td>\n",
       "      <td>3109</td>\n",
       "      <td>2003</td>\n",
       "      <td>2008</td>\n",
       "      <td>1029</td>\n",
       "      <td>5037</td>\n",
       "      <td>4157</td>\n",
       "      <td>1998</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>2000</td>\n",
       "      <td>2022</td>\n",
       "      <td>7294</td>\n",
       "      <td>1996</td>\n",
       "      <td>6971</td>\n",
       "      <td>1010</td>\n",
       "      <td>1045</td>\n",
       "      <td>1005</td>\n",
       "      <td>1049</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>2222</td>\n",
       "      <td>2022</td>\n",
       "      <td>2204</td>\n",
       "      <td>2005</td>\n",
       "      <td>3010</td>\n",
       "      <td>2945</td>\n",
       "      <td>1012</td>\n",
       "      <td>2057</td>\n",
       "      <td>2097</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>2183</td>\n",
       "      <td>2000</td>\n",
       "      <td>3288</td>\n",
       "      <td>2032</td>\n",
       "      <td>2067</td>\n",
       "      <td>1012</td>\n",
       "      <td>3357</td>\n",
       "      <td>2185</td>\n",
       "      <td>2013</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101</td>\n",
       "      <td>2054</td>\n",
       "      <td>2055</td>\n",
       "      <td>26879</td>\n",
       "      <td>17083</td>\n",
       "      <td>3051</td>\n",
       "      <td>1005</td>\n",
       "      <td>1055</td>\n",
       "      <td>4861</td>\n",
       "      <td>1029</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568108</th>\n",
       "      <td>101</td>\n",
       "      <td>1010</td>\n",
       "      <td>2012</td>\n",
       "      <td>2115</td>\n",
       "      <td>2067</td>\n",
       "      <td>1529</td>\n",
       "      <td>2022</td>\n",
       "      <td>6176</td>\n",
       "      <td>1529</td>\n",
       "      <td>2017</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568109</th>\n",
       "      <td>101</td>\n",
       "      <td>1029</td>\n",
       "      <td>2428</td>\n",
       "      <td>1029</td>\n",
       "      <td>2033</td>\n",
       "      <td>1029</td>\n",
       "      <td>2054</td>\n",
       "      <td>2055</td>\n",
       "      <td>2009</td>\n",
       "      <td>1029</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568110</th>\n",
       "      <td>101</td>\n",
       "      <td>1029</td>\n",
       "      <td>4030</td>\n",
       "      <td>2091</td>\n",
       "      <td>1529</td>\n",
       "      <td>2054</td>\n",
       "      <td>1005</td>\n",
       "      <td>1055</td>\n",
       "      <td>1999</td>\n",
       "      <td>2023</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568111</th>\n",
       "      <td>101</td>\n",
       "      <td>3040</td>\n",
       "      <td>11132</td>\n",
       "      <td>2055</td>\n",
       "      <td>2023</td>\n",
       "      <td>1029</td>\n",
       "      <td>1996</td>\n",
       "      <td>28997</td>\n",
       "      <td>2003</td>\n",
       "      <td>6728</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568112</th>\n",
       "      <td>101</td>\n",
       "      <td>1529</td>\n",
       "      <td>2954</td>\n",
       "      <td>1529</td>\n",
       "      <td>2272</td>\n",
       "      <td>1529</td>\n",
       "      <td>2886</td>\n",
       "      <td>1529</td>\n",
       "      <td>2204</td>\n",
       "      <td>1529</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>568113 rows × 384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1      2      3      4     5     6      7     8     9    ...  \\\n",
       "0       101  2054   1996   3109   2003  2008  1029   5037  4157  1998  ...   \n",
       "1       101  2000   2022   7294   1996  6971  1010   1045  1005  1049  ...   \n",
       "2       101  2222   2022   2204   2005  3010  2945   1012  2057  2097  ...   \n",
       "3       101  2183   2000   3288   2032  2067  1012   3357  2185  2013  ...   \n",
       "4       101  2054   2055  26879  17083  3051  1005   1055  4861  1029  ...   \n",
       "...     ...   ...    ...    ...    ...   ...   ...    ...   ...   ...  ...   \n",
       "568108  101  1010   2012   2115   2067  1529  2022   6176  1529  2017  ...   \n",
       "568109  101  1029   2428   1029   2033  1029  2054   2055  2009  1029  ...   \n",
       "568110  101  1029   4030   2091   1529  2054  1005   1055  1999  2023  ...   \n",
       "568111  101  3040  11132   2055   2023  1029  1996  28997  2003  6728  ...   \n",
       "568112  101  1529   2954   1529   2272  1529  2886   1529  2204  1529  ...   \n",
       "\n",
       "        374  375  376  377  378  379  380  381  382  383  \n",
       "0         0    0    0    0    0    0    0    0    0    0  \n",
       "1         0    0    0    0    0    0    0    0    0    0  \n",
       "2         0    0    0    0    0    0    0    0    0    0  \n",
       "3         0    0    0    0    0    0    0    0    0    0  \n",
       "4         0    0    0    0    0    0    0    0    0    0  \n",
       "...     ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "568108    0    0    0    0    0    0    0    0    0    0  \n",
       "568109    0    0    0    0    0    0    0    0    0    0  \n",
       "568110    0    0    0    0    0    0    0    0    0    0  \n",
       "568111    0    0    0    0    0    0    0    0    0    0  \n",
       "568112    0    0    0    0    0    0    0    0    0    0  \n",
       "\n",
       "[568113 rows x 384 columns]"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ted_train_batched[5]\n",
    "pd.read_csv('./data/open_subtitles_processed.test-batched.csv',header=None,delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtext/data/example.py:52: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from torchtext import data\n",
    "dev = data.TabularDataset(\n",
    "    path='/home/nxingyu/project/data/ted_talks_processed.dev.csv', format='csv',\n",
    "    fields={'transcript': ('transcripts', data.Field(sequential=False)),})\n",
    "# train_iter = data.BucketIterator(dataset=ted['dev'], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtext/data/iterator.py:48: UserWarning: Iterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Field' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-524-eef636d2104e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtext/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \"\"\"\n\u001b[1;32m    233\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    333\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    333\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Field' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q077IWV5-P6R",
    "outputId": "cc1542a4-b672-465b-df9c-87766c8e8a63"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-bdd308fcdec498e9/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2/cache-5964375f1255733d.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................................................................................................................................................................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "dataset=process_dataset(ted,'test')\n",
    "# dataset.view(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "id": "I1SVoyQ0TVrI",
    "outputId": "04dba423-f03b-422c-b954-80e9dc962333"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"[CLS]  rightful  space  in  public  life  .  and  as  we  saw  earlier  ,  one  of  the  most  critical  variables  in  determining  whether  a  movement  will  be  successful  or  not  is  a  movement  '  s  ideology  regarding  the  role  of  women  in  public  life  .  this  is  a  question  of  whether  we  '  re  moving  towards  more  democratic  and  peaceful  societies  .  in  a  world  where  so  much  change  is  happening  ,  and  where  change  is  bound  to  continue  at  an  increasingly  faster  pace  ,  it  is  not  a  question  of  whether  we  will  face  conflict  ,  but  rather  a  question  of  which  stories  will  shape  how  we  choose  to  wage  conflict  .  thank  you  .  [SEP]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD] \""
      ]
     },
     "execution_count": 190,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.view(3007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "id": "8p0nj-zLLiD1",
    "outputId": "3264137e-e79e-401d-c5de-07bba21f91c6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"[CLS]  of  eyes  on  her  naked  body, even  though, intellectual  ##ly, she  knows  it  wasn  '  t  her  body. and  she  has  frequent  panic  attacks, especially  when  someone  she  doesn  '  t  know  tries  to  take  her  picture. what  if  they  '  re  going  to  make  another  deep  ##fa  ##ke? she  thinks  to  herself. and  so  for  the  sake  of  individuals  like  rana  a  ##y  ##yu  ##b  and  the  sake  of  our  democracy, we  need  to  do  something  right  now. thank  you. [SEP]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD]  [PAD] \""
      ]
     },
     "execution_count": 116,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %pprint24\n",
    "dataset.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "CbLr1DzyShMq",
    "outputId": "c81a62ab-7f3a-4cef-9243-4b2fd73b6fa8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"[CLS]  re, often  concerned  with  performing  at  our  best  and, as  a  result  we  try  and  control  what  we  '  re  doing  to  force  the  best  performance  the. end  result  is  that  we  actually  screw  up  in. basketball  the, term  unconscious  is  used  to  describe  a  shooter  who  can  '  t  miss  and. san  antonio  spurs  star  tim  duncan  has  said  when, you  have  to  stop  and  think  that  '  s, when  you  mess  up  in. dance  the, great  choreographer  george, bala  ##nch  ##ine  used, to  urge  his  dancers  don  '  t, think  just, do  when. the  pressure  '  s  on  when, we  want  to  put  our  best  foot  forward  somewhat, ironically  we, often  try  and  control  what  we  '  re  doing  in  a  way  that  leads  [SEP] \""
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.view(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gj3aWlid22yl",
    "outputId": "b2c141f5-1a57-46fe-fdea-c89d37b39c02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([  101,  2627,  1303,  1110, 17136,  1118,  1297,  1223,  1103,   102]),\n",
       "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0, -100])),\n",
       " (tensor([  101,  1297,  1223,  1103,  2343, 19420,  1986,  1184,  1225,   102]),\n",
       "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  tensor([   0,    0,    0,    0,    7,    4,    2,    0,    0, -100])),\n",
       " (tensor([ 101, 1986, 1184, 1225, 1195, 1198, 1202, 2421,  112,  102]),\n",
       "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  tensor([   4,    2,    0,    0,    0,    0,    7,    0, -100, -100])),\n",
       " (tensor([  101,  1202,  2421,   112,   188,  4267, 11553,  5822,  1142,   102]),\n",
       "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  tensor([   0,    7,    0, -100, -100,    0, -100, -100,    0, -100]))]"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(iter(test_dataset))[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YaD8i_y022ym"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Disable all GPUS\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    visible_devices = tf.config.get_visible_devices()\n",
    "    for device in visible_devices:\n",
    "        assert device.device_type != 'GPU'\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJ_qLuNa22ym",
    "outputId": "c40b21c8-7337-4810-c4ec-bc055eca4c55"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/nxingyu/.cache/huggingface/datasets/csv/default-295ea44b803e5492/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2/cache-47a2b754535508b1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Set the offset as 1\n",
    "# Set the cls tag as offset-1 value\n",
    "# Set the non zeros with the offset to sum(nonzeros)\n",
    "# offset+= sum(nonzeros in first half)\n",
    "# stop when offset+ len(mapping)>len(tag)\n",
    "\n",
    "\n",
    "def dataset2tf(dataset,split):\n",
    "    data=dataset[split].map(chunk_examples_with_degree(0), batched=True, batch_size=10,remove_columns=dataset[split].column_names)\n",
    "    train_encodings=tokenizer(data['texts'], is_split_into_words=True, return_offsets_mapping=True, \n",
    "              return_overflowing_tokens=True, padding=True, truncation=True, max_length=32, stride=15,)\n",
    "\n",
    "    train_labels=encode_tags(train_encodings, data['tags'])\n",
    "    train_encodings.pop(\"offset_mapping\")\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dict(train_encodings),\n",
    "        train_labels\n",
    "    ))\n",
    "    return train_dataset\n",
    "test_dataset=dataset2tf(ted,'test')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qcFmm8Vu22yn",
    "outputId": "400460ba-9cd3-4696-ffd4-c821a8d40a8a"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TensorDataset' object has no attribute 'take'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b63c14ac9c87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TensorDataset' object has no attribute 'take'"
     ]
    }
   ],
   "source": [
    "for i in test_dataset.take(4):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPCol8RH22yn",
    "outputId": "ae5c0570-dfd6-469c-8261-2b5ca328dd74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=32, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings[146]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ZFO_z-o22yo",
    "outputId": "186588e2-764f-4bd2-879e-f9ede2e09e2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('was', ''),\n",
       " ('to', ''),\n",
       " ('focus', ''),\n",
       " ('on', ''),\n",
       " ('math', ''),\n",
       " ('and', ''),\n",
       " ('science', ','),\n",
       " ('to', ''),\n",
       " ('focus', ''),\n",
       " ('on', ''),\n",
       " ('basic', ''),\n",
       " ('research', '.'),\n",
       " ('And', ''),\n",
       " (\"that's\", ''),\n",
       " ('what', ''),\n",
       " (\"we've\", ''),\n",
       " ('done', '.'),\n",
       " ('Six', ''),\n",
       " ('years', ''),\n",
       " ('ago', ''),\n",
       " ('or', ''),\n",
       " ('so', ','),\n",
       " ('I', ''),\n",
       " ('left', ''),\n",
       " ('Renaissance', ''),\n",
       " ('and', ''),\n",
       " ('went', ''),\n",
       " ('to', ''),\n",
       " ('work', ''),\n",
       " ('at', ''),\n",
       " ('the', ''),\n",
       " ('foundation', '.'),\n",
       " ('So', ''),\n",
       " (\"that's\", ''),\n",
       " ('what', ''),\n",
       " ('we', ''),\n",
       " ('do', '.'),\n",
       " ('And', ''),\n",
       " ('so', ''),\n",
       " ('Math', ''),\n",
       " ('for', ''),\n",
       " ('America', ''),\n",
       " ('is', ''),\n",
       " ('basically', ''),\n",
       " ('investing', ''),\n",
       " ('in', ''),\n",
       " ('math', ''),\n",
       " ('teachers', ''),\n",
       " ('around', ''),\n",
       " ('the', ''),\n",
       " ('country', ','),\n",
       " ('giving', ''),\n",
       " ('them', ''),\n",
       " ('some', ''),\n",
       " ('extra', ''),\n",
       " ('income', ','),\n",
       " ('giving', ''),\n",
       " ('them', ''),\n",
       " ('support', ''),\n",
       " ('and', ''),\n",
       " ('coaching', '.'),\n",
       " ('And', ''),\n",
       " ('really', ''),\n",
       " ('trying', ''),\n",
       " ('to', ''),\n",
       " ('make', ''),\n",
       " ('that', ''),\n",
       " ('more', ''),\n",
       " ('effective', ''),\n",
       " ('and', ''),\n",
       " ('make', ''),\n",
       " ('that', ''),\n",
       " ('a', ''),\n",
       " ('calling', ''),\n",
       " ('to', ''),\n",
       " ('which', ''),\n",
       " ('teachers', ''),\n",
       " ('can', ''),\n",
       " ('aspire', '.'),\n",
       " ('Yeah', '—'),\n",
       " ('instead', ''),\n",
       " ('of', ''),\n",
       " ('beating', ''),\n",
       " ('up', ''),\n",
       " ('the', ''),\n",
       " ('bad', ''),\n",
       " ('teachers', ','),\n",
       " ('which', ''),\n",
       " ('has', ''),\n",
       " ('created', ''),\n",
       " ('morale', ''),\n",
       " ('problems', ''),\n",
       " ('all', ''),\n",
       " ('through', ''),\n",
       " ('the', ''),\n",
       " ('educational', ''),\n",
       " ('community', ','),\n",
       " ('in', ''),\n",
       " ('particular', ''),\n",
       " ('in', ''),\n",
       " ('math', ''),\n",
       " ('and', ''),\n",
       " ('science', ','),\n",
       " ('we', ''),\n",
       " ('focus', ''),\n",
       " ('on', ''),\n",
       " ('celebrating', ''),\n",
       " ('the', ''),\n",
       " ('good', ''),\n",
       " ('ones', ''),\n",
       " ('and', ''),\n",
       " ('giving', ''),\n",
       " ('them', ''),\n",
       " ('status', '.'),\n",
       " ('Yeah', ','),\n",
       " ('we', ''),\n",
       " ('give', ''),\n",
       " ('them', ''),\n",
       " ('extra', ''),\n",
       " ('money', ','),\n",
       " ('15', ','),\n",
       " ('000', ''),\n",
       " ('dollars', ''),\n",
       " ('a', ''),\n",
       " ('year', '.'),\n",
       " ('We', ''),\n",
       " ('have', ''),\n",
       " ('800', ''),\n",
       " ('math', ''),\n",
       " ('and', ''),\n",
       " ('science', ''),\n",
       " ('teachers', ''),\n",
       " ('in', ''),\n",
       " ('New', ''),\n",
       " ('York', ''),\n",
       " ('City', ''),\n",
       " ('in', ''),\n",
       " ('public', ''),\n",
       " ('schools', ''),\n",
       " ('today', ','),\n",
       " ('as', ''),\n",
       " ('part', ''),\n",
       " ('of', ''),\n",
       " ('a', ''),\n",
       " ('core', '.'),\n",
       " (\"There's\", ''),\n",
       " ('a', ''),\n",
       " ('great', ''),\n",
       " ('morale', ''),\n",
       " ('among', ''),\n",
       " ('them', '.'),\n",
       " (\"They're\", ''),\n",
       " ('staying', ''),\n",
       " ('in', ''),\n",
       " ('the', ''),\n",
       " ('field', '.'),\n",
       " ('Next', ''),\n",
       " ('year', ','),\n",
       " (\"it'll\", ''),\n",
       " ('be', ''),\n",
       " ('1', ','),\n",
       " ('000', ''),\n",
       " ('and', ''),\n",
       " (\"that'll\", ''),\n",
       " ('be', ''),\n",
       " ('10', ''),\n",
       " ('percent', ''),\n",
       " ('of', ''),\n",
       " ('the', ''),\n",
       " ('math', ''),\n",
       " ('and', ''),\n",
       " ('science', ''),\n",
       " ('teachers', ''),\n",
       " ('in', ''),\n",
       " ('New', ''),\n",
       " ('York', ''),\n",
       " ('City', ''),\n",
       " ('public', ''),\n",
       " ('schools', '.'),\n",
       " ('Jim', ','),\n",
       " (\"here's\", ''),\n",
       " ('another', ''),\n",
       " ('project', ''),\n",
       " ('that', ''),\n",
       " (\"you've\", ''),\n",
       " ('supported', ''),\n",
       " ('philanthropically', ':'),\n",
       " ('Research', ''),\n",
       " ('into', ''),\n",
       " ('origins', ''),\n",
       " ('of', ''),\n",
       " ('life', ','),\n",
       " ('I', ''),\n",
       " ('guess', '.'),\n",
       " ('What', ''),\n",
       " ('are', ''),\n",
       " ('we', ''),\n",
       " ('looking', ''),\n",
       " ('at', ''),\n",
       " ('here', '?'),\n",
       " ('Well', ','),\n",
       " (\"I'll\", ''),\n",
       " ('save', ''),\n",
       " ('that', ''),\n",
       " ('for', ''),\n",
       " ('a', ''),\n",
       " ('second', '.'),\n",
       " ('And', ''),\n",
       " ('then', ''),\n",
       " (\"I'll\", ''),\n",
       " ('tell', ''),\n",
       " ('you', ''),\n",
       " ('what', ''),\n",
       " (\"you're\", ''),\n",
       " ('looking', ''),\n",
       " ('at', '.'),\n",
       " ('Origins', ''),\n",
       " ('of', ''),\n",
       " ('life', ''),\n",
       " ('is', ''),\n",
       " ('a', ''),\n",
       " ('fascinating', ''),\n",
       " ('question', '.'),\n",
       " ('How', ''),\n",
       " ('did', ''),\n",
       " ('we', ''),\n",
       " ('get', ''),\n",
       " ('here', '?'),\n",
       " ('Well', ','),\n",
       " ('there', ''),\n",
       " ('are', ''),\n",
       " ('two', ''),\n",
       " ('questions', ':'),\n",
       " ('One', ''),\n",
       " ('is', ','),\n",
       " ('what', ''),\n",
       " ('is', ''),\n",
       " ('the', ''),\n",
       " ('route', ''),\n",
       " ('from', ''),\n",
       " ('geology', ''),\n",
       " ('to', ''),\n",
       " ('biology', '—'),\n",
       " ('how', ''),\n",
       " ('did', ''),\n",
       " ('we', ''),\n",
       " ('get', ''),\n",
       " ('here', '?'),\n",
       " ('And', ''),\n",
       " ('the', ''),\n",
       " ('other', ''),\n",
       " ('question', ''),\n",
       " ('is', ','),\n",
       " ('what', ''),\n",
       " ('did', ''),\n",
       " ('we', ''),\n",
       " ('start', ''),\n",
       " ('with', '?'),\n",
       " ('What', ''),\n",
       " ('material', ','),\n",
       " ('if', ''),\n",
       " ('any', ','),\n",
       " ('did', ''),\n",
       " ('we', ''),\n",
       " ('have', ''),\n",
       " ('to', ''),\n",
       " ('work', ''),\n",
       " ('with', ''),\n",
       " ('on', ''),\n",
       " ('this', ''),\n",
       " ('route', '?'),\n",
       " ('Those', ''),\n",
       " ('are', ''),\n",
       " ('two', ''),\n",
       " ('very', ','),\n",
       " ('very', ''),\n",
       " ('interesting', ''),\n",
       " ('questions', '.'),\n",
       " ('The', ''),\n",
       " ('first', ''),\n",
       " ('question', ''),\n",
       " ('is', ''),\n",
       " ('a', ''),\n",
       " ('tortuous', ''),\n",
       " ('path', ''),\n",
       " ('from', ''),\n",
       " ('geology', ''),\n",
       " ('up', ''),\n",
       " ('to', ''),\n",
       " ('RNA', ''),\n",
       " ('or', ''),\n",
       " ('something', ''),\n",
       " ('like', ''),\n",
       " ('that', '—'),\n",
       " ('how', ''),\n",
       " ('did', ''),\n",
       " ('that', ''),\n",
       " ('all', ''),\n",
       " ('work', '?'),\n",
       " ('And', ''),\n",
       " ('the', ''),\n",
       " ('other', ','),\n",
       " ('what', ''),\n",
       " ('do', ''),\n",
       " ('we', ''),\n",
       " ('have', ''),\n",
       " ('to', ''),\n",
       " ('work', ''),\n",
       " ('with', '?'),\n",
       " ('Well', ','),\n",
       " ('more', ''),\n",
       " ('than', ''),\n",
       " ('we', ''),\n",
       " ('think', '.'),\n",
       " ('So', ''),\n",
       " (\"what's\", ''),\n",
       " ('pictured', ''),\n",
       " ('there', ''),\n",
       " ('is', ''),\n",
       " ('a', ''),\n",
       " ('star', ''),\n",
       " ('in', ''),\n",
       " ('formation', '.'),\n",
       " ('Now', ','),\n",
       " ('every', ''),\n",
       " ('year', ''),\n",
       " ('in', ''),\n",
       " ('our', ''),\n",
       " ('Milky', ''),\n",
       " ('Way', ','),\n",
       " ('which', ''),\n",
       " ('has', ''),\n",
       " ('100', ''),\n",
       " ('billion', ''),\n",
       " ('stars', ','),\n",
       " ('about', ''),\n",
       " ('two', ''),\n",
       " ('new', ''),\n",
       " ('stars', ''),\n",
       " ('are', ''),\n",
       " ('created', '.'),\n",
       " (\"Don't\", ''),\n",
       " ('ask', ''),\n",
       " ('me', ''),\n",
       " ('how', ','),\n",
       " ('but', ''),\n",
       " (\"they're\", ''),\n",
       " ('created', '.'),\n",
       " ('And', ''),\n",
       " ('it', ''),\n",
       " ('takes', ''),\n",
       " ('them', ''),\n",
       " ('about', ''),\n",
       " ('a', ''),\n",
       " ('million', ''),\n",
       " ('years', ''),\n",
       " ('to', ''),\n",
       " ('settle', ''),\n",
       " ('out', '.'),\n",
       " ('So', ','),\n",
       " ('in', ''),\n",
       " ('steady', ''),\n",
       " ('state', ','),\n",
       " ('there', ''),\n",
       " ('are', ''),\n",
       " ('about', ''),\n",
       " ('two', ''),\n",
       " ('million', ''),\n",
       " ('stars', ''),\n",
       " ('in', ''),\n",
       " ('formation', ''),\n",
       " ('at', ''),\n",
       " ('any', ''),\n",
       " ('time', '.'),\n",
       " ('That', ''),\n",
       " ('one', ''),\n",
       " ('is', ''),\n",
       " ('somewhere', ''),\n",
       " ('along', ''),\n",
       " ('this', ''),\n",
       " ('settling', '-'),\n",
       " ('down', ''),\n",
       " ('period', '.'),\n",
       " ('And', ''),\n",
       " (\"there's\", ''),\n",
       " ('all', ''),\n",
       " ('this', ''),\n",
       " ('crap', ''),\n",
       " ('sort', ''),\n",
       " ('of', ''),\n",
       " ('circling', ''),\n",
       " ('around', ''),\n",
       " ('it', ','),\n",
       " ('dust', ''),\n",
       " ('and', ''),\n",
       " ('stuff', '.'),\n",
       " ('And', ''),\n",
       " (\"it'll\", ''),\n",
       " ('form', ''),\n",
       " ('probably', ''),\n",
       " ('a', ''),\n",
       " ('solar', ''),\n",
       " ('system', ','),\n",
       " ('or', ''),\n",
       " ('whatever', ''),\n",
       " ('it', ''),\n",
       " ('forms', '.'),\n",
       " ('But', ''),\n",
       " (\"here's\", ''),\n",
       " ('the', ''),\n",
       " ('thing', '—'),\n",
       " ('in', ''),\n",
       " ('this', ''),\n",
       " ('dust', ''),\n",
       " ('that', ''),\n",
       " ('surrounds', ''),\n",
       " ('a', ''),\n",
       " ('forming', ''),\n",
       " ('star', ''),\n",
       " ('have', ''),\n",
       " ('been', ''),\n",
       " ('found', ','),\n",
       " ('now', ','),\n",
       " ('significant', ''),\n",
       " ('organic', ''),\n",
       " ('molecules', '.'),\n",
       " ('Molecules', ''),\n",
       " ('not', ''),\n",
       " ('just', ''),\n",
       " ('like', ''),\n",
       " ('methane', ','),\n",
       " ('but', ''),\n",
       " ('formaldehyde', ''),\n",
       " ('and', ''),\n",
       " ('cyanide', '—'),\n",
       " ('things', ''),\n",
       " ('that', ''),\n",
       " ('are', ''),\n",
       " ('the', ''),\n",
       " ('building', ''),\n",
       " ('blocks', '—'),\n",
       " ('the', ''),\n",
       " ('seeds', ','),\n",
       " ('if', ''),\n",
       " ('you', ''),\n",
       " ('will', '—'),\n",
       " ('of', ''),\n",
       " ('life', '.'),\n",
       " ('So', ','),\n",
       " ('that', ''),\n",
       " ('may', ''),\n",
       " ('be', ''),\n",
       " ('typical', '.'),\n",
       " ('And', ''),\n",
       " ('it', ''),\n",
       " ('may', ''),\n",
       " ('be', ''),\n",
       " ('typical', ''),\n",
       " ('that', ''),\n",
       " ('planets', ''),\n",
       " ('around', ''),\n",
       " ('the', ''),\n",
       " ('universe', ''),\n",
       " ('start', ''),\n",
       " ('off', ''),\n",
       " ('with', ''),\n",
       " ('some', ''),\n",
       " ('of', ''),\n",
       " ('these', ''),\n",
       " ('basic', ''),\n",
       " ('building', ''),\n",
       " ('blocks', '.'),\n",
       " ('Now', ''),\n",
       " ('does', ''),\n",
       " ('that', ''),\n",
       " ('mean', ''),\n",
       " (\"there's\", ''),\n",
       " ('going', ''),\n",
       " ('to', ''),\n",
       " ('be', ''),\n",
       " ('life', ''),\n",
       " ('all', ''),\n",
       " ('around', '?'),\n",
       " ('Maybe', '.'),\n",
       " ('But', ''),\n",
       " (\"it's\", ''),\n",
       " ('a', ''),\n",
       " ('question', ''),\n",
       " ('of', ''),\n",
       " ('how', ''),\n",
       " ('tortuous', ''),\n",
       " ('this', ''),\n",
       " ('path', ''),\n",
       " ('is', ''),\n",
       " ('from', ''),\n",
       " ('those', ''),\n",
       " ('frail', ''),\n",
       " ('beginnings', ','),\n",
       " ('those', ''),\n",
       " ('seeds', ','),\n",
       " ('all', ''),\n",
       " ('the', ''),\n",
       " ('way', ''),\n",
       " ('to', ''),\n",
       " ('life', '.'),\n",
       " ('And', ''),\n",
       " ('most', ''),\n",
       " ('of', ''),\n",
       " ('those', ''),\n",
       " ('seeds', ''),\n",
       " ('will', ''),\n",
       " ('fall', ''),\n",
       " ('on', ''),\n",
       " ('fallow', ''),\n",
       " ('planets', '.'),\n",
       " ('So', ''),\n",
       " ('for', ''),\n",
       " ('you', ','),\n",
       " ('personally', ','),\n",
       " ('finding', ''),\n",
       " ('an', ''),\n",
       " ('answer', ''),\n",
       " ('to', ''),\n",
       " ('this', ''),\n",
       " ('question', ''),\n",
       " ('of', ''),\n",
       " ('where', ''),\n",
       " ('we', ''),\n",
       " ('came', ''),\n",
       " ('from', ','),\n",
       " ('of', ''),\n",
       " ('how', ''),\n",
       " ('did', ''),\n",
       " ('this', ''),\n",
       " ('thing', ''),\n",
       " ('happen', ','),\n",
       " ('that', ''),\n",
       " ('is', ''),\n",
       " ('something', ''),\n",
       " ('you', ''),\n",
       " ('would', ''),\n",
       " ('love', ''),\n",
       " ('to', ''),\n",
       " ('see', '.'),\n",
       " ('Would', ''),\n",
       " ('love', ''),\n",
       " ('to', ''),\n",
       " ('see', '.'),\n",
       " ('And', ''),\n",
       " ('like', ''),\n",
       " ('to', ''),\n",
       " ('know', '—'),\n",
       " ('if', ''),\n",
       " ('that', ''),\n",
       " ('path', ''),\n",
       " ('is', ''),\n",
       " ('tortuous', ''),\n",
       " ('enough', ','),\n",
       " ('and', ''),\n",
       " ('so', ''),\n",
       " ('improbable', ','),\n",
       " ('that', ''),\n",
       " ('no', ''),\n",
       " ('matter', ''),\n",
       " ('what', ''),\n",
       " ('you', ''),\n",
       " ('start', ''),\n",
       " ('with', ','),\n",
       " ('we', ''),\n",
       " ('could', ''),\n",
       " ('be', ''),\n",
       " ('a', ''),\n",
       " ('singularity', '.'),\n",
       " ('But', ''),\n",
       " ('on', ''),\n",
       " ('the', ''),\n",
       " ('other', ''),\n",
       " ('hand', ','),\n",
       " ('given', ''),\n",
       " ('all', ''),\n",
       " ('this', ''),\n",
       " ('organic', ''),\n",
       " ('dust', ''),\n",
       " (\"that's\", ''),\n",
       " ('floating', ''),\n",
       " ('around', ','),\n",
       " ('we', ''),\n",
       " ('could', ''),\n",
       " ('have', ''),\n",
       " ('lots', ''),\n",
       " ('of', ''),\n",
       " ('friends', ''),\n",
       " ('out', ''),\n",
       " ('there', '.'),\n",
       " (\"It'd\", ''),\n",
       " ('be', ''),\n",
       " ('great', ''),\n",
       " ('to', ''),\n",
       " ('know', '.'),\n",
       " ('Jim', ','),\n",
       " ('a', ''),\n",
       " ('couple', ''),\n",
       " ('of', ''),\n",
       " ('years', ''),\n",
       " ('ago', ','),\n",
       " ('I', ''),\n",
       " ('got', ''),\n",
       " ('the', ''),\n",
       " ('chance', ''),\n",
       " ('to', ''),\n",
       " ('speak', ''),\n",
       " ('with', ''),\n",
       " ('Elon', ''),\n",
       " ('Musk', ','),\n",
       " ('and', ''),\n",
       " ('I', ''),\n",
       " ('asked', ''),\n",
       " ('him', ''),\n",
       " ('the', ''),\n",
       " ('secret', ''),\n",
       " ('of', ''),\n",
       " ('his', ''),\n",
       " ('success', ','),\n",
       " ('and', ''),\n",
       " ('he', ''),\n",
       " ('said', ''),\n",
       " ('taking', ''),\n",
       " ('physics', ''),\n",
       " ('seriously', ''),\n",
       " ('was', ''),\n",
       " ('it', '.'),\n",
       " ('Listening', ''),\n",
       " ('to', ''),\n",
       " ('you', ','),\n",
       " ('what', ''),\n",
       " ('I', ''),\n",
       " ('hear', ''),\n",
       " ('you', ''),\n",
       " ('saying', ''),\n",
       " ('is', ''),\n",
       " ('taking', ''),\n",
       " ('math', ''),\n",
       " ('seriously', ','),\n",
       " ('that', ''),\n",
       " ('has', ''),\n",
       " ('infused', ''),\n",
       " ('your', ''),\n",
       " ('whole', ''),\n",
       " ('life', '.'),\n",
       " (\"It's\", ''),\n",
       " ('made', ''),\n",
       " ('you', ''),\n",
       " ('an', ''),\n",
       " ('absolute', ''),\n",
       " ('fortune', ','),\n",
       " ('and', ''),\n",
       " ('now', ''),\n",
       " (\"it's\", ''),\n",
       " ('allowing', ''),\n",
       " ('you', ''),\n",
       " ('to', ''),\n",
       " ('invest', ''),\n",
       " ('in', ''),\n",
       " ('the', ''),\n",
       " ('futures', ''),\n",
       " ('of', ''),\n",
       " ('thousands', ''),\n",
       " ('and', ''),\n",
       " ('thousands', ''),\n",
       " ('of', ''),\n",
       " ('kids', ''),\n",
       " ('across', ''),\n",
       " ('America', ''),\n",
       " ('and', ''),\n",
       " ('elsewhere', '.'),\n",
       " ('Could', ''),\n",
       " ('it', ''),\n",
       " ('be', ''),\n",
       " ('that', ''),\n",
       " ('science', ''),\n",
       " ('actually', ''),\n",
       " ('works', '?'),\n",
       " ('That', ''),\n",
       " ('math', ''),\n",
       " ('actually', ''),\n",
       " ('works', '?'),\n",
       " ('Well', ','),\n",
       " ('math', ''),\n",
       " ('certainly', ''),\n",
       " ('works', '.'),\n",
       " ('Math', ''),\n",
       " ('certainly', ''),\n",
       " ('works', '.'),\n",
       " ('But', ''),\n",
       " ('this', ''),\n",
       " ('has', ''),\n",
       " ('been', ''),\n",
       " ('fun', '.'),\n",
       " ('Working', ''),\n",
       " ('with', ''),\n",
       " ('Marilyn', ''),\n",
       " ('and', ''),\n",
       " ('giving', ''),\n",
       " ('it', ''),\n",
       " ('away', ''),\n",
       " ('has', ''),\n",
       " ('been', ''),\n",
       " ('very', ''),\n",
       " ('enjoyable', '.'),\n",
       " ('I', ''),\n",
       " ('just', ''),\n",
       " ('find', ''),\n",
       " ('it', '—'),\n",
       " (\"it's\", ''),\n",
       " ('an', ''),\n",
       " ('inspirational', ''),\n",
       " ('thought', ''),\n",
       " ('to', ''),\n",
       " ('me', ','),\n",
       " ('that', ''),\n",
       " ('by', ''),\n",
       " ('taking', ''),\n",
       " ('knowledge', ''),\n",
       " ('seriously', ','),\n",
       " ('so', ''),\n",
       " ('much', ''),\n",
       " ('more', ''),\n",
       " ('can', ''),\n",
       " ('come', ''),\n",
       " ('from', ''),\n",
       " ('it', '.'),\n",
       " ('So', ''),\n",
       " ('thank', ''),\n",
       " ('you', ''),\n",
       " ('for', ''),\n",
       " ('your', ''),\n",
       " ('amazing', ''),\n",
       " ('life', ','),\n",
       " ('and', ''),\n",
       " ('for', ''),\n",
       " ('coming', ''),\n",
       " ('here', ''),\n",
       " ('to', ''),\n",
       " ('TED', '.'),\n",
       " ('Thank', ''),\n",
       " ('you', '.'),\n",
       " ('Jim', ''),\n",
       " ('Simons', '!')]"
      ]
     },
     "execution_count": 80,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [len(data[i]['tags']) for i in range(144,150)]\n",
    "list(zip(data[145]['texts'][2700:],[id2tag[t] if t>0 else '' for t in data[145]['tags'][2701:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XfB9IrIn22yo"
   },
   "outputs": [],
   "source": [
    "# chunked_ted_val=ted['val'].select([0,1]).map(chunk_examples_with_degree(0), batched=True, batch_size=10,remove_columns=ted['train'].column_names)\n",
    "# encodings=tokenizer(chunked_ted['texts'], is_split_into_words=True, return_offsets_mapping=True, \n",
    "#           return_overflowing_tokens=True, padding=True, truncation=True, max_length=32, stride=15,)\n",
    "# labels=encode_tags(encodings, chunked_ted['tags'])\n",
    "# encodings.pop(\"offset_mapping\")\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((\n",
    "#     dict(encodings),\n",
    "#     labels\n",
    "# ))\n",
    "# val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q420jtr722yo",
    "outputId": "8022b197-7b97-4cc9-b926-d2b64f33b8f3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForTokenClassification: ['activation_13', 'vocab_projector', 'vocab_layer_norm', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier', 'dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFDistilBertForTokenClassification\n",
    "model = TFDistilBertForTokenClassification.from_pretrained('distilbert-base-cased', num_labels=len(tags)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqzK3sjh22yp",
    "outputId": "037e3054-0211-41b1-c7dc-af2f57e7d170"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_token_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  65190912  \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  7690      \n",
      "=================================================================\n",
      "Total params: 65,198,602\n",
      "Trainable params: 65,198,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "3310/3310 [==============================] - ETA: 0s - loss: 0.1861"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-42f8ecd6cdb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# can also use any keras loss fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1121\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[0;32m-> 1123\u001b[0;31m           val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1124\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m               \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1377\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TraceContext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1380\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss) # can also use any keras loss fn\n",
    "model.fit(test_dataset.batch(16), epochs=3, batch_size=16, validation_data=test_dataset,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_dhG96822yp"
   },
   "outputs": [],
   "source": [
    "sample_output=model.predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-RrvHyy22yq",
    "outputId": "ed8d9125-6229-455d-c361-f10dd61da9c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 2, 0, 0],\n",
       "       [5, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 5, 0, ..., 0, 0, 2],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 5, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 5, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_output['logits'].argmax(axis=2).reshape(32,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UT_eKW-S22yr",
    "outputId": "fc9c627a-a4ac-4efe-c5da-aad005e66c56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1448, 2247, 4427, 1107, 1381, 5227, 2021, 15474, 8449, 1105, 8703, 170, 1299, 1150, 102], [101, 2021, 15474, 8449, 1105, 8703, 170, 1299, 1150, 1691, 10108, 102, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]], 'offset_mapping': [[(0, 0), (0, 3), (0, 6), (0, 9), (0, 2), (0, 4), (0, 2), (0, 6), (0, 8), (0, 10), (0, 3), (0, 8), (0, 1), (0, 3), (0, 3), (0, 0)], [(0, 0), (0, 6), (0, 8), (0, 10), (0, 3), (0, 8), (0, 1), (0, 3), (0, 3), (0, 8), (0, 10), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]], 'overflow_to_sample_mapping': [0, 0]}"
      ]
     },
     "execution_count": 112,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(chunked_ted['texts'][0], is_split_into_words=True, return_offsets_mapping=True, return_overflowing_tokens=True, padding=True, truncation=True, stride=8, max_length=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HwUQbHe22yr"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "encodings.pop(\"offset_mapping\")\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(encodings),\n",
    "    labels\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hlc3eBkD22yr",
    "outputId": "3f662b45-d736-46e3-cfc9-be32e2f29f98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(25,), dtype=int32, numpy=\n",
      "array([  101,  1448,  2247,  4427,  1107,  1381,  5227,  2021, 15474,\n",
      "        8449,  1105,  8703,   170,  1299,  1150,  1691,   102,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(25,), dtype=int32, numpy=\n",
      "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(25,), dtype=int32, numpy=\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0], dtype=int32)>}, <tf.Tensor: shape=(25,), dtype=int32, numpy=\n",
      "array([-100,    0,    0,    0,    0,    2,    0,    0,    2,    0,    0,\n",
      "          0,    0,    0,    0,    0, -100, -100, -100, -100, -100, -100,\n",
      "       -100, -100, -100], dtype=int32)>)\n"
     ]
    }
   ],
   "source": [
    "for data in dataset.take(1):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EdbO0Vf122ys",
    "outputId": "ee18b7bf-d9f5-418f-f14b-dc0187a507e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 20062, 117, 1166, 122, 1553, 126, 3775, 1234, 2541, 4223, 4139, 119, 1130, 2593, 117, 1234, 1132, 2257, 1106, 10556, 1147, 1583, 117, 2128, 1166, 1405, 1550, 8940, 119, 4288, 117, 1443, 170, 4095, 117, 1132, 1103, 1211, 7386, 1105, 8018, 5256, 795, 1133, 1136, 1198, 1121, 1103, 5119, 2952, 18552, 117, 1133, 1121, 1103, 1510, 8362, 20080, 27443, 3154, 1115, 8755, 1138, 1113, 1147, 2073, 119, 1109, 5758, 1104, 1594, 1817, 1482, 1120, 170, 1842, 1344, 3187, 1111, 1103, 1718, 1104, 6438, 1105, 18560, 2645, 119, 4288, 117, 1112, 1195, 1169, 1178, 5403, 117, 1209, 1631, 4472, 117, 4963, 1105, 1120, 3187, 119, 1252, 1175, 1110, 1363, 2371, 119, 1109, 3068, 1104, 1920, 1115, 1482, 3531, 1107, 1147, 2073, 1169, 1138, 170, 1167, 2418, 2629, 1113, 1147, 1218, 118, 1217, 1190, 1121, 1103, 4315, 5758, 1104, 1594, 1115, 1152, 1138, 1151, 5490, 1106, 119, 1573, 2140, 117, 1482, 1169, 1129, 4921, 1118, 3258, 117, 5343, 6486, 1158, 1219, 1105, 1170, 4139, 119, 1130, 1349, 117, 146, 1108, 170, 1148, 118, 1214, 7735, 2377, 1107, 1103, 1239, 1104, 4280, 1323, 1104, 24797, 4052, 119, 2409, 1242, 1104, 1128, 1303, 117, 146, 2542, 1103, 5532, 1107, 7303, 8362, 10787, 1107, 1524, 1104, 1143, 1113, 1103, 1794, 119, 1422, 1266, 1110, 2034, 1121, 7303, 117, 1105, 1304, 1346, 1113, 117, 146, 1575, 1317, 1266, 1484, 1107, 1541, 16358, 14791, 16877, 3242, 119, 146, 112, 173, 3465, 1105, 146, 112, 173, 8422, 1114, 1139, 1266, 1105, 2824, 1103, 1794, 119, 1284, 112, 1396, 1155, 1562, 1343, 4429, 131, 10095, 9769, 2275, 117, 10676, 117, 5915, 1105, 1234, 7406, 1105, 1919, 119, 1135, 1108, 1579, 1103, 1234, 7406, 1105, 1919, 1115, 1541, 1400, 1143, 1103, 1211, 117, 2108, 1343, 10444, 118, 1702, 1482, 119, 146, 1108, 170, 1534, 1106, 1160, 1685, 117, 3417, 1107, 27110, 8588, 1482, 119, 1220, 1127, 1421, 1105, 1565, 1173, 117, 1120, 1126, 1425, 1187, 1152, 3417, 1455, 7424, 1105, 7424, 1104, 3243, 117, 1105, 2637, 1842, 117, 13870, 6615, 119, 1573, 117, 146, 1310, 1106, 4608, 1184, 1122, 1547, 1129, 1176, 1106, 6486, 1139, 1482, 1107, 170, 1594, 4834, 1105, 170, 15820, 3227, 119, 5718, 1139, 1482, 1849, 136, 5718, 1139, 1797, 112, 188, 3999, 117, 2816, 1257, 3857, 1147, 18978, 136, 5718, 1139, 1488, 112, 188, 1541, 8000, 1105, 1920, 26743, 2731, 1561, 22984, 1105, 9512, 136, 1731, 1156, 146, 16743, 136, 5718, 146, 1849, 136, 1249, 16979, 1116, 1105, 6486, 26657, 117, 1195, 1221, 1115, 1981, 1158, 2153, 1114, 4196, 1107, 12605, 1111, 1147, 1482, 1169, 1138, 170, 3321, 2629, 1113, 1147, 1218, 118, 1217, 117, 1105, 1195, 1840, 1142, 6486, 2013, 119, 1109, 2304, 146, 1125, 1108, 117, 1180, 6486, 2013, 2648, 1129, 5616, 1111, 2073, 1229, 1152, 1127, 1253, 1107, 1594, 10490, 1137, 15820, 7869, 136, 7426, 1195, 2519, 1172, 1114, 5566, 1137, 2013, 1115, 1156, 1494, 1172, 1194, 1292, 11998, 136, 1573, 146, 4685, 1139, 7735, 16014, 117, 2986, 4858, 11917, 2312, 117, 1114, 1103, 1911, 1104, 1606, 1139, 3397, 4196, 1106, 1294, 1199, 1849, 1107, 1103, 1842, 1362, 119, 146, 1445, 112, 189, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 9), (9, 10), (11, 15), (16, 17), (18, 23), (24, 25), (26, 33), (34, 40), (41, 51), (52, 57), (58, 66), (66, 67), (68, 70), (71, 79), (79, 80), (81, 87), (88, 91), (92, 98), (99, 101), (102, 106), (107, 112), (113, 120), (120, 121), (122, 129), (130, 134), (135, 137), (138, 145), (146, 154), (154, 155), (156, 164), (164, 165), (166, 173), (174, 175), (176, 181), (181, 182), (183, 186), (187, 190), (191, 195), (196, 204), (205, 208), (209, 219), (220, 227), (228, 229), (230, 233), (234, 237), (238, 242), (243, 247), (248, 251), (252, 259), (260, 268), (269, 276), (276, 277), (278, 281), (282, 286), (287, 290), (291, 296), (297, 299), (299, 301), (301, 305), (306, 313), (314, 318), (319, 323), (324, 328), (329, 331), (332, 337), (338, 346), (346, 347), (348, 351), (352, 363), (364, 366), (367, 370), (371, 376), (377, 385), (386, 388), (389, 390), (391, 395), (396, 400), (401, 405), (406, 409), (410, 413), (414, 425), (426, 428), (429, 438), (439, 442), (443, 453), (454, 462), (462, 463), (464, 472), (472, 473), (474, 476), (477, 479), (480, 483), (484, 488), (489, 496), (496, 497), (498, 502), (503, 507), (508, 515), (515, 516), (517, 527), (528, 531), (532, 534), (535, 539), (539, 540), (541, 544), (545, 550), (551, 553), (554, 558), (559, 563), (563, 564), (565, 568), (569, 576), (577, 579), (580, 584), (585, 589), (590, 598), (599, 606), (607, 609), (610, 615), (616, 624), (625, 628), (629, 633), (634, 635), (636, 640), (641, 652), (653, 659), (660, 662), (663, 668), (669, 673), (673, 674), (674, 679), (680, 684), (685, 689), (690, 693), (694, 700), (701, 712), (713, 715), (716, 719), (720, 724), (725, 729), (730, 734), (735, 739), (740, 747), (748, 750), (750, 751), (752, 754), (755, 763), (763, 764), (765, 773), (774, 777), (778, 780), (781, 790), (791, 793), (794, 798), (798, 799), (800, 806), (807, 813), (813, 816), (817, 823), (824, 827), (828, 833), (834, 842), (842, 843), (844, 846), (847, 851), (851, 852), (853, 854), (855, 858), (859, 860), (861, 866), (866, 867), (867, 871), (872, 875), (876, 883), (884, 886), (887, 890), (891, 901), (902, 904), (905, 915), (916, 922), (923, 925), (926, 939), (940, 948), (948, 949), (950, 954), (955, 959), (960, 962), (963, 966), (967, 971), (971, 972), (973, 974), (975, 982), (983, 986), (987, 993), (994, 996), (997, 1002), (1003, 1005), (1005, 1009), (1010, 1012), (1013, 1018), (1019, 1021), (1022, 1024), (1025, 1027), (1028, 1031), (1032, 1034), (1034, 1035), (1036, 1038), (1039, 1045), (1046, 1048), (1049, 1059), (1060, 1064), (1065, 1070), (1070, 1071), (1072, 1075), (1076, 1080), (1081, 1086), (1087, 1089), (1089, 1090), (1091, 1092), (1093, 1097), (1098, 1105), (1106, 1112), (1113, 1120), (1121, 1123), (1124, 1130), (1131, 1133), (1133, 1136), (1136, 1141), (1142, 1146), (1146, 1147), (1148, 1149), (1149, 1150), (1150, 1151), (1152, 1155), (1156, 1159), (1160, 1161), (1161, 1162), (1162, 1163), (1164, 1170), (1171, 1175), (1176, 1178), (1179, 1185), (1186, 1189), (1190, 1195), (1196, 1199), (1200, 1202), (1202, 1203), (1204, 1206), (1206, 1207), (1207, 1209), (1210, 1213), (1214, 1218), (1219, 1224), (1225, 1231), (1231, 1232), (1233, 1238), (1239, 1249), (1250, 1259), (1259, 1260), (1261, 1266), (1266, 1267), (1268, 1279), (1280, 1283), (1284, 1290), (1291, 1300), (1301, 1304), (1305, 1312), (1312, 1313), (1314, 1316), (1317, 1320), (1321, 1327), (1328, 1331), (1332, 1338), (1339, 1348), (1349, 1352), (1353, 1360), (1361, 1365), (1366, 1372), (1373, 1376), (1377, 1379), (1380, 1383), (1384, 1388), (1388, 1389), (1390, 1400), (1401, 1406), (1407, 1416), (1416, 1417), (1417, 1424), (1425, 1433), (1433, 1434), (1435, 1436), (1437, 1440), (1441, 1442), (1443, 1449), (1450, 1452), (1453, 1456), (1457, 1462), (1462, 1463), (1464, 1473), (1474, 1476), (1476, 1480), (1480, 1485), (1486, 1494), (1494, 1495), (1496, 1500), (1501, 1505), (1506, 1510), (1511, 1514), (1515, 1518), (1519, 1523), (1523, 1524), (1525, 1527), (1528, 1530), (1531, 1534), (1535, 1540), (1541, 1545), (1546, 1555), (1556, 1561), (1562, 1566), (1567, 1570), (1571, 1575), (1576, 1578), (1579, 1588), (1588, 1589), (1590, 1593), (1594, 1602), (1603, 1607), (1607, 1608), (1609, 1619), (1620, 1627), (1627, 1628), (1629, 1631), (1631, 1632), (1633, 1634), (1635, 1640), (1641, 1643), (1644, 1650), (1651, 1655), (1656, 1658), (1659, 1664), (1665, 1667), (1668, 1672), (1673, 1675), (1676, 1682), (1683, 1685), (1686, 1694), (1695, 1697), (1698, 1699), (1700, 1703), (1704, 1708), (1709, 1712), (1713, 1714), (1715, 1722), (1723, 1727), (1727, 1728), (1729, 1734), (1735, 1737), (1738, 1746), (1747, 1753), (1753, 1754), (1755, 1760), (1761, 1763), (1764, 1772), (1772, 1773), (1773, 1774), (1775, 1781), (1781, 1782), (1783, 1788), (1789, 1793), (1794, 1798), (1799, 1804), (1805, 1810), (1810, 1811), (1812, 1817), (1818, 1820), (1821, 1824), (1824, 1825), (1825, 1826), (1827, 1833), (1834, 1841), (1842, 1845), (1846, 1850), (1850, 1854), (1855, 1861), (1862, 1868), (1869, 1876), (1877, 1880), (1881, 1890), (1890, 1891), (1892, 1895), (1896, 1901), (1902, 1903), (1904, 1908), (1908, 1909), (1910, 1915), (1916, 1917), (1918, 1924), (1924, 1925), (1926, 1928), (1929, 1941), (1941, 1942), (1943, 1946), (1947, 1953), (1954, 1962), (1962, 1963), (1964, 1966), (1967, 1971), (1972, 1976), (1977, 1980), (1980, 1983), (1984, 1991), (1992, 1996), (1997, 2003), (2004, 2006), (2007, 2013), (2014, 2017), (2018, 2023), (2024, 2032), (2033, 2036), (2037, 2041), (2042, 2043), (2044, 2048), (2049, 2055), (2056, 2058), (2059, 2064), (2065, 2069), (2069, 2070), (2070, 2075), (2075, 2076), (2077, 2080), (2081, 2083), (2084, 2088), (2089, 2093), (2094, 2100), (2101, 2109), (2109, 2110), (2111, 2114), (2115, 2123), (2124, 2125), (2126, 2129), (2130, 2133), (2133, 2134), (2135, 2140), (2141, 2147), (2148, 2156), (2157, 2165), (2166, 2168), (2169, 2175), (2176, 2179), (2180, 2188), (2189, 2194), (2195, 2199), (2200, 2204), (2205, 2210), (2211, 2213), (2214, 2217), (2218, 2223), (2224, 2226), (2227, 2234), (2235, 2240), (2240, 2241), (2242, 2247), (2248, 2250), (2251, 2256), (2257, 2261), (2262, 2266), (2267, 2273), (2274, 2276), (2277, 2285), (2286, 2290), (2291, 2296), (2297, 2301), (2302, 2306), (2307, 2314), (2315, 2320), (2321, 2330), (2330, 2331), (2332, 2334), (2335, 2336), (2337, 2347), (2348, 2350), (2351, 2354), (2355, 2365), (2365, 2366), (2367, 2376), (2377, 2383), (2384, 2387), (2387, 2389), (2389, 2390), (2391, 2395), (2396, 2399), (2400, 2404), (2405, 2407), (2408, 2413), (2414, 2416), (2417, 2425), (2426, 2432), (2433, 2435), (2436, 2440), (2441, 2445), (2446, 2452), (2453, 2455), (2456, 2459), (2460, 2464), (2465, 2470), (2470, 2471), (2472, 2473), (2474, 2478), (2478, 2479), (2479, 2480), (0, 0)]}"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(ted['train'][0]['transcript'], return_offsets_mapping=True, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dleHaD9122ys"
   },
   "outputs": [],
   "source": [
    "#create_pretraining_data.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import random\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "import tensorflow as tf\n",
    "\n",
    "flags = tf.compat.v1.flags\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9o98U9Hw22ys",
    "outputId": "b5b1c2ed-a1c2-406c-ba24-9d2176900747"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow.python.platform.flags' from '/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/tensorflow/python/platform/flags.py'>"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TrainingInstance(object):\n",
    "    def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels, is_random_next):\n",
    "        self.tokens = tokens\n",
    "        self.segment_ids = segment_ids\n",
    "        self.is_random_next = is_random_next\n",
    "        self.masked_lm_positions = masked_lm_positions\n",
    "        self.masked_lm_labels = masked_lm_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXkqp_uW22ys",
    "outputId": "79090af3-44ab-43d7-9d55-ff536b82011e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-21 11:43:00,756 [INFO] b'fcfb139\\n'\n",
      "2020-12-21 11:43:00,761 [WARNING] 2020-12-21 11:43\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "root_logger= logging.getLogger()\n",
    "root_logger.setLevel(logging.DEBUG)\n",
    "handler = logging.FileHandler('test.log', 'w', 'utf-8')\n",
    "handler.setFormatter(logging.Formatter(\"%(asctime)s [%(levelname)s] %(message)s\"))\n",
    "root_logger.addHandler(handler)\n",
    "\n",
    "import subprocess\n",
    "root_logger.info(subprocess.check_output(['git', 'describe', '--always']))\n",
    "import datetime\n",
    "root_logger.warning(datetime.datetime.now().strftime('%Y-%m-%d %H:%M'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "processing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "4126b507acb94b68919a3106f811d262": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cb3a63cecafe4b998bbe7c54313f9443",
       "IPY_MODEL_8706c3fb380844378b5e5f75d2428643"
      ],
      "layout": "IPY_MODEL_77aae155cbe44998af7f5c6e7d09941a"
     }
    },
    "652d1164fb10493fa23cdaec9f216458": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "77aae155cbe44998af7f5c6e7d09941a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a7307ac70c8406199c64930c80b5c09": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8706c3fb380844378b5e5f75d2428643": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93b92df558d84a03897722ceeccdbab1",
      "placeholder": "​",
      "style": "IPY_MODEL_7a7307ac70c8406199c64930c80b5c09",
      "value": " 4/4 [00:04&lt;00:00,  1.09s/ba]"
     }
    },
    "8becc36787ec4b1f8a01bf5f6079b0a3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93b92df558d84a03897722ceeccdbab1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb3a63cecafe4b998bbe7c54313f9443": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8becc36787ec4b1f8a01bf5f6079b0a3",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_652d1164fb10493fa23cdaec9f216458",
      "value": 4
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
