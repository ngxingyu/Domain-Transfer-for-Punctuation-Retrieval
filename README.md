# Domain Transfer for punctuation retrieval

## Navigating this repository

```bash
project
└───Punctuation_with_Domain_discriminator/results/ : results referenced in report.
│
│   README.md      : This file
│   processcsv.py  : preprocess raw csv $ python ~/project/processcsv.py -i sourcepath.csv -o sourcepath_processed.csv -c 2000 (process in chunks of 2000)
│   processxml.py  : helper script for bin/xml2csv.sh
│   setup.sh       : sample script for setting up project environment. Change the CUDA version before running
│   .gitignore
│   NLP.yml        : conda environment. (I use setup.sh instead)
│
└───bin
│   │   get-data.sh     : Script for getting initial dataset before preprocessing. Get from kaggle instead
│   │   percsplit.sh    : helper for processandsplit.sh
│   │   processandsplit.sh  : Split csv into 3 files $ bash ~/project/bin/processandsplit.sh csvfilepath 8 1 1 (train dev test)
│   │   processpipeline.sh  : Outdated. Get dataset from kaggle instead
│   │   shuffle.sh  : helper script to sort or shuffle csv file with seed
│   │   xml2csv.sh  : Crawl opensubtitles folders and extract text from xml to a single csv file.
│   
└───experiment
    │   main.py     : main script for training  $ python main.py    // Set configs in config.yaml
    │   config.yaml : main config file for setting all hparams
    │   info.log    : autogenerated
    │   results.txt : ignore
    │   test_config.yaml : ignore
    │   testing.py  : script for testing. $ python testing.py // Either test a csv, perform inference on list of text, or take input from commmandline
    │                 (Set path to model ckpt file and comment out relavant section in file before running)
    │   
    └───models
    │   │   punctuation_domain_model.py   : main code for PunctuationDomainModel
    │   
    └───data	: extra scripts for dataprocessing
    │   │   punctuation_count.sh  : View punctuation distribution for files
    │   │   punctuation_datamodule.py  : Datamodule for model
    │   │   punctuation_dataset_multi.py  : PyTorch Dataset code for training and inference
    │   │   percsplit / processandsplit / shuffle / xml2csv same as above
    │   │   explode.py  : Split long examples into shorter lengths at sentence boundaries
    │
    └───core    : scripts used by PunctuationDomainModel/Dataset etc.

```

## Datasources

The chosen datasources for this project are:

1. [TED \- Ultimate Dataset \| Kaggle](https://www.kaggle.com/miguelcorraljr/ted-ultimate-dataset) \- A collection of 4005 TED talks.
2. [Untokenised Corpus files for Opensubtitles](http://opus.nlpl.eu/OpenSubtitles-v2018.php) \- Select the rightmost column language ID\, in my case en\.
3. [Switchboard corpus](https://catalog.ldc.upenn.edu/docs/LDC97S62/swb1_dialogact_annot.tar.gz) \- Concatenate all telephone conversations

## Preprocessing

The following steps are taken for preprocessing the data to a useable form.

1. Remove speaker tags or tags that are added for better readability, such as sound effects e.g.  (Narrator:  ) or (Applause).
2. Identify spoken text that are within square or round brackets and remove the brackets.
3. Remove music lyrics bounded by music note symbols i.e. `♫ Lyrics ♫` as they contain minimal punctuation information.
4. Remove empty matching tags - square brackets, parentheses, single/double quotes. Remove empty matching tags: square brackets, parentheses, single or double quotes as they are not covered under the scope of this research.
5. Convert ellipsis to the unicode version.
6. Removing Non-sentence punctuation (All punctuation that are non-readable (not @$#%&^+=€²£¥) ).
7. Replace en-dash with hyphen
8. Combining repeated patterns of punctuation, i.e. [. .] to [.], [!!!!] to [!].
9. Pronounce common symbols such as * to times, or 5.0 to 5 point 0
10. Remove excess white-spaces
11. Sort the entire corpus by chronological order
12. Remove duplicates
13. Perform train development test split of 0.8 0.1 0.1.
14. Split large document into smaller size (e.g. chunks of around 1500 characters at sentence boundaries) to reduce size difference between examples. $ python ~/project/experiment/data/explode.py -i open_subtitles_processed.csv -o open_subtitles_explode.csv -l 1500 -s 0

## Processing part-2

**The punctuation to be classified are as follows**:
{0: '', 1: '!', 2: ',', 3: '-', 4: '.', 5: ':', 6: ';', 7: '?', 8: '—', 9: '…'} with 8 being the emdash.

There are occurences of consecutive punctuation. This includes:

1. `.,` : period after abbreviation or initial
2. `?,` or `!—` etc. where the first punctuation applies to a local scope and the 2nd applies to a larger context.
3. anomalies i.e. ?! or !! or even a hyphen leading the next sentence

In most cases, it makes more sense to classify the punctuations from right to left, so for this project I'll just consider the punctuation at the right:

The process of converting continuous text to a form to be input into the model is as follows (text2mask function):

1. Taking the text and degree (0 being the punctuation at the right), convert the text into 2 lists - the first being a list of words, and the second being a list of previously classified punctuations or spaces dividing the text. (i.e. when degree is 0, the 2nd list contains all empty strings.)
2. Intialize 2 new lists, a and b. Process both lists alternately beginning with the words list, identifying the trailing punctuation and stripping all punctuations from the tail. The word will be appended to a and the id of the punctuation identified will be appended to b.

For a better idea of the entire preprocessing pipeline, refer to the file ```./experiment/data/punctuation_datamodule.py``` and all modules that it references. 

### Environment setup

``` console
# After changing the cudatoolkit version based on your GPU
$ . ./setup.sh
```

### Preprocessing commands

You can either manually process the raw data files or obtain the preprocessed version from kaggle. 

From scratch:
``` console
$ bash ~/project/get-data.sh

# Switchboard to csv
$ bash ~/project/experiment/data/disfl2csv.sh /home/nxingyu/data/LDC99T42/treebank_3/dysfl/dff/swbd /home/nxingyu/data/switchboard_processed.csv
$ bash ~/project/experiment/data/utt2csv.sh /home/nxingyu/data/utt /home/nxingyu/data/switchboardutt_processed.csv


# Preprocess csv
$ python ~/project/processcsv.py -i ~/data/switchboardutt_processed.csv -o ~/data/switchboardutt_processed.csv -c 2000
$ python ~/project/processcsv.py -i ~/data/switchboard_processed.csv -o ~/data/switchboard_processed.csv -c 2000
$ python ~/project/processcsv.py -i ~/data/ted_talks_en.csv -o ~/data/ted_talks_processed.csv -c 2000
$ python ~/project/processcsv.py -i ~/data/open_subtitles.csv -o ~/data/open_subtitles_processed.csv -c 2000

# Split train dev test
$ bash ~/project/bin/processandsplit.sh ./switchboard_processed.csv 8 1 1
$ bash ~/project/bin/processandsplit.sh ./switchboardutt_processed.csv 8 1 1
$ bash ~/project/bin/processandsplit.sh ./ted_talks_processed.csv 8 1 1
$ bash ~/project/bin/processandsplit.sh ./open_subtitles_processed.csv 8 1 1

# Explode to reduce length (splits every 2500+ characters at a sentence punctuation mark. the final chunk < 2500 characters will just be taken as it is.)
$ python ~/project/experiment/data/explode.py -i switchboardutt_processed.csv -o switchboardutt_explode.csv -l 2500 -s 0
$ python ~/project/experiment/data/explode.py -i ted_talks_processed.csv -o  ted_talks_explode.csv -l 2500 -s 0
$ python ~/project/experiment/data/explode.py -i open_subtitles_processed.csv -o open_subtitles_explode.csv -l 2500 -s 0

# Rename splits and add header to all files
$ find . -name '*.*.csv' -exec bash -c ' mv $0 ${0/explode/processed}' {} \;
$ sed -i 1i"id,transcript" *processed*

# Create low resource switchboard dataset (set NR<= 1+number of lines)
$ awk 'NR==1 || NR<=5' switchboardutt_processed.train.csv > switchboardutt_processedlow.train.csv
$ cp switchboardutt_processed.dev.csv switchboardutt_processedlow.dev.csv
$ cp switchboardutt_processed.test.csv switchboardutt_processedlow.test.csv

<!-- kaggle datasets version -m 'message' -->
```

From kaggle (Using kaggle api or downloading from [here](https://www.kaggle.com/ngxingyu/preprocessed-english-spoken-transcripts):
```bash
$ kaggle datasets download -d ngxingyu/preprocessed-english-spoken-transcripts
$ unzip preprocessed-english-spoken-transcripts.zip
$ mv *.csv ~/data/
```

### Running Experiments

#### To train:
From the experiment directory,
Update the config.yaml file
Run ```python main.py```, specifying any specific GPU if needed.

#### To view logs:
Run ```tensorboard serve --logdir diroflogs```

#### To evaluate experiment:
Update the experiment folder name containing the .ckpt file in experiment/testing.py
Uncomment the relavant section of code based on the intended task: evaluating the test set, performing inference on the list of test, and performing inference based on input from commandline.
Run ```python testing.py```



## Observations

Experiments on Switchboard corpus, performing hparams tuning on the following: (CEL, FL, DL with MLP or CRF).
Result: 
- Dice Loss performs the best. 
- CRF seems hard to train on this dataset, performing poorly on rarer classes. This might be due to the small training size
- Unfreezing of the bert layer does not affect the overall accuracy much.




<!-- ## Log for 26/1/2020

Found a bug in regex pattern: A-z also includes punctuation characters, use A-Za-z instead.
Worked on creating the model in python instead of ipynb.

## Log for 27/1/2020

Use code-server

``` console
user@instance:~$ fuser -k 9999/tcp
user@instance:~$ code-server --bind-addr 127.0.0.1:9999 --auth none &
```

To convert all to lowercase.
To strip leading before first Uppercase, after last sentence punctuation.

Repeated starts are possible i.e. similar show but different episodes. Perhaps better to remove the shuffling and just split by order? yes. I'll do this instead.

Some of the regexes are flawed, to check if spare time?

Convert from huggingface load dataset which loads all to memory to pandas chunking map save.

## Log for 28/1/2020

Found an arabic character in one of the texts "Co ِ perative" which broke the tokenizer parsing. To go through the preprocessing step in greater detail now.

## Log for 2/2/2020

Converted torch Dataset into IterableDataset with chunks, for faster loading. Each batch features a ConcatDataset looking at all children datasets which are cycled using itertools.cycle. They run until the largest batch size is fully covered.

To implement:

* Random shuffling of csv dataset each batch.
* Look at effectiveness of Dice Loss and possible hyperparameters which can improve its F score.
* Evaluate the effectiveness of smaller models -->

<!-- Git issues:

``` console
git filter-branch -f --index-filter 'git rm -rf --cached --ignore-unmatch ./experiment/nemo_experiments/Punctuation_with_Domain_discriminator/*' --tag-name-filter cat -- --all
git rev-list --objects --all |   git cat-file --batch-check='%(objecttype) %(objectname) %(objectsize) %(rest)' |   sed -n 's/^blob //p' |   sort --numeric-sort --key=2 |   cut -c 1-12,41- |   $(command -v gnumfmt || echo numfmt) --field=2 --to=iec-i --suffix=B --padding=7 --round=nearest
git gc --prune=now
``` -->
