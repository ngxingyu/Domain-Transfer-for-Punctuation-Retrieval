{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1320    THOSE LOVE PANGS RIVALS THERE'S A FEMALE HIDIN...\n",
       "Name: transcript, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_test.transcript[open_test.transcript.apply(lambda x:len(x.split()))==17]#.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default-a2ce7fdae4f5a4d8 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/nxingyu/.cache/huggingface/datasets/csv/default-a2ce7fdae4f5a4d8/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/nxingyu/.cache/huggingface/datasets/csv/default-a2ce7fdae4f5a4d8/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ted=load_dataset('csv',data_files={#'train':'/home/nxingyu/project/data/ted_talks_processed.train.csv',\n",
    "                                    #     'dev':'/home/nxingyu/project/data/ted_talks_processed.dev.csv',\n",
    "                                         'test':'/home/nxingyu/project/data/ted_talks_processed.test.csv'})\n",
    "# subtitles=load_dataset('csv',data_files={'train':'/home/nxingyu/project/data/open_subtitles_processed.train.csv',\n",
    "#                                          'dev':'/home/nxingyu/project/data/open_subtitles_processed.dev.csv',\n",
    "#                                          'test':'/home/nxingyu/project/data/open_subtitles_processed.test.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". 40835 0.3979476484690198\n",
      "? 3679 0.035852807609098175\n",
      "! 291 0.002835870349075175\n",
      ", 48727 0.4748572319566531\n",
      "; 586 0.005710721733876469\n",
      ": 1243 0.012113356851891554\n",
      "- 3874 0.03775313310074649\n",
      "— 3168 0.03087298029508644\n",
      "… 211 0.00205624963455279\n",
      "734841\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def punct_proportion(df):\n",
    "    l=[]\n",
    "    st=pd.Series(df['transcript']).str\n",
    "    for c in \".?!,;:-—…\":\n",
    "        l.append(sum(st.count(\"\\\\\"+c)))\n",
    "    [print(i[0],i[1],i[1]/sum(l)) for i in zip(list(\".?!,;:-—…\"),l)]\n",
    "    print(st.count('\\w+').sum())\n",
    "for split in ted:\n",
    "    punct_proportion(ted[split])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". 47443035 0.44392753421469083\n",
      "? 13250829 0.12398886041482206\n",
      "! 6519047 0.0609991426589736\n",
      ", 24551760 0.22973239965425646\n",
      "; 57285 0.0005360194346227758\n",
      ": 374276 0.003502124638437183\n",
      "- 10479724 0.09805945244798349\n",
      "— 9564 8.949096399986432e-05\n",
      "… 4185605 0.03916497557221373\n",
      "419201886\n",
      "\n",
      ". 5921757 0.4430048481920479\n",
      "? 1660096 0.12419127911939411\n",
      "! 813038 0.06082312660995144\n",
      ", 3062385 0.22909609462708524\n",
      "; 8073 0.0006039386856729181\n",
      ": 57019 0.004265574125899185\n",
      "- 1321067 0.09882862227992877\n",
      "— 591 4.421253105818092e-05\n",
      "… 523225 0.03914230382896229\n",
      "52422785\n",
      "\n",
      ". 5921304 0.4422626972770535\n",
      "? 1670946 0.12480309826421737\n",
      "! 834767 0.06234881793231256\n",
      ", 3063118 0.2287845428570959\n",
      "; 7344 0.0005485239820152251\n",
      ": 44933 0.0033560495756930976\n",
      "- 1324557 0.09893127451608667\n",
      "— 1210 9.037500248344532e-05\n",
      "… 520479 0.03887462059304226\n",
      "52341671\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for split in subtitles:\n",
    "    punct_proportion(subtitles[split])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['talk_id', 'transcript'],\n",
       "     num_rows: 3195\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['talk_id', 'transcript'],\n",
       "     num_rows: 399\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['talk_id', 'transcript'],\n",
       "     num_rows: 400\n",
       " }))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ted_train.sort_values('talk_id').sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "ted_train=ted['train']\n",
    "ted_dev=ted['dev']\n",
    "ted_test=ted['test']\n",
    "ted_train,ted_dev,ted_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'!': 1, ',': 2, '-': 3, '.': 4, ':': 5, ';': 6, '?': 7, '—': 8, '…': 9},\n",
       " {1: '!', 2: ',', 3: '-', 4: '.', 5: ':', 6: ';', 7: '?', 8: '—', 9: '…'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "tags=sorted(list('.?!,;:-—…'))\n",
    "tag2id = {tag: id+1 for id, tag in enumerate(tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}\n",
    "tag2id,id2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6aae1cf975e42d78226bf98ca77eb15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=40.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "..............................................................................................................................................................................................................................................................................................................................................................................................................."
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-eda27007b57d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m#         torch.tensor(encodings['attention_mask'],dtype=torch.long),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m#         torch.tensor(labels,dtype=torch.long))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;31m# dev_dataset=process_dataset(ted,'dev')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m# train_dataset=process_dataset(ted,'train')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-eda27007b57d>\u001b[0m in \u001b[0;36mprocess_dataset\u001b[0;34m(dataset, split, max_length, overlap)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;31m#         train_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m#     ))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     return torch.utils.data.TensorDataset({'input_ids':torch.tensor(encodings['input_ids'],dtype=torch.long),\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m 'labels':torch.tensor(labels,dtype=torch.long)})\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import regex as re\n",
    "import numpy as np\n",
    "\n",
    "tags=sorted(list('.?!,;:-—…'))\n",
    "tag2id = {tag: id+1 for id, tag in enumerate(tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}\n",
    "\n",
    "def text2masks(n):\n",
    "    def text2masks(text):\n",
    "        '''Converts single paragraph of text into a list of words and corresponding punctuation based on the degree requested.'''\n",
    "        if n==0: \n",
    "            refilter=\"(?<=[.?!,;:\\-—… ])(?=[^.?!,;:\\-—… ])|$\" \n",
    "        else:\n",
    "            refilter=\"[.?!,;:\\-—…]{1,%d}(?= *[^.?!,;:\\-—…]+|$)|(?<=[^.?!,;:\\-—…]) +(?=[^.?!,;:\\-—…])\"%(n)\n",
    "        word=re.split(refilter,text, flags=re.V1)\n",
    "        punct=re.findall(refilter,text, flags=re.V1)\n",
    "        wordlist,punctlist=([] for _ in range(2))\n",
    "        for i in zip(word,punct+['']):\n",
    "            w,p=i[0].strip(),i[1].strip()\n",
    "            if w!='':\n",
    "                wordlist.append(re.sub(r'[.?!,;:\\-—… ]','',w))\n",
    "                punctlist.append(0 if not w[-1] in '.?!,;:-—…' else tag2id[w[-1]])\n",
    "            if p!='':\n",
    "                wordlist.append(p)\n",
    "                punctlist.append(0)\n",
    "        return(wordlist,punctlist)\n",
    "    return text2masks\n",
    "assert(text2masks(0)('\"Hello!!')==(['\"Hello'], [1]))\n",
    "assert(text2masks(1)('\"Hello!!')==(['\"Hello', '!'], [1, 0]))\n",
    "assert(text2masks(0)('\"Hello!!, I am human.')==(['\"Hello','I','am','human'], [2,0,0,4]))\n",
    "assert(text2masks(2)('\"Hello!!, I am human.')==(['\"Hello', '!,','I','am','human','.'], [1,0,0,0,0,0]))\n",
    "def chunk_examples_with_degree(n):\n",
    "    '''Ensure batched=True if using dataset.map or ensure the examples are wrapped in lists.'''\n",
    "    def chunk_examples(examples):\n",
    "        output={}\n",
    "        output['texts']=[]\n",
    "        output['tags']=[]\n",
    "        for sentence in examples['transcript']:\n",
    "            text,tag=text2masks(n)(sentence)\n",
    "            output['texts'].append(text)\n",
    "            output['tags'].append([0]+tag if text[0]!='' else tag) # [0]+tag so that in all case, the first tag refers to [CLS]\n",
    "        return output\n",
    "    return chunk_examples\n",
    "assert(chunk_examples_with_degree(0)({'transcript':['Hello!Bye…']})=={'texts': [['Hello', 'Bye']], 'tags': [[0, 1, 9]]})\n",
    "\n",
    "def encode_tags(encodings, docs, max_length, overlap):\n",
    "    encoded_labels = []\n",
    "    doc_id=0\n",
    "    label_offset=1\n",
    "#     print(encodings.keys())\n",
    "    for doc_offset,current_doc_id in zip(encodings.offset_mapping,encodings['overflow_to_sample_mapping']):\n",
    "#         print(doc_id, end=' ')\n",
    "        if current_doc_id>doc_id:\n",
    "            doc_id+=1\n",
    "            label_offset=0\n",
    "            print('.', end='')\n",
    "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
    "        doc_enc_labels[0]=docs[doc_id][label_offset-1] # Set leading punctuation class\n",
    "#         print([id2tag[t] if t>0 else '' for t in docs[doc_id][label_offset:label_offset+len(doc_offset)]])\n",
    "        arr_offset = np.array(doc_offset)\n",
    "        arr_mask = (arr_offset[:,0] == 0) & (arr_offset[:,1] != 0) # Gives the labels that should be assigned punctuation\n",
    "        doc_enc_labels[arr_mask] = docs[doc_id][label_offset:label_offset+sum(arr_mask)]\n",
    "        encoded_labels.append(doc_enc_labels)            \n",
    "        label_offset+=sum(arr_mask[:max_length-overlap-1])\n",
    "    return encoded_labels\n",
    "\n",
    "def process_dataset(dataset, split, max_length=128, overlap=63):\n",
    "    data=dataset[split].map(chunk_examples_with_degree(0), batched=True, batch_size=max_length,remove_columns=dataset[split].column_names)\n",
    "    encodings=tokenizer(data['texts'], is_split_into_words=True, return_offsets_mapping=True,\n",
    "              return_overflowing_tokens=True, padding=True, truncation=True, max_length=max_length, stride=overlap)\n",
    "    \n",
    "    labels=encode_tags(encodings, data['tags'], max_length, overlap)\n",
    "    encodings.pop(\"offset_mapping\")\n",
    "    encodings.pop(\"overflow_to_sample_mapping\")\n",
    "#     train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "#         dict(train_encodings),\n",
    "#         train_labels\n",
    "#     ))\n",
    "    return torch.utils.data.TensorDataset({'input_ids':torch.tensor(encodings['input_ids'],dtype=torch.long),\n",
    "'attention_mask':torch.tensor(encodings['attention_mask'],dtype=torch.long),\n",
    "'labels':torch.tensor(labels,dtype=torch.long)})\n",
    "#     return torch.utils.data.TensorDataset(torch.tensor(encodings['input_ids'],dtype=torch.long),\n",
    "#         torch.tensor(encodings['attention_mask'],dtype=torch.long),\n",
    "#         torch.tensor(labels,dtype=torch.long))\n",
    "test_dataset=process_dataset(ted,'test',10,3)\n",
    "# dev_dataset=process_dataset(ted,'dev')\n",
    "# train_dataset=process_dataset(ted,'train')\n",
    "\n",
    "# for name,dataset in {'test':test_dataset}.items():# 'train':train_dataset, 'dev':dev_dataset, \n",
    "#     torch.save(dataset, './ted-'+name+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([  101,  2627,  1303,  1110, 17136,  1118,  1297,  1223,  1103,   102]),\n",
       "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0, -100])),\n",
       " (tensor([  101,  1297,  1223,  1103,  2343, 19420,  1986,  1184,  1225,   102]),\n",
       "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  tensor([   0,    0,    0,    0,    7,    4,    2,    0,    0, -100])),\n",
       " (tensor([ 101, 1986, 1184, 1225, 1195, 1198, 1202, 2421,  112,  102]),\n",
       "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  tensor([   4,    2,    0,    0,    0,    0,    7,    0, -100, -100])),\n",
       " (tensor([  101,  1202,  2421,   112,   188,  4267, 11553,  5822,  1142,   102]),\n",
       "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  tensor([   0,    7,    0, -100, -100,    0, -100, -100,    0, -100]))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(iter(test_dataset))[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Disable all GPUS\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    visible_devices = tf.config.get_visible_devices()\n",
    "    for device in visible_devices:\n",
    "        assert device.device_type != 'GPU'\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/nxingyu/.cache/huggingface/datasets/csv/default-295ea44b803e5492/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2/cache-47a2b754535508b1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Set the offset as 1\n",
    "# Set the cls tag as offset-1 value\n",
    "# Set the non zeros with the offset to sum(nonzeros)\n",
    "# offset+= sum(nonzeros in first half)\n",
    "# stop when offset+ len(mapping)>len(tag)\n",
    "\n",
    "\n",
    "def dataset2tf(dataset,split):\n",
    "    data=dataset[split].map(chunk_examples_with_degree(0), batched=True, batch_size=10,remove_columns=dataset[split].column_names)\n",
    "    train_encodings=tokenizer(data['texts'], is_split_into_words=True, return_offsets_mapping=True, \n",
    "              return_overflowing_tokens=True, padding=True, truncation=True, max_length=32, stride=15,)\n",
    "\n",
    "    train_labels=encode_tags(train_encodings, data['tags'])\n",
    "    train_encodings.pop(\"offset_mapping\")\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dict(train_encodings),\n",
    "        train_labels\n",
    "    ))\n",
    "    return train_dataset\n",
    "test_dataset=dataset2tf(ted,'test')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TensorDataset' object has no attribute 'take'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b63c14ac9c87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TensorDataset' object has no attribute 'take'"
     ]
    }
   ],
   "source": [
    "for i in test_dataset.take(4):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=32, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings[146]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('was', ''),\n",
       " ('to', ''),\n",
       " ('focus', ''),\n",
       " ('on', ''),\n",
       " ('math', ''),\n",
       " ('and', ''),\n",
       " ('science', ','),\n",
       " ('to', ''),\n",
       " ('focus', ''),\n",
       " ('on', ''),\n",
       " ('basic', ''),\n",
       " ('research', '.'),\n",
       " ('And', ''),\n",
       " (\"that's\", ''),\n",
       " ('what', ''),\n",
       " (\"we've\", ''),\n",
       " ('done', '.'),\n",
       " ('Six', ''),\n",
       " ('years', ''),\n",
       " ('ago', ''),\n",
       " ('or', ''),\n",
       " ('so', ','),\n",
       " ('I', ''),\n",
       " ('left', ''),\n",
       " ('Renaissance', ''),\n",
       " ('and', ''),\n",
       " ('went', ''),\n",
       " ('to', ''),\n",
       " ('work', ''),\n",
       " ('at', ''),\n",
       " ('the', ''),\n",
       " ('foundation', '.'),\n",
       " ('So', ''),\n",
       " (\"that's\", ''),\n",
       " ('what', ''),\n",
       " ('we', ''),\n",
       " ('do', '.'),\n",
       " ('And', ''),\n",
       " ('so', ''),\n",
       " ('Math', ''),\n",
       " ('for', ''),\n",
       " ('America', ''),\n",
       " ('is', ''),\n",
       " ('basically', ''),\n",
       " ('investing', ''),\n",
       " ('in', ''),\n",
       " ('math', ''),\n",
       " ('teachers', ''),\n",
       " ('around', ''),\n",
       " ('the', ''),\n",
       " ('country', ','),\n",
       " ('giving', ''),\n",
       " ('them', ''),\n",
       " ('some', ''),\n",
       " ('extra', ''),\n",
       " ('income', ','),\n",
       " ('giving', ''),\n",
       " ('them', ''),\n",
       " ('support', ''),\n",
       " ('and', ''),\n",
       " ('coaching', '.'),\n",
       " ('And', ''),\n",
       " ('really', ''),\n",
       " ('trying', ''),\n",
       " ('to', ''),\n",
       " ('make', ''),\n",
       " ('that', ''),\n",
       " ('more', ''),\n",
       " ('effective', ''),\n",
       " ('and', ''),\n",
       " ('make', ''),\n",
       " ('that', ''),\n",
       " ('a', ''),\n",
       " ('calling', ''),\n",
       " ('to', ''),\n",
       " ('which', ''),\n",
       " ('teachers', ''),\n",
       " ('can', ''),\n",
       " ('aspire', '.'),\n",
       " ('Yeah', '—'),\n",
       " ('instead', ''),\n",
       " ('of', ''),\n",
       " ('beating', ''),\n",
       " ('up', ''),\n",
       " ('the', ''),\n",
       " ('bad', ''),\n",
       " ('teachers', ','),\n",
       " ('which', ''),\n",
       " ('has', ''),\n",
       " ('created', ''),\n",
       " ('morale', ''),\n",
       " ('problems', ''),\n",
       " ('all', ''),\n",
       " ('through', ''),\n",
       " ('the', ''),\n",
       " ('educational', ''),\n",
       " ('community', ','),\n",
       " ('in', ''),\n",
       " ('particular', ''),\n",
       " ('in', ''),\n",
       " ('math', ''),\n",
       " ('and', ''),\n",
       " ('science', ','),\n",
       " ('we', ''),\n",
       " ('focus', ''),\n",
       " ('on', ''),\n",
       " ('celebrating', ''),\n",
       " ('the', ''),\n",
       " ('good', ''),\n",
       " ('ones', ''),\n",
       " ('and', ''),\n",
       " ('giving', ''),\n",
       " ('them', ''),\n",
       " ('status', '.'),\n",
       " ('Yeah', ','),\n",
       " ('we', ''),\n",
       " ('give', ''),\n",
       " ('them', ''),\n",
       " ('extra', ''),\n",
       " ('money', ','),\n",
       " ('15', ','),\n",
       " ('000', ''),\n",
       " ('dollars', ''),\n",
       " ('a', ''),\n",
       " ('year', '.'),\n",
       " ('We', ''),\n",
       " ('have', ''),\n",
       " ('800', ''),\n",
       " ('math', ''),\n",
       " ('and', ''),\n",
       " ('science', ''),\n",
       " ('teachers', ''),\n",
       " ('in', ''),\n",
       " ('New', ''),\n",
       " ('York', ''),\n",
       " ('City', ''),\n",
       " ('in', ''),\n",
       " ('public', ''),\n",
       " ('schools', ''),\n",
       " ('today', ','),\n",
       " ('as', ''),\n",
       " ('part', ''),\n",
       " ('of', ''),\n",
       " ('a', ''),\n",
       " ('core', '.'),\n",
       " (\"There's\", ''),\n",
       " ('a', ''),\n",
       " ('great', ''),\n",
       " ('morale', ''),\n",
       " ('among', ''),\n",
       " ('them', '.'),\n",
       " (\"They're\", ''),\n",
       " ('staying', ''),\n",
       " ('in', ''),\n",
       " ('the', ''),\n",
       " ('field', '.'),\n",
       " ('Next', ''),\n",
       " ('year', ','),\n",
       " (\"it'll\", ''),\n",
       " ('be', ''),\n",
       " ('1', ','),\n",
       " ('000', ''),\n",
       " ('and', ''),\n",
       " (\"that'll\", ''),\n",
       " ('be', ''),\n",
       " ('10', ''),\n",
       " ('percent', ''),\n",
       " ('of', ''),\n",
       " ('the', ''),\n",
       " ('math', ''),\n",
       " ('and', ''),\n",
       " ('science', ''),\n",
       " ('teachers', ''),\n",
       " ('in', ''),\n",
       " ('New', ''),\n",
       " ('York', ''),\n",
       " ('City', ''),\n",
       " ('public', ''),\n",
       " ('schools', '.'),\n",
       " ('Jim', ','),\n",
       " (\"here's\", ''),\n",
       " ('another', ''),\n",
       " ('project', ''),\n",
       " ('that', ''),\n",
       " (\"you've\", ''),\n",
       " ('supported', ''),\n",
       " ('philanthropically', ':'),\n",
       " ('Research', ''),\n",
       " ('into', ''),\n",
       " ('origins', ''),\n",
       " ('of', ''),\n",
       " ('life', ','),\n",
       " ('I', ''),\n",
       " ('guess', '.'),\n",
       " ('What', ''),\n",
       " ('are', ''),\n",
       " ('we', ''),\n",
       " ('looking', ''),\n",
       " ('at', ''),\n",
       " ('here', '?'),\n",
       " ('Well', ','),\n",
       " (\"I'll\", ''),\n",
       " ('save', ''),\n",
       " ('that', ''),\n",
       " ('for', ''),\n",
       " ('a', ''),\n",
       " ('second', '.'),\n",
       " ('And', ''),\n",
       " ('then', ''),\n",
       " (\"I'll\", ''),\n",
       " ('tell', ''),\n",
       " ('you', ''),\n",
       " ('what', ''),\n",
       " (\"you're\", ''),\n",
       " ('looking', ''),\n",
       " ('at', '.'),\n",
       " ('Origins', ''),\n",
       " ('of', ''),\n",
       " ('life', ''),\n",
       " ('is', ''),\n",
       " ('a', ''),\n",
       " ('fascinating', ''),\n",
       " ('question', '.'),\n",
       " ('How', ''),\n",
       " ('did', ''),\n",
       " ('we', ''),\n",
       " ('get', ''),\n",
       " ('here', '?'),\n",
       " ('Well', ','),\n",
       " ('there', ''),\n",
       " ('are', ''),\n",
       " ('two', ''),\n",
       " ('questions', ':'),\n",
       " ('One', ''),\n",
       " ('is', ','),\n",
       " ('what', ''),\n",
       " ('is', ''),\n",
       " ('the', ''),\n",
       " ('route', ''),\n",
       " ('from', ''),\n",
       " ('geology', ''),\n",
       " ('to', ''),\n",
       " ('biology', '—'),\n",
       " ('how', ''),\n",
       " ('did', ''),\n",
       " ('we', ''),\n",
       " ('get', ''),\n",
       " ('here', '?'),\n",
       " ('And', ''),\n",
       " ('the', ''),\n",
       " ('other', ''),\n",
       " ('question', ''),\n",
       " ('is', ','),\n",
       " ('what', ''),\n",
       " ('did', ''),\n",
       " ('we', ''),\n",
       " ('start', ''),\n",
       " ('with', '?'),\n",
       " ('What', ''),\n",
       " ('material', ','),\n",
       " ('if', ''),\n",
       " ('any', ','),\n",
       " ('did', ''),\n",
       " ('we', ''),\n",
       " ('have', ''),\n",
       " ('to', ''),\n",
       " ('work', ''),\n",
       " ('with', ''),\n",
       " ('on', ''),\n",
       " ('this', ''),\n",
       " ('route', '?'),\n",
       " ('Those', ''),\n",
       " ('are', ''),\n",
       " ('two', ''),\n",
       " ('very', ','),\n",
       " ('very', ''),\n",
       " ('interesting', ''),\n",
       " ('questions', '.'),\n",
       " ('The', ''),\n",
       " ('first', ''),\n",
       " ('question', ''),\n",
       " ('is', ''),\n",
       " ('a', ''),\n",
       " ('tortuous', ''),\n",
       " ('path', ''),\n",
       " ('from', ''),\n",
       " ('geology', ''),\n",
       " ('up', ''),\n",
       " ('to', ''),\n",
       " ('RNA', ''),\n",
       " ('or', ''),\n",
       " ('something', ''),\n",
       " ('like', ''),\n",
       " ('that', '—'),\n",
       " ('how', ''),\n",
       " ('did', ''),\n",
       " ('that', ''),\n",
       " ('all', ''),\n",
       " ('work', '?'),\n",
       " ('And', ''),\n",
       " ('the', ''),\n",
       " ('other', ','),\n",
       " ('what', ''),\n",
       " ('do', ''),\n",
       " ('we', ''),\n",
       " ('have', ''),\n",
       " ('to', ''),\n",
       " ('work', ''),\n",
       " ('with', '?'),\n",
       " ('Well', ','),\n",
       " ('more', ''),\n",
       " ('than', ''),\n",
       " ('we', ''),\n",
       " ('think', '.'),\n",
       " ('So', ''),\n",
       " (\"what's\", ''),\n",
       " ('pictured', ''),\n",
       " ('there', ''),\n",
       " ('is', ''),\n",
       " ('a', ''),\n",
       " ('star', ''),\n",
       " ('in', ''),\n",
       " ('formation', '.'),\n",
       " ('Now', ','),\n",
       " ('every', ''),\n",
       " ('year', ''),\n",
       " ('in', ''),\n",
       " ('our', ''),\n",
       " ('Milky', ''),\n",
       " ('Way', ','),\n",
       " ('which', ''),\n",
       " ('has', ''),\n",
       " ('100', ''),\n",
       " ('billion', ''),\n",
       " ('stars', ','),\n",
       " ('about', ''),\n",
       " ('two', ''),\n",
       " ('new', ''),\n",
       " ('stars', ''),\n",
       " ('are', ''),\n",
       " ('created', '.'),\n",
       " (\"Don't\", ''),\n",
       " ('ask', ''),\n",
       " ('me', ''),\n",
       " ('how', ','),\n",
       " ('but', ''),\n",
       " (\"they're\", ''),\n",
       " ('created', '.'),\n",
       " ('And', ''),\n",
       " ('it', ''),\n",
       " ('takes', ''),\n",
       " ('them', ''),\n",
       " ('about', ''),\n",
       " ('a', ''),\n",
       " ('million', ''),\n",
       " ('years', ''),\n",
       " ('to', ''),\n",
       " ('settle', ''),\n",
       " ('out', '.'),\n",
       " ('So', ','),\n",
       " ('in', ''),\n",
       " ('steady', ''),\n",
       " ('state', ','),\n",
       " ('there', ''),\n",
       " ('are', ''),\n",
       " ('about', ''),\n",
       " ('two', ''),\n",
       " ('million', ''),\n",
       " ('stars', ''),\n",
       " ('in', ''),\n",
       " ('formation', ''),\n",
       " ('at', ''),\n",
       " ('any', ''),\n",
       " ('time', '.'),\n",
       " ('That', ''),\n",
       " ('one', ''),\n",
       " ('is', ''),\n",
       " ('somewhere', ''),\n",
       " ('along', ''),\n",
       " ('this', ''),\n",
       " ('settling', '-'),\n",
       " ('down', ''),\n",
       " ('period', '.'),\n",
       " ('And', ''),\n",
       " (\"there's\", ''),\n",
       " ('all', ''),\n",
       " ('this', ''),\n",
       " ('crap', ''),\n",
       " ('sort', ''),\n",
       " ('of', ''),\n",
       " ('circling', ''),\n",
       " ('around', ''),\n",
       " ('it', ','),\n",
       " ('dust', ''),\n",
       " ('and', ''),\n",
       " ('stuff', '.'),\n",
       " ('And', ''),\n",
       " (\"it'll\", ''),\n",
       " ('form', ''),\n",
       " ('probably', ''),\n",
       " ('a', ''),\n",
       " ('solar', ''),\n",
       " ('system', ','),\n",
       " ('or', ''),\n",
       " ('whatever', ''),\n",
       " ('it', ''),\n",
       " ('forms', '.'),\n",
       " ('But', ''),\n",
       " (\"here's\", ''),\n",
       " ('the', ''),\n",
       " ('thing', '—'),\n",
       " ('in', ''),\n",
       " ('this', ''),\n",
       " ('dust', ''),\n",
       " ('that', ''),\n",
       " ('surrounds', ''),\n",
       " ('a', ''),\n",
       " ('forming', ''),\n",
       " ('star', ''),\n",
       " ('have', ''),\n",
       " ('been', ''),\n",
       " ('found', ','),\n",
       " ('now', ','),\n",
       " ('significant', ''),\n",
       " ('organic', ''),\n",
       " ('molecules', '.'),\n",
       " ('Molecules', ''),\n",
       " ('not', ''),\n",
       " ('just', ''),\n",
       " ('like', ''),\n",
       " ('methane', ','),\n",
       " ('but', ''),\n",
       " ('formaldehyde', ''),\n",
       " ('and', ''),\n",
       " ('cyanide', '—'),\n",
       " ('things', ''),\n",
       " ('that', ''),\n",
       " ('are', ''),\n",
       " ('the', ''),\n",
       " ('building', ''),\n",
       " ('blocks', '—'),\n",
       " ('the', ''),\n",
       " ('seeds', ','),\n",
       " ('if', ''),\n",
       " ('you', ''),\n",
       " ('will', '—'),\n",
       " ('of', ''),\n",
       " ('life', '.'),\n",
       " ('So', ','),\n",
       " ('that', ''),\n",
       " ('may', ''),\n",
       " ('be', ''),\n",
       " ('typical', '.'),\n",
       " ('And', ''),\n",
       " ('it', ''),\n",
       " ('may', ''),\n",
       " ('be', ''),\n",
       " ('typical', ''),\n",
       " ('that', ''),\n",
       " ('planets', ''),\n",
       " ('around', ''),\n",
       " ('the', ''),\n",
       " ('universe', ''),\n",
       " ('start', ''),\n",
       " ('off', ''),\n",
       " ('with', ''),\n",
       " ('some', ''),\n",
       " ('of', ''),\n",
       " ('these', ''),\n",
       " ('basic', ''),\n",
       " ('building', ''),\n",
       " ('blocks', '.'),\n",
       " ('Now', ''),\n",
       " ('does', ''),\n",
       " ('that', ''),\n",
       " ('mean', ''),\n",
       " (\"there's\", ''),\n",
       " ('going', ''),\n",
       " ('to', ''),\n",
       " ('be', ''),\n",
       " ('life', ''),\n",
       " ('all', ''),\n",
       " ('around', '?'),\n",
       " ('Maybe', '.'),\n",
       " ('But', ''),\n",
       " (\"it's\", ''),\n",
       " ('a', ''),\n",
       " ('question', ''),\n",
       " ('of', ''),\n",
       " ('how', ''),\n",
       " ('tortuous', ''),\n",
       " ('this', ''),\n",
       " ('path', ''),\n",
       " ('is', ''),\n",
       " ('from', ''),\n",
       " ('those', ''),\n",
       " ('frail', ''),\n",
       " ('beginnings', ','),\n",
       " ('those', ''),\n",
       " ('seeds', ','),\n",
       " ('all', ''),\n",
       " ('the', ''),\n",
       " ('way', ''),\n",
       " ('to', ''),\n",
       " ('life', '.'),\n",
       " ('And', ''),\n",
       " ('most', ''),\n",
       " ('of', ''),\n",
       " ('those', ''),\n",
       " ('seeds', ''),\n",
       " ('will', ''),\n",
       " ('fall', ''),\n",
       " ('on', ''),\n",
       " ('fallow', ''),\n",
       " ('planets', '.'),\n",
       " ('So', ''),\n",
       " ('for', ''),\n",
       " ('you', ','),\n",
       " ('personally', ','),\n",
       " ('finding', ''),\n",
       " ('an', ''),\n",
       " ('answer', ''),\n",
       " ('to', ''),\n",
       " ('this', ''),\n",
       " ('question', ''),\n",
       " ('of', ''),\n",
       " ('where', ''),\n",
       " ('we', ''),\n",
       " ('came', ''),\n",
       " ('from', ','),\n",
       " ('of', ''),\n",
       " ('how', ''),\n",
       " ('did', ''),\n",
       " ('this', ''),\n",
       " ('thing', ''),\n",
       " ('happen', ','),\n",
       " ('that', ''),\n",
       " ('is', ''),\n",
       " ('something', ''),\n",
       " ('you', ''),\n",
       " ('would', ''),\n",
       " ('love', ''),\n",
       " ('to', ''),\n",
       " ('see', '.'),\n",
       " ('Would', ''),\n",
       " ('love', ''),\n",
       " ('to', ''),\n",
       " ('see', '.'),\n",
       " ('And', ''),\n",
       " ('like', ''),\n",
       " ('to', ''),\n",
       " ('know', '—'),\n",
       " ('if', ''),\n",
       " ('that', ''),\n",
       " ('path', ''),\n",
       " ('is', ''),\n",
       " ('tortuous', ''),\n",
       " ('enough', ','),\n",
       " ('and', ''),\n",
       " ('so', ''),\n",
       " ('improbable', ','),\n",
       " ('that', ''),\n",
       " ('no', ''),\n",
       " ('matter', ''),\n",
       " ('what', ''),\n",
       " ('you', ''),\n",
       " ('start', ''),\n",
       " ('with', ','),\n",
       " ('we', ''),\n",
       " ('could', ''),\n",
       " ('be', ''),\n",
       " ('a', ''),\n",
       " ('singularity', '.'),\n",
       " ('But', ''),\n",
       " ('on', ''),\n",
       " ('the', ''),\n",
       " ('other', ''),\n",
       " ('hand', ','),\n",
       " ('given', ''),\n",
       " ('all', ''),\n",
       " ('this', ''),\n",
       " ('organic', ''),\n",
       " ('dust', ''),\n",
       " (\"that's\", ''),\n",
       " ('floating', ''),\n",
       " ('around', ','),\n",
       " ('we', ''),\n",
       " ('could', ''),\n",
       " ('have', ''),\n",
       " ('lots', ''),\n",
       " ('of', ''),\n",
       " ('friends', ''),\n",
       " ('out', ''),\n",
       " ('there', '.'),\n",
       " (\"It'd\", ''),\n",
       " ('be', ''),\n",
       " ('great', ''),\n",
       " ('to', ''),\n",
       " ('know', '.'),\n",
       " ('Jim', ','),\n",
       " ('a', ''),\n",
       " ('couple', ''),\n",
       " ('of', ''),\n",
       " ('years', ''),\n",
       " ('ago', ','),\n",
       " ('I', ''),\n",
       " ('got', ''),\n",
       " ('the', ''),\n",
       " ('chance', ''),\n",
       " ('to', ''),\n",
       " ('speak', ''),\n",
       " ('with', ''),\n",
       " ('Elon', ''),\n",
       " ('Musk', ','),\n",
       " ('and', ''),\n",
       " ('I', ''),\n",
       " ('asked', ''),\n",
       " ('him', ''),\n",
       " ('the', ''),\n",
       " ('secret', ''),\n",
       " ('of', ''),\n",
       " ('his', ''),\n",
       " ('success', ','),\n",
       " ('and', ''),\n",
       " ('he', ''),\n",
       " ('said', ''),\n",
       " ('taking', ''),\n",
       " ('physics', ''),\n",
       " ('seriously', ''),\n",
       " ('was', ''),\n",
       " ('it', '.'),\n",
       " ('Listening', ''),\n",
       " ('to', ''),\n",
       " ('you', ','),\n",
       " ('what', ''),\n",
       " ('I', ''),\n",
       " ('hear', ''),\n",
       " ('you', ''),\n",
       " ('saying', ''),\n",
       " ('is', ''),\n",
       " ('taking', ''),\n",
       " ('math', ''),\n",
       " ('seriously', ','),\n",
       " ('that', ''),\n",
       " ('has', ''),\n",
       " ('infused', ''),\n",
       " ('your', ''),\n",
       " ('whole', ''),\n",
       " ('life', '.'),\n",
       " (\"It's\", ''),\n",
       " ('made', ''),\n",
       " ('you', ''),\n",
       " ('an', ''),\n",
       " ('absolute', ''),\n",
       " ('fortune', ','),\n",
       " ('and', ''),\n",
       " ('now', ''),\n",
       " (\"it's\", ''),\n",
       " ('allowing', ''),\n",
       " ('you', ''),\n",
       " ('to', ''),\n",
       " ('invest', ''),\n",
       " ('in', ''),\n",
       " ('the', ''),\n",
       " ('futures', ''),\n",
       " ('of', ''),\n",
       " ('thousands', ''),\n",
       " ('and', ''),\n",
       " ('thousands', ''),\n",
       " ('of', ''),\n",
       " ('kids', ''),\n",
       " ('across', ''),\n",
       " ('America', ''),\n",
       " ('and', ''),\n",
       " ('elsewhere', '.'),\n",
       " ('Could', ''),\n",
       " ('it', ''),\n",
       " ('be', ''),\n",
       " ('that', ''),\n",
       " ('science', ''),\n",
       " ('actually', ''),\n",
       " ('works', '?'),\n",
       " ('That', ''),\n",
       " ('math', ''),\n",
       " ('actually', ''),\n",
       " ('works', '?'),\n",
       " ('Well', ','),\n",
       " ('math', ''),\n",
       " ('certainly', ''),\n",
       " ('works', '.'),\n",
       " ('Math', ''),\n",
       " ('certainly', ''),\n",
       " ('works', '.'),\n",
       " ('But', ''),\n",
       " ('this', ''),\n",
       " ('has', ''),\n",
       " ('been', ''),\n",
       " ('fun', '.'),\n",
       " ('Working', ''),\n",
       " ('with', ''),\n",
       " ('Marilyn', ''),\n",
       " ('and', ''),\n",
       " ('giving', ''),\n",
       " ('it', ''),\n",
       " ('away', ''),\n",
       " ('has', ''),\n",
       " ('been', ''),\n",
       " ('very', ''),\n",
       " ('enjoyable', '.'),\n",
       " ('I', ''),\n",
       " ('just', ''),\n",
       " ('find', ''),\n",
       " ('it', '—'),\n",
       " (\"it's\", ''),\n",
       " ('an', ''),\n",
       " ('inspirational', ''),\n",
       " ('thought', ''),\n",
       " ('to', ''),\n",
       " ('me', ','),\n",
       " ('that', ''),\n",
       " ('by', ''),\n",
       " ('taking', ''),\n",
       " ('knowledge', ''),\n",
       " ('seriously', ','),\n",
       " ('so', ''),\n",
       " ('much', ''),\n",
       " ('more', ''),\n",
       " ('can', ''),\n",
       " ('come', ''),\n",
       " ('from', ''),\n",
       " ('it', '.'),\n",
       " ('So', ''),\n",
       " ('thank', ''),\n",
       " ('you', ''),\n",
       " ('for', ''),\n",
       " ('your', ''),\n",
       " ('amazing', ''),\n",
       " ('life', ','),\n",
       " ('and', ''),\n",
       " ('for', ''),\n",
       " ('coming', ''),\n",
       " ('here', ''),\n",
       " ('to', ''),\n",
       " ('TED', '.'),\n",
       " ('Thank', ''),\n",
       " ('you', '.'),\n",
       " ('Jim', ''),\n",
       " ('Simons', '!')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [len(data[i]['tags']) for i in range(144,150)]\n",
    "list(zip(data[145]['texts'][2700:],[id2tag[t] if t>0 else '' for t in data[145]['tags'][2701:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunked_ted_val=ted['val'].select([0,1]).map(chunk_examples_with_degree(0), batched=True, batch_size=10,remove_columns=ted['train'].column_names)\n",
    "# encodings=tokenizer(chunked_ted['texts'], is_split_into_words=True, return_offsets_mapping=True, \n",
    "#           return_overflowing_tokens=True, padding=True, truncation=True, max_length=32, stride=15,)\n",
    "# labels=encode_tags(encodings, chunked_ted['tags'])\n",
    "# encodings.pop(\"offset_mapping\")\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((\n",
    "#     dict(encodings),\n",
    "#     labels\n",
    "# ))\n",
    "# val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForTokenClassification: ['activation_13', 'vocab_projector', 'vocab_layer_norm', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier', 'dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFDistilBertForTokenClassification\n",
    "model = TFDistilBertForTokenClassification.from_pretrained('distilbert-base-cased', num_labels=len(tags)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_token_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  65190912  \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  7690      \n",
      "=================================================================\n",
      "Total params: 65,198,602\n",
      "Trainable params: 65,198,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "3310/3310 [==============================] - ETA: 0s - loss: 0.1861"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-42f8ecd6cdb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# can also use any keras loss fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1121\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[0;32m-> 1123\u001b[0;31m           val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1124\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m               \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1377\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TraceContext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1380\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss) # can also use any keras loss fn\n",
    "model.fit(test_dataset.batch(16), epochs=3, batch_size=16, validation_data=test_dataset,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output=model.predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 2, 0, 0],\n",
       "       [5, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 5, 0, ..., 0, 0, 2],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 5, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 5, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_output['logits'].argmax(axis=2).reshape(32,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1448, 2247, 4427, 1107, 1381, 5227, 2021, 15474, 8449, 1105, 8703, 170, 1299, 1150, 102], [101, 2021, 15474, 8449, 1105, 8703, 170, 1299, 1150, 1691, 10108, 102, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]], 'offset_mapping': [[(0, 0), (0, 3), (0, 6), (0, 9), (0, 2), (0, 4), (0, 2), (0, 6), (0, 8), (0, 10), (0, 3), (0, 8), (0, 1), (0, 3), (0, 3), (0, 0)], [(0, 0), (0, 6), (0, 8), (0, 10), (0, 3), (0, 8), (0, 1), (0, 3), (0, 3), (0, 8), (0, 10), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]], 'overflow_to_sample_mapping': [0, 0]}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(chunked_ted['texts'][0], is_split_into_words=True, return_offsets_mapping=True, return_overflowing_tokens=True, padding=True, truncation=True, stride=8, max_length=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "encodings.pop(\"offset_mapping\")\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(encodings),\n",
    "    labels\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(25,), dtype=int32, numpy=\n",
      "array([  101,  1448,  2247,  4427,  1107,  1381,  5227,  2021, 15474,\n",
      "        8449,  1105,  8703,   170,  1299,  1150,  1691,   102,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(25,), dtype=int32, numpy=\n",
      "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(25,), dtype=int32, numpy=\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0], dtype=int32)>}, <tf.Tensor: shape=(25,), dtype=int32, numpy=\n",
      "array([-100,    0,    0,    0,    0,    2,    0,    0,    2,    0,    0,\n",
      "          0,    0,    0,    0,    0, -100, -100, -100, -100, -100, -100,\n",
      "       -100, -100, -100], dtype=int32)>)\n"
     ]
    }
   ],
   "source": [
    "for data in dataset.take(1):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 20062, 117, 1166, 122, 1553, 126, 3775, 1234, 2541, 4223, 4139, 119, 1130, 2593, 117, 1234, 1132, 2257, 1106, 10556, 1147, 1583, 117, 2128, 1166, 1405, 1550, 8940, 119, 4288, 117, 1443, 170, 4095, 117, 1132, 1103, 1211, 7386, 1105, 8018, 5256, 795, 1133, 1136, 1198, 1121, 1103, 5119, 2952, 18552, 117, 1133, 1121, 1103, 1510, 8362, 20080, 27443, 3154, 1115, 8755, 1138, 1113, 1147, 2073, 119, 1109, 5758, 1104, 1594, 1817, 1482, 1120, 170, 1842, 1344, 3187, 1111, 1103, 1718, 1104, 6438, 1105, 18560, 2645, 119, 4288, 117, 1112, 1195, 1169, 1178, 5403, 117, 1209, 1631, 4472, 117, 4963, 1105, 1120, 3187, 119, 1252, 1175, 1110, 1363, 2371, 119, 1109, 3068, 1104, 1920, 1115, 1482, 3531, 1107, 1147, 2073, 1169, 1138, 170, 1167, 2418, 2629, 1113, 1147, 1218, 118, 1217, 1190, 1121, 1103, 4315, 5758, 1104, 1594, 1115, 1152, 1138, 1151, 5490, 1106, 119, 1573, 2140, 117, 1482, 1169, 1129, 4921, 1118, 3258, 117, 5343, 6486, 1158, 1219, 1105, 1170, 4139, 119, 1130, 1349, 117, 146, 1108, 170, 1148, 118, 1214, 7735, 2377, 1107, 1103, 1239, 1104, 4280, 1323, 1104, 24797, 4052, 119, 2409, 1242, 1104, 1128, 1303, 117, 146, 2542, 1103, 5532, 1107, 7303, 8362, 10787, 1107, 1524, 1104, 1143, 1113, 1103, 1794, 119, 1422, 1266, 1110, 2034, 1121, 7303, 117, 1105, 1304, 1346, 1113, 117, 146, 1575, 1317, 1266, 1484, 1107, 1541, 16358, 14791, 16877, 3242, 119, 146, 112, 173, 3465, 1105, 146, 112, 173, 8422, 1114, 1139, 1266, 1105, 2824, 1103, 1794, 119, 1284, 112, 1396, 1155, 1562, 1343, 4429, 131, 10095, 9769, 2275, 117, 10676, 117, 5915, 1105, 1234, 7406, 1105, 1919, 119, 1135, 1108, 1579, 1103, 1234, 7406, 1105, 1919, 1115, 1541, 1400, 1143, 1103, 1211, 117, 2108, 1343, 10444, 118, 1702, 1482, 119, 146, 1108, 170, 1534, 1106, 1160, 1685, 117, 3417, 1107, 27110, 8588, 1482, 119, 1220, 1127, 1421, 1105, 1565, 1173, 117, 1120, 1126, 1425, 1187, 1152, 3417, 1455, 7424, 1105, 7424, 1104, 3243, 117, 1105, 2637, 1842, 117, 13870, 6615, 119, 1573, 117, 146, 1310, 1106, 4608, 1184, 1122, 1547, 1129, 1176, 1106, 6486, 1139, 1482, 1107, 170, 1594, 4834, 1105, 170, 15820, 3227, 119, 5718, 1139, 1482, 1849, 136, 5718, 1139, 1797, 112, 188, 3999, 117, 2816, 1257, 3857, 1147, 18978, 136, 5718, 1139, 1488, 112, 188, 1541, 8000, 1105, 1920, 26743, 2731, 1561, 22984, 1105, 9512, 136, 1731, 1156, 146, 16743, 136, 5718, 146, 1849, 136, 1249, 16979, 1116, 1105, 6486, 26657, 117, 1195, 1221, 1115, 1981, 1158, 2153, 1114, 4196, 1107, 12605, 1111, 1147, 1482, 1169, 1138, 170, 3321, 2629, 1113, 1147, 1218, 118, 1217, 117, 1105, 1195, 1840, 1142, 6486, 2013, 119, 1109, 2304, 146, 1125, 1108, 117, 1180, 6486, 2013, 2648, 1129, 5616, 1111, 2073, 1229, 1152, 1127, 1253, 1107, 1594, 10490, 1137, 15820, 7869, 136, 7426, 1195, 2519, 1172, 1114, 5566, 1137, 2013, 1115, 1156, 1494, 1172, 1194, 1292, 11998, 136, 1573, 146, 4685, 1139, 7735, 16014, 117, 2986, 4858, 11917, 2312, 117, 1114, 1103, 1911, 1104, 1606, 1139, 3397, 4196, 1106, 1294, 1199, 1849, 1107, 1103, 1842, 1362, 119, 146, 1445, 112, 189, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 9), (9, 10), (11, 15), (16, 17), (18, 23), (24, 25), (26, 33), (34, 40), (41, 51), (52, 57), (58, 66), (66, 67), (68, 70), (71, 79), (79, 80), (81, 87), (88, 91), (92, 98), (99, 101), (102, 106), (107, 112), (113, 120), (120, 121), (122, 129), (130, 134), (135, 137), (138, 145), (146, 154), (154, 155), (156, 164), (164, 165), (166, 173), (174, 175), (176, 181), (181, 182), (183, 186), (187, 190), (191, 195), (196, 204), (205, 208), (209, 219), (220, 227), (228, 229), (230, 233), (234, 237), (238, 242), (243, 247), (248, 251), (252, 259), (260, 268), (269, 276), (276, 277), (278, 281), (282, 286), (287, 290), (291, 296), (297, 299), (299, 301), (301, 305), (306, 313), (314, 318), (319, 323), (324, 328), (329, 331), (332, 337), (338, 346), (346, 347), (348, 351), (352, 363), (364, 366), (367, 370), (371, 376), (377, 385), (386, 388), (389, 390), (391, 395), (396, 400), (401, 405), (406, 409), (410, 413), (414, 425), (426, 428), (429, 438), (439, 442), (443, 453), (454, 462), (462, 463), (464, 472), (472, 473), (474, 476), (477, 479), (480, 483), (484, 488), (489, 496), (496, 497), (498, 502), (503, 507), (508, 515), (515, 516), (517, 527), (528, 531), (532, 534), (535, 539), (539, 540), (541, 544), (545, 550), (551, 553), (554, 558), (559, 563), (563, 564), (565, 568), (569, 576), (577, 579), (580, 584), (585, 589), (590, 598), (599, 606), (607, 609), (610, 615), (616, 624), (625, 628), (629, 633), (634, 635), (636, 640), (641, 652), (653, 659), (660, 662), (663, 668), (669, 673), (673, 674), (674, 679), (680, 684), (685, 689), (690, 693), (694, 700), (701, 712), (713, 715), (716, 719), (720, 724), (725, 729), (730, 734), (735, 739), (740, 747), (748, 750), (750, 751), (752, 754), (755, 763), (763, 764), (765, 773), (774, 777), (778, 780), (781, 790), (791, 793), (794, 798), (798, 799), (800, 806), (807, 813), (813, 816), (817, 823), (824, 827), (828, 833), (834, 842), (842, 843), (844, 846), (847, 851), (851, 852), (853, 854), (855, 858), (859, 860), (861, 866), (866, 867), (867, 871), (872, 875), (876, 883), (884, 886), (887, 890), (891, 901), (902, 904), (905, 915), (916, 922), (923, 925), (926, 939), (940, 948), (948, 949), (950, 954), (955, 959), (960, 962), (963, 966), (967, 971), (971, 972), (973, 974), (975, 982), (983, 986), (987, 993), (994, 996), (997, 1002), (1003, 1005), (1005, 1009), (1010, 1012), (1013, 1018), (1019, 1021), (1022, 1024), (1025, 1027), (1028, 1031), (1032, 1034), (1034, 1035), (1036, 1038), (1039, 1045), (1046, 1048), (1049, 1059), (1060, 1064), (1065, 1070), (1070, 1071), (1072, 1075), (1076, 1080), (1081, 1086), (1087, 1089), (1089, 1090), (1091, 1092), (1093, 1097), (1098, 1105), (1106, 1112), (1113, 1120), (1121, 1123), (1124, 1130), (1131, 1133), (1133, 1136), (1136, 1141), (1142, 1146), (1146, 1147), (1148, 1149), (1149, 1150), (1150, 1151), (1152, 1155), (1156, 1159), (1160, 1161), (1161, 1162), (1162, 1163), (1164, 1170), (1171, 1175), (1176, 1178), (1179, 1185), (1186, 1189), (1190, 1195), (1196, 1199), (1200, 1202), (1202, 1203), (1204, 1206), (1206, 1207), (1207, 1209), (1210, 1213), (1214, 1218), (1219, 1224), (1225, 1231), (1231, 1232), (1233, 1238), (1239, 1249), (1250, 1259), (1259, 1260), (1261, 1266), (1266, 1267), (1268, 1279), (1280, 1283), (1284, 1290), (1291, 1300), (1301, 1304), (1305, 1312), (1312, 1313), (1314, 1316), (1317, 1320), (1321, 1327), (1328, 1331), (1332, 1338), (1339, 1348), (1349, 1352), (1353, 1360), (1361, 1365), (1366, 1372), (1373, 1376), (1377, 1379), (1380, 1383), (1384, 1388), (1388, 1389), (1390, 1400), (1401, 1406), (1407, 1416), (1416, 1417), (1417, 1424), (1425, 1433), (1433, 1434), (1435, 1436), (1437, 1440), (1441, 1442), (1443, 1449), (1450, 1452), (1453, 1456), (1457, 1462), (1462, 1463), (1464, 1473), (1474, 1476), (1476, 1480), (1480, 1485), (1486, 1494), (1494, 1495), (1496, 1500), (1501, 1505), (1506, 1510), (1511, 1514), (1515, 1518), (1519, 1523), (1523, 1524), (1525, 1527), (1528, 1530), (1531, 1534), (1535, 1540), (1541, 1545), (1546, 1555), (1556, 1561), (1562, 1566), (1567, 1570), (1571, 1575), (1576, 1578), (1579, 1588), (1588, 1589), (1590, 1593), (1594, 1602), (1603, 1607), (1607, 1608), (1609, 1619), (1620, 1627), (1627, 1628), (1629, 1631), (1631, 1632), (1633, 1634), (1635, 1640), (1641, 1643), (1644, 1650), (1651, 1655), (1656, 1658), (1659, 1664), (1665, 1667), (1668, 1672), (1673, 1675), (1676, 1682), (1683, 1685), (1686, 1694), (1695, 1697), (1698, 1699), (1700, 1703), (1704, 1708), (1709, 1712), (1713, 1714), (1715, 1722), (1723, 1727), (1727, 1728), (1729, 1734), (1735, 1737), (1738, 1746), (1747, 1753), (1753, 1754), (1755, 1760), (1761, 1763), (1764, 1772), (1772, 1773), (1773, 1774), (1775, 1781), (1781, 1782), (1783, 1788), (1789, 1793), (1794, 1798), (1799, 1804), (1805, 1810), (1810, 1811), (1812, 1817), (1818, 1820), (1821, 1824), (1824, 1825), (1825, 1826), (1827, 1833), (1834, 1841), (1842, 1845), (1846, 1850), (1850, 1854), (1855, 1861), (1862, 1868), (1869, 1876), (1877, 1880), (1881, 1890), (1890, 1891), (1892, 1895), (1896, 1901), (1902, 1903), (1904, 1908), (1908, 1909), (1910, 1915), (1916, 1917), (1918, 1924), (1924, 1925), (1926, 1928), (1929, 1941), (1941, 1942), (1943, 1946), (1947, 1953), (1954, 1962), (1962, 1963), (1964, 1966), (1967, 1971), (1972, 1976), (1977, 1980), (1980, 1983), (1984, 1991), (1992, 1996), (1997, 2003), (2004, 2006), (2007, 2013), (2014, 2017), (2018, 2023), (2024, 2032), (2033, 2036), (2037, 2041), (2042, 2043), (2044, 2048), (2049, 2055), (2056, 2058), (2059, 2064), (2065, 2069), (2069, 2070), (2070, 2075), (2075, 2076), (2077, 2080), (2081, 2083), (2084, 2088), (2089, 2093), (2094, 2100), (2101, 2109), (2109, 2110), (2111, 2114), (2115, 2123), (2124, 2125), (2126, 2129), (2130, 2133), (2133, 2134), (2135, 2140), (2141, 2147), (2148, 2156), (2157, 2165), (2166, 2168), (2169, 2175), (2176, 2179), (2180, 2188), (2189, 2194), (2195, 2199), (2200, 2204), (2205, 2210), (2211, 2213), (2214, 2217), (2218, 2223), (2224, 2226), (2227, 2234), (2235, 2240), (2240, 2241), (2242, 2247), (2248, 2250), (2251, 2256), (2257, 2261), (2262, 2266), (2267, 2273), (2274, 2276), (2277, 2285), (2286, 2290), (2291, 2296), (2297, 2301), (2302, 2306), (2307, 2314), (2315, 2320), (2321, 2330), (2330, 2331), (2332, 2334), (2335, 2336), (2337, 2347), (2348, 2350), (2351, 2354), (2355, 2365), (2365, 2366), (2367, 2376), (2377, 2383), (2384, 2387), (2387, 2389), (2389, 2390), (2391, 2395), (2396, 2399), (2400, 2404), (2405, 2407), (2408, 2413), (2414, 2416), (2417, 2425), (2426, 2432), (2433, 2435), (2436, 2440), (2441, 2445), (2446, 2452), (2453, 2455), (2456, 2459), (2460, 2464), (2465, 2470), (2470, 2471), (2472, 2473), (2474, 2478), (2478, 2479), (2479, 2480), (0, 0)]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(ted['train'][0]['transcript'], return_offsets_mapping=True, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_pretraining_data.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import random\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "import tensorflow as tf\n",
    "\n",
    "flags = tf.compat.v1.flags\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow.python.platform.flags' from '/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/tensorflow/python/platform/flags.py'>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TrainingInstance(object):\n",
    "    def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels, is_random_next):\n",
    "        self.tokens = tokens\n",
    "        self.segment_ids = segment_ids\n",
    "        self.is_random_next = is_random_next\n",
    "        self.masked_lm_positions = masked_lm_positions\n",
    "        self.masked_lm_labels = masked_lm_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-21 11:43:00,756 [INFO] b'fcfb139\\n'\n",
      "2020-12-21 11:43:00,761 [WARNING] 2020-12-21 11:43\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "root_logger= logging.getLogger()\n",
    "root_logger.setLevel(logging.DEBUG)\n",
    "handler = logging.FileHandler('test.log', 'w', 'utf-8')\n",
    "handler.setFormatter(logging.Formatter(\"%(asctime)s [%(levelname)s] %(message)s\"))\n",
    "root_logger.addHandler(handler)\n",
    "\n",
    "import subprocess\n",
    "root_logger.info(subprocess.check_output(['git', 'describe', '--always']))\n",
    "import datetime\n",
    "root_logger.warning(datetime.datetime.now().strftime('%Y-%m-%d %H:%M'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
