{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "eval.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JILqsDFCkIcM",
        "outputId": "d51aa9cf-cfdb-4844-9e63-56b9c738b7ff"
      },
      "source": [
        "#%%Forcolabonly\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')\r\n",
        "#%%"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YlGB1lokE2T"
      },
      "source": [
        "# !pip install datasets\n",
        "# !pip install transformers\n",
        "# !pip install torchcontrib\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import BoolTensor, FloatTensor, LongTensor\n",
        "from typing import Optional\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import DistilBertTokenizerFast, BertPreTrainedModel, get_linear_schedule_with_warmup, AdamW\n",
        "from torchcontrib.optim import SWA\n",
        "import regex as re\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_PYFo1TkE2h"
      },
      "source": [
        "tags=sorted(list('.?!,;:-—…'))\n",
        "tag2id = {tag: id+1 for id, tag in enumerate(tags)}\n",
        "tag2id[' ']=0\n",
        "tag2id['']=-100\n",
        "id2tag = {id: tag for tag, id in tag2id.items()}\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "class PunctuationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, input_ids:LongTensor, attention_mask:FloatTensor, labels:Optional[LongTensor] = None) -> None:\n",
        "        \"\"\"\n",
        "        :param input_ids: tokenids\n",
        "        :param attention_mask: attention_mask, null->0\n",
        "        :param labels: true labels, optional\n",
        "        :return None\n",
        "        \"\"\"\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\":param idx: implement index\"\"\"\n",
        "        return {'input_ids': torch.as_tensor(self.input_ids[idx],dtype=torch.long),\n",
        "                'attention_mask': torch.as_tensor(self.attention_mask[idx],dtype=torch.float32),\n",
        "                'labels': torch.as_tensor(self.labels[idx],dtype=torch.long)}\n",
        "\n",
        "    def view(self,idx:int)->str:\n",
        "        \"\"\":param idx(int): returns readable format of single input_ids and labels in the form of readable text\"\"\"\n",
        "        return ' '.join([''.join(x) for x in list(zip(tokenizer.convert_ids_to_tokens(self.input_ids[idx]),[id2tag[x] for x in self.labels[idx].tolist()]))])\n",
        "\n",
        "    def __len__(self)->int:\n",
        "        return len(self.labels)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVAhMIG5pMd3"
      },
      "source": [
        "class config:\r\n",
        "  def __init__(self):\r\n",
        "    self.max_len=128\r\n",
        "    self.overlap = 126\r\n",
        "    self.train_batch_size = 4\r\n",
        "    self.dev_batch_size = 4\r\n",
        "    self.gpu_device = 'cpu' # 'cuda:0' #\r\n",
        "    self.freeze_epochs = 20\r\n",
        "    self.freeze_lr = 1e-4\r\n",
        "    self.unfreeze_epochs = 20\r\n",
        "    self.unfreeze_layers = 6\r\n",
        "    self.unfreeze_lr = 1e-5\r\n",
        "    self.base_model_path = 'distilbert-base-uncased'\r\n",
        "    self.train_dataset = '/content/gdrive/MyDrive/ASR/ted_talks_processed.train.pt'\r\n",
        "    self.dev_dataset = '/content/gdrive/MyDrive/ASR/ted_talks_processed.dev.pt'\r\n",
        "    self.alpha = 0.8\r\n",
        "    self.hidden_dropout_prob = 0.3\r\n",
        "    self.embedding_dim = 768\r\n",
        "    self.num_labels = 10\r\n",
        "    self.hidden_dim = 128\r\n",
        "    self.self_adjusting = True\r\n",
        "    self.square_denominator = False\r\n",
        "    self.use_crf = False\r\n",
        "    self.model_name = 'bertcrf'\r\n",
        "    self.model_path = \"/content/gdrive/MyDrive/ASR/logs/models/\"\r\n",
        "config = transformers.configuration_utils.PretrainedConfig.from_dict(config().__dict__)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "-NtAYQ5Vqsjg",
        "outputId": "c1c6ade6-f298-427d-9c39-fbf34c77edfd"
      },
      "source": [
        "device = torch.device(config.gpu_device) if torch.cuda.is_available() else torch.device('cpu')\r\n",
        "train_dataset=PunctuationDataset(**torch.load(config.train_dataset,map_location=device)[:])\r\n",
        "# dev_dataset=PunctuationDataset(**torch.load(config.dev_dataset,map_location=device)[:])\r\n",
        "train_dataset.view(-1000)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"[CLS]  turns  activism  into  terrorism  if  it  causes  a  loss  of  profits. now  most  people  never  even  heard  about  this  law, including  members  of  congress. less  than  one  percent  were  in  the  room  when  it  passed  the  house. the  rest  were  outside  at  a  new  memorial. they  were  praising  dr. king  as  his  style  of  activism  was  branded  as  terrorism  if  done  in  the  name  of  animals  or  the  environment. supporters  say  laws  like  this  are  needed  for  the  ex  ##tre  ##mist  ##s: the  van  ##dal  ##s, the  arson  ##ists, the  radicals. but  right  now, companies  like  trans  ##cana  ##da  are  briefing  police  in  presentations  like  this  one  about  how  to  prose  ##cute  non  ##vio  ##lent  protesters  as  terrorists. the  fbi  '  s  training  documents  on  eco- [SEP] \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGbVdBw4rl5X",
        "outputId": "40c3ad68-017f-463b-d505-3d1c55244448"
      },
      "source": [
        "train_dataloader=torch.utils.data.DataLoader(train_dataset, batch_size=config.train_batch_size, num_workers=4)\r\n",
        "# dev_dataloader=torch.utils.data.DataLoader(dev_dataset, batch_size=config.dev_batch_size, num_workers=2)\r\n",
        "{x:y.shape for x,y in next(iter(train_dataloader)).items()} #(batch_size, seq_len)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_mask': torch.Size([4, 128]),\n",
              " 'input_ids': torch.Size([4, 128]),\n",
              " 'labels': torch.Size([4, 128])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pShkHDqTroBX",
        "outputId": "c362d2d6-133b-4e92-ed60-577c99bb5478"
      },
      "source": [
        "#b_s = 4, s_l = 128, h_d = 768\r\n",
        "# bert=transformers.BertModel.from_pretrained(config.base_model_path)\r\n",
        "# input_ids, attention_mask, labels = next(iter(train_dataloader)).values() # (batch_size, seq_len) * 3\r\n",
        "# bo=bert(input_ids, attention_mask) # => last_hidden_state (b_s, s_l, h_d 768), pooler_output (b_s, h_d)\r\n",
        "# dropout = nn.Dropout(config.hidden_dropout_prob) # (b_s, s_l, h_d)\r\n",
        "# sequence_output=dropout(bo[0])\r\n",
        "# fcl = nn.Linear(config.embedding_dim, config.num_labels)\r\n",
        "# fcl_output=fcl(sequence_output) #(b_s, s_l, h_d) -> (b_s, s_l, num_labels)\r\n",
        "# dice_loss=DiceLoss(self_adjusting=True,alpha=1)(fcl_output,labels)\r\n",
        "# bert(next)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9624, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJXqCtg-Eclv",
        "outputId": "db4dd1b9-d7e5-427c-a274-0c46fbfd3c77"
      },
      "source": [
        ""
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9624, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBS1GMx3tr74",
        "outputId": "31b86143-8a8d-410d-c401-b76ad13177a8"
      },
      "source": [
        "# dice_loss(fcl_output,labels,attention_mask)\r\n",
        "#b_s,s_l,n_l\r\n",
        "# pred_soft=torch.softmax(fcl_output,-1)#,torch.softmax(fcl_output[0,0,:],-1) #apply softmax to each token\r\n",
        "# target_one_hot=F.one_hot(labels,num_classes=config.num_labels)#,labels[0,:5] (b_s, s_l) -> (b_s, s_l, n_l)\r\n",
        "# pred_factor=((1-pred_soft) ** config.alpha) if config.self_adjusting else 1\r\n",
        "# pred_prod=pred_factor*pred_soft*target_one_hot\r\n",
        "# sum(pred_prod,0).shape\r\n",
        "# smooth = 1e-8\r\n",
        "# intersection=torch.sum(pred_prod,1)\r\n",
        "intersection.shape,pred_prod.shape\r\n",
        "# cardinality =torch.sum(pred_factor*pred_soft + target_one_hot, 1)\r\n",
        "# dice_score=1-2*(intersection+smooth)/(cardinality+smooth)\r\n",
        "# dice_score[0,:]\r\n",
        "# weight=[0,0,0,0,0,0,0,1,0,0]\r\n",
        "# (dice_score[:2,:]*torch.tensor(torch.tensor(weight))).shape\r\n",
        "### torch.gather(pred_soft[0,:5],-1,index=labels[0,:5].unsqueeze(-1)) #returns the probability for the most likely example is this really needed?\r\n",
        "# target_one_hot.shape\r\n",
        "# labels.unsqueeze(2).shape,labels.shape\r\n",
        "# pred_soft[0,:5].shape"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4, 10]), torch.Size([4, 128, 10]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5oDqH1cFjiv"
      },
      "source": [
        "def validate(\r\n",
        "        h: torch.Tensor,\r\n",
        "        labels: Optional[torch.LongTensor] = None,\r\n",
        "        mask: Optional[torch.ByteTensor] = None) -> None:\r\n",
        "    if h.dim() != 3:\r\n",
        "        raise ValueError(f'h must have dimension of 3, got {h.dim()}')\r\n",
        "    if h.size(2) != config.num_labels:\r\n",
        "        raise ValueError(\r\n",
        "            f'expected last dimension of h is {config.num_labels}, '\r\n",
        "            f'got {h.size(2)}')\r\n",
        "    if labels is not None:\r\n",
        "        if h.shape[:2] != labels.shape:\r\n",
        "            raise ValueError(\r\n",
        "                'the first two dimensions of h and labels must match, '\r\n",
        "                f'got {tuple(h.shape[:2])} and {tuple(labels.shape)}')\r\n",
        "    if mask is not None:\r\n",
        "        if h.shape[:2] != mask.shape:\r\n",
        "            raise ValueError(\r\n",
        "                'the first two dimensions of h and mask must match, '\r\n",
        "                f'got {tuple(h.shape[:2])} and {tuple(mask.shape)}')\r\n",
        "validate(fcl_output,labels) #why must x be the fcl output?"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVk91EvMkE2i"
      },
      "source": [
        "class DiceLoss(nn.Module):\n",
        "    r\"\"\"\n",
        "    Creates a criterion that optimizes a multi-class Self-adjusting Dice Loss\n",
        "    (\"Dice Loss for Data-imbalanced NLP Tasks\" paper)\n",
        "    Args:\n",
        "        alpha (float): a factor to push down the weight of easy examples\n",
        "        gamma (float): a factor added to both the nominator and the denominator for smoothing purposes\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 smooth: Optional[float] = 1e-8,\n",
        "                #  square_denominator: Optional[bool] = False,\n",
        "                 self_adjusting: Optional[bool] = False,\n",
        "                #  with_logits: Optional[bool] = True,\n",
        "                 reduction: Optional[str] = \"mean\",\n",
        "                 alpha: float = 1.0,\n",
        "                #  ignore_index: int = -100,\n",
        "                 weight=1, #int or list\n",
        "                 ) -> None:\n",
        "        super(DiceLoss, self).__init__()\n",
        "        # self.ignore_index = ignore_index\n",
        "        self.reduction = reduction\n",
        "        self.self_adjusting = self_adjusting\n",
        "        self.alpha = alpha\n",
        "        self.smooth = smooth\n",
        "        # self.square_denominator = square_denominator\n",
        "        self.weight=weight\n",
        "    def forward(self,\n",
        "                pred: torch.Tensor,\n",
        "                target: torch.Tensor,\n",
        "                # mask: Optional[torch.Tensor] = None,\n",
        "                num_classes: int = 10,\n",
        "                ) -> torch.Tensor:\n",
        "        pred_soft = torch.softmax(pred,-1) #(batch_size,seq_len,num_labels)->(batch_size,seq_len,num_labels), sum along num_labels to 1\n",
        "        target_one_hot=F.one_hot(target,num_classes=num_classes) #(b_s, s_l) -> (b_s, s_l, n_l)\n",
        "        pred_factor = ((1-pred_soft) ** self.alpha) if self.self_adjusting else 1\n",
        "        # if mask is not None:\n",
        "        #     mask = mask.view(-1).float()\n",
        "        #     pred_soft = pred_soft * mask\n",
        "        #     target_one_hot = target_one_hot * mask\n",
        "        intersection = torch.sum(pred_factor * pred_soft * target_one_hot, 1) # (b_s,s_l,n_l)->(b_s,n_l)\n",
        "        cardinality = torch.sum(pred_factor * pred_soft + target_one_hot, 1)  # (b_s,s_l,n_l)->(b_s,n_l)\n",
        "        dice_score = 1. - 2. * (intersection + self.smooth) / (cardinality + self.smooth) * torch.tensor(self.weight)\n",
        "        if self.reduction == \"mean\":\n",
        "            return dice_score.mean()\n",
        "        elif self.reduction == \"sum\":\n",
        "            return dice_score.sum()\n",
        "        elif self.reduction == \"none\" or self.reduction is None:\n",
        "            return dice_score\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Reduction `{self.reduction}` is not supported.\")\n",
        "    def __str__(self):\n",
        "        return f\"Dice Loss smooth:{self.smooth}\"\n",
        "\n",
        "\n",
        "\n",
        "class DiceCRF(nn.Module):\n",
        "    def __init__(\n",
        "        self, num_labels: int, pad_idx: Optional[int] = None,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        :param num_labels: number of labels\n",
        "        :param pad_idx: padding index. default None\n",
        "        :return None\n",
        "        \"\"\"\n",
        "        if num_labels < 1: raise ValueError(\"invalid number of labels: {0}\".format(num_labels))\n",
        "        super().__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.trans_matrix = nn.Parameter(torch.empty(num_labels, num_labels))\n",
        "        self.start_trans = nn.Parameter(torch.empty(num_labels))\n",
        "        self.end_trans = nn.Parameter(torch.empty(num_labels))\n",
        "        self._initialize_parameters(pad_idx)\n",
        "    def forward(\n",
        "        self, h: FloatTensor, labels: LongTensor, mask: BoolTensor, reduction: str = 'sum',\n",
        "    ) -> FloatTensor:\n",
        "        \"\"\"\n",
        "        :param h: hidden matrix (batch_size, seq_len, num_labels)\n",
        "        :param labels: answer labels of each sequence\n",
        "                       in mini batch (batch_size, seq_len)\n",
        "        :param mask: mask tensor of each sequence\n",
        "                     in mini batch (batch_size, seq_len)\n",
        "        :return: The log-likelihood (batch_size)\n",
        "        \"\"\"\n",
        "        self._validate(h, labels=labels, mask=mask)\n",
        "        log_numerator = self._compute_numerator_log_likelihood(h, labels, mask)\n",
        "        log_denominator = self._compute_denominator_log_likelihood(h, mask)\n",
        "        llh=log_numerator - log_denominator\n",
        "        if reduction == 'none':\n",
        "            return llh\n",
        "        if reduction == 'sum':\n",
        "            return llh.sum()\n",
        "        if reduction == 'mean':\n",
        "            return llh.mean()\n",
        "        assert reduction == 'token_mean'\n",
        "        return llh.sum() / mask.float().sum()\n",
        "    def _validate(\n",
        "            self,\n",
        "            h: torch.Tensor,\n",
        "            labels: Optional[torch.LongTensor] = None,\n",
        "            mask: Optional[torch.ByteTensor] = None) -> None:\n",
        "        if h.dim() != 3:\n",
        "            raise ValueError(f'h must have dimension of 3, got {h.dim()}')\n",
        "        if h.size(2) != self.num_labels:\n",
        "            raise ValueError(\n",
        "                f'expected last dimension of h is {self.num_labels}, '\n",
        "                f'got {h.size(2)}')\n",
        "        if labels is not None:\n",
        "            if h.shape[:2] != labels.shape:\n",
        "                raise ValueError(\n",
        "                    'the first two dimensions of h and labels must match, '\n",
        "                    f'got {tuple(h.shape[:2])} and {tuple(labels.shape)}')\n",
        "        if mask is not None:\n",
        "            if h.shape[:2] != mask.shape:\n",
        "                raise ValueError(\n",
        "                    'the first two dimensions of h and mask must match, '\n",
        "                    f'got {tuple(h.shape[:2])} and {tuple(mask.shape)}')\n",
        "#             no_empty_seq = not self.batch_first and mask[0].all()\n",
        "#             no_empty_seq_bf = self.batch_first and mask[:, 0].all()\n",
        "#             if not no_empty_seq and not no_empty_seq_bf:\n",
        "#                 raise ValueError('mask of the first timestep must all be on')\n",
        "    def viterbi_decode(self, h: FloatTensor, mask: BoolTensor) -> List[List[int]]:\n",
        "        \"\"\"\n",
        "        decode labels using viterbi algorithm\n",
        "        :param h: hidden matrix (batch_size, seq_len, num_labels)\n",
        "        :param mask: mask tensor of each sequence\n",
        "                     in mini batch (batch_size, batch_size)\n",
        "        :return: labels of each sequence in mini batch\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = h.size()\n",
        "        # prepare the sequence lengths in each sequence\n",
        "        seq_lens = mask.sum(dim=1)\n",
        "        # In mini batch, prepare the score\n",
        "        # from the start sequence to the first label\n",
        "        score = [self.start_trans.data + h[:, 0]]\n",
        "        path = []\n",
        "        for t in range(1, seq_len):\n",
        "            # extract the score of previous sequence\n",
        "            # (batch_size, num_labels, 1)\n",
        "            previous_score = score[t - 1].view(batch_size, -1, 1)\n",
        "            # extract the score of hidden matrix of sequence\n",
        "            # (batch_size, 1, num_labels)\n",
        "            h_t = h[:, t].view(batch_size, 1, -1)\n",
        "            # extract the score in transition\n",
        "            # from label of t-1 sequence to label of sequence of t\n",
        "            # self.trans_matrix has the score of the transition\n",
        "            # from sequence A to sequence B\n",
        "            # (batch_size, num_labels, num_labels)\n",
        "            score_t = previous_score + self.trans_matrix + h_t\n",
        "            # keep the maximum value\n",
        "            # and point where maximum value of each sequence\n",
        "            # (batch_size, num_labels)\n",
        "            best_score, best_path = score_t.max(1)\n",
        "            score.append(best_score)\n",
        "            path.append(best_path)\n",
        "        # predict labels of mini batch\n",
        "        best_paths = [\n",
        "            self._viterbi_compute_best_path(i, seq_lens, score, path)\n",
        "            for i in range(batch_size)\n",
        "        ]\n",
        "        return best_paths\n",
        "    def _viterbi_compute_best_path(\n",
        "        self,\n",
        "        batch_idx: int,\n",
        "        seq_lens: torch.LongTensor,\n",
        "        score: List[FloatTensor],\n",
        "        path: List[torch.LongTensor],\n",
        "    ) -> List[int]:\n",
        "        \"\"\"\n",
        "        return labels using viterbi algorithm\n",
        "        :param batch_idx: index of batch\n",
        "        :param seq_lens: sequence lengths in mini batch (batch_size)\n",
        "        :param score: transition scores of length max sequence size\n",
        "                      in mini batch [(batch_size, num_labels)]\n",
        "        :param path: transition paths of length max sequence size\n",
        "                     in mini batch [(batch_size, num_labels)]\n",
        "        :return: labels of batch_idx-th sequence\n",
        "        \"\"\"\n",
        "        seq_end_idx = seq_lens[batch_idx] - 1\n",
        "        # extract label of end sequence\n",
        "        _, best_last_label = (score[seq_end_idx][batch_idx] + self.end_trans).max(0)\n",
        "        best_labels = [int(best_last_label)]\n",
        "        # predict labels from back using viterbi algorithm\n",
        "        for p in reversed(path[:seq_end_idx]):\n",
        "            best_last_label = p[batch_idx][best_labels[0]]\n",
        "            best_labels.insert(0, int(best_last_label))\n",
        "        return best_labels\n",
        "    def _compute_denominator_log_likelihood(self, h: FloatTensor, mask: BoolTensor):\n",
        "        \"\"\"\n",
        "        compute the denominator term for the log-likelihood\n",
        "        compute the partition function in log-space using the forward-algorithm.\n",
        "        :param h: hidden matrix (batch_size, seq_len, num_labels)\n",
        "        :param mask: mask tensor of each sequence\n",
        "                     in mini batch (batch_size, seq_len)\n",
        "        :return: The score of denominator term for the log-likelihood\n",
        "        \"\"\"\n",
        "        device = h.device\n",
        "        batch_size, seq_len, _ = h.size()\n",
        "        # (num_labels, num_labels) -> (1, num_labels, num_labels)\n",
        "        trans = self.trans_matrix.unsqueeze(0)\n",
        "        # add the score from beginning to each label\n",
        "        # and the first score of each label\n",
        "        score = self.start_trans + h[:, 0]\n",
        "        # iterate through processing for the number of words in the mini batch\n",
        "        for t in range(1, seq_len):\n",
        "            # (batch_size, self.num_labels, 1)\n",
        "            before_score = score.unsqueeze(2)\n",
        "            # prepare t-th mask of sequences in each sequence\n",
        "            # (batch_size, 1)\n",
        "            mask_t = mask[:, t].unsqueeze(1)\n",
        "            mask_t = mask_t.to(device)\n",
        "            # prepare the transition probability of the t-th sequence label\n",
        "            # in each sequence\n",
        "            # (batch_size, 1, num_labels)\n",
        "            h_t = h[:, t].unsqueeze(1)\n",
        "            # calculate t-th scores in each sequence\n",
        "            # (batch_size, num_labels)\n",
        "            score_t = before_score + h_t + trans\n",
        "            score_t = torch.logsumexp(score_t, 1)\n",
        "            # update scores\n",
        "            # (batch_size, num_labels)\n",
        "            score = torch.where(mask_t, score_t, score)\n",
        "        # add the end score of each label\n",
        "        score += self.end_trans\n",
        "        # return the log likely food of all data in mini batch\n",
        "        return torch.logsumexp(score, 1)\n",
        "    def _compute_numerator_log_likelihood(\n",
        "        self, h: FloatTensor, y: LongTensor, mask: BoolTensor\n",
        "    ) -> FloatTensor:\n",
        "        \"\"\"\n",
        "        compute the numerator term for the log-likelihood\n",
        "        :param h: hidden matrix (batch_size, seq_len, num_labels)\n",
        "        :param y: answer labels of each sequence\n",
        "                  in mini batch (batch_size, seq_len)\n",
        "        :param mask: mask tensor of each sequence\n",
        "                     in mini batch (batch_size, seq_len)\n",
        "        :return: The score of numerator term for the log-likelihood\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = h.size()\n",
        "        h_unsqueezed = h.unsqueeze(-1)\n",
        "        trans = self.trans_matrix.unsqueeze(-1)\n",
        "        arange_b = torch.arange(batch_size)\n",
        "        # extract first vector of sequences in mini batch\n",
        "        calc_range = seq_len - 1\n",
        "        score = self.start_trans[y[:, 0]] + sum(\n",
        "            [self._calc_trans_score_for_num_llh(\n",
        "                h_unsqueezed, y, trans, mask, t, arange_b\n",
        "            ) for t in range(calc_range)])\n",
        "        # extract end label number of each sequence in mini batch\n",
        "        # (batch_size)\n",
        "        last_mask_index = mask.sum(1) - 1\n",
        "        last_labels = y[arange_b, last_mask_index]\n",
        "        each_last_score = h[arange_b, -1, last_labels] * mask[:, -1]\n",
        "        # Add the score of the sequences of the maximum length in mini batch\n",
        "        # Add the scores from the last tag of each sequence to EOS\n",
        "        score += each_last_score + self.end_trans[last_labels]\n",
        "        return score\n",
        "    def _calc_trans_score_for_num_llh(\n",
        "        self,\n",
        "        h: FloatTensor,\n",
        "        y: LongTensor,\n",
        "        trans: FloatTensor,\n",
        "        mask: BoolTensor,\n",
        "        t: int,\n",
        "        arange_b: FloatTensor,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        calculate transition score for computing numberator llh\n",
        "        :param h: hidden matrix (batch_size, seq_len, num_labels)\n",
        "        :param y: answer labels of each sequence\n",
        "                  in mini batch (batch_size, seq_len)\n",
        "        :param trans: transition score\n",
        "        :param mask: mask tensor of each sequence\n",
        "                     in mini batch (batch_size, seq_len)\n",
        "        :paramt t: index of hidden, transition, and mask matrixex\n",
        "        :param arange_b: this param is seted torch.arange(batch_size)\n",
        "        :param batch_size: batch size of this calculation\n",
        "        \"\"\"\n",
        "        device = h.device\n",
        "        mask_t = mask[:, t]\n",
        "        mask_t = mask_t.to(device)\n",
        "        mask_t1 = mask[:, t + 1]\n",
        "        mask_t1 = mask_t1.to(device)\n",
        "        # extract the score of t+1 label\n",
        "        # (batch_size)\n",
        "        h_t = h[arange_b, t, y[:, t]].squeeze(1)\n",
        "        # extract the transition score from t-th label to t+1 label\n",
        "        # (batch_size)\n",
        "        trans_t = trans[y[:, t], y[:, t + 1]].squeeze(1)\n",
        "        # add the score of t+1 and the transition score\n",
        "        # (batch_size)\n",
        "        return h_t * mask_t + trans_t * mask_t1\n",
        "    def _initialize_parameters(self, pad_idx: Optional[int]) -> None:\n",
        "        \"\"\"\n",
        "        initialize transition parameters\n",
        "        :param: pad_idx: if not None, additional initialize\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        nn.init.uniform_(self.trans_matrix, -0.1, 0.1)\n",
        "        nn.init.uniform_(self.start_trans, -0.1, 0.1)\n",
        "        nn.init.uniform_(self.end_trans, -0.1, 0.1)\n",
        "        if pad_idx is not None:\n",
        "            self.start_trans[pad_idx] = -10000.0\n",
        "            self.trans_matrix[pad_idx, :] = -10000.0\n",
        "            self.trans_matrix[:, pad_idx] = -10000.0\n",
        "            self.trans_matrix[pad_idx, pad_idx] = 0.0\n"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4urD6kW9kE2l"
      },
      "source": [
        "### engine.py\n",
        "# from tqdm import tqdm\n",
        "def train_step(trainer,batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    batch=[_data.to(device) for _data in batch]\n",
        "    _, loss = model(batch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    return loss.item()\n",
        "\n",
        "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
        "    model.train()\n",
        "    final_loss = 0\n",
        "    for data in tqdm(data_loader, total=len(data_loader)):\n",
        "        if torch.cuda.is_available(): data=[_data.to(device) for _data in data]\n",
        "        optimizer.zero_grad()\n",
        "        _, loss = model(data)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        final_loss += loss.item()\n",
        "    return final_loss / len(data_loader)\n",
        "\n",
        "\n",
        "def eval_fn(data_loader, model, device):\n",
        "    model.eval()\n",
        "    final_loss = 0\n",
        "    for data in tqdm(data_loader, total=len(data_loader)):\n",
        "        if torch.cuda.is_available(): data=[_data.to(device) for _data in data]\n",
        "        punct, loss = model(data)\n",
        "        final_loss += loss.item()\n",
        "    return final_loss / len(data_loader)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT1awx_tkE2p",
        "outputId": "3971bea4-a88c-442c-e986-21395efa8750"
      },
      "source": [
        "model = BertCRFModel(num_punct=10, embedding_dim=config.EMBEDDING_DIM, hidden_dim=config.HIDDEN_DIM, use_crf=config.USE_CRF)\n",
        "for i,param in enumerate(model.bert.parameters()):\n",
        "    param.requires_grad = False\n",
        "model.to(device)\n",
        "optimizer = AdamW(optimizer_parameters, lr=config.FREEZE_LEARNING_RATE)\n",
        "num_train_steps = train_dataset.tensors[0].size()[0] / config.TRAIN_BATCH_SIZE * config.UNFREEZE_EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_train_steps\n",
        ")\n",
        "# optimizer = SWA(base_opt)\n",
        "\n",
        "trainer = Engine(train_step)\n",
        "val_metrics = {\n",
        "    \"precision\": Precision(),\n",
        "    \"recall\": Recall(),\n",
        "#     \"Dice\": DiceCoefficient(cm=),\n",
        "    \"F1\": Fbeta(1),\n",
        "}\n",
        "evaluator = create_supervised_evaluator(model, metrics=val_metrics)\n",
        "def log_metrics(engine, title):\n",
        "    print(\"Epoch: {} - {} accuracy: {:.2f}\"\n",
        "           .format(trainer.state.epoch, title, engine.state.metrics[\"acc\"]))\n",
        "\n",
        "@trainer.on(Events.EPOCH_COMPLETED)\n",
        "def evaluate(trainer):\n",
        "    with evaluator.add_event_handler(Events.COMPLETED, log_metrics, \"train\"):\n",
        "        evaluator.run(train_dataloader)\n",
        "\n",
        "    with evaluator.add_event_handler(Events.COMPLETED, log_metrics, \"dev\"):\n",
        "        evaluator.run(dev_dataloader)\n",
        "\n",
        "trainer.run(train_dataloader, max_epochs=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertModel: ['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Current run is terminating due to exception: too many values to unpack (expected 2).\n",
            "Engine run is terminating due to exception: too many values to unpack (expected 2).\n",
            "Engine run is terminating due to exception: too many values to unpack (expected 2).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-5d0c641fc8ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Engine run is terminating due to exception: %s.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    735\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTERMINATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhandlers_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m                 \u001b[0;31m# update time wrt handlers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_fire_event\u001b[0;34m(self, event_name, *event_args, **event_kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                 \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mothers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                 \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mothers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-70-5d0c641fc8ec>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(trainer)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_event_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOMPLETED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_event_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOMPLETED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dev\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Engine run is terminating due to exception: %s.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    728\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Current run is terminating due to exception: %s.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    809\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/ignite/engine/__init__.py\u001b[0m in \u001b[0;36m_inference\u001b[0;34m(engine, batch)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/ignite/engine/__init__.py\u001b[0m in \u001b[0;36m_prepare_batch\u001b[0;34m(batch, device, non_blocking)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \"\"\"\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     return (\n\u001b[1;32m     36\u001b[0m         \u001b[0mconvert_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "n3AxdqJJkE2p"
      },
      "source": [
        "torch.utils.data.DataLoader??"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}