{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "eval.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JILqsDFCkIcM",
        "outputId": "7858fa0a-09ce-41d7-9662-9089316a2499"
      },
      "source": [
        "#%%Forcolabonly\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')\r\n",
        "#%%"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YlGB1lokE2T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3ac7c97-9614-48f5-e3ed-27bd88a64235"
      },
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install torchcontrib\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import BoolTensor, FloatTensor, LongTensor, ByteTensor\n",
        "from typing import List, Optional, Sequence, Union, Callable, Dict, Any, Tuple\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import DistilBertTokenizerFast, BertPreTrainedModel, get_linear_schedule_with_warmup, AdamW\n",
        "from torchcontrib.optim import SWA\n",
        "import regex as re\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/78/5873ac1e27bf25a2cbf3447d6704edd3136b1b3ff0eb3bfab38a45d2a1ff/datasets-1.2.0-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 5.7MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Collecting pyarrow>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n",
            "\u001b[K     |████████████████████████████████| 17.7MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.19.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, pyarrow, datasets\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed datasets-1.2.0 pyarrow-2.0.0 xxhash-2.0.0\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 21.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 39.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=97aa639cf7a504721177ad7dc9850e988f797c72b2a3014c18be81905a176f23\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n",
            "Collecting torchcontrib\n",
            "  Downloading https://files.pythonhosted.org/packages/72/36/45d475035ab35353911e72a03c1c1210eba63b71e5a6917a9e78a046aa10/torchcontrib-0.0.2.tar.gz\n",
            "Building wheels for collected packages: torchcontrib\n",
            "  Building wheel for torchcontrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchcontrib: filename=torchcontrib-0.0.2-cp36-none-any.whl size=7531 sha256=bca5e45d429ad99b6cb2c025341d795b9515745dd2e693cc05615a5ceca5c037\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/06/7b/a5f5920bbf4f12a2c927e438fac17d4cd9560f8336b00e9a99\n",
            "Successfully built torchcontrib\n",
            "Installing collected packages: torchcontrib\n",
            "Successfully installed torchcontrib-0.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_PYFo1TkE2h"
      },
      "source": [
        "tags=sorted(list('.?!,;:-—…'))\n",
        "tag2id = {tag: id+1 for id, tag in enumerate(tags)}\n",
        "tag2id[' ']=0\n",
        "tag2id['']=-100\n",
        "id2tag = {id: tag for tag, id in tag2id.items()}\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "class PunctuationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, input_ids:LongTensor, attention_mask:FloatTensor, labels:Optional[LongTensor] = None) -> None:\n",
        "        \"\"\"\n",
        "        :param input_ids: tokenids\n",
        "        :param attention_mask: attention_mask, null->0\n",
        "        :param labels: true labels, optional\n",
        "        :return None\n",
        "        \"\"\"\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\":param idx: implement index\"\"\"\n",
        "        return {'input_ids': torch.as_tensor(self.input_ids[idx],dtype=torch.long),\n",
        "                'attention_mask': torch.as_tensor(self.attention_mask[idx],dtype=torch.uint8),\n",
        "                'labels': torch.as_tensor(self.labels[idx],dtype=torch.long)}\n",
        "\n",
        "    def view(self,idx:int)->str:\n",
        "        \"\"\":param idx(int): returns readable format of single input_ids and labels in the form of readable text\"\"\"\n",
        "        return ' '.join([''.join(x) for x in list(zip(tokenizer.convert_ids_to_tokens(self.input_ids[idx]),[id2tag[x] for x in self.labels[idx].tolist()]))])\n",
        "\n",
        "    def __len__(self)->int:\n",
        "        return len(self.labels)"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVAhMIG5pMd3"
      },
      "source": [
        "class config:\r\n",
        "  def __init__(self):\r\n",
        "    self.max_len=128\r\n",
        "    self.overlap = 126\r\n",
        "    self.train_batch_size = 4\r\n",
        "    self.dev_batch_size = 4\r\n",
        "    self.gpu_device = 'cpu' # 'cuda:0' #\r\n",
        "    self.freeze_epochs = 20\r\n",
        "    self.freeze_lr = 1e-4\r\n",
        "    self.unfreeze_epochs = 20\r\n",
        "    self.unfreeze_layers = 6\r\n",
        "    self.unfreeze_lr = 1e-5\r\n",
        "    self.base_model_path = 'distilbert-base-uncased'\r\n",
        "    self.train_dataset = '/content/gdrive/MyDrive/ASR/ted_talks_processed.train.pt'\r\n",
        "    self.dev_dataset = '/content/gdrive/MyDrive/ASR/ted_talks_processed.dev.pt'\r\n",
        "    self.alpha = 0.8\r\n",
        "    self.hidden_dropout_prob = 0.3\r\n",
        "    self.embedding_dim = 768\r\n",
        "    self.num_labels = 10\r\n",
        "    self.hidden_dim = 128\r\n",
        "    self.self_adjusting = True\r\n",
        "    self.square_denominator = False\r\n",
        "    self.use_crf = False\r\n",
        "    self.use_dice = False\r\n",
        "    self.reduction='mean'\r\n",
        "    self.model_name = 'bertcrf'\r\n",
        "    self.model_path = \"/content/gdrive/MyDrive/ASR/logs/models/\"\r\n",
        "    self.log_dir='/tmp/tensorboard_logs'\r\n",
        "config = transformers.configuration_utils.PretrainedConfig.from_dict(config().__dict__)"
      ],
      "execution_count": 284,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "-NtAYQ5Vqsjg",
        "outputId": "90515a02-c466-4c5f-ae39-897c99fe333b"
      },
      "source": [
        "device = torch.device(config.gpu_device) if torch.cuda.is_available() else torch.device('cpu')\r\n",
        "train_dataset=PunctuationDataset(**torch.load(config.train_dataset,map_location=device)[:])\r\n",
        "dev_dataset=PunctuationDataset(**torch.load(config.dev_dataset,map_location=device)[:])\r\n",
        "train_dataset.view(-1000)"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"[CLS]  turns  activism  into  terrorism  if  it  causes  a  loss  of  profits. now  most  people  never  even  heard  about  this  law, including  members  of  congress. less  than  one  percent  were  in  the  room  when  it  passed  the  house. the  rest  were  outside  at  a  new  memorial. they  were  praising  dr. king  as  his  style  of  activism  was  branded  as  terrorism  if  done  in  the  name  of  animals  or  the  environment. supporters  say  laws  like  this  are  needed  for  the  ex  ##tre  ##mist  ##s: the  van  ##dal  ##s, the  arson  ##ists, the  radicals. but  right  now, companies  like  trans  ##cana  ##da  are  briefing  police  in  presentations  like  this  one  about  how  to  prose  ##cute  non  ##vio  ##lent  protesters  as  terrorists. the  fbi  '  s  training  documents  on  eco- [SEP] \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGbVdBw4rl5X",
        "outputId": "0d5d06a8-544d-469f-c2d7-67e444e948dc"
      },
      "source": [
        "train_dataloader=torch.utils.data.DataLoader(train_dataset, batch_size=config.train_batch_size, num_workers=4)\r\n",
        "dev_dataloader=torch.utils.data.DataLoader(dev_dataset, batch_size=config.dev_batch_size, num_workers=2)\r\n",
        "{x:y.shape for x,y in next(iter(train_dataloader)).items()},{x:y.shape for x,y in next(iter(dev_dataloader)).items()} #(batch_size, seq_len)"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'attention_mask': torch.Size([4, 128]),\n",
              "  'input_ids': torch.Size([4, 128]),\n",
              "  'labels': torch.Size([4, 128])},\n",
              " {'attention_mask': torch.Size([4, 128]),\n",
              "  'input_ids': torch.Size([4, 128]),\n",
              "  'labels': torch.Size([4, 128])})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVk91EvMkE2i"
      },
      "source": [
        "class DiceLoss(nn.Module):\n",
        "    r\"\"\"\n",
        "    Creates a criterion that optimizes a multi-class Self-adjusting Dice Loss\n",
        "    (\"Dice Loss for Data-imbalanced NLP Tasks\" paper)\n",
        "    Args:\n",
        "        alpha (float): a factor to push down the weight of easy examples\n",
        "        gamma (float): a factor added to both the nominator and the denominator for smoothing purposes\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 smooth: Optional[float] = 1e-8,\n",
        "                #  square_denominator: Optional[bool] = False,\n",
        "                 self_adjusting: Optional[bool] = False,\n",
        "                 reduction: Optional[str] = \"mean\",\n",
        "                 alpha: float = 1.0,\n",
        "                 num_classes: int = 10,\n",
        "                 weight=1, #int or list\n",
        "                 ) -> None:\n",
        "        super(DiceLoss, self).__init__()\n",
        "        # self.ignore_index = ignore_index\n",
        "        self.reduction = reduction\n",
        "        self.self_adjusting = self_adjusting\n",
        "        self.num_classes = num_classes\n",
        "        self.alpha = alpha\n",
        "        self.smooth = smooth\n",
        "        # self.square_denominator = square_denominator\n",
        "        self.weight=weight\n",
        "    def forward(self,\n",
        "                pred: FloatTensor,\n",
        "                target: LongTensor,\n",
        "                mask: ByteTensor,\n",
        "                ) -> torch.Tensor:\n",
        "        pred_soft = torch.softmax(pred,-1) #(batch_size,seq_len,num_labels)->(batch_size,seq_len,num_labels), sum along num_labels to 1\n",
        "        target_one_hot=F.one_hot(target,num_classes=self.num_classes) #(b_s, s_l) -> (b_s, s_l, n_l)\n",
        "        pred_factor = ((1-pred_soft) ** self.alpha) if self.self_adjusting else 1\n",
        "        if mask is not None:\n",
        "            pred_soft = pred_soft * mask.unsqueeze(-1)\n",
        "            target_one_hot = target_one_hot * mask.unsqueeze(-1)\n",
        "        intersection = torch.sum(pred_factor * pred_soft * target_one_hot, 1) # (b_s,s_l,n_l)->(b_s,n_l)\n",
        "        cardinality = torch.sum(pred_factor * pred_soft + target_one_hot, 1)  # (b_s,s_l,n_l)->(b_s,n_l)\n",
        "        dice_score = 1. - 2. * (intersection + self.smooth) / (cardinality + self.smooth) * torch.tensor(self.weight)\n",
        "        if self.reduction == \"mean\":\n",
        "            return dice_score.mean()\n",
        "        elif self.reduction == \"sum\":\n",
        "            return dice_score.sum()\n",
        "        elif self.reduction == \"none\" or self.reduction is None:\n",
        "            return dice_score\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Reduction `{self.reduction}` is not supported.\")\n",
        "    def __str__(self):\n",
        "        return f\"Dice Loss smooth:{self.smooth}\"\n",
        "\n",
        "class DiceCRF(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        num_labels: int, \n",
        "        pad_idx: Optional[int] = None,\n",
        "        reduction: Optional[str] = \"mean\",\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        :param num_labels: number of labels\n",
        "        :param pad_idx: padding index. default None\n",
        "        :return None\n",
        "        \"\"\"\n",
        "        if num_labels < 1: raise ValueError(\"invalid number of labels: {0}\".format(num_labels))\n",
        "        super().__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.trans_matrix = nn.Parameter(torch.empty(num_labels, num_labels))\n",
        "        self.start_trans = nn.Parameter(torch.empty(num_labels))\n",
        "        self.end_trans = nn.Parameter(torch.empty(num_labels))\n",
        "        self.reduction = reduction\n",
        "        self._initialize_parameters(pad_idx)\n",
        "    def forward(\n",
        "        self, h: FloatTensor, labels: LongTensor, mask: ByteTensor,\n",
        "    ) -> FloatTensor:\n",
        "        \"\"\"\n",
        "        :param h: hidden matrix (batch_size, seq_len, num_labels)\n",
        "        :param labels: answer labels of each sequence\n",
        "                       in mini batch (batch_size, seq_len)\n",
        "        :param mask: mask tensor of each sequence #attention_mask\n",
        "                     in mini batch (batch_size, seq_len)\n",
        "        :param reduction: Specifies  the reduction to apply to the output:\n",
        "                ``none|sum|mean|token_mean``. ``none``: no reduction will be applied.\n",
        "                ``sum``: the output will be summed over batches. ``mean``: the output will be\n",
        "                averaged over batches. ``token_mean``: the output will be averaged over tokens\n",
        "        :return: The log-likelihood (batch_size,) if reduction==none,  else ()\n",
        "        \"\"\"\n",
        "        self._validate(h, labels=labels, mask=mask)\n",
        "        if self.reduction not in ('none', 'sum', 'mean', 'token_mean'):\n",
        "            raise ValueError(f'invalid reduction: {self.reduction}')\n",
        "        if mask is None:\n",
        "            mask = torch.ones_like(tags, dtype=torch.uint8)\n",
        "\n",
        "        log_numerator = self._compute_numerator_log_likelihood(h, labels, mask) #score\n",
        "        log_denominator = self._compute_denominator_log_likelihood(h, mask) #partition\n",
        "        llh=log_numerator - log_denominator\n",
        "        if self.reduction == 'none':\n",
        "            return -llh\n",
        "        if self.reduction == 'sum':\n",
        "            return -llh.sum()\n",
        "        if self.reduction == 'mean':\n",
        "            return -llh.mean()\n",
        "        assert self.reduction == 'token_mean'\n",
        "        return -llh.sum() / mask.type_as(h).sum()\n",
        "\n",
        "    def _validate(\n",
        "            self,\n",
        "            h: torch.Tensor,\n",
        "            labels: Optional[LongTensor] = None,\n",
        "            mask: Optional[ByteTensor] = None) -> None:\n",
        "        if h.dim() != 3:\n",
        "            raise ValueError(f'h must have dimension of 3, got {h.dim()}')\n",
        "        if h.size(2) != self.num_labels:\n",
        "            raise ValueError(\n",
        "                f'expected last dimension of h is {self.num_labels}, '\n",
        "                f'got {h.size(2)}')\n",
        "        if labels is not None:\n",
        "            if h.shape[:2] != labels.shape:\n",
        "                raise ValueError(\n",
        "                    'the first two dimensions of h and labels must match, '\n",
        "                    f'got {tuple(h.shape[:2])} and {tuple(labels.shape)}')\n",
        "        if mask is not None:\n",
        "            if h.shape[:2] != mask.shape:\n",
        "                raise ValueError(\n",
        "                    'the first two dimensions of h and mask must match, '\n",
        "                    f'got {tuple(h.shape[:2])} and {tuple(mask.shape)}')\n",
        "    def decode(self, h: FloatTensor,\n",
        "               mask: Optional[ByteTensor] = None) -> torch.Tensor:\n",
        "        self._validate(h, mask=mask)\n",
        "\n",
        "        if mask is None:\n",
        "            mask = emissions.new_ones(emissions.shape[:2], dtype=torch.uint8)\n",
        "        return self._viterbi_decode(h,mask)\n",
        "\n",
        "    def _viterbi_decode(self, h: FloatTensor, mask: ByteTensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        decode labels using viterbi algorithm\n",
        "        :param h: hidden matrix (batch_size, seq_len, num_labels)\n",
        "        :param mask: mask tensor of each sequence\n",
        "                     in mini batch (batch_size, seq_len)\n",
        "        :return: labels of each sequence in mini batch\n",
        "        \"\"\"\n",
        "        assert h.dim() == 3 and mask.dim() == 2\n",
        "        assert h.shape[:2] == mask.shape\n",
        "        assert h.size(2) == self.num_labels\n",
        "        assert mask[0].all()\n",
        "\n",
        "        batch_size, seq_len, _ = h.shape\n",
        "        # prepare the sequence lengths in each sequence\n",
        "        seq_lens = mask.sum(dim=1)\n",
        "        # In mini batch, prepare the score\n",
        "        # from the start sequence to the first label\n",
        "        score = [self.start_trans.data + h[:, 0]]\n",
        "        path = []\n",
        "        for t in range(1, seq_len):\n",
        "            # extract the score of previous sequence\n",
        "            # (batch_size, num_labels, 1)\n",
        "            previous_score = score[t - 1].view(batch_size, -1, 1)\n",
        "            # extract the score of hidden matrix of sequence\n",
        "            # (batch_size, 1, num_labels)\n",
        "            h_t = h[:, t].view(batch_size, 1, -1)\n",
        "            # extract the score in transition\n",
        "            # from label of t-1 sequence to label of sequence of t\n",
        "            # self.trans_matrix has the score of the transition\n",
        "            # from sequence A to sequence B\n",
        "            # (batch_size, num_labels, num_labels)\n",
        "            score_t = previous_score + self.trans_matrix + h_t\n",
        "            # keep the maximum value\n",
        "            # and point where maximum value of each sequence\n",
        "            # (batch_size, num_labels)\n",
        "            best_score, best_path = score_t.max(1)\n",
        "            score.append(best_score)\n",
        "            path.append(best_path)\n",
        "        # predict labels of mini batch\n",
        "        best_paths = [\n",
        "            self._viterbi_compute_best_path(i, seq_lens, score, path)\n",
        "            for i in range(batch_size)\n",
        "        ]\n",
        "        return torch.tensor(best_paths)\n",
        "\n",
        "    def _viterbi_compute_best_path(\n",
        "        self,\n",
        "        batch_idx: int,\n",
        "        seq_lens: torch.LongTensor,\n",
        "        score: List[FloatTensor],\n",
        "        path: List[torch.LongTensor],\n",
        "    ) -> List[int]:\n",
        "        \"\"\"\n",
        "        return labels using viterbi algorithm\n",
        "        :param batch_idx: index of batch\n",
        "        :param seq_lens: sequence lengths in mini batch (batch_size)\n",
        "        :param score: transition scores of length max sequence size\n",
        "                      in mini batch [(batch_size, num_labels)]\n",
        "        :param path: transition paths of length max sequence size\n",
        "                     in mini batch [(batch_size, num_labels)]\n",
        "        :return: labels of batch_idx-th sequence\n",
        "        \"\"\"\n",
        "        seq_end_idx = seq_lens[batch_idx] - 1\n",
        "        # extract label of end sequence\n",
        "        _, best_last_label = (score[seq_end_idx][batch_idx] + self.end_trans).max(0)\n",
        "        best_labels = [int(best_last_label)]\n",
        "        # predict labels from back using viterbi algorithm\n",
        "        for p in reversed(path[:seq_end_idx]):\n",
        "            best_last_label = p[batch_idx][best_labels[0]]\n",
        "            best_labels.insert(0, int(best_last_label))\n",
        "        return best_labels\n",
        "\n",
        "    def _compute_denominator_log_likelihood(self, h: FloatTensor, mask: ByteTensor):\n",
        "        \"\"\"\n",
        "        compute the denominator term for the log-likelihood\n",
        "        compute the partition function in log-space using the forward-algorithm.\n",
        "        :param h: hidden matrix (batch_size, seq_len, num_labels)\n",
        "        :param mask: mask tensor of each sequence\n",
        "                     in mini batch (batch_size, seq_len)\n",
        "        :return: The score of denominator term for the log-likelihood\n",
        "        \"\"\"\n",
        "        device = h.device\n",
        "        batch_size, seq_len, _ = h.size()\n",
        "        # (num_labels, num_labels) -> (1, num_labels, num_labels)\n",
        "        trans = self.trans_matrix.unsqueeze(0)\n",
        "        # add the score from beginning to each label\n",
        "        # and the first score of each label\n",
        "        score = self.start_trans + h[:, 0]\n",
        "        # iterate through processing for the number of words in the mini batch\n",
        "        for t in range(1, seq_len):\n",
        "            # (batch_size, self.num_labels, 1)\n",
        "            before_score = score.unsqueeze(2)\n",
        "            # prepare t-th mask of sequences in each sequence\n",
        "            # (batch_size, 1)\n",
        "            mask_t = mask[:, t].unsqueeze(1)\n",
        "            mask_t = mask_t.to(device)\n",
        "            # prepare the transition probability of the t-th sequence label\n",
        "            # in each sequence\n",
        "            # (batch_size, 1, num_labels)\n",
        "            h_t = h[:, t].unsqueeze(1)\n",
        "            # calculate t-th scores in each sequence\n",
        "            # (batch_size, num_labels)\n",
        "            score_t = before_score + h_t + trans\n",
        "            score_t = torch.logsumexp(score_t, 1)\n",
        "            # update scores\n",
        "            # (batch_size, num_labels)\n",
        "            score = torch.where(mask_t, score_t, score)\n",
        "        # add the end score of each label\n",
        "        score += self.end_trans\n",
        "        # return the log likely food of all data in mini batch\n",
        "        return torch.logsumexp(score, 1)\n",
        "\n",
        "    def _compute_numerator_log_likelihood(\n",
        "        self, h: FloatTensor, y: LongTensor, mask: ByteTensor\n",
        "    ) -> FloatTensor:\n",
        "        \"\"\"\n",
        "        compute the numerator term for the log-likelihood\n",
        "        :param h: hidden matrix (batch_size, seq_len, num_labels)\n",
        "        :param y: answer labels of each sequence\n",
        "                  in mini batch (batch_size, seq_len)\n",
        "        :param mask: mask tensor of each sequence\n",
        "                     in mini batch (batch_size, seq_len)\n",
        "        :return: The score of numerator term for the log-likelihood\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = h.size()\n",
        "        h_unsqueezed = h.unsqueeze(-1)\n",
        "        trans = self.trans_matrix.unsqueeze(-1)\n",
        "        arange_b = torch.arange(batch_size)\n",
        "        # extract first vector of sequences in mini batch\n",
        "        last_mask_index = mask.sum(1) - 1\n",
        "        calc_range = seq_len - 1 #should calc_range be last_mask_index? No since parallel faster\n",
        "        # calc_range = last_mask_index\n",
        "        score = self.start_trans[y[:, 0]] + sum(\n",
        "            [self._calc_trans_score_for_num_llh(\n",
        "                h_unsqueezed, y, trans, mask, t, arange_b\n",
        "            ) for t in range(calc_range)])\n",
        "        # extract end label number of each sequence in mini batch\n",
        "        # (batch_size)\n",
        "        last_labels = y[arange_b,last_mask_index]\n",
        "        each_last_score = h[arange_b, -1, last_labels].squeeze(-1) * mask[:, -1]\n",
        "        # Add the score of the sequences of the maximum length in mini batch\n",
        "        # Add the scores from the last tag of each sequence to EOS\n",
        "        score += each_last_score + self.end_trans[last_labels]\n",
        "        return score\n",
        "    def _calc_trans_score_for_num_llh(\n",
        "        self,\n",
        "        h: FloatTensor,\n",
        "        y: LongTensor,\n",
        "        trans: FloatTensor,\n",
        "        mask: ByteTensor,\n",
        "        t: int,\n",
        "        arange_b: FloatTensor,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        calculate transition score for computing numerator llh\n",
        "        :param h: hidden matrix (batch_size, seq_len, num_labels)\n",
        "        :param y: answer labels of each sequence\n",
        "                  in mini batch (batch_size, seq_len)\n",
        "        :param trans: transition score\n",
        "        :param mask: mask tensor of each sequence\n",
        "                     in mini batch (batch_size, seq_len)\n",
        "        :paramt t: index of hidden, transition, and mask matrixex\n",
        "        :param arange_b: this param is seted torch.arange(batch_size)\n",
        "        :param batch_size: batch size of this calculation\n",
        "        \"\"\"\n",
        "        device = h.device\n",
        "        mask_t = mask[:, t]\n",
        "        mask_t = mask_t.to(device)\n",
        "        mask_t1 = mask[:, t + 1]\n",
        "        mask_t1 = mask_t1.to(device)\n",
        "        # extract the score of t label\n",
        "        # (batch_size)\n",
        "        h_t = h[arange_b, t, y[:, t]].squeeze(1)  ##Changed from t to t\n",
        "        # extract the transition score from t-th label to t+1 label\n",
        "        # (batch_size)\n",
        "        trans_t = trans[y[:, t], y[:, t + 1]].squeeze(1)\n",
        "        # add the score of t+1 and the transition score\n",
        "        # (batch_size)\n",
        "        return h_t * mask_t + trans_t * mask_t1\n",
        "    def _initialize_parameters(self, pad_idx: Optional[int]) -> None:\n",
        "        \"\"\"\n",
        "        initialize transition parameters\n",
        "        :param: pad_idx: if not None, additional initialize\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        nn.init.uniform_(self.trans_matrix, -0.1, 0.1)\n",
        "        nn.init.uniform_(self.start_trans, -0.1, 0.1)\n",
        "        nn.init.uniform_(self.end_trans, -0.1, 0.1)\n",
        "        punct=torch.arange(1,self.trans_matrix.shape[0])\n",
        "        # inaccurate since consecutive punctuation is possible (\"Hello! Bye!\")\n",
        "        # with torch.no_grad():\n",
        "        #   self.trans_matrix[punct,punct]=-100\n",
        "        if pad_idx is not None:\n",
        "            self.start_trans[pad_idx] = -10000.0\n",
        "            self.trans_matrix[pad_idx, :] = -10000.0\n",
        "            self.trans_matrix[:, pad_idx] = -10000.0\n",
        "            self.trans_matrix[pad_idx, pad_idx] = 0.0\n",
        "\n",
        "class CrossEntropyLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    This criterion (`CrossEntropyLoss`) combines `LogSoftMax` and `NLLLoss` in one single class.\n",
        "    \n",
        "    NOTE: Computes per-element losses for a mini-batch (instead of the average loss over the entire mini-batch).\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, weight: Optional[torch.Tensor] = None, ignore_index: int = -100, reduction: Optional[str] = 'mean') -> None:\n",
        "        super().__init__()\n",
        "        self.weight=weight\n",
        "        self.ignore_index = ignore_index\n",
        "        self.reduction=reduction\n",
        "\n",
        "    def forward(self, input: FloatTensor, target: LongTensor, mask: ByteTensor) -> torch.Tensor:\n",
        "        assert self.weight is None or isinstance(self.weight, torch.Tensor)\n",
        "        return F.cross_entropy(input[mask].view(-1,10).float(), target[mask].long(), weight=self.weight,\n",
        "                               ignore_index=self.ignore_index, reduction=self.reduction)"
      ],
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pShkHDqTroBX",
        "outputId": "4330b28d-1a2a-455f-a506-fed6734b174c"
      },
      "source": [
        "#b_s = 4, s_l = 128, h_d = 768\r\n",
        "bert=transformers.BertModel.from_pretrained(config.base_model_path)\r\n",
        "input_ids, attention_mask, labels = next(iter(train_dataloader)).values() # (batch_size, seq_len) * 3\r\n",
        "bo=bert(input_ids, attention_mask) # => last_hidden_state (b_s, s_l, h_d 768), pooler_output (b_s, h_d)\r\n",
        "dropout = nn.Dropout(config.hidden_dropout_prob) # (b_s, s_l, h_d)\r\n",
        "sequence_output=dropout(bo[0])\r\n",
        "fcl = nn.Linear(config.embedding_dim, config.num_labels)\r\n",
        "fcl_output=fcl(sequence_output) #(b_s, s_l, h_d) -> (b_s, s_l, num_labels)\r\n",
        "dice_loss=DiceLoss(self_adjusting=True,alpha=1)(fcl_output,labels,attention_mask)\r\n",
        "## bert(next)"
      ],
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertModel: ['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-KLOv1T0AIN",
        "outputId": "23c79caa-992d-4748-cdc2-4dd82b4314b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class BertCRFModel(BertPreTrainedModel):\r\n",
        "    def __init__(self,config):\r\n",
        "        super().__init__(config)\r\n",
        "        self.num_labels=config.num_labels\r\n",
        "        self.embedding_dim=config.embedding_dim\r\n",
        "        self.use_crf = config.use_crf\r\n",
        "        self.use_dice = config.use_dice\r\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n",
        "        self.bert = transformers.BertModel.from_pretrained(config.base_model_path)\r\n",
        "        self.fcl = nn.Linear(self.embedding_dim, self.num_labels)\r\n",
        "        if self.use_crf:\r\n",
        "            self.loss=DiceCRF(self.num_labels,reduction=config.reduction)\r\n",
        "        elif self.use_dice:\r\n",
        "            self.loss=DiceLoss(self_adjusting=config.self_adjusting,alpha=config.alpha, num_classes=self.num_labels, reduction=config.reduction)\r\n",
        "        else:\r\n",
        "            self.loss=CrossEntropyLoss(reduction=config.reduction)\r\n",
        "    def forward(self, input_ids:FloatTensor, attn_masks:ByteTensor, labels:Optional[LongTensor]=None):\r\n",
        "        \"\"\"\r\n",
        "        :param input_ids: tokenids (batch_size,seq_len)\r\n",
        "        :param attention_mask: attention_mask, null->0 (batch_size,seq_len)\r\n",
        "        :param labels: true labels, optional (batch_size,seq_len)\r\n",
        "        :return None\r\n",
        "        \"\"\"\r\n",
        "        o1 = self.bert(input_ids,attn_masks)[0] #(batch_size,seq_len), (batch_size,seq_len)-> last_hidden_state (batch_size,seq_len,hidden_size)\r\n",
        "        sequence_output = self.dropout(o1) #(batch_size,seq_len,hidden_size)\r\n",
        "        punct = self.fcl(sequence_output) #(batch_size,seq_len,hidden_size) -> (batch_size,seq_len,num_labels)\r\n",
        "        if labels is not None:\r\n",
        "            return self.loss(punct, labels, attn_masks)\r\n",
        "        else:\r\n",
        "            prediction = F.one_hot(self.loss.decode(punct, attn_masks).flatten(),10) if self.use_crf else punct.view(-1,10)\r\n",
        "            #(batch_size*seq_len,10)\r\n",
        "            return prediction\r\n",
        "        #loss = (loss_tag + loss_pos) / 2\r\n",
        "model=BertCRFModel(config)\r\n",
        "for i,param in enumerate(model.bert.parameters()):\r\n",
        "    param.requires_grad = False\r\n",
        "# model.to(device)\r\n"
      ],
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertModel: ['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfOhw0kt93X3",
        "outputId": "2f163786-c860-4aee-afd8-cc5b46b1ea43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# model(input_ids, attention_mask, labels)\r\n",
        "# model.train()\r\n",
        "# model(input_ids, attention_mask,labels)\r\n",
        "# fcl_output.view(-1,10).shape\r\n",
        "# labels=DiceCRF(10).decode(fcl_output,attention_mask)"
      ],
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        ...,\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True],\n",
              "        [True, True, True,  ..., True, True, True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 261
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XRHSAWN32dk",
        "outputId": "4a85d039-8411-4db4-fb86-39fb4bfd6405",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# train_dataloader, dev_dataloader\r\n",
        "from ignite.utils import setup_logger, convert_tensor\r\n",
        "from ignite.engine.engine import Engine\r\n",
        "from ignite.metrics import Fbeta, ConfusionMatrix, DiceCoefficient, Precision, Recall\r\n",
        "from ignite.handlers import ModelCheckpoint\r\n",
        "from ignite.metrics import Metric\r\n",
        "from ignite.contrib.handlers.tensorboard_logger import *\r\n",
        "\r\n",
        "\r\n",
        "model.train()\r\n",
        "param_optimizer = list(model.named_parameters())\r\n",
        "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\r\n",
        "optimizer_parameters = [\r\n",
        "    {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\r\n",
        "        \"weight_decay\": 0.001},\r\n",
        "    {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\r\n",
        "     \"weight_decay\": 0.0}\r\n",
        "     ]\r\n",
        "base_opt = AdamW(optimizer_parameters, lr=config.freeze_lr)\r\n",
        "optimizer = SWA(base_opt)\r\n",
        "\r\n",
        "def _prepare_batch(\r\n",
        "    batch: Sequence[torch.Tensor], device: Optional[Union[str, torch.device]] = None, non_blocking: bool = False\r\n",
        "):\r\n",
        "    \"\"\"Prepare batch for training: pass to a device with options.\"\"\"\r\n",
        "    x, y, z = batch.values()\r\n",
        "    return (convert_tensor(x, device=device, non_blocking=non_blocking),\r\n",
        "            convert_tensor(y, device=device, non_blocking=non_blocking),\r\n",
        "            convert_tensor(y, device=device, non_blocking=non_blocking))\r\n",
        "\r\n",
        "def create_supervised_trainer(\r\n",
        "    model: torch.nn.Module,\r\n",
        "    optimizer: torch.optim.Optimizer,\r\n",
        "    loss_fn: Optional[Union[Callable, torch.nn.Module]] = None,\r\n",
        "    device: Optional[Union[str, torch.device]] = None,\r\n",
        "    non_blocking: bool = False,\r\n",
        "    prepare_batch: Callable = _prepare_batch,\r\n",
        "    output_transform: Callable = lambda x, y, loss: loss.item(),\r\n",
        "    deterministic: bool = False) -> Engine:\r\n",
        "\r\n",
        "    device_type = device.type if isinstance(device, torch.device) else device\r\n",
        "    on_tpu = \"xla\" in device_type if device_type is not None else False\r\n",
        "\r\n",
        "    if on_tpu and not idist.has_xla_support:\r\n",
        "        raise RuntimeError(\"In order to run on TPU, please install PyTorch XLA\")\r\n",
        "\r\n",
        "    def _update(engine: Engine, batch: Sequence[torch.Tensor]) -> Union[Any, Tuple[torch.Tensor]]:\r\n",
        "        model.train()\r\n",
        "        optimizer.zero_grad()\r\n",
        "        input_ids, attention_mask, labels = prepare_batch(batch, device=device, non_blocking=non_blocking)\r\n",
        "        loss = model(input_ids, attention_mask, labels)\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        if on_tpu:\r\n",
        "            xm.optimizer_step(optimizer, barrier=True)\r\n",
        "        else:\r\n",
        "            optimizer.step()\r\n",
        "\r\n",
        "        return output_transform(input_ids, attention_mask, loss)\r\n",
        "\r\n",
        "    trainer = Engine(_update) if not deterministic else DeterministicEngine(_update)\r\n",
        "\r\n",
        "    return trainer\r\n",
        "\r\n",
        "def create_supervised_evaluator(\r\n",
        "    model: torch.nn.Module,\r\n",
        "    metrics: Optional[Dict[str, Metric]] = None,\r\n",
        "    device: Optional[Union[str, torch.device]] = None,\r\n",
        "    non_blocking: bool = False,\r\n",
        "    prepare_batch: Callable = _prepare_batch,\r\n",
        "    output_transform: Callable = lambda x, y, y_pred: (y_pred, y)) -> Engine:\r\n",
        "    metrics = metrics or {}\r\n",
        "\r\n",
        "    def _inference(engine: Engine, batch: Sequence[torch.Tensor]) -> Union[Any, Tuple[torch.Tensor]]:\r\n",
        "        model.eval()\r\n",
        "        with torch.no_grad():\r\n",
        "            input_ids, attention_mask, labels = prepare_batch(batch, device=device, non_blocking=non_blocking)\r\n",
        "            y_pred = model(input_ids, attention_mask)\r\n",
        "            return output_transform(x, y, y_pred)\r\n",
        "\r\n",
        "    evaluator = Engine(_inference)\r\n",
        "\r\n",
        "    for name, metric in metrics.items():\r\n",
        "        metric.attach(evaluator, name)\r\n",
        "\r\n",
        "    return evaluator\r\n",
        "\r\n",
        "trainer = create_supervised_trainer(model, optimizer, device=device)\r\n",
        "trainer.logger = setup_logger(\"Trainer\")\r\n",
        "metrics = {\"F1\": Fbeta(1), \"dice\": DiceCoefficient(ConfusionMatrix(num_classes=10)),\"p\": Precision(), 'r': Recall()}\r\n",
        "train_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\r\n",
        "train_evaluator.logger = setup_logger(\"Train Evaluator\")\r\n",
        "dev_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\r\n",
        "dev_evaluator.logger = setup_logger(\"Dev Evaluator\")\r\n",
        "\r\n",
        "@trainer.on(Events.EPOCH_COMPLETED)\r\n",
        "def compute_metrics(engine):\r\n",
        "    train_evaluator.run(train_dataloader)\r\n",
        "    dev_evaluator.run(dev_dataloader)\r\n",
        "\r\n",
        "tb_logger = TensorboardLogger(log_dir=config.log_dir)\r\n",
        "tb_logger.attach_output_handler(\r\n",
        "    trainer,\r\n",
        "    event_name=Events.ITERATION_COMPLETED(every=100),\r\n",
        "    tag=\"training\",\r\n",
        "    output_transform=lambda loss: {\"batchloss\": loss},\r\n",
        "    metric_names=\"all\",\r\n",
        ")\r\n",
        "for tag, evaluator in [(\"training\", train_evaluator), (\"validation\", dev_evaluator)]:\r\n",
        "    tb_logger.attach_output_handler(\r\n",
        "        evaluator,\r\n",
        "        event_name=Events.EPOCH_COMPLETED,\r\n",
        "        tag=tag,\r\n",
        "        metric_names=[\"F1\", \"dice\"],\r\n",
        "        global_step_transform=global_step_from_engine(trainer),\r\n",
        "    )\r\n",
        "\r\n",
        "def score_function(engine):\r\n",
        "    return engine.state.metrics[\"F1\"]\r\n",
        "\r\n",
        "model_checkpoint = ModelCheckpoint(\r\n",
        "    config.log_dir,\r\n",
        "    n_saved=2,\r\n",
        "    filename_prefix=\"best\",\r\n",
        "    score_function=score_function,\r\n",
        "    score_name=\"validation_accuracy\",\r\n",
        "    global_step_transform=global_step_from_engine(trainer),\r\n",
        ")\r\n",
        "\r\n",
        "dev_evaluator.add_event_handler(Events.COMPLETED, model_checkpoint, {\"model\": model})\r\n",
        "\r\n",
        "trainer.run(train_dataloader, max_epochs=config.freeze_epochs)\r\n",
        "tb_logger.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-08 04:25:47,378 Trainer INFO: Engine run starting with max_epochs=20.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:348: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py:132: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "  allow_unreachable=True)  # allow_unreachable flag\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBUoWkY3NQvp",
        "outputId": "e63c9e7c-06d5-4f67-d1c7-80f9cdeaa01a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pytorch-ignite\r\n",
        "# !pip uninstall ignite"
      ],
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Requirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.6/dist-packages (from pytorch-ignite) (1.7.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch<2,>=1.3->pytorch-ignite) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch<2,>=1.3->pytorch-ignite) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch<2,>=1.3->pytorch-ignite) (1.19.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX3_Gk5LvE5V"
      },
      "source": [
        "#exploration dice and crf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBS1GMx3tr74",
        "outputId": "31b86143-8a8d-410d-c401-b76ad13177a8"
      },
      "source": [
        "# dice_loss(fcl_output,labels,attention_mask)\r\n",
        "#b_s,s_l,n_l\r\n",
        "# pred_soft=torch.softmax(fcl_output,-1)#,torch.softmax(fcl_output[0,0,:],-1) #apply softmax to each token\r\n",
        "# target_one_hot=F.one_hot(labels,num_classes=config.num_labels)#,labels[0,:5] (b_s, s_l) -> (b_s, s_l, n_l)\r\n",
        "# pred_factor=((1-pred_soft) ** config.alpha) if config.self_adjusting else 1\r\n",
        "# pred_prod=pred_factor*pred_soft*target_one_hot\r\n",
        "# sum(pred_prod,0).shape\r\n",
        "# smooth = 1e-8\r\n",
        "# intersection=torch.sum(pred_prod,1)\r\n",
        "intersection.shape,pred_prod.shape\r\n",
        "# cardinality =torch.sum(pred_factor*pred_soft + target_one_hot, 1)\r\n",
        "# dice_score=1-2*(intersection+smooth)/(cardinality+smooth)\r\n",
        "# dice_score[0,:]\r\n",
        "# weight=[0,0,0,0,0,0,0,1,0,0]\r\n",
        "# (dice_score[:2,:]*torch.tensor(torch.tensor(weight))).shape\r\n",
        "### torch.gather(pred_soft[0,:5],-1,index=labels[0,:5].unsqueeze(-1)) #returns the probability for the most likely example is this really needed?\r\n",
        "# target_one_hot.shape\r\n",
        "# labels.unsqueeze(2).shape,labels.shape\r\n",
        "# pred_soft[0,:5].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4, 10]), torch.Size([4, 128, 10]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJXqCtg-Eclv"
      },
      "source": [
        "# crf_score=DiceCRF(10)(fcl_output,labels,attention_mask,'mean')\r\n",
        "# crf_score\r\n",
        "DiceCRF(10).decode(fcl_output,attention_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5oDqH1cFjiv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ef36cae-19ea-4e27-9ce6-eef752f5fe39"
      },
      "source": [
        "def validate(\r\n",
        "        h: torch.Tensor,\r\n",
        "        labels: Optional[torch.LongTensor] = None,\r\n",
        "        mask: Optional[torch.ByteTensor] = None) -> None:\r\n",
        "    if h.dim() != 3:\r\n",
        "        raise ValueError(f'h must have dimension of 3, got {h.dim()}')\r\n",
        "    if h.size(2) != config.num_labels:\r\n",
        "        raise ValueError(\r\n",
        "            f'expected last dimension of h is {config.num_labels}, '\r\n",
        "            f'got {h.size(2)}')\r\n",
        "    if labels is not None:\r\n",
        "        if h.shape[:2] != labels.shape:\r\n",
        "            raise ValueError(\r\n",
        "                'the first two dimensions of h and labels must match, '\r\n",
        "                f'got {tuple(h.shape[:2])} and {tuple(labels.shape)}')\r\n",
        "    if mask is not None:\r\n",
        "        if h.shape[:2] != mask.shape:\r\n",
        "            raise ValueError(\r\n",
        "                'the first two dimensions of h and mask must match, '\r\n",
        "                f'got {tuple(h.shape[:2])} and {tuple(mask.shape)}')\r\n",
        "# validate(fcl_output,labels) #why must x be the fcl output?\r\n",
        "\r\n",
        "# num_labels=10\r\n",
        "# trans_matrix = nn.Parameter(torch.empty(num_labels, num_labels))\r\n",
        "# start_trans = nn.Parameter(torch.empty(num_labels))\r\n",
        "# end_trans = nn.Parameter(torch.empty(num_labels))\r\n",
        "# def _initialize_parameters(pad_idx: Optional[int]) -> None:\r\n",
        "#       \"\"\"\r\n",
        "#       initialize transition parameters\r\n",
        "#       :param: pad_idx: if not None, additional initialize\r\n",
        "#       :return: None\r\n",
        "#       \"\"\"\r\n",
        "#       nn.init.uniform_(trans_matrix, -0.1, 0.1)\r\n",
        "#       nn.init.uniform_(start_trans, -0.1, 0.1)\r\n",
        "#       nn.init.uniform_(end_trans, -0.1, 0.1)\r\n",
        "#       punct=torch.arange(1,trans_matrix.shape[0])\r\n",
        "#       # inaccurate since consecutive punctuation is possible (\"Hello! Bye!\")\r\n",
        "#       # with torch.no_grad():\r\n",
        "#       #   self.trans_matrix[punct,punct]=-100\r\n",
        "#       if pad_idx is not None:\r\n",
        "#           start_trans[pad_idx] = -10000.0\r\n",
        "#           trans_matrix[pad_idx, :] = -10000.0\r\n",
        "#           trans_matrix[:, pad_idx] = -10000.0\r\n",
        "#           trans_matrix[pad_idx, pad_idx] = 0.0\r\n",
        "# _initialize_parameters(None)\r\n",
        "\r\n",
        "def _compute_numerator_log_likelihood(\r\n",
        "    h: FloatTensor, y: LongTensor, mask: ByteTensor\r\n",
        ") -> FloatTensor:\r\n",
        "    \"\"\"\r\n",
        "    compute the numerator term for the log-likelihood\r\n",
        "    :param h: hidden matrix (batch_size, seq_len, num_labels)\r\n",
        "    :param y: answer labels of each sequence\r\n",
        "              in mini batch (batch_size, seq_len)\r\n",
        "    :param mask: mask tensor of each sequence\r\n",
        "                  in mini batch (batch_size, seq_len)\r\n",
        "    :return: The score of numerator term for the log-likelihood\r\n",
        "    \"\"\"\r\n",
        "    batch_size, seq_len, _ = h.size()\r\n",
        "    h_unsqueezed = h.unsqueeze(-1)\r\n",
        "    trans = trans_matrix.unsqueeze(-1)\r\n",
        "    arange_b = torch.arange(batch_size)\r\n",
        "    # extract first vector of sequences in mini batch\r\n",
        "    last_mask_index = mask.sum(1) - 1\r\n",
        "    calc_range = seq_len - 1 #should calc_range be last_mask_index? No since parallel faster\r\n",
        "    # calc_range = last_mask_index\r\n",
        "    score = start_trans[y[:, 0]] + sum(\r\n",
        "        [_calc_trans_score_for_num_llh(\r\n",
        "            h_unsqueezed, y, trans, mask, t, arange_b\r\n",
        "        ) for t in range(calc_range)])\r\n",
        "    # extract end label number of each sequence in mini batch\r\n",
        "    # (batch_size)\r\n",
        "    last_labels = y[arange_b,last_mask_index]\r\n",
        "    each_last_score = h[arange_b, -1, last_labels].squeeze(-1) * mask[:, -1]\r\n",
        "    # Add the score of the sequences of the maximum length in mini batch\r\n",
        "    # Add the scores from the last tag of each sequence to EOS\r\n",
        "    score += each_last_score + end_trans[last_labels]\r\n",
        "    return score\r\n",
        "def _calc_trans_score_for_num_llh(\r\n",
        "    h: FloatTensor,\r\n",
        "    y: LongTensor,\r\n",
        "    trans: FloatTensor,\r\n",
        "    mask: ByteTensor,\r\n",
        "    t: int,\r\n",
        "    arange_b: FloatTensor,\r\n",
        ") -> torch.Tensor:\r\n",
        "    \"\"\"\r\n",
        "    calculate transition score for computing numerator llh\r\n",
        "    :param h: hidden matrix (batch_size, seq_len, num_labels)\r\n",
        "    :param y: answer labels of each sequence\r\n",
        "              in mini batch (batch_size, seq_len)\r\n",
        "    :param trans: transition score\r\n",
        "    :param mask: mask tensor of each sequence\r\n",
        "                  in mini batch (batch_size, seq_len)\r\n",
        "    :paramt t: index of hidden, transition, and mask matrixex\r\n",
        "    :param arange_b: this param is seted torch.arange(batch_size)\r\n",
        "    :param batch_size: batch size of this calculation\r\n",
        "    \"\"\"\r\n",
        "    device = h.device\r\n",
        "    mask_t = mask[:, t]\r\n",
        "    mask_t = mask_t.to(device)\r\n",
        "    mask_t1 = mask[:, t + 1]\r\n",
        "    mask_t1 = mask_t1.to(device)\r\n",
        "    # extract the score of t label\r\n",
        "    # (batch_size)\r\n",
        "    h_t = h[arange_b, t, y[:, t]].squeeze(1)  ##Changed from t to t\r\n",
        "    # extract the transition score from t-th label to t+1 label\r\n",
        "    # (batch_size)\r\n",
        "    trans_t = trans[y[:, t], y[:, t + 1]].squeeze(1)\r\n",
        "    # add the score of t+1 and the transition score\r\n",
        "    # (batch_size)\r\n",
        "    return h_t * mask_t + trans_t * mask_t1\r\n",
        "\r\n",
        "# numerator_log_likelihood=_compute_numerator_log_likelihood(fcl_output,labels,attention_mask.byte())\r\n",
        "# numerator_log_likelihood\r\n",
        "def _compute_denominator_log_likelihood(h: FloatTensor, mask: ByteTensor):\r\n",
        "        device = h.device\r\n",
        "        batch_size, seq_len, _ = h.size()\r\n",
        "        # (num_labels, num_labels) -> (1, num_labels, num_labels)\r\n",
        "        trans = trans_matrix.unsqueeze(0)\r\n",
        "        # add the score from beginning to each label\r\n",
        "        # and the first score of each label\r\n",
        "        score = start_trans + h[:, 0]\r\n",
        "        # iterate through processing for the number of words in the mini batch\r\n",
        "        for t in range(1, seq_len):\r\n",
        "            before_score = score.unsqueeze(2) # (batch_size, num_labels) -> (batch_size, num_labels, 1)\r\n",
        "            # prepare t-th mask of sequences in each sequence\r\n",
        "            # (batch_size, 1)\r\n",
        "            mask_t = mask[:, t].unsqueeze(1)\r\n",
        "            mask_t = mask_t.to(device)\r\n",
        "            # prepare the transition probability of the t-th sequence label\r\n",
        "            # in each sequence\r\n",
        "            # (batch_size, 1, num_labels)\r\n",
        "            h_t = h[:, t].unsqueeze(1)\r\n",
        "            # calculate t-th scores in each sequence\r\n",
        "            # (batch_size, num_labels)\r\n",
        "            score_t = before_score + h_t + trans\r\n",
        "            score_t = torch.logsumexp(score_t, 1)\r\n",
        "            # update scores\r\n",
        "            # (batch_size, num_labels)\r\n",
        "            score = torch.where(mask_t, score_t, score)\r\n",
        "        print(score)\r\n",
        "        # add the end score of each label\r\n",
        "        score += end_trans\r\n",
        "        print(score)\r\n",
        "        # return the log likelihood of all data in mini batch\r\n",
        "        return torch.logsumexp(score, 1)\r\n",
        "# denominator_log_likelihood=_compute_denominator_log_likelihood(fcl_output,attention_mask.byte())\r\n",
        "# denominator_log_likelihood\r\n",
        "\r\n",
        "# llh=numerator_log_likelihood - denominator_log_likelihood\r\n",
        "numerator_log_likelihood"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([46.9585, 50.8604, 49.0791, 51.3038], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKQ9o4-hvJJS"
      },
      "source": [
        "#-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4urD6kW9kE2l"
      },
      "source": [
        "### engine.py\n",
        "# from tqdm import tqdm\n",
        "def train_step(trainer,batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    batch=[_data.to(device) for _data in batch]\n",
        "    _, loss = model(batch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    return loss.item()\n",
        "\n",
        "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
        "    model.train()\n",
        "    final_loss = 0\n",
        "    for data in tqdm(data_loader, total=len(data_loader)):\n",
        "        if torch.cuda.is_available(): data=[_data.to(device) for _data in data]\n",
        "        optimizer.zero_grad()\n",
        "        _, loss = model(data)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        final_loss += loss.item()\n",
        "    return final_loss / len(data_loader)\n",
        "\n",
        "\n",
        "def eval_fn(data_loader, model, device):\n",
        "    model.eval()\n",
        "    final_loss = 0\n",
        "    for data in tqdm(data_loader, total=len(data_loader)):\n",
        "        if torch.cuda.is_available(): data=[_data.to(device) for _data in data]\n",
        "        punct, loss = model(data)\n",
        "        final_loss += loss.item()\n",
        "    return final_loss / len(data_loader)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT1awx_tkE2p",
        "outputId": "3971bea4-a88c-442c-e986-21395efa8750"
      },
      "source": [
        "model = BertCRFModel(num_punct=10, embedding_dim=config.EMBEDDING_DIM, hidden_dim=config.HIDDEN_DIM, use_crf=config.USE_CRF)\n",
        "for i,param in enumerate(model.bert.parameters()):\n",
        "    param.requires_grad = False\n",
        "model.to(device)\n",
        "optimizer = AdamW(optimizer_parameters, lr=config.FREEZE_LEARNING_RATE)\n",
        "num_train_steps = train_dataset.tensors[0].size()[0] / config.TRAIN_BATCH_SIZE * config.UNFREEZE_EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_train_steps\n",
        ")\n",
        "# optimizer = SWA(base_opt)\n",
        "\n",
        "trainer = Engine(train_step)\n",
        "val_metrics = {\n",
        "    \"precision\": Precision(),\n",
        "    \"recall\": Recall(),\n",
        "#     \"Dice\": DiceCoefficient(cm=),\n",
        "    \"F1\": Fbeta(1),\n",
        "}\n",
        "evaluator = create_supervised_evaluator(model, metrics=val_metrics)\n",
        "def log_metrics(engine, title):\n",
        "    print(\"Epoch: {} - {} accuracy: {:.2f}\"\n",
        "           .format(trainer.state.epoch, title, engine.state.metrics[\"acc\"]))\n",
        "\n",
        "@trainer.on(Events.EPOCH_COMPLETED)\n",
        "def evaluate(trainer):\n",
        "    with evaluator.add_event_handler(Events.COMPLETED, log_metrics, \"train\"):\n",
        "        evaluator.run(train_dataloader)\n",
        "\n",
        "    with evaluator.add_event_handler(Events.COMPLETED, log_metrics, \"dev\"):\n",
        "        evaluator.run(dev_dataloader)\n",
        "\n",
        "trainer.run(train_dataloader, max_epochs=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertModel: ['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Current run is terminating due to exception: too many values to unpack (expected 2).\n",
            "Engine run is terminating due to exception: too many values to unpack (expected 2).\n",
            "Engine run is terminating due to exception: too many values to unpack (expected 2).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-5d0c641fc8ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Engine run is terminating due to exception: %s.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    735\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTERMINATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhandlers_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m                 \u001b[0;31m# update time wrt handlers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_fire_event\u001b[0;34m(self, event_name, *event_args, **event_kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                 \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mothers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                 \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mothers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-70-5d0c641fc8ec>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(trainer)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_event_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOMPLETED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_event_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOMPLETED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dev\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Engine run is terminating due to exception: %s.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    728\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Current run is terminating due to exception: %s.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    809\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/ignite/engine/__init__.py\u001b[0m in \u001b[0;36m_inference\u001b[0;34m(engine, batch)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/ignite/engine/__init__.py\u001b[0m in \u001b[0;36m_prepare_batch\u001b[0;34m(batch, device, non_blocking)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \"\"\"\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     return (\n\u001b[1;32m     36\u001b[0m         \u001b[0mconvert_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "n3AxdqJJkE2p"
      },
      "source": [
        "torch.utils.data.DataLoader??"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}