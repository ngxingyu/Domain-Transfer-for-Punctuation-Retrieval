@misc{
	dynamiccrf,
	author = {W. Lu and H.T. Ng},
	title = {Better punctuation prediction with dynamic conditional random fields},
	year = {2010},
	abstract = {This paper focuses on the task of inserting punctuation symbols into transcribed conversational speech texts, without relying on prosodic cues. We investigate limitations associated with previous methods, and propose a novel approach based on dynamic conditional random fields. Different from previous work, our proposed approach is designed to jointly perform both sentence boundary and sentence type prediction, and punctuation prediction on speech utterances. We performed evaluations on a transcribed conversational speech domain consisting of both English and Chinese texts. Empirical results show that our method outperforms an approach based on linear-chain conditional random fields and other previous approaches. © 2010 Association for Computational Linguistics.}
}

@inproceedings{targetshiftadv,
  title={On target shift in adversarial domain adaptation},
  author={Li, Yitong and Murias, Michael and Major, Samantha and Dawson, Geraldine and Carlson, David},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={616--625},
  year={2019},
  organization={PMLR}
}

@article{jointlearningcorrbirnn, title={Joint Learning of Correlated Sequence Labeling Tasks Using Bidirectional Recurrent Neural Networks}, DOI={10.21437/interspeech.2017-1247}, journal={Interspeech 2017}, author={Pahuja, Vardaan and Laha, Anirban and Mirkin, Shachar and Raykar, Vikas and Kotlerman, Lili and Lev, Guy}, year={2017}, month={Jul}} 

@inproceedings{birnnattention,
  title={Bidirectional Recurrent Neural Network with Attention Mechanism for Punctuation Restoration},
  author={Ottokar Tilk and Tanel Alum{\"a}e},
  booktitle={INTERSPEECH},
  year={2016}
}

@article{kim_2019, title={Deep Recurrent Neural Networks with Layer-wise Multi-head Attentions for Punctuation Restoration}, DOI={10.1109/icassp.2019.8682418}, journal={ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, author={Kim, Seokhwan}, year={2019}, month={Apr}} 

@article{chunkmerging, title={Fast and Accurate Capitalization and Punctuation for Automatic Speech Recognition Using Transformer and Chunk Merging}, DOI={10.1109/o-cocosda46868.2019.9041202}, journal={2019 22nd Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA)}, author={Nguyen, Binh and Nguyen, Vu Bao Hung and Nguyen, Hien and Phuong, Pham Ngoc and Nguyen, The-Loc and Do, Quoc Truong and Mai, Luong Chi}, year={2019}, month={Aug}}

@inproceedings{pandababa,  author={K. {Makhija} and T. {Ho} and E. {Chng}},  booktitle={2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)},   title={\href{https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9023200}{Transfer Learning for Punctuation Prediction}},   year={2019},  volume={},  number={},  pages={268-273},  note={Code available at \url{https://github.com/panda-baba/bert\_punct}}, doi={10.1109/APSIPAASC47483.2019.9023200}}

@INPROCEEDINGS{2009,
  author={A. {Gravano} and M. {Jansche} and M. {Bacchiani}},
  booktitle={2009 IEEE International Conference on Acoustics, Speech and Signal Processing}, 
  title={Restoring punctuation and capitalization in transcribed speech}, 
  year={2009},
  volume={},
  number={},
  pages={4741-4744},
  doi={10.1109/ICASSP.2009.4960690}}

@article{adversarial,
  title={Adversarial Transfer Learning for Punctuation Restoration},
  author={Jiangyan Yi and J. Tao and Ye Bai and Z. Tian and Cunhang Fan},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.00248}
}

@misc{bertcrf,
    title={Portuguese Named Entity Recognition using BERT-CRF},
    author={Fábio Souza and Rodrigo Nogueira and Roberto Lotufo},
    year={2019},
    eprint={1909.10649},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{chinesebertbilstm,
  title={Using bidirectional LSTM with BERT for Chinese punctuation prediction},
  author={Fang, Mingfeng and Zhao, Haifeng and Song, Xiao and Wang, Xin and Huang, Shilei},
  booktitle={2019 IEEE International Conference on Signal, Information and Data Processing (ICSIDP)},
  pages={1--5},
  year={2019},
  organization={IEEE}
}

@inproceedings{glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}

@article{floater,
  title={Learning to Encode Position for Transformer with Continuous Dynamical Model},
  author={Liu, Xuanqing and Yu, Hsiang-Fu and Dhillon, Inderjit and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:2003.09229},
  year={2020},
  note={Code available at \url{https://github.com/xuanqing94/FLOATER}}
}

@article{memnet,
  title={End-to-end memory networks},
  author={Sukhbaatar, Sainbayar and Weston, Jason and Fergus, Rob and others},
  journal={Advances in neural information processing systems},
  volume={28},
  pages={2440--2448},
  year={2015}
}

@conference{crf, title={{Conditional random fields: Probabilistic models for segmenting and labeling sequence data}}, author={Lafferty, J. and McCallum, A. and Pereira, F.}, booktitle={MACHINE LEARNING : INTERNATIONAL WORKSHOP THEN CONFERENCE }, pages={282—289}, year={2001}, organization={Citeseer} }

@article{crfhighorderdependencies,
  author  = {Nguyen Viet Cuong and Nan Ye and Wee Sun Lee and Hai Leong Chieu},
  title   = {Conditional Random Field with High-order Dependencies for Sequence Labeling and Segmentation},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {28},
  pages   = {981-1009},
  url     = {http://jmlr.org/papers/v15/cuong14a.html}
}

@article{hybridsemimarkovcrf,
  title={Hybrid semi-markov crf for neural sequence labeling},
  author={Ye, Zhi-Xiu and Ling, Zhen-Hua},
  journal={arXiv preprint arXiv:1805.03838},
  year={2018}
}

@inproceedings{grsemicrf,
  title={Segment-level sequence modeling using gated recursive semi-markov conditional random fields},
  author={Zhuo, Jingwei and Cao, Yong and Zhu, Jun and Zhang, Bo and Nie, Zaiqing},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1413--1423},
  year={2016}
}

@ARTICLE{dicescorejaccardindex,  author={T. {Eelbode} and J. {Bertels} and M. {Berman} and D. {Vandermeulen} and F. {Maes} and R. {Bisschops} and M. B. {Blaschko}},  journal={IEEE Transactions on Medical Imaging},   title={Optimization for Medical Image Segmentation: Theory and Practice When Evaluating With Dice Score or Jaccard Index},   year={2020},  volume={39},  number={11},  pages={3679-3690},  doi={10.1109/TMI.2020.3002417}}

@article{positionembedding,
  title={What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding},
  author={Wang, Yu-An and Chen, Yun-Nung},
  journal={arXiv preprint arXiv:2010.04903},
  year={2020}
}

@inproceedings{mecrf,
    title = "Capturing Long-range Contextual Dependencies with Memory-enhanced Conditional Random Fields",
    author = "Liu, Fei  and
      Baldwin, Timothy  and
      Cohn, Trevor",
    booktitle = "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://www.aclweb.org/anthology/I17-1056",
    pages = "555--565",
    abstract = "Despite successful applications across a broad range of NLP tasks, conditional random fields ({``}CRFs{''}), in particular the linear-chain variant, are only able to model local features. While this has important benefits in terms of inference tractability, it limits the ability of the model to capture long-range dependencies between items. Attempts to extend CRFs to capture long-range dependencies have largely come at the cost of computational complexity and approximate inference. In this work, we propose an extension to CRFs by integrating external memory, taking inspiration from memory networks, thereby allowing CRFs to incorporate information far beyond neighbouring steps. Experiments across two tasks show substantial improvements over strong CRF and LSTM baselines.",
}

@inproceedings{lstmbase,
  title={LSTM for punctuation restoration in speech transcripts},
  author={Tilk, Ottokar and Alum{\"a}e, Tanel},
  booktitle={Sixteenth annual conference of the international speech communication association},
  year={2015}
}

@inproceedings{memnet,   
  title={Memnet: A persistent memory network for image restoration},
  author={Tai, Ying and Yang, Jian and Liu, Xiaoming and Xu, Chunyan},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={4539--4547},
  year={2017}
}

@article{keung2019adversarial,
  title={Adversarial learning with contextual embeddings for zero-resource cross-lingual classification and NER},
  author={Keung, Phillip and Lu, Yichao and Bhardwaj, Vikas},
  journal={arXiv preprint arXiv:1909.00153},
  year={2019}
}

@article{adaptivenerunbalanceddata,
  title={Adaptive Name Entity Recognition under Highly Unbalanced Data},
  author={Nguyen, Thong and Nguyen, Duy and Rao, Pramod},
  journal={arXiv preprint arXiv:2003.10296},
  year={2020}
}

@inproceedings{attentionisallyouneed,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@inproceedings{focaltversky,
  title={A novel focal tversky loss function with improved attention u-net for lesion segmentation},
  author={Abraham, Nabila and Khan, Naimul Mefraz},
  booktitle={2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)},
  pages={683--687},
  year={2019},
  organization={IEEE}
}

@inproceedings{focallosspunct,
  author={Jiangyan Yi and Jianhua Tao and Zhengkun Tian and Ye Bai and Cunhang Fan},
  title={{Focal Loss for Punctuation Prediction}},
  year=2020,
  booktitle={Proc. Interspeech 2020},
  pages={721--725},
  doi={10.21437/Interspeech.2020-1638},
  url={http://dx.doi.org/10.21437/Interspeech.2020-1638}
}

@article{nagy2021automatic,
  title={Automatic punctuation restoration with BERT models},
  author={Nagy, Attila and Bial, Bence and {\'A}cs, Judit},
  journal={arXiv preprint arXiv:2101.07343},
  year={2021}
}

@inproceedings{efficientbertrobust,
    title = "Efficient Automatic Punctuation Restoration Using Bidirectional Transformers with Robust Inference",
    author = "Courtland, Maury  and
      Faulkner, Adam  and
      McElvain, Gayle",
    booktitle = "Proceedings of the 17th International Conference on Spoken Language Translation",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.iwslt-1.33",
    doi = "10.18653/v1/2020.iwslt-1.33",
    pages = "272--279",
    abstract = "Though people rarely speak in complete sentences, punctuation confers many benefits to the readers of transcribed speech. Unfortunately, most ASR systems do not produce punctuated output. To address this, we propose a solution for automatic punctuation that is both cost efficient and easy to train. Our solution benefits from the recent trend in fine-tuning transformer-based language models. We also modify the typical framing of this task by predicting punctuation for sequences rather than individual tokens, which makes for more efficient training and inference. Finally, we find that aggregating predictions across multiple context windows improves accuracy even further. Our best model achieves a new state of the art on benchmark data (TED Talks) with a combined F1 of 83.9, representing a 48.7{\%} relative improvement (15.3 absolute) over the previous state of the art.",
}

@inproceedings{DATnet,
  title={Dual adversarial neural transfer for low-resource named entity recognition},
  author={Zhou, Joey Tianyi and Zhang, Hao and Jin, Di and Zhu, Hongyuan and Fang, Meng and Goh, Rick Siow Mong and Kwok, Kenneth},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={3461--3471},
  year={2019}
}

@misc{TEDUltimate, title={TED – Ultimate Dataset}, author={Miguel Corral Jr.}} 

@misc{xyng_2021, title={Preprocessed English Spoken Transcripts}, url={https://www.kaggle.com/ds/1149348}, DOI={10.34740/KAGGLE/DS/1149348}, publisher={Kaggle}, author={Xing Yu Ng}, year={2021}}

@INPROCEEDINGS{translation,  author={F. {Wang} and W. {Chen} and Z. {Yang} and B. {Xu}},  booktitle={2018 24th International Conference on Pattern Recognition (ICPR)},   title={Self-Attention Based Network for Punctuation Restoration},   year={2018},  volume={},  number={},  pages={2803-2808},  doi={10.1109/ICPR.2018.8545470}}

@misc{speechtranslationrobust,
      title={Improving the Robustness of Speech Translation}, 
      author={Xiang Li and Haiyang Xue and Wei Chen and Yang Liu and Yang Feng and Qun Liu},
      year={2018},
      eprint={1811.00728},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{noisy,
    title = "Punctuation Restoration using Transformer Models for High-and Low-Resource Languages",
    author = "Alam, Tanvirul  and
      Khan, Akib  and
      Alam, Firoj",
    booktitle = "Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.wnut-1.18",
    doi = "10.18653/v1/2020.wnut-1.18",
    pages = "132--142",
    abstract = "Punctuation restoration is a common post-processing problem for Automatic Speech Recognition (ASR) systems. It is important to improve the readability of the transcribed text for the human reader and facilitate NLP tasks. Current state-of-art address this problem using different deep learning models. Recently, transformer models have proven their success in downstream NLP tasks, and these models have been explored very little for the punctuation restoration problem. In this work, we explore different transformer based models and propose an augmentation strategy for this task, focusing on high-resource (English) and low-resource (Bangla) languages. For English, we obtain comparable state-of-the-art results, while for Bangla, it is the first reported work, which can serve as a strong baseline for future work. We have made our developed Bangla dataset publicly available for the research community.",
}

@inproceedings{domainAdaptationBERT,
  title={Domain Adaptation with BERT-based Domain Classification and Data Selection},
  author={Ma, Xiaofei and Xu, Peng and Wang, Zhiguo and Nallapati, Ramesh and Xiang, Bing},
  booktitle={Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)},
  pages={76--83},
  year={2019}
}

@article{clim,
  title={Cross-Domain Sentiment Classification With Contrastive Learning and Mutual Information Maximization},
  author={Li, Tian and Chen, Xiang and Zhang, Shanghang and Dong, Zhen and Keutzer, Kurt},
  journal={arXiv preprint arXiv:2010.16088},
  year={2020}
}

@misc{rosvall2019comparison,
  title={Comparison of sequence classification techniques with BERT for named entity recognition},
  author={Rosvall, Erik},
  year={2019}
}

@article{tuneornottotune,
  title={To tune or not to tune? adapting pretrained representations to diverse tasks},
  author={Peters, Matthew E and Ruder, Sebastian and Smith, Noah A},
  journal={arXiv preprint arXiv:1903.05987},
  year={2019}
}

@article{robertaAcquireLinguisticPreference,
  title={Learning which features matter: Roberta acquires a preference for linguistic generalizations (eventually)},
  author={Warstadt, Alex and Zhang, Yian and Li, Haau-Sing and Liu, Haokun and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2010.05358},
  year={2020}
}

@misc{medicalasr,
      title={Robust Prediction of Punctuation and Truecasing for Medical ASR}, 
      author={Monica Sunkara and Srikanth Ronanki and Kalpit Dixit and Sravan Bodapati and Katrin Kirchhoff},
      year={2020},
      eprint={2007.02025},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{multimodalsemi,
      title={Multimodal Semi-supervised Learning Framework for Punctuation Prediction in Conversational Speech}, 
      author={Monica Sunkara and Srikanth Ronanki and Dhanush Bekal and Sravan Bodapati and Katrin Kirchhoff},
      year={2020},
      eprint={2008.00702},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}

@misc{li2020dice,
      title={Dice Loss for Data-imbalanced NLP Tasks}, 
      author={Xiaoya Li and Xiaofei Sun and Yuxian Meng and Junjun Liang and Fei Wu and Jiwei Li},
      year={2020},
      eprint={1911.02855},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}