\documentclass[a4paper]{article}
\usepackage[utf8x]{inputenc}
\usepackage[left=1.5in,right=1.5in,top=1in,bottom=1in]{geometry}
\usepackage{verbatim} %comments
\usepackage{natbib}
\bibliographystyle{apalike}

\usepackage{hyperref}
\title{Review of research conducted for Punctuation Retrieval.}
\author{Ng Xing Yu}
\date{}
\begin{document}
\maketitle

\section{Introduction}
\label{introduction}
Punctuation Retrieval is an important aspect in any Automatic Speech Recognition (ASR) pipeline for two reasons: to improve readability of auto-generated transcripts for videos or podcast subtitling or voice dictation applications, and to better capture the meaning of speech transcripts to improve the performance of downstream Natural Language Processing (NLP) tasks.

This review will look into the ideas taken into account by different authors and discuss potential areas of exploration to improve ASR output.

\section{Punctuation Features}
The majority of research into punctuation retrieval on English speech transcripts condense all punctuation into four classes --- (Period .), (Comma ,), (Question Mark ?) and (None), using a custom-defined mapping function to replace other punctuation with the four classes. This is done to combat the issue of an imbalance in punctuation occurrence in most datasets, with less frequent punctuation like semicolons or dashes occurring under 1\% in the entire \href{http://opus.nlpl.eu/OpenSubtitles-v2016.php}{OpenSubtitles v2016 english corpus}. An earlier paper by \cite{dynamiccrf} included the (Exclamation mark !) class, but did not comment on the performance of prediction on the less common classes. 
The paper by \cite{birnnattention} also used a different mapping scheme for the two languages evaluated --- Estonian and English, with the following differences:
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
 &\multicolumn{2}{|c|}{Mapped to} \\
 \hline
From & Estonian & English \\
\hline
; ! & Period . & Period . \\
: & Period . & Comma , \\
- --- & None & Comma ,\\
\hline
\end{tabular}
\end{center}
While these mappings are logical, with (?) and (!) marking sentence boundaries and the emdash (---) indicating a break in sentence structure, the mapping would result in some meaning and context being lost. Hence, expanding the set of punctuation classes to all punctuation which can be inferred from a speech, for instance \{! , - . : ; ? --- \textellipsis "\} for English, would improve the readibility of the ASR output and allow the text to better represent the meaning within the audio input.
\section{Data}
The data sources for training punctuation retrieval tasks can be categorised into purely textual, and aligned audio with text.
The most common source used for training punctuation retrieval is the IWSLT dataset - many versions of which features transcripts from TED-talks and OpenSubtitles. This textual datasource is used to train a model that only uses lexical features for punctuation retrieval. \cite{jointlearningcorrbirnn} also used transcripts from the Intelligence squared debate show. \cite{medicalasr} used an internal medical speech transcript dataset to demonstrate the ability of their model to transfer to a domain with large vocabulary and data scarcity.

\textbf{Use of non-lexical features} \cite{multimodalsemi} used the audio and text data from the Fisher corpus in training a model using both acoustic and lexical features. \cite{adversarial} also made use of the Penn Treebank-3 dataset to train a POS tagger to improve the performance of the punctuation retrieval class. \cite{birnnattention} made use of Estonian text annotated with pause duration (e.g. $\langle sil=0.030\rangle$ for 30 milliseconds of silence) 

\textbf{Data Augmentation} In performing ASR, the generated text from speech might contain word errors in the form of insertions, deletions, and substitutions, which would affect the accuracy of the downstream punctuation retrieval task. To make the model more robust to such word errors, \cite{noisy} simulated the three forms of word errors, inserting or substituting existing tokens with the unknown token and randomly deleting tokens. Their RoBERTa-large model obtained higher F1 scores on the test set --- ASR output generated by the Google Cloud Speech API to simulate word errors --- when trained on augmented text, demonstrating the effectiveness of introducing noise into the training data in improving the model's robustness to word errors. 

\cite{speechtranslationrobust} presented four strategies to generate noise for ASR training: Substitution with placeholder, substitution with random token, substitution with more frequent character, and substitution with homophone based character. Being trained and evaluated on a Chinese corpus, many homophonous words have different meaning. The proposed strategies proved effective in improving speech to text translation and can be adapted to the field of punctuation retrieval.

Other possible noise injection techniques include splitting of words into subwords or homophone or random substitution/insertion/deletion at the subword level.

\section{Models}
There are various models that performed well on the task of punctuation retrieval. 
The model by \cite{translation} treats punctuation retrieval as a token labelling task which assigns a punctuation label to each token in the sequence.
\subsection{Pre-trained models}
Glove and BERT
\subsection{RNN/Bidirectional RNN/Multi-layered RNN}
\subsection{Conditional Random Fields}

\subsection{Multi-task learning}
Punctuation with POS-tagger, adversarial learning

Capitalization

Sentence Boundary \cite{dynamiccrf}

Use of acoustic features


\section{Training process}
\subsection{Chunking}
\subsection{Objective}
Sørensen–Dice coefficient / F1 Score
Cross-entropy

% \section*{}

\bibliography{asr.bib}

\end{document}