seed: 42
trainer:
    gpus: 1 # the number of gpus, 0 for CPU
    num_nodes: 1
    max_epochs: 8
    max_steps: null # precedence over max_epochs
    accumulate_grad_batches: 4 # accumulates grads every k batches
    gradient_clip_val: 0
    amp_level: O1 # O1/O2 for mixed precision
    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
    accelerator: ddp
    checkpoint_callback: false  # Provided by exp_manager
    logger: false #false  # Provided by exp_manager
    log_every_n_steps: 1  # Interval of logging.
    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
    resume_from_checkpoint: null

    # gpus: 0 # the number of gpus, 0 for CPU
    # num_nodes: 1
    # max_epochs: 10
    # max_steps: null # precedence over max_epochs
    # accumulate_grad_batches: 4 # accumulates grads every k batches
    # gradient_clip_val: 0
    # amp_level: O0 # O1/O2 for mixed precision
    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
    # # accelerator: ddp
    # checkpoint_callback: false  # Provided by exp_manager
    # logger: false #false  # Provided by exp_manager
    # log_every_n_steps: 1  # Interval of logging.
    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
    # reload_dataloaders_every_epoch: true
    # resume_from_checkpoint: null

exp_manager:
    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
    name: Punctuation_with_Domain_discriminator  # The name of your model
    create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
    create_checkpoint_callback: true 
base_path: /root/data # /home/nxingyu2/data # 
tmp_path: /tmp # /home/nxingyu2/data/tmp # 

model:
    nemo_path: null
    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
    unfrozen: 0
    maximum_unfrozen: 2
    unfreeze_step: 1
    # unfreeze_every: 3
    punct_label_ids:
        - ""
        - "!"
        - ","
        - "-"
        - "."
        - ":"
        - ";"
        - "?"
        - "—"
        - "…"

    punct_class_weights: true
    
    dataset:
        data_dir: /root/data # /home/nxingyu2/data # 
        labelled:
            - ${base_path}/ted_talks_processed #
        unlabelled:
            # - ${base_path}/open_subtitles_processed #  
            # parameters for dataset preprocessing
        max_seq_length: 128
        pad_label: ''
        ignore_extra_tokens: false
        ignore_start_end: false
        use_cache: false
        # shared among dataloaders
        num_workers:  0
        pin_memory: true
        drop_last: false
        num_labels: 10
        num_domains: 1
        test_unlabelled: true

        train_ds:
            shuffle: true
            num_samples: -1
            batch_size: 8

        validation_ds:
            # if evaluation data is not in the model.dataset.data_dir as the training data or multiple datasets are used for
            # evaluationis needed, specify ds_item, otherwise by default data_dir is used
            # ds_item: null # expected format: [PATH_TO_DEV1,PATH_TO_DEV2] (Note no space between the paths and square brackets)
            shuffle: true
            num_samples: -1
            batch_size: 8

    tokenizer:
        tokenizer_name: ${model.language_model.pretrained_model_name} # or sentencepiece
        vocab_file: null # path to vocab file
        tokenizer_model: null # only used if tokenizer is sentencepiece
        special_tokens: null

    language_model:
        pretrained_model_name: ${model.transformer_path} # bert-base-uncased
        lm_checkpoint: null
        config_file: null # json file, precedence over config
        config: null
        # unfrozen_layers: 1

    punct_head:
        punct_num_fc_layers: 1
        fc_dropout: 0.1
        activation: 'relu'
        log_softmax: false
        use_transformer_init: true
        loss: 'dice'

    domain_head:
        domain_num_fc_layers: 1
        fc_dropout: 0.1
        activation: 'relu'
        log_softmax: false
        use_transformer_init: true
        loss: 'cel'
        gamma: 0.1 # coefficient of gradient reversal
    
    dice_loss:
        epsilon: 0.01
        alpha: 5
        macro_average: true

    focal_loss: 
        gamma: 5

    optim:
        name: adamw
        lr: 1e-3
        weight_decay: 0.00

        sched:
            name: WarmupAnnealing #CyclicLR
            # Scheduler params
            warmup_steps: null
            warmup_ratio: 0.1
            last_epoch: -1

            # pytorch lightning args
            monitor: val_loss
            reduce_on_plateau: true
hydra:
    run:
        dir: .
    job_logging:
        formatters:
            simple:
                format: '[%(levelname)s] - %(message)s'
        handlers:
          console:
            class: logging.StreamHandler
            formatter: simple
            stream: ext://sys.stdout
          file:
            class: logging.FileHandler
            level: INFO
            formatter: simple
            filename: info.log
            encoding: utf8
            mode: w
        root:
          handlers: [console, file]
