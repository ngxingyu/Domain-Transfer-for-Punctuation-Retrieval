seed: 42
trainer:
    gpus: 1 # the number of gpus, 0 for CPU
    num_nodes: 1
    max_epochs: 8
    max_steps: null # precedence over max_epochs
    accumulate_grad_batches: 2 # accumulates grads every k batches
    gradient_clip_val: 10
    amp_level: O1 # O1/O2 for mixed precision
    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
    accelerator: ddp
    checkpoint_callback: false  # Provided by exp_manager
    logger: false #false  # Provided by exp_manager
    log_every_n_steps: 1  # Interval of logging.
    val_check_interval: 1.0 #0.2  # Set to 0.25 to check 4 times per epoch, 1.0 for normal, or an int for number of iterations
    resume_from_checkpoint: null

    # gpus: 0 # the number of gpus, 0 for CPU
    # num_nodes: 1
    # max_epochs: 1
    # max_steps: null # precedence over max_epochs
    # accumulate_grad_batches: 4 # accumulates grads every k batches
    # gradient_clip_val: 0
    # amp_level: O0 # O1/O2 for mixed precision
    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
    # # accelerator: ddp
    # checkpoint_callback: false # Provided by exp_manager
    # logger: false #false  # Provided by exp_manager
    # log_every_n_steps: 1  # Interval of logging.
    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
    # reload_dataloaders_every_epoch: true
    # resume_from_checkpoint: null

exp_manager:
    exp_dir: /home/${env:USER}/project/ # /root/project # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
    name: Punctuation_with_Domain_discriminator  # The name of your model
    create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
    create_checkpoint_callback: true 
base_path: /home/${env:USER}/data # /root/data # 
tmp_path: /home/${env:USER}/data/tmp # /tmp # 
log_dir: null

model:
    nemo_path: null
    transformer_path: google/electra-base-discriminator # roberta-base #google/electra-base-discriminator # distilbert-base-uncased # filename to save the model and associated artifacts to .nemo file
    unfrozen: 0
    maximum_unfrozen: 1
    unfreeze_step: 1
    test_every_layer: false
    punct_label_ids:
        - ""
        - ","
        - "."
        - "?"

        # - "!"
        # - "-"
        # - "…"
        # - ":"

        # - "—"
        # - ";"

    label_map:

        "!": "."
        "-": ","
        ":": ","
        "…": "."

        "—": ","
        ";": "."

        
    no_space_label: '#'
    test_chunk_percent: 0.5
    punct_class_weight_factor: 0.6
    punct_class_weights:  #0 if don't weight
    
    dataset:
        data_dir: /home/${env:USER}/data # /root/data # 
        labelled:
            # - ${base_path}/ted2010 #
            - ${base_path}/ted_talks_processed #
            - ${base_path}/open_subtitles_processed #  
            # - ${base_path}/switchboardutt_processed #
        unlabelled:
            # - ${base_path}/ted_talks_processed #
            # - ${base_path}/open_subtitles_processed #  
            - ${base_path}/switchboardutt_processed
        # parameters for dataset preprocessing
        max_seq_length: 128
        pad_label: ''
        ignore_extra_tokens: false
        ignore_start_end: false
        use_cache: false
        # shared among dataloaders
        num_workers:  8
        pin_memory: false
        drop_last: true
        num_labels: 9
        num_domains: 2
        test_unlabelled: true
        attach_label_to_end: #None # false if attach to start none if dont mask
        pad_start: 0
        train_ds:
            shuffle: true
            num_samples: -1
            batch_size: 12
            manual_len: 8000 #default 0 84074

        validation_ds:
            # if evaluation data is not in the model.dataset.data_dir as the training data or multiple datasets are used for
            # evaluationis needed, specify ds_item, otherwise by default data_dir is used
            # ds_item: null # expected format: [PATH_TO_DEV1,PATH_TO_DEV2] (Note no space between the paths and square brackets)
            shuffle: true
            num_samples: -1
            batch_size: 12 #4

    tokenizer:
        tokenizer_name: ${model.language_model.pretrained_model_name} # or sentencepiece
        vocab_file: null # path to vocab file
        tokenizer_model: null # only used if tokenizer is sentencepiece
        special_tokens: null

    language_model:
        pretrained_model_name: ${model.transformer_path} # bert-base-uncased
        lm_checkpoint: null
        config_file: null # json file, precedence over config
        config: null
        # unfrozen_layers: 1
    
    punct_head:
        punct_num_fc_layers: 3
        fc_dropout: 0.1
        activation: 'gelu'
        log_softmax: false
        use_transformer_init: true
        loss: 'dice'
        bilstm: false

    domain_head:
        predict_labelled: false # if false: treats every domain separately, if true: splits into labelled and unlabelled
        domain_num_fc_layers: 1
        fc_dropout: 0.1
        activation: 'relu'
        log_softmax: false
        use_transformer_init: true
        loss: 'cel'
        gamma: 
        gamma_factor: 1.0 #0.1 # coefficient of gradient reversal
        pooling: 'mean_max' # 'mean' # 'mean_max' # 'token'
        idx_conditioned_on: 0
        weight: # ensure length is equal to number of domains if predict_labelled is false. 
            - 1
            - 1
            - 1
            # - 0.3
            # - 0.3
            # - 0.4


    dice_loss:
        epsilon: 0.01
        alpha: 1
        macro_average: true

    focal_loss: 
        gamma: 2

    frozen_lr:
        - 1e-2
        - 1e-3
        - 1e-4
        - 1e-5
        - 1e-5
        - 1e-7
        - 1e-8

    # gamma:
    #     - 1
    #     - 1
    #     - 1
    #     - 1
    #     - 1
        # - 1e-0
        # - 5e-1
        # - 4e-1
        # - 1e-4
        # - 1e-5
        
    optim:
        name: adamw #novograd #adamw
        lr: 1e-2 #1e-3
        weight_decay: 0.00
        sched:
            # name: CyclicLR
            # base_lr: 1e-5
            # max_lr: 1e-1
            # mode: 'triangular2'
            # last_epoch: -1

            name: CosineAnnealing #CosineAnneali3ng #WarmupAnnealing #CyclicLR
            # Scheduler params
            warmup_steps: null
            warmup_ratio: 0.1
            min_lr: 1e-8
            # hold_steps: 6
            last_epoch: -1

            # pytorch lightning args
            monitor: val_loss
            reduce_on_plateau: false
hydra:
    run:
        dir: .
    job_logging:
        formatters:
            simple:
                format: '[%(levelname)s] - %(message)s'
        handlers:
            console:
                class: logging.StreamHandler
                formatter: simple
                stream: ext://sys.stdout
            file:
                class: logging.FileHandler
                level: INFO
                formatter: simple
                filename: info.log
                encoding: utf8
                mode: w
        root:
            handlers: [console, file]
