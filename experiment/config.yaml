seed: 42 # pytorch lightning seed_everything
inference: false
trainer:
    # For GPU
    # gpus: 1 # the number of gpus, 0 for CPU
    # num_nodes: 1
    # max_epochs: 4 # Number of epoch every unfreeze
    # max_steps: null # precedence over max_epochs
    # accumulate_grad_batches: 2 # accumulates grads every k batches
    # gradient_clip_val: 10
    # amp_level: O1 # O1/O2 for mixed precision
    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
    # accelerator: ddp
    # checkpoint_callback: false  # Provided by exp_manager
    # logger: false #false  # Provided by exp_manager
    # log_every_n_steps: 1  # Interval of logging.
    # val_check_interval: 1.0 #0.2  # Set to 0.25 to check 4 times per epoch, 1.0 for normal, or an int for number of iterations
    # resume_from_checkpoint: null

    ## For CPU
    gpus: 0 # the number of gpus, 0 for CPU
    num_nodes: 1
    max_epochs: 1
    max_steps: null # precedence over max_epochs
    accumulate_grad_batches: 4 # accumulates grads every k batches
    gradient_clip_val: 0
    amp_level: O0 # O1/O2 for mixed precision
    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
    # accelerator: ddp
    checkpoint_callback: false # Provided by exp_manager
    logger: false #false  # Provided by exp_manager
    log_every_n_steps: 1  # Interval of logging.
    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
    reload_dataloaders_every_epoch: true
    resume_from_checkpoint: null

exp_manager:
    exp_dir: /root/project # /home/${env:USER}/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
    name: Punctuation_with_Domain_discriminator  # The name of your model
    create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
    create_checkpoint_callback: true 
base_path: /root/data # /home/${env:USER}/data # location of data source files
tmp_path: /tmp # /home/${env:USER}/data/tmp # location of tmp directory to store the tmp training files (in case /tmp out of space)
log_dir: null

model:
    nemo_path: null
    transformer_path: bert-base-uncased # google/electra-base-discriminator # distilbert-base-uncased # filename to save the model and associated artifacts to .nemo file
    unfrozen: 6                 # Initial number of layers unfrozen
    maximum_unfrozen: 6         # Maximum number of layers unfrozen.
    unfreeze_step: 2            # Number of layers unfreezed at once after each epoch
    punct_label_ids:
        - ""
        - ","
        - "."
        - "?"

        # - "!"
        # - "-"
        # - "…"
        # - ":"

        # - "—"
        # - ";"

    label_map: # Dict containing ( original punctuation: replacement punctuation. )

        "-": ","
        "…": "."
        "!": "."
        ":": ","

        "—": ","
        ";": "."

        
    no_space_label: '' # default to #, will be appended to punct_label_ids
    test_chunk_percent: # Leave blank unless you want to test with x% chunked on test_epoch
    punct_class_weight_factor: 0 # 0 if don't weight
    punct_class_weights:  # Leave blank.
    cat_domain_and_states: false # concat the domain pooled outputs with each hidden_state to feed into Punctuation Classifier.
    cat_domain_logits: true # concat the domain logits. if false, concat the pooled output.
    dataset:
        data_dir: /root/data # /home/${env:USER}/data # 
        labelled:
            - ${base_path}/ted2010_processed  #### IWSLT dataset
            # - ${base_path}/ted_talks_processed #
            # - ${base_path}/open_subtitles_processed #  
            # - ${base_path}/switchboardutt_processed
            # - ${base_path}/switchboardutt_processedlow #### Switchboard with just (2 or userdefined) examples in train set.
        unlabelled: ## the first ${low_resource_labelled_count} examples will be labelled, rest will be used as unlabelled.
            ########## If low_resource_labelled_count is 0, all examples will be unlabelled.
            ########## To just simulate low resource without unlabelled, refer to Create low resource switchboard dataset in README.md
            # - ${base_path}/ted_talks_processed #
            # - ${base_path}/open_subtitles_processed #  
            # - ${base_path}/switchboardutt_processed
        # parameters for dataset preprocessing
        low_resource_labelled_count: 2
        max_seq_length: 128
        pad_label: ''  # Unused
        ignore_extra_tokens: false
        ignore_start_end: false
        use_cache: false
        # shared among dataloaders
        num_workers:  2
        pin_memory: false
        drop_last: false
        num_labels: 7
        num_domains: 3        
        val_unlabelled: true    # validation check unlabelled if theres unlabelled else labelled
        test_unlabelled: true   # test check unlabelled if theres unlabelled else labelled
        attach_label_to_end: true #None # false if attach to start none if dont mask
        alpha_sub: 0.0
        alpha_del: 0.0
        alpha_ins: 0.0
        alpha_swp: 0
        alpha_spl: 0.0
        stride: 42 # 0 or blank if no stride. Ensure stride divides max_seq_length-2
        train_ds:
            shuffle: true
            num_samples: -1
            batch_size: 4
            manual_len: 6000 #default 0 84074

        validation_ds:
            # if evaluation data is not in the model.dataset.data_dir as the training data or multiple datasets are used for
            # evaluationis needed, specify ds_item, otherwise by default data_dir is used
            # ds_item: null # expected format: [PATH_TO_DEV1,PATH_TO_DEV2] (Note no space between the paths and square brackets)
            shuffle: false
            num_samples: -1
            batch_size: 32 #4

    mask:
        type: normal #uniform, #, normal, trapezoid, 
        sigma: 0.1 # If uniform, set as stride/(max_seq_length-2) else define it.

    tokenizer:
        tokenizer_name: ${model.language_model.pretrained_model_name} # or sentencepiece
        vocab_file: null # path to vocab file
        tokenizer_model: null # only used if tokenizer is sentencepiece
        special_tokens: null

    language_model:
        pretrained_model_name: ${model.transformer_path} # bert-base-uncased
        lm_checkpoint: null
        config_file: null # json file, precedence over config
        config: null
        
        # embedding_dropout: 0.1
        # ffn_dropout: 0.1
        # attn_score_dropout: 0.1
        # attn_layer_dropout: 0.1
    
    punct_head:
        punct_num_fc_layers: 2
        fc_dropout: 0.2
        activation: 'gelu'
        log_softmax: false
        use_transformer_init: true
        loss: 'dice'
        bilstm: false

        dice_loss:
            epsilon: 0.01
            alpha: 1
            macro_average: true

        focal_loss: 
            alpha: 2

    domain_head:
        predict_labelled: false # if false: treats every domain separately, if true: splits into labelled and unlabelled
        domain_num_fc_layers: 2 # Number of fully_connected layers. minimum 1. (no. fcl = max(1,domain_num_fc_layers))
        fc_dropout: 0.6
        activation: 'gelu'
        log_softmax: false
        use_transformer_init: true
        loss: 'dice'
        gamma: # Leave blank. auto fill per step
        gamma_factor: 0.000 #0.1 # coefficient of gradient reversal
        pooling: 'attention' # 'mean' # 'mean_max' # 'token' # 'attention' # #blank# (per token) 
        idx_conditioned_on: 0 # 0 = use first (CLS) token, only considered when pooling = token
        weight_tokens: true # Assign weight to each domain label
        weight: # ensure length is equal to number of domains if predict_labelled is false. 
            # - 0.05
            # - 0.05
            # - 0.9

            # - 2
            # - 1
            # - 4

            # - 0.1
            # - 0.45
            # - 0.45

            # - 0.1
            # - 0.9

            # - 1
            # - 1

            # - 0.1
            # - 0.9

        dice_loss:
            epsilon: 0.01
            alpha: 4
            macro_average: true

        # focal_loss: 
        #     alpha: 2

    frozen_lr:
        # - 1e-2
        # - 5e-3
        # - 1e-3
        # - 1e-3
        # - 5e-4
        # - 5e-4
        # - 1e-4
        - 4e-5
        - 1e-5
        - 5e-6
        - 1e-6
        - 5e-7
        - 1e-7
        # - 1e-7
        # - 1e-8

    # gamma:
    #     - 1
    #     - 1
    #     - 1
    #     - 1
    #     - 1
        # - 1e-0
        # - 5e-1
        # - 4e-1
        # - 1e-4
        # - 1e-5
    differential_lr: 0.8 # base layers multiplied by this
    optim:
        name: adamw #novograd #adamw
        lr: 1e-2 #1e-3
        weight_decay: 0.00
        sched:
            # name: CyclicLR
            # base_lr: 1e-5
            # max_lr: 1e-1
            # mode: 'triangular2'
            # last_epoch: -1

            name: CosineAnnealing #CosineAnneali3ng #WarmupAnnealing #CyclicLR
            # Scheduler params
            warmup_steps: null
            warmup_ratio: 0.1
            min_lr: 1e-8
            # hold_steps: 6
            last_epoch: -1

            # pytorch lightning args
            monitor: val_loss
            reduce_on_plateau: false
hydra:
    run:
        dir: .
    job_logging:
        formatters:
            simple:
                format: '[%(levelname)s] - %(message)s'
        handlers:
            console:
                class: logging.StreamHandler
                formatter: simple
                stream: ext://sys.stdout
            file:
                class: logging.FileHandler
                level: INFO
                formatter: simple
                filename: info.log
                encoding: utf8
                mode: w
        root:
            handlers: [console, file]
