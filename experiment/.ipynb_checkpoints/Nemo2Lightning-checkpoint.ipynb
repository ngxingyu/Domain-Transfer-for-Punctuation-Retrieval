{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Tesla T4\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "torch.rand(10, device=device)\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "# from models import PunctuationDomainModel\n",
    "import hydra\n",
    "from icecream import install\n",
    "install()\n",
    "def toString(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return 'array shape: '+obj.shape.__str__() + obj.__str__()\n",
    "    if isinstance(obj, torch.Tensor):\n",
    "        return 'tensor shape: '+obj.shape.__str__()\n",
    "    if isinstance(obj, dict):\n",
    "        return {_[0]:toString(_[1]) for _ in obj.items()}.__str__()\n",
    "    return repr(obj)\n",
    "ic.configureOutput(argToStringFunction=toString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_path': '/home/nxingyu2/data', 'trainer': {'gpus': 1, 'num_nodes': 1, 'max_epochs': 3, 'max_steps': None, 'accumulate_grad_batches': 1, 'gradient_clip_val': 0.0, 'amp_level': 'O0', 'precision': 16, 'accelerator': 'ddp', 'checkpoint_callback': False, 'logger': True, 'log_every_n_steps': 1, 'val_check_interval': 1.0, 'resume_from_checkpoint': None}, 'exp_manager': {'exp_dir': None, 'name': 'Punctuation_with_Domain_discriminator', 'create_tensorboard_logger': True, 'create_checkpoint_callback': True}, 'model': {'transformer_path': '${base_path}/electra-base-discriminator', 'punct_label_ids': ['', '!', ',', '-', '.', ':', ';', '?', '—', '…'], 'dataset': {'data_dir': '${base_path}', 'labelled': ['${base_path}/ted_talks_processed'], 'unlabelled': ['${base_path}/open_subtitles_processed'], 'max_seq_length': 128, 'pad_label': '', 'ignore_extra_tokens': False, 'ignore_start_end': False, 'use_cache': True, 'num_workers': 2, 'pin_memory': False, 'drop_last': False}, 'train_ds': {'shuffle': True, 'num_samples': -1, 'batch_size': 64}, 'validation_ds': {'ds_item': None, 'shuffle': False, 'num_samples': -1, 'batch_size': 64}, 'tokenizer': {'tokenizer_name': '${model.language_model.pretrained_model_name}', 'vocab_file': None, 'tokenizer_model': None, 'special_tokens': None}, 'language_model': {'pretrained_model_name': '${model.transformer_path}', 'lm_checkpoint': None, 'config_file': None, 'config': None, 'unfrozen_layers': 0}, 'punct_head': {'punct_num_fc_layers': 1, 'fc_dropout': 0.1, 'activation': 'relu', 'use_transformer_init': True, 'loss': 'cel'}, 'domain_head': {'domain_num_fc_layers': 1, 'fc_dropout': 0.1, 'activation': 'relu', 'log_softmax': True, 'use_transformer_init': True, 'loss': 'cel', 'lbd': 1}, 'dice_loss': {'alpha': 0.8, 'gamma': 2}, 'optim': {'name': 'adam', 'lr': 0.0001, 'weight_decay': 0.0}, 'sched': {'name': 'WarmupAnnealing', 'warmup_steps': None, 'warmup_ratio': 0.1, 'last_epoch': -1, 'monitor': 'val_loss', 'reduce_on_plateau': False}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Using hydra\n",
    "from hydra.experimental import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(config_path=\"core/config\")\n",
    "cfg=compose(\n",
    "    config_name=\"config.yaml\", \n",
    ")\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "from nemo.core.neural_types import ChannelType, LabelsType, MaskType, NeuralType\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from typing import List, Optional, Dict\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "class PunctuationDomainDataset(Dataset):\n",
    "\n",
    "    @property\n",
    "    def output_types(self) -> Optional[Dict[str, NeuralType]]:\n",
    "        \"\"\"Returns definitions of module output ports.\n",
    "               \"\"\"\n",
    "        return {\n",
    "            \"input_ids\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"attention_mask\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"labels\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"domain\": NeuralType(('B'), ChannelType()),\n",
    "        }\n",
    "\n",
    "    def __init__(self, \n",
    "        csv_file:str, \n",
    "        tokenizer,\n",
    "        num_samples:int=256,\n",
    "        max_seq_length:int=256,\n",
    "        punct_label_ids: Dict[str, int] = None,\n",
    "        domain=0,\n",
    "        labelled=True,\n",
    "    ):\n",
    "        if not (os.path.exists(csv_file)):\n",
    "            raise FileNotFoundError(\n",
    "                f'{csv_file} not found. The data should be joined in 1 csv file.\\\n",
    "                    Each line of the file contains the subword token ids, masks and class labels per row.'\n",
    "            )\n",
    "\n",
    "        data_dir = os.path.dirname(csv_file)\n",
    "        filename = os.path.basename(csv_file)\n",
    "\n",
    "        if not filename.endswith('.csv'):\n",
    "            raise ValueError(\"{text_file} should have extension .csv\")\n",
    "        # filename = filename[:-4]\n",
    "        \n",
    "        self.csv_file = csv_file\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.set_num_samples(csv_file, num_samples)\n",
    "        self.domain=domain\n",
    "        self.labelled=labelled\n",
    "        self.tokenizer=tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = next(\n",
    "            pd.read_csv(\n",
    "                self.csv_file,\n",
    "                skiprows=(idx % self.len)*self.num_samples,\n",
    "                chunksize=self.num_samples,\n",
    "                header=None,\n",
    "                delimiter=' '))\n",
    "        x = torch.from_numpy(x.values).reshape(-1,3,self.max_seq_length) #x.shape[-1]//3\n",
    "        return {'input_ids': torch.as_tensor(x[:,0,:], dtype=torch.long),\n",
    "                'attention_mask': torch.as_tensor(x[:,1,:],dtype=torch.bool)if self.labelled else torch.zeros_like(x[:,1,:],dtype=torch.bool),\n",
    "                'labels': torch.as_tensor(x[:,2,:],dtype=torch.long),\n",
    "                'domain':self.domain*torch.ones(x.shape[0],1,dtype=torch.long)}\n",
    "\n",
    "    def set_num_samples(self,csv_file,num_samples):\n",
    "        self.num_samples = num_samples\n",
    "        self.total_samples=int(subprocess.Popen(['wc', '-l', csv_file], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0])\n",
    "        self.len = int(self.total_samples / self.num_samples)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def view(d)->list:\n",
    "        \"\"\":param d(dictionary): returns readable format of single input_ids and labels in the form of readable text\"\"\"\n",
    "        a,_,c=d.values()\n",
    "        return [' '.join([_[0]+_[1] for _ in list(zip(self.tokenizer.convert_ids_to_tokens(_[0]),[id2tag[id] for id in _[1].tolist()]))]) for _ in zip(a,c)]\n",
    "    \n",
    "    def shuffle(self, sorted=False, seed=42):\n",
    "        os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {}'.format(self.csv_file, self.csv_file, ['false','true'][sorted], seed))\n",
    "\n",
    "class PunctuationDomainDatasets(Dataset):\n",
    "    \n",
    "    @property\n",
    "    def output_types(self) -> Optional[Dict[str, NeuralType]]:\n",
    "        \"\"\"Returns definitions of module output ports. \"\"\"\n",
    "        return {\n",
    "            \"input_ids\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"attention_mask\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"labels\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"domain\": NeuralType(('B'), ChannelType()),\n",
    "        }\n",
    "\n",
    "    def __init__(self, *datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        b={k:torch.vstack([d[i][k] for d in self.datasets]) for k in ['input_ids','attention_mask','labels','domain']}\n",
    "        rand=torch.randperm(b['labels'].size()[0])\n",
    "        return {k:v[rand] for k,v in b.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(d) for d in self.datasets)\n",
    "\n",
    "class PunctuationInferenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates dataset to use during inference for punctuation and capitalization tasks with a pretrained model.\n",
    "    For dataset to use during training with labels, see BertPunctuationCapitalizationDataset.\n",
    "    Args:\n",
    "        queries file to sequences, each line should a sentence, no header.\n",
    "        max_seq_length: max sequence length minus 2 for [CLS] and [SEP]\n",
    "        tokenizer: such as AutoTokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def output_types(self) -> Optional[Dict[str, NeuralType]]:\n",
    "        \"\"\"Returns definitions of module output ports.\n",
    "               \"\"\"\n",
    "        return {\n",
    "            'input_ids': NeuralType(('B', 'T'), ChannelType()),\n",
    "            'attention_mask': NeuralType(('B', 'T'), MaskType()),\n",
    "        }\n",
    "\n",
    "    def __init__(self, queries: List[str], max_seq_length: int, tokenizer):\n",
    "        \"\"\" Initializes BertPunctuationInferDataset. \"\"\"\n",
    "        features = get_features(queries=queries, max_seq_length=max_seq_length, tokenizer=tokenizer)\n",
    "        self.all_input_ids = features['input_ids']\n",
    "        self.all_attention_mask = features['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids':self.all_input_ids[idx],\n",
    "            'attention_mask':self.all_attention_mask[idx],}\n",
    "        \n",
    "\n",
    "def get_features(\n",
    "    queries:str, \n",
    "    max_seq_length:int,\n",
    "    tokenizer,\n",
    "    punct_label_ids: dict = None,):\n",
    "\n",
    "    def flatten(list_of_lists):\n",
    "        for list in list_of_lists:\n",
    "            for item in list:\n",
    "                yield item\n",
    "\n",
    "    def pad_to_len(max_length,ids):\n",
    "        o=np.zeros(max_length, dtype=np.int)\n",
    "        o[:len(ids)]=np.array(ids)\n",
    "        return o\n",
    "\n",
    "    def position_to_mask(max_length,indices):\n",
    "        o=np.zeros(max_length,dtype=np.int)\n",
    "        o[indices%(max_length-2)+1]=1\n",
    "        return o\n",
    "\n",
    "    batch_ids=[]\n",
    "    batch_masks=[]\n",
    "    for query in queries:\n",
    "        wordlist=re.split('[^a-zA-Z0-9]+',query)\n",
    "        subwords=list(map(tokenizer.tokenize,wordlist))\n",
    "        subword_lengths=list(map(len,subwords))\n",
    "        subwords=list(flatten(subwords))\n",
    "        token_end_idxs=np.cumsum([0]+subword_lengths[:-1])+np.array(subword_lengths)-1\n",
    "        teim=token_end_idxs%(max_seq_length-2)\n",
    "        split_token_end_idxs=np.array_split(token_end_idxs,(np.argwhere((teim[1:])<teim[:-1]).flatten()+1).tolist())\n",
    "        split_subwords=np.array_split(subwords,np.arange(max_length-2,len(subwords),max_seq_length-2)) \n",
    "        ids=torch.tensor([pad_to_len(max_seq_length,tokenizer.convert_tokens_to_ids(['[CLS]']+list(_)+['[SEP]'])) for _ in split_subwords], dtype=torch.long)\n",
    "        masks=[position_to_mask(max_length,_) for _ in split_token_end_idxs]\n",
    "        batch_ids.append(ids)\n",
    "        batch_masks.append(masks)\n",
    "    return {'input_ids': torch.as_tensor(batch_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.as_tensor(batch_masks,dtype=torch.bool)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_path': '/home/nxingyu2/data', 'trainer': {'gpus': 1, 'num_nodes': 1, 'max_epochs': 3, 'max_steps': None, 'accumulate_grad_batches': 1, 'gradient_clip_val': 0.0, 'amp_level': 'O0', 'precision': 16, 'accelerator': 'ddp', 'checkpoint_callback': False, 'logger': True, 'log_every_n_steps': 1, 'val_check_interval': 1.0, 'resume_from_checkpoint': None}, 'exp_manager': {'exp_dir': None, 'name': 'Punctuation_with_Domain_discriminator', 'create_tensorboard_logger': True, 'create_checkpoint_callback': True}, 'model': {'transformer_path': '${base_path}/electra-base-discriminator', 'punct_label_ids': ['', '!', ',', '-', '.', ':', ';', '?', '—', '…'], 'dataset': {'data_dir': '${base_path}', 'labelled': ['${base_path}/ted_talks_processed'], 'unlabelled': ['${base_path}/open_subtitles_processed'], 'max_seq_length': 128, 'pad_label': '', 'ignore_extra_tokens': False, 'ignore_start_end': False, 'use_cache': True, 'num_workers': 2, 'pin_memory': False, 'drop_last': False}, 'train_ds': {'shuffle': True, 'num_samples': -1, 'batch_size': 64}, 'validation_ds': {'ds_item': None, 'shuffle': False, 'num_samples': -1, 'batch_size': 64}, 'tokenizer': {'tokenizer_name': '${model.language_model.pretrained_model_name}', 'vocab_file': None, 'tokenizer_model': None, 'special_tokens': None}, 'language_model': {'pretrained_model_name': '${model.transformer_path}', 'lm_checkpoint': None, 'config_file': None, 'config': None, 'unfrozen_layers': 0}, 'punct_head': {'punct_num_fc_layers': 1, 'fc_dropout': 0.1, 'activation': 'relu', 'use_transformer_init': True, 'loss': 'cel'}, 'domain_head': {'domain_num_fc_layers': 1, 'fc_dropout': 0.1, 'activation': 'relu', 'log_softmax': True, 'use_transformer_init': True, 'loss': 'cel', 'lbd': 1}, 'dice_loss': {'alpha': 0.8, 'gamma': 2}, 'optim': {'name': 'adam', 'lr': 0.0001, 'weight_decay': 0.0}, 'sched': {'name': 'WarmupAnnealing', 'warmup_steps': None, 'warmup_ratio': 0.1, 'last_epoch': -1, 'monitor': 'val_loss', 'reduce_on_plateau': False}}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model.punct_label_ids=OmegaConf.create(sorted(cfg.model.punct_label_ids))\n",
    "ids_to_labels = {_[0]:_[1] for _ in enumerate(cfg.model.punct_label_ids)}\n",
    "labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
    "cfg.base_path='/home/nxingyu2/data' #/home/nxingyu/data\n",
    "# cfg.base_path\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PunctuationDomainDataset(Dataset):\n",
    "\n",
    "    @property\n",
    "    def output_types(self) -> Optional[Dict[str, NeuralType]]:\n",
    "        \"\"\"Returns definitions of module output ports.\n",
    "               \"\"\"\n",
    "        return {\n",
    "            \"input_ids\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"attention_mask\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"labels\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"domain\": NeuralType(('B'), ChannelType()),\n",
    "        }\n",
    "\n",
    "    def __init__(self, \n",
    "        csv_file:str, \n",
    "        tokenizer,\n",
    "        num_samples:int=256,\n",
    "        max_seq_length:int=256,\n",
    "        punct_label_ids: Dict[str, int] = None,\n",
    "        domain=0,\n",
    "        labelled=True,\n",
    "    ):\n",
    "        if not (os.path.exists(csv_file)):\n",
    "            raise FileNotFoundError(\n",
    "                f'{csv_file} not found. The data should be joined in 1 csv file.\\\n",
    "                    Each line of the file contains the subword token ids, masks and class labels per row.'\n",
    "            )\n",
    "\n",
    "        data_dir = os.path.dirname(csv_file)\n",
    "        filename = os.path.basename(csv_file)\n",
    "\n",
    "        if not filename.endswith('.csv'):\n",
    "            raise ValueError(\"{text_file} should have extension .csv\")\n",
    "        # filename = filename[:-4]\n",
    "        \n",
    "        self.csv_file = ic(  csv_file)\n",
    "        self.max_seq_length = ic(  max_seq_length)\n",
    "        self.set_num_samples(csv_file, num_samples)\n",
    "        self.domain= ic( domain)\n",
    "        self.labelled= ic( labelled)\n",
    "        self.tokenizer= ic( tokenizer)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = next(\n",
    "            pd.read_csv(\n",
    "                self.csv_file,\n",
    "                skiprows=(idx % self.len)*self.num_samples,\n",
    "                header=None,\n",
    "                dtype=str,\n",
    "                chunksize=self.num_samples,\n",
    "                ))[1]\n",
    "        chunked=chunk_examples_with_degree(0)(x)\n",
    "        batched=chunk_to_len_batch(self.max_seq_length,self.tokenizer,chunked['texts'],chunked['tags'],self.labelled)\n",
    "        batched['domain']=self.domain*torch.ones(batched['input_ids'].shape[0],1,dtype=torch.long)\n",
    "        rand=torch.randperm(batched['domain'].size()[0])\n",
    "        return {k:v[rand] for k,v in batched.items()}\n",
    "#        {'input_ids': torch.as_tensor(x[:,0,:], dtype=torch.long),\n",
    "#         'attention_mask': torch.as_tensor(x[:,1,:],dtype=torch.bool)if self.labelled else torch.zeros_like(x[:,1,:],dtype=torch.bool),\n",
    "#         'labels': torch.as_tensor(x[:,2,:],dtype=torch.long),\n",
    "#         'domain':self.domain*torch.ones(x.shape[0],1,dtype=torch.long)}\n",
    "\n",
    "    def set_num_samples(self,csv_file,num_samples):\n",
    "        self.num_samples = num_samples\n",
    "        self.total_samples=int(subprocess.Popen(['wc', '-l', csv_file], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0])\n",
    "        self.len = int(self.total_samples / self.num_samples)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def view(d)->list:\n",
    "        \"\"\":param d(dictionary): returns readable format of single input_ids and labels in the form of readable text\"\"\"\n",
    "        a,_,c=d.values()\n",
    "        return [' '.join([_[0]+_[1] for _ in list(zip(self.tokenizer.convert_ids_to_tokens(_[0]),[id2tag[id] for id in _[1].tolist()]))]) for _ in zip(a,c)]\n",
    "    \n",
    "    def shuffle(self, sorted=False, seed=42):\n",
    "        os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {}'.format(self.csv_file, self.csv_file, ['false','true'][sorted], seed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| csv_file: '/home/nxingyu2/data/ted_talks_processed.train.csv'\n",
      "ic| max_seq_length: 128\n",
      "ic| domain: 0\n",
      "ic| labelled: True\n",
      "ic| tokenizer: PreTrainedTokenizerFast(name_or_path='/home/nxingyu2/data/electra-base-discriminator', vocab_size=30522, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "ds=PunctuationDomainDataset( \n",
    "    csv_file=cfg.model.dataset.labelled[0]+'.train.csv', \n",
    "    tokenizer=AutoTokenizer.from_pretrained(cfg.model.transformer_path),\n",
    "    num_samples=16,\n",
    "    max_seq_length=128,\n",
    "    punct_label_ids=labels_to_ids,\n",
    "    domain=0,\n",
    "    labelled=True,\n",
    ")\n",
    "# ds.shuffle(sorted=True)\n",
    "# ds.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([596, 1])"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ds0=ds[0]\n",
    "ds0['domain'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PunctuationDomainDatasets(Dataset):\n",
    "    \n",
    "    @property\n",
    "    def output_types(self) -> Optional[Dict[str, NeuralType]]:\n",
    "        \"\"\"Returns definitions of module output ports. \"\"\"\n",
    "        return {\n",
    "            \"input_ids\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"attention_mask\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"subtoken_mask\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"labels\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"domain\": NeuralType(('B'), ChannelType()),\n",
    "        }\n",
    "\n",
    "    def __init__(self, \n",
    "                 split:str,\n",
    "                 num_samples:int,\n",
    "                 max_seq_length:int,\n",
    "                 punct_label_ids: Dict[str, int],\n",
    "                 labelled: List[str],\n",
    "                 unlabelled: List[str],\n",
    "                 tokenizer):\n",
    "        \n",
    "        self.datasets = []\n",
    "        for i,path in enumerate(labelled):\n",
    "            self.datasets.append(PunctuationDomainDataset(\n",
    "                    csv_file=f'{path}.{split}.csv', tokenizer=tokenizer,\n",
    "                    num_samples=num_samples,max_seq_length=max_seq_length,\n",
    "                    punct_label_ids=punct_label_ids,domain=i,labelled=True,))\n",
    "            \n",
    "        for i,path in enumerate(unlabelled):\n",
    "            self.datasets.append(PunctuationDomainDataset(\n",
    "                    csv_file=f'{path}.{split}.csv', tokenizer=tokenizer,\n",
    "                    num_samples=num_samples,max_seq_length=max_seq_length,\n",
    "                    punct_label_ids=punct_label_ids,domain=len(labelled)+i,labelled=False,))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        ds=[d[i] for d in self.datasets]\n",
    "        min_batch=1000000\n",
    "        for d in ds:\n",
    "            size=d['domain'].size()[0]\n",
    "            if size<min_batch:\n",
    "                min_batch=size\n",
    "        #Ensure all domains are evenly represented\n",
    "        b={k:torch.vstack([d[k][:min_batch] for d in ds]) for k in ['input_ids','attention_mask','subtoken_mask','labels','domain']}\n",
    "        rand=torch.randperm(b['labels'].size()[0])\n",
    "        return {k:v[rand] for k,v in b.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(d) for d in self.datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_dir': '${base_path}', 'labelled': ['${base_path}/ted_talks_processed'], 'unlabelled': ['${base_path}/open_subtitles_processed'], 'max_seq_length': 128, 'pad_label': '', 'ignore_extra_tokens': False, 'ignore_start_end': False, 'use_cache': True, 'num_workers': 2, 'pin_memory': False, 'drop_last': False}"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| csv_file: '/home/nxingyu2/data/ted_talks_processed.train.csv'\n",
      "ic| max_seq_length: 128\n",
      "ic| domain: 0\n",
      "ic| labelled: True\n",
      "ic| tokenizer: PreTrainedTokenizerFast(name_or_path='/home/nxingyu2/data/electra-base-discriminator', vocab_size=30522, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "ic| csv_file: '/home/nxingyu2/data/open_subtitles_processed.train.csv'\n",
      "ic| max_seq_length: 128\n",
      "ic| domain: 1\n",
      "ic| labelled: False\n",
      "ic| tokenizer: PreTrainedTokenizerFast(name_or_path='/home/nxingyu2/data/electra-base-discriminator', vocab_size=30522, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "dstrain=PunctuationDomainDatasets(\n",
    "        split='train',\n",
    "        tokenizer=AutoTokenizer.from_pretrained(cfg.model.transformer_path),\n",
    "        num_samples=8,\n",
    "        max_seq_length=128,\n",
    "        punct_label_ids=labels_to_ids,\n",
    "        labelled=list(cfg.model.dataset.labelled),\n",
    "        unlabelled=list(cfg.model.dataset.unlabelled)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([199])"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dstrain0=dstrain[0]\n",
    "# dstrain0['input_ids'].shape\n",
    "# r=torch.randperm(414)\n",
    "sum(dstrain0['domain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningDataModule\n",
    "from torch import dtype\n",
    "from data import PunctuationDomainDataset, PunctuationDomainDatasets\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from nemo.utils import logging\n",
    "\n",
    "class PunctuationDataModule(LightningDataModule):\n",
    "    def __init__(self, \n",
    "            tokenizer,\n",
    "            labelled: List[str], \n",
    "            unlabelled: List[str], \n",
    "            train_batch_size: int,\n",
    "            max_seq_length:int = 256,\n",
    "            val_batch_size:int = 256, \n",
    "            num_workers:int = 1,\n",
    "            pin_memory:bool = False,\n",
    "            drop_last:bool = False\n",
    "            ):\n",
    "        #unlabelled=[], batch_size = 256, max_seq_length = 256, num_workers=1):\n",
    "        super().__init__()\n",
    "        self.labelled=labelled\n",
    "        self.tokenizer=tokenizer\n",
    "        self.unlabelled=unlabelled\n",
    "        self.num_domains=len(labelled)+len(unlabelled)\n",
    "        self.train_batch_size = max(1,train_batch_size//self.num_domains)\n",
    "        logging.info(f\"using training batch_size of {self.train_batch_size} for each domain\")\n",
    "        self.val_batch_size = max(1,val_batch_size//self.num_domains)\n",
    "        logging.info(f\"using dev batch_size of {self.train_batch_size} for each domain\")\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.num_workers=num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "        self.train_dataset={}\n",
    "        self.dev_dataset={}\n",
    "        self.test_dataset={}\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        for unlabelled,l in enumerate([self.labelled,self.unlabelled]):\n",
    "            for i,p in enumerate(l):\n",
    "                domain=i+unlabelled*len(self.labelled) #unlabelled domain is increasing after labelled\n",
    "                try:\n",
    "                    with open(\"{}.train-stride.csv\".format(p),'r') as f:\n",
    "                        s=len(f.readline().split(' '))//3\n",
    "                except IOError:\n",
    "                    s=0\n",
    "                if (s!=self.max_seq_length):\n",
    "                    logging.info(f\"copying train file from {p}.train-batched.csv to {p}.train-stride.csv\")\n",
    "                    os.system(\"cp {} {}\".format(p+'.train-batched.csv',p+'.train-stride.csv'))\n",
    "                    if (self.max_seq_length!=256):\n",
    "                        logging.info(f'generating training strides: {self.max_seq_length}')\n",
    "                        n=np.loadtxt(open(p+\".train-stride.csv\", \"rb\"))\n",
    "                        np.savetxt(p+\".train-stride.csv\", self.with_stride_split(n,self.max_seq_length),fmt='%d')\n",
    "\n",
    "                if stage=='fit' or None:\n",
    "                    self.train_dataset[domain] = PunctuationDomainDataset(p+'.train-stride.csv', num_samples=self.train_batch_size, max_seq_length=self.max_seq_length, domain = domain, labelled=bool(1-unlabelled), tokenizer=self.tokenizer)\n",
    "                    self.dev_dataset[domain] =  PunctuationDomainDataset(p+'.dev-batched.csv', num_samples=self.val_batch_size, max_seq_length=self.max_seq_length, domain = domain, labelled=bool(1-unlabelled), tokenizer=self.tokenizer)\n",
    "                    ic(self.train_dataset[domain].shuffle(sorted=True))\n",
    "                    ic(self.train_dataset[domain].shuffle())\n",
    "\n",
    "                if stage == 'test' or stage is None:\n",
    "                    self.test_dataset[domain] =  PunctuationDomainDataset(p+'.test-batched.csv', num_samples=self.val_batch_size, max_seq_length=self.max_seq_length, domain = domain, labelled=bool(1-unlabelled), tokenizer=self.tokenizer)\n",
    "\n",
    "    def shuffle(self):\n",
    "        for dataset in self.train_dataset.values():\n",
    "            dataset.shuffle()\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(PunctuationDomainDatasets(*self.train_dataset.values()),batch_size=None,num_workers=self.num_workers,pin_memory=self.pin_memory,drop_last=self.drop_last)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(PunctuationDomainDatasets(*self.dev_dataset.values()),batch_size=None,num_workers=self.num_workers,pin_memory=self.pin_memory,drop_last=self.drop_last)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(PunctuationDomainDatasets(*self.test_dataset.values()),batch_size=None,num_workers=self.num_workers,pin_memory=self.pin_memory,drop_last=self.drop_last)\n",
    "\n",
    "    def with_stride_split(n,l):\n",
    "        def with_stride(t,l):\n",
    "            a=t[0,0]\n",
    "            z=t[0,-1]\n",
    "            t=t[:,1:-1].flatten()\n",
    "            t=np.trim_zeros(t,'b')\n",
    "            s=t.shape[0]\n",
    "            nh=-(-s//(l-2))\n",
    "            f=np.zeros((nh*(l-2),1))  \n",
    "            f[:s,0]=t\n",
    "            return np.hstack([np.ones((nh,1))*a,np.reshape(f,(-1,l-2)),np.ones((nh,1))*z])\n",
    "        s=n.shape[1]\n",
    "        a,b,c=n[:,:s//3],n[:,s//3:2*s//3],n[:,2*s//3:]\n",
    "        a,b,c=with_stride(a,l), with_stride(b,l), with_stride(c,l)\n",
    "        c1=np.zeros(a.shape)\n",
    "        c1[:c.shape[0],:]=c\n",
    "        return np.hstack([a,b,c1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def flatten(list_of_lists):\n",
    "    for l in list_of_lists:\n",
    "        for item in l:\n",
    "            yield item\n",
    "\n",
    "def pad_to_len(max_seq_length,ids):\n",
    "    '''[0, 1, 2] -> array([0, 1, 2, 0, 0, 0, 0, 0, 0, 0])'''\n",
    "    o=np.zeros(max_seq_length, dtype=np.int)\n",
    "    o[:len(ids)]=np.array(ids)\n",
    "    return o\n",
    "\n",
    "def position_to_mask(max_seq_length:int,indices:list):\n",
    "    '''[0, 2, 5] -> array([0, 1, 0, 1, 0, 0, 1, 0, 0, 0])'''\n",
    "    o=np.zeros(max_seq_length,dtype=np.int)\n",
    "    o[np.array(indices)%(max_seq_length-2)+1]=1\n",
    "    return o\n",
    "\n",
    "def align_labels_to_mask(mask,labels):\n",
    "    '''[0,1,0],[2] -> [0,2,0]'''\n",
    "    assert(sum(mask)==len(labels))\n",
    "    mask[mask>0]=torch.tensor(labels)\n",
    "    return mask.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PunctuationInferenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates dataset to use during inference for punctuation and capitalization tasks with a pretrained model.\n",
    "    For dataset to use during training with labels, see BertPunctuationCapitalizationDataset.\n",
    "    Args:\n",
    "        queries file to sequences, each line should a sentence, no header.\n",
    "        max_seq_length: max sequence length minus 2 for [CLS] and [SEP]\n",
    "        tokenizer: such as AutoTokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def output_types(self) -> Optional[Dict[str, NeuralType]]:\n",
    "        \"\"\"Returns definitions of module output ports.\n",
    "               \"\"\"\n",
    "        return {\n",
    "            'input_ids': NeuralType(('B', 'T'), ChannelType()),\n",
    "            'attention_mask': NeuralType(('B', 'T'), MaskType()),\n",
    "            'subtoken_mask': NeuralType(('B', 'T'), MaskType()),\n",
    "            \"labels\": NeuralType(('B', 'T'), ChannelType()),\n",
    "        }\n",
    "\n",
    "    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, degree:int=0,):\n",
    "        \"\"\" Initializes BertPunctuationInferDataset. \"\"\"\n",
    "        self.degree=degree\n",
    "        chunked=chunk_examples_with_degree(self.degree)(queries)\n",
    "        features = chunk_to_len_batch(max_seq_length=max_seq_length, tokenizer=tokenizer,tokens=chunked['texts'],labelled=False)\n",
    "        self.all_input_ids = ic(features['input_ids'])\n",
    "        self.all_attention_mask = ic(features['attention_mask'])\n",
    "        self.all_subtoken_mask = ic(features['subtoken_mask'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids':self.all_input_ids[idx],\n",
    "            'attention_mask':self.all_attention_mask[idx],\n",
    "               'subtoken_mask':self.all_subtoken_mask[idx]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "def text2masks(n):\n",
    "    def text2masks(text):\n",
    "        '''Converts single paragraph of text into a list of words and corresponding punctuation based on the degree requested.'''\n",
    "        if n==0: \n",
    "            refilter=\"(?<=[.?!,;:\\-—… ])(?=[^.?!,;:\\-—… ])|$\"\n",
    "        else:\n",
    "            refilter=\"[.?!,;:\\-—…]{1,%d}(?= *[^.?!,;:\\-—…]+|$)|(?<=[^.?!,;:\\-—…]) +(?=[^.?!,;:\\-—…])\"%(n)\n",
    "        text=re.sub(r'^[_\\W]*','',text)\n",
    "        word=re.split(refilter,text, flags=re.V1)\n",
    "        punct=re.findall(refilter,text, flags=re.V1)\n",
    "        wordlist,punctlist=([] for _ in range(2))\n",
    "        if word[-1]=='': # ensures text aligns\n",
    "            word.pop()\n",
    "        else:\n",
    "            punct.append('')\n",
    "        \n",
    "        for i in zip(word,punct): #+[''] to correspond to the last word or '' after the last punctuation.\n",
    "            w,p=i[0].strip(),i[1].strip()\n",
    "            if w!='':\n",
    "                wordlist.append(re.sub(r'[.?!,;:\\-—… ]','',w))\n",
    "                punctlist.append(0 if not w[-1] in '.?!,;:-—…' else labels_to_ids[w[-1]])\n",
    "            if p!='':\n",
    "                wordlist.append(p)\n",
    "                punctlist.append(0)\n",
    "        return(wordlist,punctlist)\n",
    "    return text2masks\n",
    "def chunk_examples_with_degree(n):\n",
    "    '''Ensure batched=True if using dataset.map or ensure the examples are wrapped in lists.'''\n",
    "    def chunk_examples(examples):\n",
    "        output={}\n",
    "        output['texts']=[]\n",
    "        output['tags']=[]\n",
    "        for sentence in examples:\n",
    "            text,tag=text2masks(n)(sentence)\n",
    "            output['texts'].append(text)\n",
    "            output['tags'].append(tag)\n",
    "            # output['tags'].append([0]+tag if text[0]!='' else tag) # [0]+tag so that in all case, the first tag refers to [CLS]\n",
    "            # not necessary since all the leading punctuations are stripped\n",
    "        return output\n",
    "    return chunk_examples\n",
    "assert(chunk_examples_with_degree(0)(['Hello!Bye…'])=={'texts': [['Hello', 'Bye']], 'tags': [[1, 9]]})\n",
    "\n",
    "def subword_tokenize(tokenizer,tokens):\n",
    "    subwords = list(map(tokenizer.tokenize, tokens))\n",
    "    subword_lengths = list(map(len, subwords))\n",
    "    subwords = list(flatten(subwords))\n",
    "    token_end_idxs = np.cumsum([0]+subword_lengths[:-1])+np.array(subword_lengths)-1\n",
    "    return subwords, token_end_idxs\n",
    "\n",
    "def chunk_to_len(max_seq_length,tokenizer,tokens,labels=None):\n",
    "    subwords,token_end_idxs = subword_tokenize(tokenizer,tokens)\n",
    "    teim=token_end_idxs%(max_seq_length-2)\n",
    "    breakpoints=(np.argwhere(teim[1:]<teim[:-1]).flatten()+1).tolist()\n",
    "    split_token_end_idxs=np.array_split(token_end_idxs,breakpoints)\n",
    "    split_subwords=np.array_split(subwords,np.arange(max_seq_length-2,len(subwords),max_seq_length-2))\n",
    "    ids=[pad_to_len(max_seq_length,tokenizer.convert_tokens_to_ids(['[CLS]']+list(_)+['[SEP]'])) for _ in split_subwords]\n",
    "    masks=[position_to_mask(max_seq_length,_) for _ in split_token_end_idxs]\n",
    "    padded_labels=None\n",
    "    if labels!=None:\n",
    "        split_labels=np.array_split(labels,breakpoints)\n",
    "        padded_labels=[pad_to_len(max_seq_length,align_labels_to_mask(*_)) for _ in zip(masks,split_labels)]\n",
    "    return ids,masks,padded_labels\n",
    "    \n",
    "def chunk_to_len_batch(max_seq_length,tokenizer,tokens,labels=None,labelled=True):\n",
    "    batch_ids=[]\n",
    "    batch_masks=[]\n",
    "    batch_labels=[]\n",
    "    for i,_ in enumerate(zip(tokens,tokens) if labels==None else zip(tokens,labels)):\n",
    "        a,b,c=chunk_to_len(max_seq_length,tokenizer,*_) if labels else chunk_to_len(max_seq_length,tokenizer,_[0])\n",
    "        batch_ids.extend(a)\n",
    "        batch_masks.extend(b)\n",
    "        if labelled==True:\n",
    "            batch_labels.extend(c)\n",
    "    output = {'input_ids': torch.as_tensor(batch_ids, dtype=torch.long),\n",
    "              'attention_mask': torch.as_tensor(batch_ids, dtype=torch.bool),\n",
    "              'subtoken_mask': torch.as_tensor(batch_masks,dtype=torch.bool)*labelled}\n",
    "    output['labels']=torch.as_tensor(batch_labels,dtype=torch.short) if labelled==True else torch.zeros_like(output['input_ids'],dtype=torch.short)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| features['input_ids']: tensor shape: torch.Size([5, 5])\n",
      "ic| features['attention_mask']: tensor shape: torch.Size([5, 5])\n",
      "ic| features['subtoken_mask']: tensor shape: torch.Size([5, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  7592,  9541,  9541,   102],\n",
       "         [  101,   999,  8038,  2100,   102],\n",
       "         [  101,   999,  9061,  2203,   102],\n",
       "         [  101, 14141,  1012,   102,     0],\n",
       "         [  101,  7592,   102,     0,     0]]),\n",
       " 'attention_mask': tensor([[ True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True, False],\n",
       "         [ True,  True,  True, False, False]]),\n",
       " 'subtoken_mask': tensor([[False, False, False, False, False],\n",
       "         [False, False, False, False, False],\n",
       "         [False, False, False, False, False],\n",
       "         [False, False, False, False, False],\n",
       "         [False, False, False, False, False]])}"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # split='train'\n",
    "# # o=pd.read_csv(f'{cfg.model.dataset.labelled[0]}.{split}.csv',\n",
    "# #                   dtype='str',\n",
    "# #                   header=None,\n",
    "# #                   chunksize=10)\n",
    "# # t=next(iter(o))\n",
    "# sample_out = chunk_examples_with_degree(0)(t[1])\n",
    "# # tokenizer=AutoTokenizer.from_pretrained(cfg.model.transformer_path)\n",
    "# # subword_tokenize(sample_out['texts'][0])\n",
    "# sample_out\n",
    "# sample_out['texts'],sample_out['tags']\n",
    "# chunk_to_len_batch(1000,tokenizer,sample_out['texts'][:10],sample_out['tags'][:10])\n",
    "# chunk_examples_with_degree(0)(t[1])\n",
    "# chunk_examples_with_degree(0)(['!!Hellooooo! Yay! Bye Enddd.',\"Hello\"])\n",
    "inferData=PunctuationInferenceDataset(tokenizer=tokenizer, queries=['!!Hellooooo! Yay! Bye Enddd.',\"Hello\"], max_seq_length=5,degree=1)\n",
    "inferData[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| features['input_ids']: tensor shape: torch.Size([4, 5])\n",
      "ic| features['attention_mask']: tensor shape: torch.Size([4, 5])\n",
      "ic| features['subtoken_mask']: tensor shape: torch.Size([4, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  7592,  9541,  9541,   102],\n",
       "         [  101,  8038,  2100,  9061,   102],\n",
       "         [  101,  2203, 14141,   102,     0],\n",
       "         [  101,  7592,   102,     0,     0]]),\n",
       " 'attention_mask': tensor([[ True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True, False],\n",
       "         [ True,  True,  True, False, False]]),\n",
       " 'subtoken_mask': tensor([[False, False, False, False, False],\n",
       "         [False, False, False, False, False],\n",
       "         [False, False, False, False, False],\n",
       "         [False, False, False, False, False]])}"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inferData=PunctuationInferenceDataset(tokenizer=tokenizer, queries=['!!Hellooooo! Yay! Bye Enddd.',\"Hello\"], max_seq_length=5,degree=0)\n",
    "inferData[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PunctuationInferDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates dataset to use during inference for punctuation and capitalization tasks with a pretrained model.\n",
    "    For dataset to use during training with labels, see BertPunctuationCapitalizationDataset.\n",
    "    Args:\n",
    "        queries file to sequences, each line should a sentence, no header.\n",
    "        max_seq_length: max sequence length minus 2 for [CLS] and [SEP]\n",
    "        tokenizer: such as AutoTokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def output_types(self) -> Optional[Dict[str, NeuralType]]:\n",
    "        \"\"\"Returns definitions of module output ports.\n",
    "               \"\"\"\n",
    "        return {\n",
    "            'input_ids': NeuralType(('B', 'T'), ChannelType()),\n",
    "            'attention_mask': NeuralType(('B', 'T'), MaskType()),\n",
    "        }\n",
    "\n",
    "    def __init__(self, queries: List[str], max_seq_length: int, tokenizer):\n",
    "        \"\"\" Initializes BertPunctuationInferDataset. \"\"\"\n",
    "        features = ic(get_features(queries=queries, max_seq_length=max_seq_length, tokenizer=tokenizer))\n",
    "        self.all_input_ids = ic(features['input_ids'])\n",
    "        self.all_attention_mask = ic(features['attention_mask'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids':self.all_input_ids[idx],\n",
    "            'attention_mask':self.all_attention_mask[idx]}\n",
    "\n",
    "def get_features(\n",
    "    queries:str, \n",
    "    tokenizer,\n",
    "    max_seq_length:int,\n",
    "    degree:int=0,\n",
    "    punct_label_ids: dict = None,):\n",
    "\n",
    "    batch_ids=[]\n",
    "    batch_masks=[]\n",
    "    for query in queries: #\n",
    "        #'Hellooooo! Yay! Bye Endd.'\n",
    "        wordlist=ic(re.split('[^a-zA-Z0-9]+',query,flags=re.V1)) #If end with punctuation, this includes a trailing ''\n",
    "        if wordlist[-1]=='': #Not necessary since the masks would ignore repeated end idxs.\n",
    "            wordlist=wordlist[:-1] \n",
    "        #['Hellooooo', 'Yay', 'Bye', 'Endd', '']\n",
    "        subwords=ic(list(map(tokenizer.tokenize,wordlist))) # [['hello', '##oo', '##oo'], ['ya', '##y'], ['bye'], ['end', '##d']]\n",
    "        subword_lengths=ic(list(map(len,subwords))) # [3, 2, 1, 1]\n",
    "        subwords=ic(list(flatten(subwords))) # ['hello', '##oo', '##oo', 'ya', '##y', 'bye', 'end', '##d']\n",
    "        token_end_idxs=ic(np.cumsum([0]+subword_lengths[:-1])+np.array(subword_lengths)-1) #'[2 4 5 6]'\n",
    "        teim=ic(token_end_idxs%(max_seq_length-2)) #'[2 0 1 2]'\n",
    "        ic(np.argwhere(teim[1:]<teim[:-1]).flatten()) #[0] returns last labels for each chunk.\n",
    "        split_token_end_idxs=np.array_split(token_end_idxs,(np.argwhere(teim[1:]<teim[:-1]).flatten()+1).tolist())\n",
    "        #[array([2]), array([4, 5, 6])]\n",
    "        ic(split_token_end_idxs)\n",
    "        split_subwords=ic(np.array_split(subwords,np.arange(max_seq_length-2,len(subwords),max_seq_length-2)))\n",
    "        #[array(['hello', '##oo', '##oo', 'ya'], dtype='<U5'), array(['##y', 'bye', 'end'], dtype='<U5')]\n",
    "        ids=ic([pad_to_len(max_seq_length,tokenizer.convert_tokens_to_ids(['[CLS]']+list(_)+['[SEP]'])) for _ in split_subwords])\n",
    "        #[array([ 101, 7592, 9541, 9541, 8038,  102]), array([ 101, 2100, 9061, 2203,  102,    0])]\n",
    "        masks=ic([position_to_mask(max_seq_length,_) for _ in split_token_end_idxs])\n",
    "        batch_ids.append(ids) #[[array([ 101, 7592, 9541, 9541, 8038,  102]), array([ 101, 2100, 9061, 2203, 2094,  102])]]\n",
    "        batch_masks.append(masks) #[[array([0, 0, 0, 1, 0, 0]), array([0, 1, 1, 1, 0, 0])]]\n",
    "    \n",
    "    return ic({'input_ids': torch.as_tensor(batch_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.as_tensor(batch_masks,dtype=torch.bool)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| re.split('[^a-zA-Z0-9]+',query,flags=re.V1): ['Hellooooo', 'Yay', 'Bye', 'Enddd', '']\n",
      "ic| list(map(tokenizer.tokenize,wordlist)): [['hello', '##oo', '##oo'], ['ya', '##y'], ['bye'], ['end', '##dd']]\n",
      "ic| list(map(len,subwords)): [3, 2, 1, 2]\n",
      "ic| list(flatten(subwords)): ['hello', '##oo', '##oo', 'ya', '##y', 'bye', 'end', '##dd']\n",
      "ic| np.cumsum([0]+subword_lengths[:-1])+np.array(subword_lengths)-1: array shape: (4,)[2 4 5 7]\n",
      "ic| token_end_idxs%(max_seq_length-2): array shape: (4,)[2 0 1 3]\n",
      "ic| np.argwhere(teim[1:]<teim[:-1]).flatten(): array shape: (1,)[0]\n",
      "ic| split_token_end_idxs: [array([2]), array([4, 5, 7])]\n",
      "ic| np.array_split(subwords,np.arange(max_seq_length-2,len(subwords),max_seq_length-2)): [array(['hello', '##oo', '##oo', 'ya'], dtype='<U5'), array(['##y', 'bye', 'end', '##dd'], dtype='<U5')]\n",
      "ic| [pad_ids_to_len(max_seq_length,tokenizer.convert_tokens_to_ids(['[CLS]']+list(_)+['[SEP]'])) for _ in split_subwords]: [array([ 101, 7592, 9541, 9541, 8038,  102]), array([  101,  2100,  9061,  2203, 14141,   102])]\n",
      "ic| [position_to_mask(max_seq_length,_) for _ in split_token_end_idxs]: [array([0, 0, 0, 1, 0, 0]), array([0, 1, 1, 0, 1, 0])]\n",
      "ic| {'input_ids': torch.as_tensor(batch_ids, dtype=torch.long),\n",
      "    'attention_mask': torch.as_tensor(batch_masks,dtype=torch.bool)}: {'input_ids': 'tensor shape: torch.Size([1, 2, 6])', 'attention_mask': 'tensor shape: torch.Size([1, 2, 6])'}\n",
      "ic| get_features(queries=queries, max_seq_length=max_seq_length, tokenizer=tokenizer): {'input_ids': 'tensor shape: torch.Size([1, 2, 6])', 'attention_mask': 'tensor shape: torch.Size([1, 2, 6])'}\n",
      "ic| features['input_ids']: tensor shape: torch.Size([1, 2, 6])\n",
      "ic| features['attention_mask']: tensor shape: torch.Size([1, 2, 6])\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "ifds=PunctuationInferDataset(queries=['Hellooooo! Yay! Bye Enddd.'], max_seq_length=6, tokenizer=AutoTokenizer.from_pretrained(cfg.model.transformer_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_aligned(texts,tags,tokenizer,labels_to_ids):\n",
    "        return [re.sub(' ##','',' '.join([_[0]+_[1] for _ in list(zip(tokenizer.convert_ids_to_tokens(ic(_[0])),\n",
    "                                                      [labels_to_ids[id] for id in _[1].tolist()\n",
    "                                                      ]\n",
    "                                                     )\n",
    "                                                 )\n",
    "                         ]\n",
    "                        )\n",
    "                      ) for _ in zip(texts,tags)\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| _[0]: tensor shape: torch.Size([5])\n",
      "ic| _[0]: tensor shape: torch.Size([5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS] hellooooo! [SEP]', '[CLS] bye… [SEP] [PAD] [PAD]']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
    "ids_to_labels = {_[0]:_[1] for _ in enumerate(cfg.model.punct_label_ids)}\n",
    "t=chunk_examples_with_degree(0)(['Hellooooo!Bye…'])\n",
    "tokenizer=AutoTokenizer.from_pretrained(cfg.model.transformer_path)\n",
    "t=chunk_to_len_batch(5,tokenizer,t['texts'],t['tags'])\n",
    "view_aligned(t['input_ids'],np.array(t['labels']),tokenizer,ids_to_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 1, 2, 1]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten(list_of_lists):\n",
    "    for l in list_of_lists:\n",
    "        for item in l:\n",
    "            yield item\n",
    "list(flatten([[0],[0,1],[0,1,2],[],[],[1]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
