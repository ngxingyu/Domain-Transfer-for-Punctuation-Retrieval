{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "GeForce GTX 1080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "torch.rand(10, device=device)\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed': 42, 'trainer': {'gpus': 1, 'num_nodes': 1, 'max_epochs': 10, 'max_steps': None, 'accumulate_grad_batches': 4, 'gradient_clip_val': 0, 'amp_level': 'O1', 'precision': 16, 'accelerator': 'ddp', 'checkpoint_callback': False, 'logger': False, 'log_every_n_steps': 1, 'val_check_interval': 1.0, 'resume_from_checkpoint': None}, 'exp_manager': {'exp_dir': '/home/nxingyu/project/', 'name': 'Punctuation_with_Domain_discriminator', 'create_tensorboard_logger': True, 'create_checkpoint_callback': True}, 'base_path': '/home/nxingyu/data', 'tmp_path': '/home/nxingyu/data/tmp', 'model': {'nemo_path': None, 'transformer_path': 'google/electra-small-discriminator', 'unfrozen': 0, 'maximum_unfrozen': 1, 'unfreeze_step': 1, 'punct_label_ids': ['', '!', ',', '-', '.', ':', ';', '?', '—', '…'], 'punct_class_weights': False, 'dataset': {'data_dir': '/home/nxingyu/data', 'labelled': ['${base_path}/ted_talks_processed'], 'unlabelled': ['${base_path}/open_subtitles_processed'], 'max_seq_length': 128, 'pad_label': '', 'ignore_extra_tokens': False, 'ignore_start_end': False, 'use_cache': False, 'num_workers': 4, 'pin_memory': True, 'drop_last': False, 'num_labels': 10, 'num_domains': 2, 'test_unlabelled': True, 'train_ds': {'shuffle': True, 'num_samples': -1, 'batch_size': 4}, 'validation_ds': {'shuffle': True, 'num_samples': -1, 'batch_size': 4}}, 'tokenizer': {'tokenizer_name': '${model.language_model.pretrained_model_name}', 'vocab_file': None, 'tokenizer_model': None, 'special_tokens': None}, 'language_model': {'pretrained_model_name': '${model.transformer_path}', 'lm_checkpoint': None, 'config_file': None, 'config': None}, 'punct_head': {'punct_num_fc_layers': 1, 'fc_dropout': 0.1, 'activation': 'relu', 'log_softmax': False, 'use_transformer_init': True, 'loss': 'dice'}, 'domain_head': {'domain_num_fc_layers': 1, 'fc_dropout': 0.1, 'activation': 'relu', 'log_softmax': False, 'use_transformer_init': True, 'loss': 'cel', 'gamma': 0, 'pooling': 'mean_max', 'idx_conditioned_on': 0}, 'dice_loss': {'epsilon': 0.01, 'alpha': 3, 'macro_average': True}, 'focal_loss': {'gamma': 1}, 'optim': {'name': 'adamw', 'lr': 0.001, 'weight_decay': 0.0, 'sched': {'name': 'WarmupAnnealing', 'warmup_steps': None, 'warmup_ratio': 0.1, 'last_epoch': -1, 'monitor': 'val_loss', 'reduce_on_plateau': False}}}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hydra\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import snoop\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from data import PunctuationDataModule, PunctuationInferenceDataset\n",
    "\n",
    "snoop.install()\n",
    "\n",
    "from hydra.experimental import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize()\n",
    "cfg=compose(\n",
    "    config_name=\"config.yaml\", \n",
    ")\n",
    "cfg.model.punct_label_ids=OmegaConf.create(sorted(cfg.model.punct_label_ids))\n",
    "labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
    "ids_to_labels = {_[0]:_[1] for _ in enumerate(cfg.model.punct_label_ids)}\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:11:13.98 LOG:\n",
      "10:11:14.02 .... 'cel none' = 'cel none'\n",
      "10:11:14.02 .... output = tensor([1.5514, 1.5514, 0.5514, 0.5514], grad_fn=<NllLossBackward>)\n",
      "10:11:14.02 LOG:\n",
      "10:11:14.02 .... 'cel mean' = 'cel mean'\n",
      "10:11:14.02 .... output = tensor(1.0514, grad_fn=<NllLossBackward>)\n",
      "10:11:14.03 LOG:\n",
      "10:11:14.03 .... 'cel sum' = 'cel sum'\n",
      "10:11:14.03 .... output = tensor(4.2058, grad_fn=<NllLossBackward>)\n",
      "10:11:14.08 LOG:\n",
      "10:11:14.08 .... 'focal sum' = 'focal sum'\n",
      "10:11:14.08 .... loss(inp, tar) = tensor(6.7352, grad_fn=<SumBackward0>)\n",
      "10:11:14.08 LOG:\n",
      "10:11:14.08 .... 'focal mean' = 'focal mean'\n",
      "10:11:14.09 .... loss(inp, tar) = tensor(0.4210, grad_fn=<MeanBackward0>)\n",
      "10:11:14.09 LOG:\n",
      "10:11:14.09 .... 'focal none' = 'focal none'\n",
      "10:11:14.10 .... loss(inp, tar) = tensor([0.9635, 0.9635, 0.0991, 0.0991], grad_fn=<MulBackward0>)\n",
      "10:11:14.10 LOG:\n",
      "10:11:14.10 .... 'focal none' = 'focal none'\n",
      "10:11:14.10 .... loss(inp, tar) = tensor([0.4716, 0.4716, 0.0075, 0.0075], grad_fn=<MulBackward0>)\n",
      "10:11:14.11 LOG:\n",
      "10:11:14.12 .... 'crf,none' = 'crf,none'\n",
      "10:11:14.12 .... output = tensor([4.1689], grad_fn=<NegBackward>)\n",
      "10:11:14.12 LOG:\n",
      "10:11:14.12 .... 'crf,mean' = 'crf,mean'\n",
      "10:11:14.12 .... output = tensor(4.0544, grad_fn=<NegBackward>)\n",
      "10:11:14.12 LOG:\n",
      "10:11:14.13 .... 'crf,sum' = 'crf,sum'\n",
      "10:11:14.13 .... output = tensor(4.1272, grad_fn=<NegBackward>)\n",
      "10:11:14.13 LOG:\n",
      "10:11:14.13 .... 'crf,token_mean' = 'crf,token_mean'\n",
      "10:11:14.13 .... output = tensor(1.0815, grad_fn=<DivBackward0>)\n",
      "10:11:14.13 LOG:\n",
      "10:11:14.14 .... 'dice none,micro' = 'dice none,micro'\n",
      "10:11:14.14 .... output = tensor(0.4331, grad_fn=<PowBackward0>)\n",
      "10:11:14.14 LOG:\n",
      "10:11:14.14 .... 'dice mean,micro' = 'dice mean,micro'\n",
      "10:11:14.14 .... output = tensor(0.1444, grad_fn=<DivBackward0>)\n",
      "10:11:14.14 LOG:\n",
      "10:11:14.14 .... 'dice sum,micro' = 'dice sum,micro'\n",
      "10:11:14.15 .... output = tensor(0.4331, grad_fn=<SumBackward0>)\n",
      "10:11:14.15 LOG:\n",
      "10:11:14.15 .... 'dice sum,micro' = 'dice sum,micro'\n",
      "10:11:14.15 .... output = tensor(0.0812, grad_fn=<SumBackward0>)\n",
      "10:11:14.15 LOG:\n",
      "10:11:14.15 .... 'dice none,macro' = 'dice none,macro'\n",
      "10:11:14.16 .... loss(inp, tar) = tensor([5.9547, 4.4872, 2.0551], grad_fn=<MulBackward0>)\n",
      "10:11:14.16 LOG:\n",
      "10:11:14.16 .... 'dice mean,macro' = 'dice mean,macro'\n",
      "10:11:14.16 .... loss(inp, tar) = tensor(0.4314, grad_fn=<DivBackward0>)\n",
      "10:11:14.16 LOG:\n",
      "10:11:14.16 .... 'dice sum,macro' = 'dice sum,macro'\n",
      "10:11:14.16 .... loss(inp, tar) = tensor(1.2943, grad_fn=<SumBackward0>)\n",
      "10:11:14.17 LOG:\n",
      "10:11:14.17 .... 'dice none,macro' = 'dice none,macro'\n",
      "10:11:14.17 .... loss(inp, tar) = tensor([0.3340, 0.6546, 0.3057], grad_fn=<MulBackward0>)\n",
      "10:11:14.17 LOG:\n",
      "10:11:14.17 .... 'dice none,macro' = 'dice none,macro'\n",
      "10:11:14.17 .... loss(inp, tar) = tensor([0.0373, 0.2805, 0.0286], grad_fn=<MulBackward0>)\n",
      "10:11:14.18 LOG:\n",
      "10:11:14.18 .... 'dice sum,macro' = 'dice sum,macro'\n",
      "10:11:14.18 .... output = tensor([0.5989, 0.7696, 0.2411], grad_fn=<MulBackward0>)\n",
      "10:11:14.18 LOG:\n",
      "10:11:14.18 .... 'dice sum,macro' = 'dice sum,macro'\n",
      "10:11:14.18 .... output = tensor([0.2148, 0.4559, 0.0140], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('dice sum,macro', tensor([0.2148, 0.4559, 0.0140], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from core.losses import FocalDiceLoss, CrossEntropyLoss, LinearChainCRF, AggregatorLoss, FocalLoss\n",
    "\n",
    "# inp = pp(torch.randn(3, 4, 5, requires_grad=True))\n",
    "# tar = pp(torch.empty(3, 4, dtype=torch.long).random_(5))\n",
    "inp = torch.tensor([[[0,1,0],[0,0,1],[0,0,1],[1,0,0]]],dtype=torch.float, requires_grad=True)\n",
    "tar = torch.tensor([[0,1,2,0]],dtype=torch.long)\n",
    "mask=torch.tensor([[1,1,1,1]],dtype=torch.bool)\n",
    "\n",
    "loss = CrossEntropyLoss(reduction='none')\n",
    "output = loss(inp, tar)\n",
    "# output.backward()\n",
    "pp('cel none',output)\n",
    "\n",
    "loss = CrossEntropyLoss(reduction='mean')\n",
    "output = loss(inp, tar)\n",
    "# output.backward()\n",
    "pp('cel mean',output)\n",
    "\n",
    "loss = CrossEntropyLoss(reduction='sum')\n",
    "output = loss(inp, tar)\n",
    "# output.backward()\n",
    "pp('cel sum',output)\n",
    "\n",
    "loss = FocalLoss(reduction='sum')\n",
    "output = loss(inp, tar)\n",
    "# output.backward()\n",
    "pp('focal sum',loss(inp, tar))\n",
    "\n",
    "loss = FocalLoss(reduction='mean')\n",
    "output = loss(inp, tar)\n",
    "# output.backward()\n",
    "pp('focal mean',loss(inp, tar))\n",
    "\n",
    "loss = FocalLoss(reduction='none')\n",
    "output = loss(inp, tar)\n",
    "# output.backward()\n",
    "pp('focal none',loss(inp, tar))\n",
    "\n",
    "loss = FocalLoss(reduction='none', gamma=5)\n",
    "output = loss(inp, tar)\n",
    "# output.backward()\n",
    "pp('focal none',loss(inp, tar))\n",
    "\n",
    "loss = LinearChainCRF(num_labels=3,reduction='none')\n",
    "output = loss(inp, tar,mask)\n",
    "# output.backward()\n",
    "pp('crf,none',output)\n",
    "\n",
    "loss = LinearChainCRF(num_labels=3,reduction='mean')\n",
    "output = loss(inp, tar,mask)\n",
    "# output.backward()\n",
    "pp('crf,mean',output)\n",
    "\n",
    "loss = LinearChainCRF(num_labels=3,reduction='sum')\n",
    "output = loss(inp, tar,mask)\n",
    "# output.backward()\n",
    "pp('crf,sum',output)\n",
    "\n",
    "loss = LinearChainCRF(num_labels=3,reduction='token_mean')\n",
    "output = loss(inp, tar,mask)\n",
    "# output.backward()\n",
    "pp('crf,token_mean',output)\n",
    "\n",
    "loss = FocalDiceLoss(reduction='none', macro_average=False)\n",
    "output = loss(inp, tar)\n",
    "# output.backward()\n",
    "pp('dice none,micro',output)\n",
    "\n",
    "loss = FocalDiceLoss(reduction='mean', macro_average=False)\n",
    "output = loss(inp, tar)\n",
    "# output.backward()\n",
    "pp('dice mean,micro',output)\n",
    "\n",
    "loss = FocalDiceLoss(reduction='sum', macro_average=False)\n",
    "output = loss(inp, tar)\n",
    "# output.backward()\n",
    "pp('dice sum,micro',output)\n",
    "\n",
    "loss = FocalDiceLoss(reduction='sum', alpha=3, macro_average=False)\n",
    "output = loss(inp, tar)\n",
    "# output.backward()\n",
    "pp('dice sum,micro',output)\n",
    "\n",
    "loss = FocalDiceLoss(reduction='none',macro_average=True,alpha=5.0, log_softmax=True)\n",
    "output = loss(inp, tar)\n",
    "# output.backward()\n",
    "pp('dice none,macro',loss(inp, tar))\n",
    "\n",
    "loss = FocalDiceLoss(reduction='mean',macro_average=True)\n",
    "output = loss(inp, tar)\n",
    "# output.backward()\n",
    "pp('dice mean,macro',loss(inp, tar))\n",
    "\n",
    "loss = FocalDiceLoss(reduction='sum',macro_average=True)\n",
    "output = loss(inp, tar)\n",
    "# output.backward()\n",
    "pp('dice sum,macro',loss(inp, tar))\n",
    "\n",
    "loss = FocalDiceLoss(reduction='none',macro_average=True,alpha=1.0)\n",
    "output = loss(inp, tar)\n",
    "# output.backward()\n",
    "pp('dice none,macro',loss(inp, tar))\n",
    "\n",
    "loss = FocalDiceLoss(reduction='none',macro_average=True, alpha=3)\n",
    "output = loss(inp, tar)\n",
    "# output.backward()\n",
    "pp('dice none,macro',loss(inp, tar))\n",
    "\n",
    "inp = torch.tensor([[[0,1,0],[1,0,1],[0,0,1],[0,1,0]]],dtype=torch.float, requires_grad=True)\n",
    "tar = torch.tensor([[0,1,2,0]],dtype=torch.long)\n",
    "mask=torch.tensor([[1,1,1,1]],dtype=torch.bool)\n",
    "\n",
    "loss = FocalDiceLoss(reduction='none', alpha=1, macro_average=True)\n",
    "output = loss(inp, tar)\n",
    "# output.backward()\n",
    "pp('dice sum,macro',output)\n",
    "\n",
    "loss = FocalDiceLoss(reduction='none', alpha=3, macro_average=True)\n",
    "output = loss(inp, tar)\n",
    "# output.backward()\n",
    "pp('dice sum,macro',output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b22a0d7714b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mdata_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_prepared_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/experiment/data/punctuation_datamodule.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'fit'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             self.train_dataset = PunctuationDomainDatasets(split='train',\n\u001b[0m\u001b[1;32m     64\u001b[0m                     \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                     \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/experiment/data/punctuation_dataset_multi.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, split, num_samples, max_seq_length, punct_label_ids, labelled, unlabelled, tokenizer, randomize, data_id, tmp_path)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munlabelled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             dataset=PunctuationDomainDataset(\n\u001b[0m\u001b[1;32m    187\u001b[0m                     \u001b[0mcsv_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'{path}.{split}.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                     \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/experiment/data/punctuation_dataset_multi.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, csv_file, tokenizer, num_samples, max_seq_length, degree, punct_label_ids, domain, labelled, randomize, target_file, tmp_path, start, end)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv_file\u001b[0m \u001b[0;34m=\u001b[0m   \u001b[0mcsv_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m   \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m=\u001b[0m  \u001b[0mdomain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunct_label_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpunct_label_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/experiment/data/punctuation_dataset_multi.py\u001b[0m in \u001b[0;36mset_num_samples\u001b[0;34m(self, csv_file, num_samples)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-l'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTDOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_samples\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1009\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stdin_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m                 \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_config = cfg.model.dataset\n",
    "cfg.model.punct_label_ids=OmegaConf.create(sorted(cfg.model.punct_label_ids))\n",
    "labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
    "data_config.num_labels=len(cfg.model.punct_label_ids)\n",
    "data_config.labelled = OmegaConf.create([] if data_config.labelled==None else data_config.labelled)\n",
    "data_config.unlabelled = OmegaConf.create([] if data_config.unlabelled==None else data_config.unlabelled)\n",
    "data_config.num_domains = len(data_config.labelled)+len(data_config.unlabelled)\n",
    "dm=PunctuationDataModule(\n",
    "    tokenizer= cfg.model.transformer_path,\n",
    "    labelled= list(data_config.labelled),\n",
    "    unlabelled= list(data_config.unlabelled),\n",
    "    punct_label_ids= labels_to_ids,\n",
    "    train_batch_size= data_config.train_ds.batch_size,\n",
    "    max_seq_length= data_config.max_seq_length,\n",
    "    val_batch_size= data_config.validation_ds.batch_size,\n",
    "    num_workers= data_config.num_workers,\n",
    "    pin_memory= data_config.pin_memory,\n",
    "    train_shuffle= data_config.train_ds.shuffle,\n",
    "    val_shuffle= data_config.validation_ds.shuffle,\n",
    "    seed=cfg.seed,\n",
    "    data_id='0'\n",
    ")\n",
    "dm.setup()\n",
    "len(dm.train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it=dm.train_dataset\n",
    "# ni=next(it)\n",
    "# it=dm.train_dataset.datasets[0]\n",
    "dm.train_dataset.__len__()#determine_class_weights()\n",
    "# ct=torch.zeros(10)\n",
    "# for _ in range(64):\n",
    "#     print('.',end='')\n",
    "#     ni=next(it)\n",
    "#     ct+=torch.bincount(ni['labels'].view(-1))\n",
    "# return ct/sum(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from core.optim import get_optimizer\n",
    "optimizer=get_optimizer('adam')\n",
    "optimizer=optimizer([torch.tensor(5)],**{'lr':0.01})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4794, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = FocalDiceLoss(reduction='mean',macro_average=True)\n",
    "output1 = loss(inp, tar)\n",
    "loss1 = LinearChainCRF(num_labels=5,reduction='token_mean')\n",
    "output2 = loss1(inp, tar)\n",
    "agg=AggregatorLoss(num_inputs=2,weights=[0.5,1])\n",
    "agg(a=output1,b=output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 1, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 1, 0],\n",
       "        [0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.one_hot(LinearChainCRF(num_labels=5,reduction='sum').decode(inp).flatten().long(),5)\n",
    "\n",
    "# inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm=PunctuationDataModule(\n",
    "            tokenizer= cfg.model.transformer_path,\n",
    "            labelled= list(cfg.model.dataset.labelled),\n",
    "            unlabelled= list(cfg.model.dataset.unlabelled),\n",
    "            punct_label_ids= labels_to_ids,\n",
    "            train_batch_size= cfg.model.train_ds.batch_size,\n",
    "            max_seq_length= cfg.model.dataset.max_seq_length,\n",
    "            val_batch_size= cfg.model.validation_ds.batch_size,\n",
    "            num_workers= cfg.model.dataset.num_workers,\n",
    "            pin_memory= cfg.model.dataset.pin_memory,\n",
    "            train_shuffle= cfg.model.train_ds.shuffle,\n",
    "            val_shuffle= cfg.model.validation_ds.shuffle,\n",
    ")\n",
    "dm.setup('fit')\n",
    "dl=dm.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1005,  1056,  ...,  2748,  2008,   102],\n",
       "         [  101,  3818,  2000,  ...,  2202,  2115,   102],\n",
       "         [  101,  2599, 19366,  ...,  8491, 23161,   102],\n",
       "         ...,\n",
       "         [  101,  2037, 15451,  ...,  2035,  1997,   102],\n",
       "         [  101,  2041,  1997,  ...,  1997, 15451,   102],\n",
       "         [  101,  2028,  2182,  ...,  2518,  7910,   102]]),\n",
       " 'attention_mask': tensor([[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]]),\n",
       " 'subtoken_mask': tensor([[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False,  True, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]]),\n",
       " 'labels': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 4, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.int16),\n",
       " 'domain': tensor([[1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1]], dtype=torch.int16)}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import AutoModel\n",
    "# transformer=AutoModel.from_pretrained('google/electra-small-discriminator')\n",
    "transformer.encoder.layer.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 25, 10])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cat([torch.ones(10,1),torch.ones(10,1)],dim=-2)\n",
    "# torch.cat([torch.ones(50,25,10),torch.ones(50,25,10)],dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embeddings(\n",
       "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import transformers\n",
    "# t=transformers.AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "t.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PunctuationDomainModel(pl.LightningModule):\n",
    "\n",
    "    @property\n",
    "    def input_types(self) -> Optional[Dict[str, NeuralType]]:\n",
    "        return {\n",
    "            \"input_ids\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"attention_mask\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"subtoken_mask\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"labels\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"domain\": NeuralType(('B'), ChannelType()),\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def output_types(self) -> Optional[Dict[str, NeuralType]]:\n",
    "        return {\n",
    "            \"punct_logits\": NeuralType(('B', 'T', 'D'), LogitsType()),\n",
    "            \"domain_logits\": NeuralType(('B', 'D'), LogitsType()),\n",
    "        }\n",
    "\n",
    "    def __init__(self, cfg: DictConfig): \n",
    "        # num_labels: int = 10, \n",
    "        # embedding_dim: int = 768, \n",
    "        # lossfn: str = '', \n",
    "        # hidden_dropout_prob:float=0.1, \n",
    "        # base_model_path:str='google/electra-base-discriminator', \n",
    "        # reduction:str='mean',\n",
    "        # stride:int=256,\n",
    "        # unfrozen_layers=0,\n",
    "        # alpha='0.8',\n",
    "        # gamma='2',\n",
    "        # lbd=1, #coefficient of gradient reversal.\n",
    "        # domains: int = 1):\n",
    "        # self.setup_tokenizer(cfg.tokenizer)\n",
    "        super().__init__(label)\n",
    "\n",
    "        self._cfg.punct_label_ids=OmegaConf.create(sorted(self._cfg.punct_label_ids))\n",
    "        self.labels_to_ids = {_[0]:_[1] for _ in enumerate(self._cfg.punct_label_ids)}\n",
    "        self.ids_to_labels = {_[1]:_[0] for _ in enumerate(self._cfg.punct_label_ids)}\n",
    "        self.num_domains = len(self._cfg.dataset.labelled)+len(self._cfg.dataset.unlabelled)\n",
    "        \n",
    "        self.bert_model = get_lm_model(\n",
    "            pretrained_model_name=cfg.language_model.pretrained_model_name,\n",
    "            config_file=cfg.language_model.config_file,\n",
    "            config_dict=OmegaConf.to_container(cfg.language_model.config) if cfg.language_model.config else None,\n",
    "            checkpoint_file=cfg.language_model.lm_checkpoint,\n",
    "        )\n",
    "\n",
    "        self.punct_classifier = TokenClassifier(\n",
    "            hidden_size=self.bert_model.config.hidden_size,\n",
    "            num_classes=len(self._cfg.punct_label_ids),\n",
    "            activation=cfg.punct_head.activation,\n",
    "            log_softmax=False,\n",
    "            dropout=cfg.punct_head.fc_dropout,\n",
    "            num_layers=cfg.punct_head.punct_num_fc_layers,\n",
    "            use_transformer_init=cfg.punct_head.use_transformer_init,\n",
    "        )\n",
    "\n",
    "        self.domain_classifier = SequenceClassifier(\n",
    "            hidden_size=self.bert_model.config.hidden_size,\n",
    "            num_classes=self.num_domains,\n",
    "            num_layers=cfg.domain_head.domain_num_fc_layers,\n",
    "            activation=cfg.domain_head.activation,\n",
    "            log_softmax=False,\n",
    "            dropout=cfg.domain_head.fc_dropout,\n",
    "            use_transformer_init=cfg.domain_head.use_transformer_init,\n",
    "        )\n",
    "\n",
    "        self.punctuation_loss = CrossEntropyLoss(logits_ndim=3)\n",
    "        self.domain_loss = CrossEntropyLoss(logits_ndim=2)\n",
    "        self.agg_loss = AggregatorLoss(num_inputs=2)\n",
    "\n",
    "        self.punct_class_report = ClassificationReport(\n",
    "            num_classes=len(self._cfg.punct_label_ids),\n",
    "            label_ids=self.labels_to_ids,\n",
    "            mode='macro',\n",
    "            dist_sync_on_step=True,\n",
    "        )\n",
    "        self.domain_class_report = ClassificationReport(\n",
    "            num_classes=self.num_domains,\n",
    "            label_ids=list(range(self.num_domains)),\n",
    "            mode='macro',\n",
    "            dist_sync_on_step=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    @typecheck()\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None, domain_ids=None):\n",
    "        \"\"\"\n",
    "        No special modification required for Lightning, define it as you normally would\n",
    "        in the `nn.Module` in vanilla PyTorch.\n",
    "        \"\"\"\n",
    "        hidden_states = self.bert_model(\n",
    "            input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n",
    "        )\n",
    "        punct_logits = self.punct_classifier(hidden_states=hidden_states)\n",
    "        reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)\n",
    "        domain_logits = self.domain_classifier(hidden_states=reverse_grad_hidden_states)\n",
    "        return punct_logits, domain_logits\n",
    "\n",
    "    def _make_step(self, batch):\n",
    "        input_ids=batch['input_ids']\n",
    "        attention_mask=batch['attention_mask']\n",
    "        punct_labels=batch['labels']\n",
    "        domain_labels=batch['domain'][:,0,:]\n",
    "        # input_ids, input_type_ids, input_mask, subtokens_mask, loss_mask, punct_labels, domain_labels = batch\n",
    "        punct_logits, domain_logits = self(\n",
    "            input_ids=input_ids, attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        punct_loss = self.punct_loss(logits=punct_logits, labels=punct_labels, loss_mask=attention_mask)\n",
    "        domain_loss = self.domain_loss(logits=domain_logits, labels=domain_labels)\n",
    "        loss = self.agg_loss(loss_1=punct_loss, loss_2=domain_loss)\n",
    "        return loss, punct_logits, domain_logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Lightning calls this inside the training loop with the data from the training dataloader\n",
    "        passed in as `batch`.\n",
    "        \"\"\"\n",
    "        loss, _, _ = self._make_step(batch)\n",
    "        lr = self._optimizer.param_groups[0]['lr']\n",
    "\n",
    "        self.log('lr', lr, prog_bar=True)\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        return {'loss': loss, 'lr': lr}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        \"\"\"\n",
    "        Lightning calls this inside the validation loop with the data from the validation dataloader\n",
    "        passed in as `batch`.\n",
    "        \"\"\"\n",
    "        input_ids=batch['input_ids']\n",
    "        attention_mask=batch['attention_mask']\n",
    "        punct_labels=batch['labels']\n",
    "        domain_labels=batch['domain'][:,0,:]\n",
    "\n",
    "        val_loss, punct_logits, domain_logits = self._make_step(batch)\n",
    "\n",
    "        # attention_mask = attention_mask > 0.5\n",
    "        punct_preds = torch.argmax(punct_logits, axis=-1)[attention_mask]\n",
    "        punct_labels = punct_labels[attention_mask]\n",
    "        self.punct_class_report.update(punct_preds, punct_labels)\n",
    "\n",
    "        domain_preds = torch.argmax(domain_logits, axis=-1)[attention_mask]\n",
    "        domain_labels = domain_labels[attention_mask]\n",
    "        self.domain_class_report.update(domain_preds, domain_labels)\n",
    "\n",
    "        return {\n",
    "            'val_loss': val_loss,\n",
    "            'punct_tp': self.punct_class_report.tp,\n",
    "            'punct_fn': self.punct_class_report.fn,\n",
    "            'punct_fp': self.punct_class_report.fp,\n",
    "            'domain_tp': self.domain_class_report.tp,\n",
    "            'domain_fn': self.domain_class_report.fn,\n",
    "            'domain_fp': self.domain_class_report.fp,\n",
    "        }\n",
    "\n",
    "    def test_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        \"\"\"\n",
    "        Lightning calls this inside the validation loop with the data from the validation dataloader\n",
    "        passed in as `batch`.\n",
    "        \"\"\"\n",
    "        input_ids=batch['input_ids']\n",
    "        attention_mask=batch['attention_mask']\n",
    "        punct_labels=batch['labels']\n",
    "        domain_labels=batch['domain'][:,0,:]\n",
    "\n",
    "        test_loss, punct_logits, domain_logits = self._make_step(batch)\n",
    "\n",
    "        # attention_mask = attention_mask > 0.5\n",
    "        punct_preds = torch.argmax(punct_logits, axis=-1)[attention_mask]\n",
    "        punct_labels = punct_labels[attention_mask]\n",
    "        self.punct_class_report.update(punct_preds, punct_labels)\n",
    "\n",
    "        domain_preds = torch.argmax(domain_logits, axis=-1)[attention_mask]\n",
    "        domain_labels = domain_labels[attention_mask]\n",
    "        self.domain_class_report.update(domain_preds, domain_labels)\n",
    "\n",
    "        return {\n",
    "            'test_loss': test_loss,\n",
    "            'punct_tp': self.punct_class_report.tp,\n",
    "            'punct_fn': self.punct_class_report.fn,\n",
    "            'punct_fp': self.punct_class_report.fp,\n",
    "            'domain_tp': self.domain_class_report.tp,\n",
    "            'domain_fn': self.domain_class_report.fn,\n",
    "            'domain_fp': self.domain_class_report.fp,\n",
    "        }\n",
    "    \n",
    "    def multi_validation_epoch_end(self, outputs, dataloader_idx: int = 0):\n",
    "        \"\"\"\n",
    "        Called at the end of validation to aggregate outputs.\n",
    "        outputs: list of individual outputs of each validation step.\n",
    "        \"\"\"\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "\n",
    "        # calculate metrics and log classification report for Punctuation task\n",
    "        punct_precision, punct_recall, punct_f1, punct_report = self.punct_class_report.compute()\n",
    "        logging.info(f'Punctuation report: {punct_report}')\n",
    "\n",
    "        # calculate metrics and log classification report for domainalization task\n",
    "        domain_precision, domain_recall, domain_f1, domain_report = self.domain_class_report.compute()\n",
    "        logging.info(f'Domain report: {domain_report}')\n",
    "\n",
    "        self.log('val_loss', avg_loss, prog_bar=True)\n",
    "        self.log('punct_precision', punct_precision)\n",
    "        self.log('punct_f1', punct_f1)\n",
    "        self.log('punct_recall', punct_recall)\n",
    "        self.log('domain_precision', domain_precision)\n",
    "        self.log('domain_f1', domain_f1)\n",
    "        self.log('domain_recall', domain_recall)\n",
    "\n",
    "    def multi_test_epoch_end(self, outputs, dataloader_idx: int = 0):\n",
    "        \"\"\"\n",
    "            Called at the end of test to aggregate outputs.\n",
    "            outputs: list of individual outputs of each validation step.\n",
    "        \"\"\"\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "\n",
    "        # calculate metrics and log classification report for Punctuation task\n",
    "        punct_precision, punct_recall, punct_f1, punct_report = self.punct_class_report.compute()\n",
    "        logging.info(f'Punctuation report: {punct_report}')\n",
    "\n",
    "        # calculate metrics and log classification report for domainalization task\n",
    "        domain_precision, domain_recall, domain_f1, domain_report = self.domain_class_report.compute()\n",
    "        logging.info(f'Domain report: {domain_report}')\n",
    "\n",
    "        self.log('test_loss', avg_loss, prog_bar=True)\n",
    "        self.log('punct_precision', punct_precision)\n",
    "        self.log('punct_f1', punct_f1)\n",
    "        self.log('punct_recall', punct_recall)\n",
    "        self.log('domain_precision', domain_precision)\n",
    "        self.log('domain_f1', domain_f1)\n",
    "        self.log('domain_recall', domain_recall)\n",
    "    \n",
    "    def update_data_dir(self, data_dir: str) -> None:\n",
    "        \"\"\"\n",
    "        Update data directory\n",
    "        Args:\n",
    "            data_dir: path to data directory\n",
    "        \"\"\"\n",
    "        if os.path.exists(data_dir):\n",
    "            logging.info(f'Setting model.dataset.data_dir to {data_dir}.')\n",
    "            self._cfg.dataset.data_dir = data_dir\n",
    "        else:\n",
    "            raise ValueError(f'{data_dir} not found')\n",
    "\n",
    "    def setup_datamodule(self, cfg: Optional[DictConfig] = None):\n",
    "        if cfg is None:\n",
    "            cfg = self._cfg.train_ds\n",
    "            \n",
    "        self.data_module = PunctuationDataModule(\n",
    "            labelled=list(cfg.model.dataset.labelled),\n",
    "            unlabeled=list(cfg.model.dataset.unlabelled),\n",
    "            train_batch_size=cfg.model.train_ds.batch_size,\n",
    "            val_batch_size=cfg.model.validation_ds.batch_size,\n",
    "            max_seq_length=self._cfg.max_seq_length,\n",
    "            num_workers=cfg.model.dataset.num_workers,\n",
    "            pin_memory=cfg.model.dataset.pin_memory,\n",
    "            drop_last=cfg.model.dataset.drop_last,\n",
    "            tokenizer=self.tokenizer,\n",
    "        )\n",
    "        self._train_dl=self.data_module.train_dataloader\n",
    "        self._validation_dl=self.data_module.dev_dataloader\n",
    "        self._test_dl=self.data_module.test_dataloader\n",
    "    \n",
    "    def _setup_infer_dataloader(self, queries: List[str], batch_size: int) -> 'torch.utils.data.DataLoader':\n",
    "        \"\"\"\n",
    "        Setup function for a infer data loader.\n",
    "        Args:\n",
    "            queries: lower cased text without punctuation\n",
    "            batch_size: batch size to use during inference\n",
    "        Returns:\n",
    "            A pytorch DataLoader.\n",
    "        \"\"\"\n",
    "\n",
    "        dataset = BertPunctuationInferDataset(\n",
    "            tokenizer=self.tokenizer, queries=queries, max_seq_length=self._cfg.dataset.max_seq_length\n",
    "        )\n",
    "\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset=dataset,\n",
    "            collate_fn=dataset.collate_fn,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self._cfg.dataset.num_workers,\n",
    "            pin_memory=self._cfg.dataset.pin_memory,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "    def add_punctuation_capitalization(self, queries: List[str], batch_size: int = None) -> List[str]:\n",
    "        \"\"\"\n",
    "        Adds punctuation and capitalization to the queries. Use this method for debugging and prototyping.\n",
    "        Args:\n",
    "            queries: Text\n",
    "            batch_size: batch size to use during inference\n",
    "        Returns:\n",
    "            result: text with punctuation\n",
    "        \"\"\"\n",
    "\n",
    "        if queries is None or len(queries) == 0:\n",
    "            return []\n",
    "\n",
    "        if batch_size is None:\n",
    "            batch_size = len(queries)\n",
    "            logging.info(f'Using batch size {batch_size} for inference')\n",
    "\n",
    "        # We will store the output here\n",
    "        result = []\n",
    "\n",
    "        # Model's mode and device\n",
    "        mode = self.training\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        try:\n",
    "            # Switch model to evaluation mode\n",
    "            self.eval()\n",
    "            self = self.to(device)\n",
    "            infer_datalayer = self._setup_infer_dataloader(queries, batch_size)\n",
    "\n",
    "            # store predictions for all queries in a single list\n",
    "            all_punct_preds = []\n",
    "\n",
    "            for batch in infer_datalayer:\n",
    "                input_ids = batch['input_ids']\n",
    "                attention_mask = batch['attention_mask']\n",
    "\n",
    "                punct_logits, _ = self.forward(\n",
    "                    input_ids=input_ids.to(device),\n",
    "                    attention_mask=input_mask.to(device),\n",
    "                )\n",
    "                punct_preds = tensor2list(torch.argmax(punct_logits, axis=-1)[subtokens_mask])\n",
    "                all_punct_preds.extend(punct_preds)\n",
    "            id2tag = {v: k for k, v in self._cfg.punct_label_ids.items()}\n",
    "            result.extend([' '.join([_[0]+_[1] for _ in \\\n",
    "                list(zip(self.tokenizer.convert_ids_to_tokens(_[0]),\n",
    "                            [id2tag[id] for id in _[1].tolist()])\n",
    "                    )]) for _ in zip(infer_datalayer['input_ids'],all_punct_preds)])\n",
    "        finally:\n",
    "            # set mode back to its original value\n",
    "            self.train(mode=mode)\n",
    "        return result\n",
    "\n",
    "\n",
    "#%%\n",
    "'''\n",
    "        self.num_labels=num_labels\n",
    "        self.embedding_dim=embedding_dim\n",
    "        self.domains = domains\n",
    "        self.reduction=reduction\n",
    "        self.unfrozen_layers=unfrozen_layers\n",
    "        self.alpha=alpha\n",
    "        self.gamma=gamma\n",
    "        self.lossfn=lossfn\n",
    "        self.stride=stride\n",
    "        self.grad_reverse=GradientReverse\n",
    "        self.grad_reverse.scale=lbd\n",
    "        self.dropout = torch.nn.Dropout(hidden_dropout_prob)\n",
    "        self.transformer = transformers.ElectraModel.from_pretrained(base_model_path)\n",
    "        self.freeze()\n",
    "        self.fcl = torch.nn.Linear(self.embedding_dim, self.num_labels)\n",
    "        if lossfn == 'crf':\n",
    "            self.loss=DiceCRF(self.num_labels,reduction=self.reduction)\n",
    "        elif lossfn == 'dice':\n",
    "            self.loss=DiceLoss(gamma=self.gamma,alpha=self.alpha, num_classes=self.num_labels, reduction=self.reduction)\n",
    "        else:\n",
    "            self.loss=CrossEntropyLoss(reduction=self.reduction)\n",
    "        if self.domains>1:\n",
    "            self.domainfcl=torch.nn.Linear(self.embedding_dim, self.domains)\n",
    "            self.domain_loss=CrossEntropyLoss(reduction=self.reduction, punct_classifier=False)\n",
    "            self.agg_loss=AggregatorLoss(weights=[1,0.5])\n",
    "            \n",
    "        self.punct_class_report = ClassificationReport(\n",
    "            num_classes=self.num_labels,\n",
    "            label_ids={'': 0, '!': 1, ',': 2, '-': 3, '.': 4, ':': 5, ';': 6, '?': 7, '—': 8, '…': 9},\n",
    "            mode='macro',\n",
    "            dist_sync_on_step=True,\n",
    "        )\n",
    "        if self.domains>1:\n",
    "            self.domain_class_report = ClassificationReport(\n",
    "                num_classes=self.domains,\n",
    "                mode='macro',\n",
    "                dist_sync_on_step=True)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        o1 = self.transformer(x['input_ids'],x['attention_mask'])[0]\n",
    "        d1 = self.dropout(o1)\n",
    "        p = self.fcl(d1)\n",
    "        if self.domains>1: ##relook\n",
    "            d1r= self.grad_reverse.apply(d1)\n",
    "            d= self.domainfcl(d1r[:,0,:])\n",
    "            return p, d\n",
    "        return p\n",
    "\n",
    "    def _make_step(self, batch):\n",
    "        punct_logits, domain_logits = self(batch)\n",
    "        print('make_step',punct_logits.shape,domain_logits.shape)\n",
    "        punct_loss = self.loss(punct_logits, batch['labels'], batch['attention_mask'])\n",
    "        if self.domains>1:\n",
    "            domain_loss = self.domain_loss(domain_logits, batch['domain'])\n",
    "        loss = punct_loss if self.domains==1 else self.agg_loss(loss_1=punct_loss, loss_2=domain_loss)\n",
    "        return loss, punct_logits, domain_logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss,_,_ = self._make_step(batch)\n",
    "        lr = self._optimizer.param_groups[0]['lr']\n",
    "        self.log('lr', lr, prog_bar=True)\n",
    "        self.log('train_loss', loss)\n",
    "        return {'loss': loss, 'lr': lr}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        val_loss, punct_logits, domain_logits = self._make_step(batch)\n",
    "        punct_preds = F.one_hot(self.loss.decode(punct_logits, batch['attention_mask']).flatten(),self.num_labels).to(device) if self.lossfn=='crf' else punct_logits.view(-1,self.num_labels)\n",
    "        punct_labels = F.one_hot(batch['labels'].flatten(),self.num_labels)\n",
    "        print('punct pred, labels',punct_preds.shape,punct_labels.shape)\n",
    "        self.punct_class_report.update(punct_preds, punct_labels)\n",
    "        if self.domains>1:\n",
    "            domain_labels=F.one_hot(batch['domain'].flatten(),self.domains)\n",
    "            domain_preds = domain_logits.view(-1,self.domains)\n",
    "            print('domain pred,label,logits',domain_preds.shape,domain_labels.shape, domain_logits.shape)\n",
    "            self.domain_class_report.update(domain_preds, domain_labels)\n",
    "            return {\n",
    "                'val_loss': val_loss,\n",
    "                'punct_tp': self.punct_class_report.tp,\n",
    "                'punct_fn': self.punct_class_report.fn,\n",
    "                'punct_fp': self.punct_class_report.fp,\n",
    "                'domain_tp': self.domain_class_report.tp,\n",
    "                'domain_fn': self.domain_class_report.fn,\n",
    "                'domain_fp': self.domain_class_report.fp,\n",
    "            }\n",
    "        return {\n",
    "            'val_loss': val_loss,\n",
    "            'punct_tp': self.punct_class_report.tp,\n",
    "            'punct_fn': self.punct_class_report.fn,\n",
    "            'punct_fp': self.punct_class_report.fp,\n",
    "        }\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        test_loss, punct_logits, domain_logits = self._make_step(batch)\n",
    "        punct_preds = F.one_hot(self.loss.decode(punct_logits, batch['attention_mask']).flatten(),self.num_labels).to(device) if self.loss_fn=='crf' else punct_logits.view(-1,self.num_labels)\n",
    "        punct_labels = F.one_hot(batch['labels'].flatten(),self.num_labels)\n",
    "        self.punct_class_report.update(punct_preds, punct_labels)\n",
    "        if self.domains>1:\n",
    "            domain_labels=F.one_hot(batch['domain'],self.domains)\n",
    "            domain_preds = domain_logits.view(-1,self.domains)\n",
    "            self.domain_class_report.update(domain_preds, domain_labels)\n",
    "            return {\n",
    "                'test_loss': test_loss,\n",
    "                'punct_tp': self.punct_class_report.tp,\n",
    "                'punct_fn': self.punct_class_report.fn,\n",
    "                'punct_fp': self.punct_class_report.fp,\n",
    "                'domain_tp': self.domain_class_report.tp,\n",
    "                'domain_fn': self.domain_class_report.fn,\n",
    "                'domain_fp': self.domain_class_report.fp,\n",
    "            }\n",
    "        return {\n",
    "                'test_loss': test_loss,\n",
    "                'punct_tp': self.punct_class_report.tp,\n",
    "                'punct_fn': self.punct_class_report.fn,\n",
    "                'punct_fp': self.punct_class_report.fp,\n",
    "            }\n",
    "    #https://github.com/NVIDIA/NeMo/blob/bb86f88143c89231f970e5b6bd9f78999fc45a90/nemo/collections/nlp/models/token_classification/punctuation_domainalization_model.py#L42\n",
    "    def multi_validation_epoch_end(self, outputs, dataloader_idx: int = 0):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "\n",
    "        # calculate metrics and log classification report for Punctuation task\n",
    "        punct_precision, punct_recall, punct_f1, punct_report = self.punct_class_report.compute()\n",
    "        logging.info(f'Punctuation report: {punct_report}')\n",
    "        self.log('val_loss', avg_loss, prog_bar=True)\n",
    "        self.log('punct_precision', punct_precision)\n",
    "        self.log('punct_f1', punct_f1)\n",
    "        self.log('punct_recall', punct_recall)\n",
    "        if self.domains>1:\n",
    "            # calculate metrics and log classification report for domainalization task\n",
    "            domain_precision, domain_recall, domain_f1, domain_report = self.domain_class_report.compute()\n",
    "            logging.info(f'Domain report: {domain_report}')\n",
    "            self.log('domain_precision', domain_precision)\n",
    "            self.log('domain_f1', domain_f1)\n",
    "            self.log('domain_recall', domain_recall)\n",
    "    def multi_test_epoch_end(self, outputs, dataloader_idx: int = 0):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        # calculate metrics and log classification report for Punctuation task\n",
    "        punct_precision, punct_recall, punct_f1, punct_report = self.punct_class_report.compute()\n",
    "        logging.info(f'Punctuation report: {punct_report}')\n",
    "        # calculate metrics and log classification report for domainalization task\n",
    "        self.log('test_loss', avg_loss, prog_bar=True)\n",
    "        self.log('punct_precision', punct_precision)\n",
    "        self.log('punct_f1', punct_f1)\n",
    "        self.log('punct_recall', punct_recall)\n",
    "        if self.domains>1:\n",
    "            domain_precision, domain_recall, domain_f1, domain_report = self.domain_class_report.compute()\n",
    "            logging.info(f'Domain report: {domain_report}')\n",
    "            self.log('domain_precision', domain_precision)\n",
    "            self.log('domain_f1', domain_f1)\n",
    "            self.log('domain_recall', domain_recall)\n",
    "        \n",
    "    def freeze_transformer_to(self, n:int, exclude_types=(torch.nn.BatchNorm1d, torch.nn.BatchNorm2d, torch.nn.BatchNorm3d)) -> None:\n",
    "        \"\"\"Freeze layers up to layer group `n`.\n",
    "        Look at each group, and freeze each paraemeter, except excluded types\n",
    "        \"\"\"\n",
    "        print(f\"freeze 1st {n} encoder layers of transformer\")\n",
    "        def set_requires_grad_for_module(module: torch.nn.Module, requires_grad: bool):\n",
    "            \"Sets each parameter in lthe module to the `requires_grad` value\"\n",
    "            params = list(module.parameters())\n",
    "            for param in params: \n",
    "                param.requires_grad = requires_grad\n",
    "            \n",
    "        for layer in list(self.transformer.encoder.layer)[:n]:\n",
    "            if not isinstance(layer, exclude_types): \n",
    "                set_requires_grad_for_module(layer, False)\n",
    "        \n",
    "        for layer in list(self.transformer.encoder.layer)[n:]:\n",
    "            set_requires_grad_for_module(layer, True)\n",
    "\n",
    "    def freeze(self) -> None:\n",
    "        for param in self.transformer.embeddings.parameters():\n",
    "            param.requires_grad=False\n",
    "\n",
    "        self.frozen=len(self.transformer.encoder.layer)\n",
    "        self.freeze_transformer_to(self.frozen)\n",
    "\n",
    "    def unfreeze(self,i:int=1):\n",
    "        self.freeze_transformer_to(max(0,self.frozen-i))\n",
    "        self.frozen-=1;\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.02)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "from nemo.core.neural_types import ChannelType, LabelsType, MaskType, NeuralType\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from typing import List, Optional, Dict\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "class PunctuationDomainDataset(Dataset):\n",
    "\n",
    "    @property\n",
    "    def output_types(self) -> Optional[Dict[str, NeuralType]]:\n",
    "        \"\"\"Returns definitions of module output ports.\n",
    "               \"\"\"\n",
    "        return {\n",
    "            \"input_ids\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"attention_mask\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"labels\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"domain\": NeuralType(('B'), ChannelType()),\n",
    "        }\n",
    "\n",
    "    def __init__(self, \n",
    "        csv_file:str, \n",
    "        tokenizer,\n",
    "        num_samples:int=256,\n",
    "        max_seq_length:int=256,\n",
    "        punct_label_ids: Dict[str, int] = None,\n",
    "        domain=0,\n",
    "        labelled=True,\n",
    "    ):\n",
    "        if not (os.path.exists(csv_file)):\n",
    "            raise FileNotFoundError(\n",
    "                f'{csv_file} not found. The data should be joined in 1 csv file.\\\n",
    "                    Each line of the file contains the subword token ids, masks and class labels per row.'\n",
    "            )\n",
    "\n",
    "        data_dir = os.path.dirname(csv_file)\n",
    "        filename = os.path.basename(csv_file)\n",
    "\n",
    "        if not filename.endswith('.csv'):\n",
    "            raise ValueError(\"{text_file} should have extension .csv\")\n",
    "        # filename = filename[:-4]\n",
    "        \n",
    "        self.csv_file = csv_file\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.set_num_samples(csv_file, num_samples)\n",
    "        self.domain=domain\n",
    "        self.labelled=labelled\n",
    "        self.tokenizer=tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = next(\n",
    "            pd.read_csv(\n",
    "                self.csv_file,\n",
    "                skiprows=(idx % self.len)*self.num_samples,\n",
    "                chunksize=self.num_samples,\n",
    "                header=None,\n",
    "                delimiter=' '))\n",
    "        x = torch.from_numpy(x.values).reshape(-1,3,self.max_seq_length) #x.shape[-1]//3\n",
    "        return {'input_ids': torch.as_tensor(x[:,0,:], dtype=torch.long),\n",
    "                'attention_mask': torch.as_tensor(x[:,1,:],dtype=torch.bool)if self.labelled else torch.zeros_like(x[:,1,:],dtype=torch.bool),\n",
    "                'labels': torch.as_tensor(x[:,2,:],dtype=torch.long),\n",
    "                'domain':self.domain*torch.ones(x.shape[0],1,dtype=torch.long)}\n",
    "\n",
    "    def set_num_samples(self,csv_file,num_samples):\n",
    "        self.num_samples = num_samples\n",
    "        self.total_samples=int(subprocess.Popen(['wc', '-l', csv_file], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0])\n",
    "        self.len = int(self.total_samples / self.num_samples)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def view(d)->list:\n",
    "        \"\"\":param d(dictionary): returns readable format of single input_ids and labels in the form of readable text\"\"\"\n",
    "        a,_,c=d.values()\n",
    "        return [' '.join([_[0]+_[1] for _ in list(zip(self.tokenizer.convert_ids_to_tokens(_[0]),[id2tag[id] for id in _[1].tolist()]))]) for _ in zip(a,c)]\n",
    "    \n",
    "    def shuffle(self, sorted=False, seed=42):\n",
    "        os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {}'.format(self.csv_file, self.csv_file, ['false','true'][sorted], seed))\n",
    "\n",
    "class PunctuationDomainDatasets(Dataset):\n",
    "    \n",
    "    @property\n",
    "    def output_types(self) -> Optional[Dict[str, NeuralType]]:\n",
    "        \"\"\"Returns definitions of module output ports. \"\"\"\n",
    "        return {\n",
    "            \"input_ids\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"attention_mask\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"labels\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"domain\": NeuralType(('B'), ChannelType()),\n",
    "        }\n",
    "\n",
    "    def __init__(self, *datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        b={k:torch.vstack([d[i][k] for d in self.datasets]) for k in ['input_ids','attention_mask','labels','domain']}\n",
    "        rand=torch.randperm(b['labels'].size()[0])\n",
    "        return {k:v[rand] for k,v in b.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(d) for d in self.datasets)\n",
    "\n",
    "class PunctuationInferenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates dataset to use during inference for punctuation and capitalization tasks with a pretrained model.\n",
    "    For dataset to use during training with labels, see BertPunctuationCapitalizationDataset.\n",
    "    Args:\n",
    "        queries file to sequences, each line should a sentence, no header.\n",
    "        max_seq_length: max sequence length minus 2 for [CLS] and [SEP]\n",
    "        tokenizer: such as AutoTokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def output_types(self) -> Optional[Dict[str, NeuralType]]:\n",
    "        \"\"\"Returns definitions of module output ports.\n",
    "               \"\"\"\n",
    "        return {\n",
    "            'input_ids': NeuralType(('B', 'T'), ChannelType()),\n",
    "            'attention_mask': NeuralType(('B', 'T'), MaskType()),\n",
    "        }\n",
    "\n",
    "    def __init__(self, queries: List[str], max_seq_length: int, tokenizer):\n",
    "        \"\"\" Initializes BertPunctuationInferDataset. \"\"\"\n",
    "        features = get_features(queries=queries, max_seq_length=max_seq_length, tokenizer=tokenizer)\n",
    "        self.all_input_ids = features['input_ids']\n",
    "        self.all_attention_mask = features['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids':self.all_input_ids[idx],\n",
    "            'attention_mask':self.all_attention_mask[idx],}\n",
    "        \n",
    "\n",
    "def get_features(\n",
    "    queries:str, \n",
    "    max_seq_length:int,\n",
    "    tokenizer,\n",
    "    punct_label_ids: dict = None,):\n",
    "\n",
    "    def flatten(list_of_lists):\n",
    "        for list in list_of_lists:\n",
    "            for item in list:\n",
    "                yield item\n",
    "\n",
    "    def pad_to_len(max_length,ids):\n",
    "        o=np.zeros(max_length, dtype=np.int)\n",
    "        o[:len(ids)]=np.array(ids)\n",
    "        return o\n",
    "\n",
    "    def position_to_mask(max_length,indices):\n",
    "        o=np.zeros(max_length,dtype=np.int)\n",
    "        o[indices%(max_length-2)+1]=1\n",
    "        return o\n",
    "\n",
    "    batch_ids=[]\n",
    "    batch_masks=[]\n",
    "    for query in queries:\n",
    "        wordlist=re.split('[^a-zA-Z0-9]+',query)\n",
    "        subwords=list(map(tokenizer.tokenize,wordlist))\n",
    "        subword_lengths=list(map(len,subwords))\n",
    "        subwords=list(flatten(subwords))\n",
    "        token_end_idxs=np.cumsum([0]+subword_lengths[:-1])+np.array(subword_lengths)-1\n",
    "        teim=token_end_idxs%(max_seq_length-2)\n",
    "        split_token_end_idxs=np.array_split(token_end_idxs,(np.argwhere((teim[1:])<teim[:-1]).flatten()+1).tolist())\n",
    "        split_subwords=np.array_split(subwords,np.arange(max_length-2,len(subwords),max_seq_length-2)) \n",
    "        ids=torch.tensor([pad_to_len(max_seq_length,tokenizer.convert_tokens_to_ids(['[CLS]']+list(_)+['[SEP]'])) for _ in split_subwords], dtype=torch.long)\n",
    "        masks=[position_to_mask(max_length,_) for _ in split_token_end_idxs]\n",
    "        batch_ids.append(ids)\n",
    "        batch_masks.append(masks)\n",
    "    return {'input_ids': torch.as_tensor(batch_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.as_tensor(batch_masks,dtype=torch.bool)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed': 42, 'trainer': {'gpus': 1, 'num_nodes': 1, 'max_epochs': 3, 'max_steps': None, 'accumulate_grad_batches': 1, 'gradient_clip_val': 0.0, 'amp_level': 'O0', 'precision': 16, 'accelerator': 'ddp', 'checkpoint_callback': False, 'logger': False, 'log_every_n_steps': 1, 'val_check_interval': 1.0, 'resume_from_checkpoint': None}, 'exp_manager': {'exp_dir': None, 'name': 'Punctuation_with_Domain_discriminator', 'create_tensorboard_logger': True, 'create_checkpoint_callback': True}, 'base_path': '/home/nxingyu2/data', 'model': {'nemo_path': None, 'transformer_path': 'google/electra-small-discriminator', 'punct_label_ids': ['', '!', ',', '-', '.', ':', ';', '?', '—', '…'], 'dataset': {'data_dir': '${base_path}', 'labelled': ['${base_path}/ted_talks_processed'], 'unlabelled': ['${base_path}/open_subtitles_processed'], 'max_seq_length': 128, 'pad_label': '', 'ignore_extra_tokens': False, 'ignore_start_end': False, 'use_cache': True, 'num_workers': 4, 'pin_memory': False, 'drop_last': False, 'num_labels': 10, 'num_domains': 2}, 'train_ds': {'shuffle': True, 'num_samples': -1, 'batch_size': 4}, 'validation_ds': {'shuffle': True, 'num_samples': -1, 'batch_size': 16}, 'tokenizer': {'tokenizer_name': '${model.language_model.pretrained_model_name}', 'vocab_file': None, 'tokenizer_model': None, 'special_tokens': None}, 'language_model': {'pretrained_model_name': '${model.transformer_path}', 'lm_checkpoint': None, 'config_file': None, 'config': None, 'unfrozen_layers': 0}, 'punct_head': {'punct_num_fc_layers': 1, 'fc_dropout': 0.1, 'activation': 'relu', 'log_softmax': False, 'use_transformer_init': True, 'loss': 'cel'}, 'domain_head': {'domain_num_fc_layers': 1, 'fc_dropout': 0.1, 'activation': 'relu', 'log_softmax': False, 'use_transformer_init': True, 'loss': 'cel', 'gamma': 1}, 'dice_loss': {'epsilon': 0.05, 'alpha': 2}, 'optim': {'name': 'adam', 'lr': 0.0001, 'weight_decay': 0.0, 'sched': {'name': 'WarmupAnnealing', 'warmup_steps': None, 'warmup_ratio': 0.1, 'last_epoch': -1, 'monitor': 'val_loss', 'reduce_on_plateau': False}}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model.punct_label_ids=OmegaConf.create(sorted(cfg.model.punct_label_ids))\n",
    "ids_to_labels = {_[0]:_[1] for _ in enumerate(cfg.model.punct_label_ids)}\n",
    "labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
    "cfg.base_path='/home/nxingyu2/data' #/home/nxingyu/data\n",
    "# cfg.base_path\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1612232037"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PunctuationDomainDataset(Dataset):\n",
    "\n",
    "    @property\n",
    "    def output_types(self) -> Optional[Dict[str, NeuralType]]:\n",
    "        \"\"\"Returns definitions of module output ports.\n",
    "               \"\"\"\n",
    "        return {\n",
    "            \"input_ids\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"attention_mask\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"labels\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"domain\": NeuralType(('B'), ChannelType()),\n",
    "        }\n",
    "\n",
    "    def __init__(self, \n",
    "        csv_file:str, \n",
    "        tokenizer,\n",
    "        num_samples:int=256,\n",
    "        max_seq_length:int=256,\n",
    "        punct_label_ids: Dict[str, int] = None,\n",
    "        domain=0,\n",
    "        labelled=True,\n",
    "    ):\n",
    "        if not (os.path.exists(csv_file)):\n",
    "            raise FileNotFoundError(\n",
    "                f'{csv_file} not found. The data should be joined in 1 csv file.\\\n",
    "                    Each line of the file contains the subword token ids, masks and class labels per row.'\n",
    "            )\n",
    "\n",
    "        data_dir = os.path.dirname(csv_file)\n",
    "        filename = os.path.basename(csv_file)\n",
    "\n",
    "        if not filename.endswith('.csv'):\n",
    "            raise ValueError(\"{text_file} should have extension .csv\")\n",
    "        # filename = filename[:-4]\n",
    "        \n",
    "        self.csv_file = pp(  csv_file)\n",
    "        self.max_seq_length = pp(  max_seq_length)\n",
    "        self.set_num_samples(csv_file, num_samples)\n",
    "        self.domain= pp( domain)\n",
    "        self.labelled= pp( labelled)\n",
    "        self.tokenizer= pp( tokenizer)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = next(\n",
    "            pd.read_csv(\n",
    "                self.csv_file,\n",
    "                skiprows=(idx % self.len)*self.num_samples,\n",
    "                header=None,\n",
    "                dtype=str,\n",
    "                chunksize=self.num_samples,\n",
    "                ))[1]\n",
    "        chunked=chunk_examples_with_degree(0)(x)\n",
    "        batched=chunk_to_len_batch(self.max_seq_length,self.tokenizer,chunked['texts'],chunked['tags'],self.labelled)\n",
    "        batched['domain']=self.domain*torch.ones(batched['input_ids'].shape[0],1,dtype=torch.long)\n",
    "        rand=torch.randperm(batched['domain'].size()[0])\n",
    "        return {k:v[rand] for k,v in batched.items()}\n",
    "#        {'input_ids': torch.as_tensor(x[:,0,:], dtype=torch.long),\n",
    "#         'attention_mask': torch.as_tensor(x[:,1,:],dtype=torch.bool)if self.labelled else torch.zeros_like(x[:,1,:],dtype=torch.bool),\n",
    "#         'labels': torch.as_tensor(x[:,2,:],dtype=torch.long),\n",
    "#         'domain':self.domain*torch.ones(x.shape[0],1,dtype=torch.long)}\n",
    "\n",
    "    def set_num_samples(self,csv_file,num_samples):\n",
    "        self.num_samples = num_samples\n",
    "        self.total_samples=int(subprocess.Popen(['wc', '-l', csv_file], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0])\n",
    "        self.len = int(self.total_samples / self.num_samples)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def view(d)->list:\n",
    "        \"\"\":param d(dictionary): returns readable format of single input_ids and labels in the form of readable text\"\"\"\n",
    "        a,_,c=d.values()\n",
    "        return [' '.join([_[0]+_[1] for _ in list(zip(self.tokenizer.convert_ids_to_tokens(_[0]),[id2tag[id] for id in _[1].tolist()]))]) for _ in zip(a,c)]\n",
    "    \n",
    "    def shuffle(self, sorted=False, seed=42):\n",
    "        os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {}'.format(self.csv_file, self.csv_file, ['false','true'][sorted], seed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "from itertools import cycle\n",
    "class PunctuationDomainDataset(IterableDataset):\n",
    "    def __init__(self, \n",
    "        csv_file:str, \n",
    "        tokenizer,\n",
    "        num_samples:int=256,\n",
    "        max_seq_length:int=256,\n",
    "        punct_label_ids: Dict[str, int] = None,\n",
    "        domain=0,\n",
    "        labelled=True,\n",
    "    ):\n",
    "        if not (os.path.exists(csv_file)):\n",
    "            raise FileNotFoundError(\n",
    "                f'{csv_file} not found. The data should be joined in 1 csv file.\\\n",
    "                    Each line of the file contains the subword token ids, masks and class labels per row.'\n",
    "            )\n",
    "\n",
    "        data_dir = os.path.dirname(csv_file)\n",
    "        filename = os.path.basename(csv_file)\n",
    "\n",
    "        if not filename.endswith('.csv'):\n",
    "            raise ValueError(\"{text_file} should have extension .csv\")\n",
    "        # filename = filename[:-4]\n",
    "        \n",
    "        self.csv_file = csv_file\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.set_num_samples(csv_file, num_samples)\n",
    "        self.domain= domain\n",
    "        self.labelled= labelled\n",
    "        self.tokenizer= tokenizer\n",
    "    def __iter__(self):\n",
    "        self.dataset=iter(pd.read_csv(\n",
    "                self.csv_file,\n",
    "                skiprows=(0 % self.len)*self.num_samples,\n",
    "                header=None,\n",
    "                dtype=str,\n",
    "                chunksize=self.num_samples,\n",
    "                ))\n",
    "        return self\n",
    "        \n",
    "    \n",
    "    def __next__(self):\n",
    "        x = next(self.dataset)[1]\n",
    "        chunked=chunk_examples_with_degree(0)(x)\n",
    "        batched=chunk_to_len_batch(self.max_seq_length,self.tokenizer,chunked['texts'],chunked['tags'],self.labelled)\n",
    "        batched['domain']=self.domain*torch.ones(batched['input_ids'].shape[0],1,dtype=torch.long)\n",
    "        rand=torch.randperm(batched['domain'].size()[0])\n",
    "        return {k:v[rand] for k,v in batched.items()}\n",
    "#        {'input_ids': torch.as_tensor(x[:,0,:], dtype=torch.long),\n",
    "#         'attention_mask': torch.as_tensor(x[:,1,:],dtype=torch.bool)if self.labelled else torch.zeros_like(x[:,1,:],dtype=torch.bool),\n",
    "#         'labels': torch.as_tensor(x[:,2,:],dtype=torch.long),\n",
    "#         'domain':self.domain*torch.ones(x.shape[0],1,dtype=torch.long)}\n",
    "\n",
    "    def set_num_samples(self,csv_file,num_samples):\n",
    "        self.num_samples = num_samples\n",
    "        self.total_samples=int(subprocess.Popen(['wc', '-l', csv_file], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0])\n",
    "        self.len = int(self.total_samples / self.num_samples)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def view(d)->list:\n",
    "        \"\"\":param d(dictionary): returns readable format of single input_ids and labels in the form of readable text\"\"\"\n",
    "        a,_,c=d.values()\n",
    "        return [' '.join([_[0]+_[1] for _ in list(zip(self.tokenizer.convert_ids_to_tokens(_[0]),[id2tag[id] for id in _[1].tolist()]))]) for _ in zip(a,c)]\n",
    "    \n",
    "    def shuffle(self, sorted=False, seed=42):\n",
    "        os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {}'.format(self.csv_file, self.csv_file, ['false','true'][sorted], seed))\n",
    "\n",
    "class PunctuationDomainDatasets(IterableDataset):\n",
    "    \n",
    "    @property\n",
    "    def output_types(self) -> Optional[Dict[str, NeuralType]]:\n",
    "        \"\"\"Returns definitions of module output ports. \"\"\"\n",
    "        return {\n",
    "            \"input_ids\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"attention_mask\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"subtoken_mask\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"labels\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"domain\": NeuralType(('B'), ChannelType()),\n",
    "        }\n",
    "\n",
    "    def __init__(self, \n",
    "                 split:str,\n",
    "                 num_samples:int,\n",
    "                 max_seq_length:int,\n",
    "                 punct_label_ids: Dict[str, int],\n",
    "                 labelled: List[str],\n",
    "                 unlabelled: List[str],\n",
    "                 tokenizer):\n",
    "        \n",
    "        self.datasets = []\n",
    "        self.iterators=[]\n",
    "        for i,path in enumerate(labelled):\n",
    "            dataset=PunctuationDomainDataset(\n",
    "                    csv_file=f'{path}.{split}.csv', tokenizer=tokenizer,\n",
    "                    num_samples=num_samples,max_seq_length=max_seq_length,\n",
    "                    punct_label_ids=punct_label_ids,domain=i,labelled=True,)\n",
    "            self.datasets.append(dataset)\n",
    "            self.iterators.append(cycle(dataset))\n",
    "            \n",
    "        for i,path in enumerate(unlabelled):\n",
    "            dataset=PunctuationDomainDataset(\n",
    "                    csv_file=f'{path}.{split}.csv', tokenizer=tokenizer,\n",
    "                    num_samples=num_samples,max_seq_length=max_seq_length,\n",
    "                    punct_label_ids=punct_label_ids,domain=len(labelled)+i,labelled=False,)\n",
    "            self.datasets.append(dataset)\n",
    "            self.iterators.append(cycle(dataset))\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.iterators=[]\n",
    "        for dataset in self.datasets:\n",
    "            self.iterators.append(cycle(dataset))\n",
    "        return self\n",
    "            \n",
    "    def __next__(self):\n",
    "        ds=[next(d) for d in self.datasets]\n",
    "        min_batch=1000000\n",
    "        for d in ds:\n",
    "            size=d['domain'].size()[0]\n",
    "            if size<min_batch:\n",
    "                min_batch=size\n",
    "        #Ensure all domains are evenly represented\n",
    "        b={k:torch.cat([d[k][:min_batch] for d in ds],axis=0) for k in ['input_ids','attention_mask','subtoken_mask','labels','domain']}\n",
    "        rand=torch.randperm(b['labels'].size()[0])\n",
    "        return {k:v[rand] for k,v in b.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(d) for d in self.datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstrain=PunctuationDomainDatasets(\n",
    "        split='train',\n",
    "        tokenizer=AutoTokenizer.from_pretrained(cfg.model.transformer_path),\n",
    "        num_samples=1,\n",
    "        max_seq_length=128,\n",
    "        punct_label_ids=labels_to_ids,\n",
    "        labelled=['/home/nxingyu/data/ted_talks_processed'],\n",
    "        unlabelled=[]\n",
    "    )\n",
    "\n",
    "# ds=PunctuationDomainDataset( \n",
    "#     csv_file='/home/nxingyu/data/ted_talks_processed.train.csv', \n",
    "#     tokenizer=AutoTokenizer.from_pretrained(cfg.model.transformer_path),\n",
    "#     num_samples=16,\n",
    "#     max_seq_length=128,\n",
    "#     punct_label_ids=labels_to_ids,\n",
    "#     domain=0,\n",
    "#     labelled=True,\n",
    "# )\n",
    "# ds.shuffle(sorted=True)\n",
    "# ds.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2064,  1005,  ...,  2056,  2024,   102],\n",
       "         [  101,  1998,  1996,  ...,  2128,  2183,   102],\n",
       "         [  101,  2512,  6299,  ...,   102,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2036,  2000,  ...,  2001,  1037,   102],\n",
       "         [  101,  2000,  2828,  ...,  2009,  1005,   102],\n",
       "         [  101,  2009, 11901,  ...,  2000,  2030,   102]]),\n",
       " 'attention_mask': tensor([[ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True, False, False],\n",
       "         ...,\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True]]),\n",
       " 'subtoken_mask': tensor([[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False,  True,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]]),\n",
       " 'labels': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 4,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.int16),\n",
       " 'domain': tensor([[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]])}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it=iter(dstrain)\n",
    "next(it)\n",
    "# len(dstrain)\n",
    "# next(it)\n",
    "# next(it)\n",
    "# next(it)\n",
    "# next(iter(dstrain))\n",
    "# dstrain.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  5654,  7028,  ...,  2008,  1996,   102],\n",
       "         [  101,  1040,  2185,  ...,  1045,  1005,   102],\n",
       "         [  101,  2054,  2017,  ...,  1040,  3305,   102],\n",
       "         ...,\n",
       "         [  101, 13346,  2000,  ...,  2022,  2006,   102],\n",
       "         [  101,  2054,  2065,  ...,  1005,  1055,   102],\n",
       "         [  101,  1056,  2079,  ...,  1997,  2256,   102]]),\n",
       " 'attention_mask': tensor([[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]]),\n",
       " 'subtoken_mask': tensor([[False, False,  True,  ..., False, False, False],\n",
       "         [False, False,  True,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ...,  True, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False,  True,  ..., False, False, False]]),\n",
       " 'labels': tensor([[0, 0, 2,  ..., 0, 0, 0],\n",
       "         [0, 0, 4,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 4, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 4,  ..., 0, 0, 0]], dtype=torch.int16),\n",
       " 'domain': tensor([[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]])}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([596, 1])"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ds0=ds[0]\n",
    "ds0['domain'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PunctuationDomainDatasets(Dataset):\n",
    "    \n",
    "    @property\n",
    "    def output_types(self) -> Optional[Dict[str, NeuralType]]:\n",
    "        \"\"\"Returns definitions of module output ports. \"\"\"\n",
    "        return {\n",
    "            \"input_ids\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"attention_mask\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"subtoken_mask\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"labels\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"domain\": NeuralType(('B'), ChannelType()),\n",
    "        }\n",
    "\n",
    "    def __init__(self, \n",
    "                 split:str,\n",
    "                 num_samples:int,\n",
    "                 max_seq_length:int,\n",
    "                 punct_label_ids: Dict[str, int],\n",
    "                 labelled: List[str],\n",
    "                 unlabelled: List[str],\n",
    "                 tokenizer):\n",
    "        \n",
    "        self.datasets = []\n",
    "        for i,path in enumerate(labelled):\n",
    "            self.datasets.append(PunctuationDomainDataset(\n",
    "                    csv_file=f'{path}.{split}.csv', tokenizer=tokenizer,\n",
    "                    num_samples=num_samples,max_seq_length=max_seq_length,\n",
    "                    punct_label_ids=punct_label_ids,domain=i,labelled=True,))\n",
    "            \n",
    "        for i,path in enumerate(unlabelled):\n",
    "            self.datasets.append(PunctuationDomainDataset(\n",
    "                    csv_file=f'{path}.{split}.csv', tokenizer=tokenizer,\n",
    "                    num_samples=num_samples,max_seq_length=max_seq_length,\n",
    "                    punct_label_ids=punct_label_ids,domain=len(labelled)+i,labelled=False,))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        ds=[d[i] for d in self.datasets]\n",
    "        min_batch=1000000\n",
    "        for d in ds:\n",
    "            size=d['domain'].size()[0]\n",
    "            if size<min_batch:\n",
    "                min_batch=size\n",
    "        #Ensure all domains are evenly represented\n",
    "        b={k:torch.vstack([d[k][:min_batch] for d in ds]) for k in ['input_ids','attention_mask','subtoken_mask','labels','domain']}\n",
    "        rand=torch.randperm(b['labels'].size()[0])\n",
    "        return {k:v[rand] for k,v in b.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(d) for d in self.datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_dir': '${base_path}', 'labelled': ['${base_path}/ted_talks_processed'], 'unlabelled': ['${base_path}/open_subtitles_processed'], 'max_seq_length': 128, 'pad_label': '', 'ignore_extra_tokens': False, 'ignore_start_end': False, 'use_cache': True, 'num_workers': 2, 'pin_memory': False, 'drop_last': False}"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-a38055a18a85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# d.describe()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# x=torch.utils.data.DataLoader(dstrain)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-2be02ae2878c>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mmin_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-2be02ae2878c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mmin_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-0443db41d089>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 delimiter=' '))\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#x.shape[-1]//3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         return {'input_ids': torch.as_tensor(x[:,0,:], dtype=torch.long),\n\u001b[1;32m     64\u001b[0m                 \u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabelled\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "# d=pd.read_csv(f'{cfg.model.dataset.labelled[0]}.csv',header=None)[1].str.split().map(len)\n",
    "# d.describe()\n",
    "# x=torch.utils.data.DataLoader(dstrain)\n",
    "next(iter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstrain=PunctuationDomainDatasets(\n",
    "        split='train',\n",
    "        tokenizer=AutoTokenizer.from_pretrained(cfg.model.transformer_path),\n",
    "        num_samples=1,\n",
    "        max_seq_length=128,\n",
    "        punct_label_ids=labels_to_ids,\n",
    "        labelled=list(cfg.model.dataset.labelled),\n",
    "        unlabelled=list(cfg.model.dataset.unlabelled)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([199])"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dstrain0=dstrain[0]\n",
    "# dstrain0['input_ids'].shape\n",
    "# r=torch.randperm(414)\n",
    "sum(dstrain0['domain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningDataModule\n",
    "from torch import dtype\n",
    "from data import PunctuationDomainDataset, PunctuationDomainDatasets\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from nemo.utils import logging\n",
    "\n",
    "class PunctuationDataModule(LightningDataModule):\n",
    "    def __init__(self, \n",
    "            tokenizer,\n",
    "            labelled: List[str], \n",
    "            unlabelled: List[str], \n",
    "            train_batch_size: int,\n",
    "            max_seq_length:int = 256,\n",
    "            val_batch_size:int = 256, \n",
    "            num_workers:int = 1,\n",
    "            pin_memory:bool = False,\n",
    "            drop_last:bool = False\n",
    "            ):\n",
    "        #unlabelled=[], batch_size = 256, max_seq_length = 256, num_workers=1):\n",
    "        super().__init__()\n",
    "        self.labelled=labelled\n",
    "        self.tokenizer=tokenizer\n",
    "        self.unlabelled=unlabelled\n",
    "        self.num_domains=len(labelled)+len(unlabelled)\n",
    "        self.train_batch_size = max(1,train_batch_size//self.num_domains)\n",
    "        logging.info(f\"using training batch_size of {self.train_batch_size} for each domain\")\n",
    "        self.val_batch_size = max(1,val_batch_size//self.num_domains)\n",
    "        logging.info(f\"using dev batch_size of {self.train_batch_size} for each domain\")\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.num_workers=num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "        self.train_dataset={}\n",
    "        self.dev_dataset={}\n",
    "        self.test_dataset={}\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        for unlabelled,l in enumerate([self.labelled,self.unlabelled]):\n",
    "            for i,p in enumerate(l):\n",
    "                domain=i+unlabelled*len(self.labelled) #unlabelled domain is increasing after labelled\n",
    "                try:\n",
    "                    with open(\"{}.train-stride.csv\".format(p),'r') as f:\n",
    "                        s=len(f.readline().split(' '))//3\n",
    "                except IOError:\n",
    "                    s=0\n",
    "                if (s!=self.max_seq_length):\n",
    "                    logging.info(f\"copying train file from {p}.train-batched.csv to {p}.train-stride.csv\")\n",
    "                    os.system(\"cp {} {}\".format(p+'.train-batched.csv',p+'.train-stride.csv'))\n",
    "                    if (self.max_seq_length!=256):\n",
    "                        logging.info(f'generating training strides: {self.max_seq_length}')\n",
    "                        n=np.loadtxt(open(p+\".train-stride.csv\", \"rb\"))\n",
    "                        np.savetxt(p+\".train-stride.csv\", self.with_stride_split(n,self.max_seq_length),fmt='%d')\n",
    "\n",
    "                if stage=='fit' or None:\n",
    "                    self.train_dataset[domain] = PunctuationDomainDataset(p+'.train-stride.csv', num_samples=self.train_batch_size, max_seq_length=self.max_seq_length, domain = domain, labelled=bool(1-unlabelled), tokenizer=self.tokenizer)\n",
    "                    self.dev_dataset[domain] =  PunctuationDomainDataset(p+'.dev-batched.csv', num_samples=self.val_batch_size, max_seq_length=self.max_seq_length, domain = domain, labelled=bool(1-unlabelled), tokenizer=self.tokenizer)\n",
    "                    pp(self.train_dataset[domain].shuffle(sorted=True))\n",
    "                    pp(self.train_dataset[domain].shuffle())\n",
    "\n",
    "                if stage == 'test' or stage is None:\n",
    "                    self.test_dataset[domain] =  PunctuationDomainDataset(p+'.test-batched.csv', num_samples=self.val_batch_size, max_seq_length=self.max_seq_length, domain = domain, labelled=bool(1-unlabelled), tokenizer=self.tokenizer)\n",
    "\n",
    "    def shuffle(self):\n",
    "        for dataset in self.train_dataset.values():\n",
    "            dataset.shuffle()\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(PunctuationDomainDatasets(*self.train_dataset.values()),batch_size=None,num_workers=self.num_workers,pin_memory=self.pin_memory,drop_last=self.drop_last)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(PunctuationDomainDatasets(*self.dev_dataset.values()),batch_size=None,num_workers=self.num_workers,pin_memory=self.pin_memory,drop_last=self.drop_last)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(PunctuationDomainDatasets(*self.test_dataset.values()),batch_size=None,num_workers=self.num_workers,pin_memory=self.pin_memory,drop_last=self.drop_last)\n",
    "\n",
    "    def with_stride_split(n,l):\n",
    "        def with_stride(t,l):\n",
    "            a=t[0,0]\n",
    "            z=t[0,-1]\n",
    "            t=t[:,1:-1].flatten()\n",
    "            t=np.trim_zeros(t,'b')\n",
    "            s=t.shape[0]\n",
    "            nh=-(-s//(l-2))\n",
    "            f=np.zeros((nh*(l-2),1))  \n",
    "            f[:s,0]=t\n",
    "            return np.hstack([np.ones((nh,1))*a,np.reshape(f,(-1,l-2)),np.ones((nh,1))*z])\n",
    "        s=n.shape[1]\n",
    "        a,b,c=n[:,:s//3],n[:,s//3:2*s//3],n[:,2*s//3:]\n",
    "        a,b,c=with_stride(a,l), with_stride(b,l), with_stride(c,l)\n",
    "        c1=np.zeros(a.shape)\n",
    "        c1[:c.shape[0],:]=c\n",
    "        return np.hstack([a,b,c1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def flatten(list_of_lists):\n",
    "    for l in list_of_lists:\n",
    "        for item in l:\n",
    "            yield item\n",
    "\n",
    "def pad_to_len(max_seq_length,ids):\n",
    "    '''[0, 1, 2] -> array([0, 1, 2, 0, 0, 0, 0, 0, 0, 0])'''\n",
    "    o=np.zeros(max_seq_length, dtype=np.int)\n",
    "    o[:len(ids)]=np.array(ids)\n",
    "    return o\n",
    "\n",
    "def position_to_mask(max_seq_length:int,indices:list):\n",
    "    '''[0, 2, 5] -> array([0, 1, 0, 1, 0, 0, 1, 0, 0, 0])'''\n",
    "    o=np.zeros(max_seq_length,dtype=np.int)\n",
    "    o[np.array(indices)%(max_seq_length-2)+1]=1\n",
    "    return o\n",
    "\n",
    "def align_labels_to_mask(mask,labels):\n",
    "    '''[0,1,0],[2] -> [0,2,0]'''\n",
    "    assert(sum(mask)==len(labels))\n",
    "    mask[mask>0]=torch.tensor(labels)\n",
    "    return mask.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "def text2masks(n):\n",
    "    def text2masks(text):\n",
    "        '''Converts single paragraph of text into a list of words and corresponding punctuation based on the degree requested.'''\n",
    "        if n==0: \n",
    "            refilter=\"(?<=[.?!,;:\\-—… ])(?=[^.?!,;:\\-—… ])|$\"\n",
    "        else:\n",
    "            refilter=\"[.?!,;:\\-—…]{1,%d}(?= *[^.?!,;:\\-—…]+|$)|(?<=[^.?!,;:\\-—…]) +(?=[^.?!,;:\\-—…])\"%(n)\n",
    "        text=re.sub(r'^[_\\W]*','',text)\n",
    "        word=re.split(refilter,text, flags=re.V1)\n",
    "        punct=re.findall(refilter,text, flags=re.V1)\n",
    "        wordlist,punctlist=([] for _ in range(2))\n",
    "        if word[-1]=='': # ensures text aligns\n",
    "            word.pop()\n",
    "        else:\n",
    "            punct.append('')\n",
    "        \n",
    "        for i in zip(word,punct): #+[''] to correspond to the last word or '' after the last punctuation.\n",
    "            w,p=i[0].strip(),i[1].strip()\n",
    "            if w!='':\n",
    "                wordlist.append(re.sub(r'[.?!,;:\\-—… ]','',w))\n",
    "                punctlist.append(0 if not w[-1] in '.?!,;:-—…' else labels_to_ids[w[-1]])\n",
    "            if p!='':\n",
    "                wordlist.append(p)\n",
    "                punctlist.append(0)\n",
    "        return(wordlist,punctlist)\n",
    "    return text2masks\n",
    "def chunk_examples_with_degree(n):\n",
    "    '''Ensure batched=True if using dataset.map or ensure the examples are wrapped in lists.'''\n",
    "    def chunk_examples(examples):\n",
    "        output={}\n",
    "        output['texts']=[]\n",
    "        output['tags']=[]\n",
    "        for sentence in examples:\n",
    "            text,tag=text2masks(n)(sentence)\n",
    "            output['texts'].append(text)\n",
    "            output['tags'].append(tag)\n",
    "            # output['tags'].append([0]+tag if text[0]!='' else tag) # [0]+tag so that in all case, the first tag refers to [CLS]\n",
    "            # not necessary since all the leading punctuations are stripped\n",
    "        return output\n",
    "    return chunk_examples\n",
    "assert(chunk_examples_with_degree(0)(['Hello!Bye…'])=={'texts': [['Hello', 'Bye']], 'tags': [[1, 9]]})\n",
    "\n",
    "def subword_tokenize(tokenizer,tokens):\n",
    "    subwords = list(map(tokenizer.tokenize, tokens))\n",
    "    subword_lengths = list(map(len, subwords))\n",
    "    subwords = list(flatten(subwords))\n",
    "    token_end_idxs = np.cumsum([0]+subword_lengths[:-1])+np.array(subword_lengths)-1\n",
    "    return subwords, token_end_idxs\n",
    "\n",
    "def chunk_to_len(max_seq_length,tokenizer,tokens,labels=None):\n",
    "    subwords,token_end_idxs = subword_tokenize(tokenizer,tokens)\n",
    "    teim=token_end_idxs%(max_seq_length-2)\n",
    "    breakpoints=(np.argwhere(teim[1:]<teim[:-1]).flatten()+1).tolist()\n",
    "    split_token_end_idxs=np.array_split(token_end_idxs,breakpoints)\n",
    "    split_subwords=np.array_split(subwords,np.arange(max_seq_length-2,len(subwords),max_seq_length-2))\n",
    "    ids=[pad_to_len(max_seq_length,tokenizer.convert_tokens_to_ids(['[CLS]']+list(_)+['[SEP]'])) for _ in split_subwords]\n",
    "    masks=[position_to_mask(max_seq_length,_) for _ in split_token_end_idxs]\n",
    "    padded_labels=None\n",
    "    if labels!=None:\n",
    "        split_labels=np.array_split(labels,breakpoints)\n",
    "        padded_labels=[pad_to_len(max_seq_length,align_labels_to_mask(*_)) for _ in zip(masks,split_labels)]\n",
    "    return ids,masks,padded_labels\n",
    "    \n",
    "def chunk_to_len_batch(max_seq_length,tokenizer,tokens,labels=None,labelled=True, ignore_index=-100):\n",
    "    batch_ids=[]\n",
    "    batch_masks=[]\n",
    "    batch_labels=[]\n",
    "    for i,_ in enumerate(zip(tokens,tokens) if labels==None else zip(tokens,labels)):\n",
    "        a,b,c=chunk_to_len(max_seq_length,tokenizer,*_) if labels else chunk_to_len(max_seq_length,tokenizer,_[0])\n",
    "        batch_ids.extend(a)\n",
    "        batch_masks.extend(b)\n",
    "        if labelled==True:\n",
    "            batch_labels.extend(c)\n",
    "    output = {'input_ids': torch.as_tensor(batch_ids, dtype=torch.long),\n",
    "              'attention_mask': torch.as_tensor(batch_ids, dtype=torch.bool),\n",
    "              'subtoken_mask': torch.as_tensor(batch_masks,dtype=torch.bool)}#*labelled\n",
    "    output['subtoken_mask']|=(output['input_ids']==101)|(output['input_ids']==102)\n",
    "    output['subtoken_mask']&=labelled\n",
    "#     output['input_ids']+=ignore_index*(~output['subtoken_mask'])\n",
    "    \n",
    "    output['labels']=torch.as_tensor(batch_labels,dtype=torch.short) if labelled==True else torch.zeros_like(output['input_ids'],dtype=torch.short)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PunctuationInferenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates dataset to use during inference for punctuation and capitalization tasks with a pretrained model.\n",
    "    For dataset to use during training with labels, see BertPunctuationCapitalizationDataset.\n",
    "    Args:\n",
    "        queries file to sequences, each line should a sentence, no header.\n",
    "        max_seq_length: max sequence length minus 2 for [CLS] and [SEP]\n",
    "        tokenizer: such as AutoTokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def output_types(self) -> Optional[Dict[str, NeuralType]]:\n",
    "        \"\"\"Returns definitions of module output ports.\n",
    "               \"\"\"\n",
    "        return {\n",
    "            'input_ids': NeuralType(('B', 'T'), ChannelType()),\n",
    "            'attention_mask': NeuralType(('B', 'T'), MaskType()),\n",
    "            'subtoken_mask': NeuralType(('B', 'T'), MaskType()),\n",
    "            \"labels\": NeuralType(('B', 'T'), ChannelType()),\n",
    "        }\n",
    "\n",
    "    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, degree:int=0,):\n",
    "        \"\"\" Initializes BertPunctuationInferDataset. \"\"\"\n",
    "        self.degree=degree\n",
    "        chunked=chunk_examples_with_degree(self.degree)(queries)\n",
    "        features = chunk_to_len_batch(max_seq_length=max_seq_length, tokenizer=tokenizer,tokens=chunked['texts'],labelled=False)\n",
    "        self.all_input_ids = pp(features['input_ids'])\n",
    "        self.all_attention_mask = pp(features['attention_mask'])\n",
    "        self.all_subtoken_mask = pp(features['subtoken_mask'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids':self.all_input_ids[idx],\n",
    "            'attention_mask':self.all_attention_mask[idx],\n",
    "               'subtoken_mask':self.all_subtoken_mask[idx]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| features['input_ids']: tensor shape torch.Size([5, 5]) type torch.int64\n",
      "ic| features['attention_mask']: tensor shape torch.Size([5, 5]) type torch.bool\n",
      "ic| features['subtoken_mask']: tensor shape torch.Size([5, 5]) type torch.bool\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  7592,  9541,  9541,   102],\n",
       "         [  101,   999,  8038,  2100,   102],\n",
       "         [  101,   999,  9061,  2203,   102],\n",
       "         [  101, 14141,  1012,   102,     0],\n",
       "         [  101,  7592,   102,     0,     0]]),\n",
       " 'attention_mask': tensor([[ True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True, False],\n",
       "         [ True,  True,  True, False, False]]),\n",
       " 'subtoken_mask': tensor([[False, False, False, False, False],\n",
       "         [False, False, False, False, False],\n",
       "         [False, False, False, False, False],\n",
       "         [False, False, False, False, False],\n",
       "         [False, False, False, False, False]])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # split='train'\n",
    "# # o=pd.read_csv(f'{cfg.model.dataset.labelled[0]}.{split}.csv',\n",
    "# #                   dtype='str',\n",
    "# #                   header=None,\n",
    "# #                   chunksize=10)\n",
    "# # t=next(iter(o))\n",
    "# sample_out = chunk_examples_with_degree(0)(t[1])\n",
    "# tokenizer=AutoTokenizer.from_pretrained(cfg.model.transformer_path)\n",
    "# # subword_tokenize(sample_out['texts'][0])\n",
    "# sample_out\n",
    "# sample_out['texts'],sample_out['tags']\n",
    "# chunk_to_len_batch(1000,tokenizer,sample_out['texts'][:10],sample_out['tags'][:10])\n",
    "# chunk_examples_with_degree(0)(t[1])\n",
    "# chunk_examples_with_degree(0)(['!!Hellooooo! Yay! Bye Enddd.',\"Hello\"])\n",
    "inferData=PunctuationInferenceDataset(tokenizer=tokenizer, queries=['!!Hellooooo! Yay! Bye Enddd.',\"Hello\"], max_seq_length=5,degree=1)\n",
    "inferData[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| features['input_ids']: tensor tensor([[  101,  7592,  9541,  9541,   102],\n",
      "                                   [  101,  8038,  2100,  9061,   102],\n",
      "                                   [  101,  2203, 14141,   102,     0],\n",
      "                                   [  101,  7592,   102,     0,     0]]) shape torch.Size([4, 5]) type torch.int64\n",
      "ic| features['attention_mask']: tensor tensor([[ True,  True,  True,  True,  True],\n",
      "                                        [ True,  True,  True,  True,  True],\n",
      "                                        [ True,  True,  True,  True, False],\n",
      "                                        [ True,  True,  True, False, False]]) shape torch.Size([4, 5]) type torch.bool\n",
      "ic| features['subtoken_mask']: tensor tensor([[False, False, False,  True, False],\n",
      "                                       [False, False,  True,  True, False],\n",
      "                                       [False, False,  True, False, False],\n",
      "                                       [False,  True, False, False, False]]) shape torch.Size([4, 5]) type torch.bool\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  7592,  9541,  9541,   102],\n",
       "         [  101,  8038,  2100,  9061,   102],\n",
       "         [  101,  2203, 14141,   102,     0],\n",
       "         [  101,  7592,   102,     0,     0]]),\n",
       " 'attention_mask': tensor([[ True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True, False],\n",
       "         [ True,  True,  True, False, False]]),\n",
       " 'subtoken_mask': tensor([[False, False, False,  True, False],\n",
       "         [False, False,  True,  True, False],\n",
       "         [False, False,  True, False, False],\n",
       "         [False,  True, False, False, False]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inferData=PunctuationInferenceDataset(tokenizer=tokenizer, queries=['!!Hellooooo! Yay! Bye Enddd.',\"Hello\"], max_seq_length=5,degree=0)\n",
    "inferData[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PunctuationInferDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates dataset to use during inference for punctuation and capitalization tasks with a pretrained model.\n",
    "    For dataset to use during training with labels, see BertPunctuationCapitalizationDataset.\n",
    "    Args:\n",
    "        queries file to sequences, each line should a sentence, no header.\n",
    "        max_seq_length: max sequence length minus 2 for [CLS] and [SEP]\n",
    "        tokenizer: such as AutoTokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def output_types(self) -> Optional[Dict[str, NeuralType]]:\n",
    "        \"\"\"Returns definitions of module output ports.\n",
    "               \"\"\"\n",
    "        return {\n",
    "            'input_ids': NeuralType(('B', 'T'), ChannelType()),\n",
    "            'attention_mask': NeuralType(('B', 'T'), MaskType()),\n",
    "        }\n",
    "\n",
    "    def __init__(self, queries: List[str], max_seq_length: int, tokenizer):\n",
    "        \"\"\" Initializes BertPunctuationInferDataset. \"\"\"\n",
    "        features = pp(get_features(queries=queries, max_seq_length=max_seq_length, tokenizer=tokenizer))\n",
    "        self.all_input_ids = pp(features['input_ids'])\n",
    "        self.all_attention_mask = pp(features['attention_mask'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids':self.all_input_ids[idx],\n",
    "            'attention_mask':self.all_attention_mask[idx]}\n",
    "\n",
    "def get_features(\n",
    "    queries:str, \n",
    "    tokenizer,\n",
    "    max_seq_length:int,\n",
    "    degree:int=0,\n",
    "    punct_label_ids: dict = None,):\n",
    "\n",
    "    batch_ids=[]\n",
    "    batch_masks=[]\n",
    "    for query in queries: #\n",
    "        #'Hellooooo! Yay! Bye Endd.'\n",
    "        wordlist=ic(re.split('[^a-zA-Z0-9]+',query,flags=re.V1)) #If end with punctuation, this includes a trailing ''\n",
    "        if wordlist[-1]=='': #Not necessary since the masks would ignore repeated end idxs.\n",
    "            wordlist=wordlist[:-1] \n",
    "        #['Hellooooo', 'Yay', 'Bye', 'Endd', '']\n",
    "        subwords=ic(list(map(tokenizer.tokenize,wordlist))) # [['hello', '##oo', '##oo'], ['ya', '##y'], ['bye'], ['end', '##d']]\n",
    "        subword_lengths=ic(list(map(len,subwords))) # [3, 2, 1, 1]\n",
    "        subwords=ic(list(flatten(subwords))) # ['hello', '##oo', '##oo', 'ya', '##y', 'bye', 'end', '##d']\n",
    "        token_end_idxs=ic(np.cumsum([0]+subword_lengths[:-1])+np.array(subword_lengths)-1) #'[2 4 5 6]'\n",
    "        teim=ic(token_end_idxs%(max_seq_length-2)) #'[2 0 1 2]'\n",
    "        pp(np.argwhere(teim[1:]<teim[:-1]).flatten()) #[0] returns last labels for each chunk.\n",
    "        split_token_end_idxs=np.array_split(token_end_idxs,(np.argwhere(teim[1:]<teim[:-1]).flatten()+1).tolist())\n",
    "        #[array([2]), array([4, 5, 6])]\n",
    "        pp(split_token_end_idxs)\n",
    "        split_subwords=ic(np.array_split(subwords,np.arange(max_seq_length-2,len(subwords),max_seq_length-2)))\n",
    "        #[array(['hello', '##oo', '##oo', 'ya'], dtype='<U5'), array(['##y', 'bye', 'end'], dtype='<U5')]\n",
    "        ids=ic([pad_to_len(max_seq_length,tokenizer.convert_tokens_to_ids(['[CLS]']+list(_)+['[SEP]'])) for _ in split_subwords])\n",
    "        #[array([ 101, 7592, 9541, 9541, 8038,  102]), array([ 101, 2100, 9061, 2203,  102,    0])]\n",
    "        masks=ic([position_to_mask(max_seq_length,_) for _ in split_token_end_idxs])\n",
    "        batch_ids.append(ids) #[[array([ 101, 7592, 9541, 9541, 8038,  102]), array([ 101, 2100, 9061, 2203, 2094,  102])]]\n",
    "        batch_masks.append(masks) #[[array([0, 0, 0, 1, 0, 0]), array([0, 1, 1, 1, 0, 0])]]\n",
    "    \n",
    "    return pp({'input_ids': torch.as_tensor(batch_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.as_tensor(batch_masks,dtype=torch.bool)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| re.split('[^a-zA-Z0-9]+',query,flags=re.V1): ['Hellooooo', 'Yay', 'Bye', 'Enddd', '']\n",
      "ic| list(map(tokenizer.tokenize,wordlist)): [['hello', '##oo', '##oo'], ['ya', '##y'], ['bye'], ['end', '##dd']]\n",
      "ic| list(map(len,subwords)): [3, 2, 1, 2]\n",
      "ic| list(flatten(subwords)): ['hello', '##oo', '##oo', 'ya', '##y', 'bye', 'end', '##dd']\n",
      "ic| np.cumsum([0]+subword_lengths[:-1])+np.array(subword_lengths)-1: array shape: (4,)[2 4 5 7]\n",
      "ic| token_end_idxs%(max_seq_length-2): array shape: (4,)[2 0 1 3]\n",
      "ic| np.argwhere(teim[1:]<teim[:-1]).flatten(): array shape: (1,)[0]\n",
      "ic| split_token_end_idxs: [array([2]), array([4, 5, 7])]\n",
      "ic| np.array_split(subwords,np.arange(max_seq_length-2,len(subwords),max_seq_length-2)): [array(['hello', '##oo', '##oo', 'ya'], dtype='<U5'), array(['##y', 'bye', 'end', '##dd'], dtype='<U5')]\n",
      "ic| [pad_ids_to_len(max_seq_length,tokenizer.convert_tokens_to_ids(['[CLS]']+list(_)+['[SEP]'])) for _ in split_subwords]: [array([ 101, 7592, 9541, 9541, 8038,  102]), array([  101,  2100,  9061,  2203, 14141,   102])]\n",
      "ic| [position_to_mask(max_seq_length,_) for _ in split_token_end_idxs]: [array([0, 0, 0, 1, 0, 0]), array([0, 1, 1, 0, 1, 0])]\n",
      "ic| {'input_ids': torch.as_tensor(batch_ids, dtype=torch.long),\n",
      "    'attention_mask': torch.as_tensor(batch_masks,dtype=torch.bool)}: {'input_ids': 'tensor shape: torch.Size([1, 2, 6])', 'attention_mask': 'tensor shape: torch.Size([1, 2, 6])'}\n",
      "ic| get_features(queries=queries, max_seq_length=max_seq_length, tokenizer=tokenizer): {'input_ids': 'tensor shape: torch.Size([1, 2, 6])', 'attention_mask': 'tensor shape: torch.Size([1, 2, 6])'}\n",
      "ic| features['input_ids']: tensor shape: torch.Size([1, 2, 6])\n",
      "ic| features['attention_mask']: tensor shape: torch.Size([1, 2, 6])\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "ifds=PunctuationInferDataset(queries=['Hellooooo! Yay! Bye Enddd.'], max_seq_length=6, tokenizer=AutoTokenizer.from_pretrained(cfg.model.transformer_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0, '!': 1, ',': 2, '-': 3, '.': 4, ':': 5, ';': 6, '?': 7, '—': 8, '…': 9}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| _[0]: tensor shape: torch.Size([5])\n",
      "ic| _[0]: tensor shape: torch.Size([5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS] hellooooo! [SEP]', '[CLS] bye… [SEP] [PAD] [PAD]']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
    "ids_to_labels = {_[0]:_[1] for _ in enumerate(cfg.model.punct_label_ids)}\n",
    "t=chunk_examples_with_degree(0)(['Hellooooo!Bye…'])\n",
    "tokenizer=AutoTokenizer.from_pretrained(cfg.model.transformer_path)\n",
    "t=chunk_to_len_batch(5,tokenizer,t['texts'],t['tags'])\n",
    "view_aligned(t['input_ids'],np.array(t['labels']),tokenizer,ids_to_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 1, 2, 1]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten(list_of_lists):\n",
    "    for l in list_of_lists:\n",
    "        for item in l:\n",
    "            yield item\n",
    "list(flatten([[0],[0,1],[0,1,2],[],[],[1]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
