{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "brazilian-locator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "outstanding-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset, Dataset, get_worker_info\n",
    "from nemo.core.neural_types import ChannelType, LabelsType, MaskType, NeuralType\n",
    "import gc\n",
    "import numpy as np\n",
    "from typing import List, Optional, Dict\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import subprocess\n",
    "from time import time\n",
    "from itertools import cycle, chain, islice, repeat\n",
    "from core.utils import chunk_examples_with_degree, chunk_to_len_batch\n",
    "\n",
    "class PunctuationDomainDataset(IterableDataset):\n",
    "\n",
    "    @property\n",
    "    def output_types(self) -> Optional[Dict[str, NeuralType]]:\n",
    "        \"\"\"Returns definitions of module output ports.\"\"\"\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"attention_mask\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"subtoken_mask\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"labels\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"domain\": NeuralType(('B'), ChannelType()),\n",
    "        }\n",
    "\n",
    "    def __init__(self, \n",
    "        csv_file:str, \n",
    "        tokenizer,\n",
    "        num_samples:int=256,\n",
    "        max_seq_length:int=256,\n",
    "        degree=0,\n",
    "        punct_label_ids: Dict[str, int] = None,\n",
    "        label_map:Dict[str,str] = None,\n",
    "        domain=0,\n",
    "        labelled=True,\n",
    "        randomize=True,\n",
    "        target_file='',\n",
    "        tmp_path='~/data/tmp',\n",
    "        start=0,\n",
    "        end=-1,\n",
    "        attach_label_to_end=None,\n",
    "        no_space_label=None,\n",
    "        manual_len=0,\n",
    "        pad_start=0,\n",
    "    ):\n",
    "        if not (os.path.exists(csv_file)):\n",
    "            raise FileNotFoundError(\n",
    "                f'{csv_file} not found. The 2nd column of the file contains the transcripts.'\n",
    "            )\n",
    "\n",
    "        data_dir = os.path.dirname(csv_file)\n",
    "        filename = os.path.basename(csv_file)\n",
    "\n",
    "        if not filename.endswith('.csv'):\n",
    "            raise ValueError(\"{text_file} should have extension .csv\")\n",
    "        # filename = filename[:-4]\n",
    "        \n",
    "        self.csv_file =   csv_file\n",
    "        self.max_seq_length =   max_seq_length\n",
    "        self.manual_len=manual_len\n",
    "        self.domain=  domain\n",
    "        self.punct_label_ids=punct_label_ids\n",
    "        self.label_map=label_map\n",
    "        self.labelled=  labelled\n",
    "        self.tokenizer= tokenizer\n",
    "        self.degree=degree\n",
    "        self.randomize=randomize\n",
    "        self.target_file=target_file\n",
    "        self.tmp_path=tmp_path\n",
    "        self.attach_label_to_end=attach_label_to_end\n",
    "        self.no_space_label=no_space_label\n",
    "        self.pad_start=pad_start\n",
    "        if not (os.path.exists(self.target_file)):\n",
    "            os.system(f\"sed '1d' {self.csv_file} > {self.target_file}\")\n",
    "        self.set_num_samples(self.target_file, num_samples, manual_len)\n",
    "    def __iter__(self):\n",
    "        self.dataset=iter(pd.read_csv(\n",
    "                self.target_file,\n",
    "                skiprows=(0 % self.len)*self.num_samples,\n",
    "                header=None,\n",
    "                dtype=str,\n",
    "                chunksize=self.num_samples,\n",
    "                ))\n",
    "        self.id=0\n",
    "        return self\n",
    "        \n",
    "\n",
    "    def __next__(self):\n",
    "        self.id+=1\n",
    "        pp(self.id)\n",
    "        batch = next(self.dataset)[1]\n",
    "\n",
    "        # l=batch.str.split().map(len).values\n",
    "        # n=16\n",
    "        # a=np.maximum((l-self.max_seq_length*n).clip(min=0),(l*np.random.random(l.__len__())).astype(int))\n",
    "        # b=np.minimum(l,a+self.max_seq_length*n)\n",
    "        # batch=pd.DataFrame({'t':batch,'a':a,'b':b}).apply(lambda row: ' '.join(row.t.split()[row.a:row.b]),axis=1)\n",
    "\n",
    "        chunked=chunk_examples_with_degree(self.degree, self.punct_label_ids, self.label_map)(batch)\n",
    "        batched=chunk_to_len_batch(self.max_seq_length,self.tokenizer,chunked['texts'],chunked['tags'],self.labelled,attach_label_to_end=self.attach_label_to_end,no_space_label=self.no_space_label, pad_start=self.pad_start)\n",
    "        num_samples=batched['labels'].shape[0]\n",
    "        batched['domain']=self.domain*torch.ones(num_samples,1,dtype=torch.long)\n",
    "        gc.collect()\n",
    "        if self.randomize:\n",
    "            rand=torch.randperm(num_samples)\n",
    "            return {k:v[rand] for k,v in batched.items()}\n",
    "        else:\n",
    "            return batched\n",
    "\n",
    "    def set_num_samples(self,csv_file,num_samples, manual_len):\n",
    "        self.num_samples = num_samples\n",
    "        self.total_samples=int(subprocess.Popen(['wc', '-l', csv_file], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0])\n",
    "        if manual_len>0:\n",
    "            self.total_samples=min(manual_len,self.total_samples)\n",
    "        self.num_samples=min(self.num_samples,self.total_samples)\n",
    "        self.len = max(1,int(self.total_samples / self.num_samples))\n",
    "        pp(self.num_samples,self.len,self.total_samples)\n",
    "\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        pp('dataset')\n",
    "        return pp(self.len)\n",
    "    \n",
    "    def shuffle(self, randomize=True, seed=42):\n",
    "        pp(int(subprocess.Popen(['wc', '-l', self.target_file], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0]))\n",
    "        pp(os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)))\n",
    "        pp(int(subprocess.Popen(['wc', '-l', self.target_file], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0]))\n",
    "        pp(self.len,self.num_samples)\n",
    "        self.dataset=iter(pd.read_csv(\n",
    "                self.target_file,\n",
    "                skiprows=(0 % self.len)*self.num_samples,\n",
    "                header=None,\n",
    "                dtype=str,\n",
    "                chunksize=self.num_samples,\n",
    "                ))\n",
    "        self.id=0\n",
    "    \n",
    "    def determine_class_weights(self):\n",
    "        it=iter(self)\n",
    "        ct=torch.zeros(len(self.punct_label_ids))\n",
    "        for _ in range(20):\n",
    "            print('.',end='')\n",
    "            ni=next(it)\n",
    "            ct+=torch.bincount(ni['labels'].view(-1),minlength=len(self.punct_label_ids))\n",
    "        return ct/sum(ct)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PunctuationDomainDatasets(IterableDataset):\n",
    "\n",
    "    def __init__(self, \n",
    "                 split:str,\n",
    "                 num_samples:int,\n",
    "                 max_seq_length:int,\n",
    "                 punct_label_ids: Dict[str, int],\n",
    "                 label_map:Dict[str,str],\n",
    "                 labelled: List[str],\n",
    "                 unlabelled: List[str],\n",
    "                 tokenizer,\n",
    "                 randomize:bool=True,\n",
    "                 data_id='',\n",
    "                 tmp_path='~/data/tmp',\n",
    "                 attach_label_to_end=None,\n",
    "                 manual_len:int=0,\n",
    "                 no_space_label:int=None,\n",
    "                 pad_start:int=0,\n",
    "                 ):\n",
    "        worker_info = get_worker_info()\n",
    "        self.num_workers=1 if worker_info is None else worker_info.num_workers\n",
    "        self.num_labelled=len(labelled)\n",
    "        self.datasets = []\n",
    "        self.iterables=[]\n",
    "        self.randomize=randomize\n",
    "        self.punct_label_ids=punct_label_ids\n",
    "        self.label_map=label_map\n",
    "        self.ds_lengths=[]\n",
    "        self.labelled=labelled\n",
    "        for path in labelled+unlabelled:\n",
    "            if manual_len>0:\n",
    "                self.ds_lengths.append(min(manual_len,int(subprocess.Popen(['wc', '-l', f'{path}.{split}.csv'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0])))\n",
    "            else:\n",
    "                self.ds_lengths.append(int(subprocess.Popen(['wc', '-l', f'{path}.{split}.csv'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0]))\n",
    "        pp(self.ds_lengths)\n",
    "        self.max_length=max(self.ds_lengths) \n",
    "        self.per_worker=int(self.max_length/self.num_workers)\n",
    "        self.len=int(self.per_worker/num_samples) \n",
    "        self.class_weights=None\n",
    "\n",
    "\n",
    "        for i,path in enumerate(labelled):\n",
    "            pp(min(num_samples,self.ds_lengths[i]))\n",
    "            target=os.path.join(tmp_path,os.path.split(path)[1])\n",
    "            dataset=PunctuationDomainDataset(\n",
    "                    csv_file=f'{path}.{split}.csv', tokenizer=tokenizer,\n",
    "                    num_samples=num_samples,max_seq_length=max_seq_length,\n",
    "                    punct_label_ids=punct_label_ids,\n",
    "                    label_map=label_map,\n",
    "                    domain=i,labelled=True,\n",
    "                    randomize=randomize,\n",
    "                    target_file=f'{target}.{split}.{data_id}.csv',\n",
    "                    tmp_path=tmp_path,\n",
    "                    attach_label_to_end=attach_label_to_end,\n",
    "                    no_space_label=no_space_label,\n",
    "                    manual_len=manual_len,\n",
    "                    pad_start=pad_start,)\n",
    "            self.datasets.append(dataset)\n",
    "            self.iterables.append(cycle(dataset))\n",
    "            \n",
    "        for i,path in enumerate(unlabelled):\n",
    "            pp(min(num_samples,self.ds_lengths[i]))\n",
    "            target=os.path.join(tmp_path,os.path.split(path)[1])\n",
    "            dataset=PunctuationDomainDataset(\n",
    "                    csv_file=f'{path}.{split}.csv', tokenizer=tokenizer,\n",
    "                    num_samples=num_samples,max_seq_length=max_seq_length,\n",
    "                    punct_label_ids=punct_label_ids,\n",
    "                    label_map=label_map,domain=len(labelled)+i,labelled=False,\n",
    "                    randomize=randomize,\n",
    "                    target_file=f'{target}.{split}.{data_id}.csv',\n",
    "                    tmp_path=tmp_path,\n",
    "                    attach_label_to_end=attach_label_to_end,\n",
    "                    no_space_label=no_space_label,\n",
    "                    manual_len=manual_len,\n",
    "                    pad_start=pad_start,)\n",
    "            self.datasets.append(dataset)\n",
    "            self.iterables.append(cycle(dataset))\n",
    "        \n",
    "    # def __getitem__(self, i):\n",
    "    #     ds=[d[i] for d in self.datasets]\n",
    "\n",
    "    def __iter__(self):\n",
    "        pp('iteriteriter')\n",
    "        worker_info = get_worker_info()\n",
    "        worker_id = 0 if worker_info is None else worker_info.id\n",
    "        self.iterables=[]\n",
    "        for ds_length, dataset in zip(self.ds_lengths,self.datasets):\n",
    "            start = (worker_id*self.per_worker)%ds_length\n",
    "            pp(ds_length,start)\n",
    "            self.iterables.append(cycle(chain(islice(iter(dataset),start,None),islice(iter(dataset),start))))\n",
    "        return islice(self,self.len)\n",
    "\n",
    "    def __next__(self):\n",
    "        ds=[next(d) for d in self.iterables]\n",
    "        if self.randomize:\n",
    "            min_batch=1000000\n",
    "            for d in ds:\n",
    "                size=d['domain'].shape[0]\n",
    "                if size<min_batch:\n",
    "                    min_batch=size\n",
    "            #Ensure all domains are evenly represented\n",
    "            b={k:torch.cat([d[k][:min_batch] for d in ds], dim=0) for k in ['input_ids','attention_mask','subtoken_mask','labels','domain']}\n",
    "            rand=torch.randperm(b['labels'].shape[0])\n",
    "            return {k:v[rand] for k,v in b.items()}\n",
    "        else:\n",
    "            return {k:torch.cat([d[k] for d in ds], dim=0) for k in ['input_ids','attention_mask','subtoken_mask','labels','domain']}\n",
    "\n",
    "    def __len__(self):\n",
    "        pp('datasets',self.len)\n",
    "        return self.len\n",
    "\n",
    "    def shuffle(self, randomize=True, seed=42):\n",
    "        worker_info = get_worker_info()\n",
    "        worker_id = 0 if worker_info is None else worker_info.id\n",
    "        if worker_id==0:\n",
    "            for _ in self.datasets:\n",
    "                print(f\"shuffling {_}\")\n",
    "                _.shuffle(randomize,seed)\n",
    "    \n",
    "    def determine_class_weights(self):\n",
    "        if self.class_weights is None:\n",
    "            ct=torch.zeros(len(self.punct_label_ids))\n",
    "            for _ in range(self.num_labelled):\n",
    "                ct+=self.datasets[_].determine_class_weights()\n",
    "            self.class_weights=self.num_labelled/ct\n",
    "        return self.class_weights\n",
    "\n",
    "\n",
    "class PunctuationInferenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates dataset to use during inference for punctuation and capitalization tasks with a pretrained model.\n",
    "    For dataset to use during training with labels, see BertPunctuationCapitalizationDataset.\n",
    "    Args:\n",
    "        queries file to sequences, each line should a sentence, no header.\n",
    "        max_seq_length: max sequence length minus 2 for [CLS] and [SEP]\n",
    "        tokenizer: such as AutoTokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def output_types(self) -> Optional[Dict[str, NeuralType]]:\n",
    "        \"\"\"Returns definitions of module output ports.\n",
    "               \"\"\"\n",
    "        return {\n",
    "            'input_ids': NeuralType(('B', 'T'), ChannelType()),\n",
    "            'attention_mask': NeuralType(('B', 'T'), MaskType()),\n",
    "            'subtoken_mask': NeuralType(('B', 'T'), MaskType()),\n",
    "            \"labels\": NeuralType(('B', 'T'), ChannelType()),\n",
    "        }\n",
    "\n",
    "    def __init__(self, \n",
    "    tokenizer, \n",
    "    queries: List[str], \n",
    "    max_seq_length: int, \n",
    "    punct_label_ids:Dict[str,int], \n",
    "    label_map:Dict[str,str], \n",
    "    num_samples:int=256, \n",
    "    degree:int = 0, \n",
    "    attach_label_to_end:bool=None,\n",
    "    no_space_label=None,\n",
    "    pad_start:int=0,\n",
    "    ):\n",
    "        \"\"\" Initializes BertPunctuationInferDataset. \"\"\"\n",
    "        self.degree=degree\n",
    "        self.punct_label_ids=punct_label_ids\n",
    "        self.label_map = label_map\n",
    "        chunked=chunk_examples_with_degree(self.degree, self.punct_label_ids, self.label_map)(queries)\n",
    "        self.features = chunk_to_len_batch(max_seq_length, tokenizer,chunked['texts'],chunked['tags'],attach_label_to_end=attach_label_to_end,no_space_label=no_space_label,pad_start=pad_start)\n",
    "        self.attach_label_to_end=attach_label_to_end\n",
    "        # self.all_input_ids = features['input_ids']\n",
    "        # self.all_attention_mask = features['attention_mask']\n",
    "        # self.all_subtoken_mask = features['subtoken_mask']\n",
    "        self.num_samples=num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.all_input_ids)/self.num_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # lower=idx*self.num_samples\n",
    "        # print(idx)\n",
    "        # (int(idx)+1)*self.num_samples+1)\n",
    "        # upper=min(len(self.all_input_ids),(int(idx)+1)*self.num_samples+1)\n",
    "        return {k:v for k,v in self.features.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "accompanied-binding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nxingyu2/project/experiment\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: '', 1: ',', 2: '.', 3: '?'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hydra\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from data import PunctuationDataModule, PunctuationInferenceDataset\n",
    "import os\n",
    "from models import PunctuationDomainModel\n",
    "\n",
    "from nemo.utils.exp_manager import exp_manager\n",
    "from time import time\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import atexit\n",
    "from copy import deepcopy\n",
    "import snoop\n",
    "snoop.install()\n",
    "\n",
    "from hydra.experimental import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "\n",
    "# initialize(config_path=\"../Punctuation_with_Domain_discriminator/\"+folder) \n",
    "initialize()\n",
    "!pwd\n",
    "cfg=compose(\n",
    "#     config_name=\"hparams.yaml\", \n",
    "    config_name=\"config.yaml\",\n",
    ")\n",
    "cfg.model.punct_label_ids=OmegaConf.create(sorted(cfg.model.punct_label_ids))\n",
    "labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
    "ids_to_labels = {_[0]:_[1] for _ in enumerate(cfg.model.punct_label_ids)}\n",
    "ids_to_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "quarterly-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PunctuationDomainDataset(IterableDataset):\n",
    "\n",
    "    @property\n",
    "    def output_types(self) -> Optional[Dict[str, NeuralType]]:\n",
    "        \"\"\"Returns definitions of module output ports.\"\"\"\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"attention_mask\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"subtoken_mask\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"labels\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"domain\": NeuralType(('B'), ChannelType()),\n",
    "        }\n",
    "\n",
    "    def __init__(self, \n",
    "        csv_file:str, \n",
    "        tokenizer,\n",
    "        num_samples:int=256,\n",
    "        max_seq_length:int=256,\n",
    "        degree=0,\n",
    "        punct_label_ids: Dict[str, int] = None,\n",
    "        label_map:Dict[str,str] = None,\n",
    "        domain=0,\n",
    "        labelled=True,\n",
    "        randomize=True,\n",
    "        target_file='',\n",
    "        tmp_path='~/data/tmp',\n",
    "        start=0,\n",
    "        end=-1,\n",
    "        attach_label_to_end=None,\n",
    "        no_space_label=None,\n",
    "        manual_len=0,\n",
    "        pad_start=0,\n",
    "    ):\n",
    "        if not (os.path.exists(csv_file)):\n",
    "            raise FileNotFoundError(\n",
    "                f'{csv_file} not found. The 2nd column of the file contains the transcripts.'\n",
    "            )\n",
    "\n",
    "        data_dir = os.path.dirname(csv_file)\n",
    "        filename = os.path.basename(csv_file)\n",
    "\n",
    "        if not filename.endswith('.csv'):\n",
    "            raise ValueError(\"{text_file} should have extension .csv\")\n",
    "        # filename = filename[:-4]\n",
    "        \n",
    "        self.csv_file =   csv_file\n",
    "        self.max_seq_length =   max_seq_length\n",
    "        self.manual_len=manual_len\n",
    "        self.domain=  domain\n",
    "        self.punct_label_ids=punct_label_ids\n",
    "        self.label_map=label_map\n",
    "        self.labelled=  labelled\n",
    "        self.tokenizer= tokenizer\n",
    "        self.degree=degree\n",
    "        self.randomize=randomize\n",
    "        self.target_file=target_file\n",
    "        self.tmp_path=tmp_path\n",
    "        self.attach_label_to_end=attach_label_to_end\n",
    "        self.no_space_label=no_space_label\n",
    "        self.pad_start=pad_start\n",
    "        if not (os.path.exists(self.target_file)):\n",
    "            os.system(f\"sed '1d' {self.csv_file} > {self.target_file}\")\n",
    "        self.set_num_samples(self.target_file, num_samples, manual_len)\n",
    "    def __iter__(self):\n",
    "        self.dataset=iter(pd.read_csv(\n",
    "                self.target_file,\n",
    "                skiprows=(0 % self.len)*self.num_samples,\n",
    "                header=None,\n",
    "                dtype=str,\n",
    "                chunksize=self.num_samples,\n",
    "                ))\n",
    "        self.id=0\n",
    "        return self\n",
    "        \n",
    "\n",
    "    def __next__(self):\n",
    "        self.id+=1\n",
    "        pp(self.id)\n",
    "        batch = next(self.dataset)[1]\n",
    "\n",
    "        # l=batch.str.split().map(len).values\n",
    "        # n=16\n",
    "        # a=np.maximum((l-self.max_seq_length*n).clip(min=0),(l*np.random.random(l.__len__())).astype(int))\n",
    "        # b=np.minimum(l,a+self.max_seq_length*n)\n",
    "        # batch=pd.DataFrame({'t':batch,'a':a,'b':b}).apply(lambda row: ' '.join(row.t.split()[row.a:row.b]),axis=1)\n",
    "\n",
    "        chunked=chunk_examples_with_degree(self.degree, self.punct_label_ids, self.label_map)(batch)\n",
    "        batched=chunk_to_len_batch(self.max_seq_length,self.tokenizer,chunked['texts'],chunked['tags'],self.labelled,attach_label_to_end=self.attach_label_to_end,no_space_label=self.no_space_label, pad_start=self.pad_start)\n",
    "        num_samples=batched['labels'].shape[0]\n",
    "        batched['domain']=self.domain*torch.ones(num_samples,1,dtype=torch.long)\n",
    "        gc.collect()\n",
    "        if self.randomize:\n",
    "            rand=torch.randperm(num_samples)\n",
    "            return {k:v[rand] for k,v in batched.items()}\n",
    "        else:\n",
    "            return batched\n",
    "\n",
    "    def set_num_samples(self,csv_file,num_samples, manual_len):\n",
    "        self.num_samples = num_samples\n",
    "        self.total_samples=int(subprocess.Popen(['wc', '-l', csv_file], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0])\n",
    "        if manual_len>0:\n",
    "            self.total_samples=min(manual_len,self.total_samples)\n",
    "        self.num_samples=min(self.num_samples,self.total_samples)\n",
    "        self.len = max(1,int(self.total_samples / self.num_samples))\n",
    "        pp(self.num_samples,self.len,self.total_samples)\n",
    "\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        pp('dataset')\n",
    "        return pp(self.len)\n",
    "    \n",
    "    def shuffle(self, randomize=True, seed=42):\n",
    "        pp(int(subprocess.Popen(['wc', '-l', self.target_file], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0]))\n",
    "        pp(os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)))\n",
    "        pp(int(subprocess.Popen(['wc', '-l', self.target_file], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0]))\n",
    "        pp(self.len,self.num_samples)\n",
    "        self.dataset=iter(pd.read_csv(\n",
    "                self.target_file,\n",
    "                skiprows=(0 % self.len)*self.num_samples,\n",
    "                header=None,\n",
    "                dtype=str,\n",
    "                chunksize=self.num_samples,\n",
    "                ))\n",
    "        self.id=0\n",
    "    \n",
    "    def determine_class_weights(self):\n",
    "        it=iter(self)\n",
    "        ct=torch.zeros(len(self.punct_label_ids))\n",
    "        for _ in range(20):\n",
    "            print('.',end='')\n",
    "            ni=next(it)\n",
    "            ct+=torch.bincount(ni['labels'].view(-1),minlength=len(self.punct_label_ids))\n",
    "        return ct/sum(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "defensive-lawsuit",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:26:08.45 LOG:\n",
      "12:26:08.52 .... self.num_samples = 8\n",
      "12:26:08.52 .... self.len = 13\n",
      "12:26:08.52 .... self.total_samples = 107\n"
     ]
    }
   ],
   "source": [
    "# tokenizer=AutoTokenizer.from_pretrained(cfg.model.transformer_path)\n",
    "# if cfg.model.no_space_label is not None and cfg.model.dataset.attach_label_to_end is None:\n",
    "#     s=set(cfg.model.punct_label_ids)\n",
    "#     s.add(cfg.model.no_space_label)\n",
    "#     cfg.model.punct_label_ids=sorted(list(s))\n",
    "# ids_to_labels = {_[0]: _[1]\n",
    "#                       for _ in enumerate(cfg.model.punct_label_ids)}\n",
    "# labels_to_ids = {v:k\n",
    "#                       for k,v in ids_to_labels.items()}\n",
    "# labels_to_ids\n",
    "tds=PunctuationDomainDataset(\n",
    "                    csv_file=f'/home/nxingyu2/data/switchboardutt_processed.train.csv', tokenizer=tokenizer,\n",
    "                    num_samples=8,max_seq_length=128,\n",
    "                    punct_label_ids= labels_to_ids,\n",
    "                    label_map= cfg.model.label_map,\n",
    "                    domain=0,labelled=True,\n",
    "                    randomize=True,\n",
    "                    target_file='switchboardutt_processed.train.target.csv',\n",
    "                    tmp_path='/home/nxingyu2/data/tmp',\n",
    "                    attach_label_to_end=None,\n",
    "                    no_space_label=2,\n",
    "                    manual_len=0,\n",
    "                    pad_start=0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "focal-robin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "historical-cloud",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def it(x):\n",
    "    return iter(range(x))\n",
    "start=0\n",
    "num_samples=13\n",
    "ds_lengths=[5,12]\n",
    "max_length=max(ds_lengths)\n",
    "num_workers=2\n",
    "per_worker=int(max_length/num_workers)\n",
    "length=max(1,int(per_worker/num_samples))\n",
    "\n",
    "# itx=cycle(chain(islice(it(5),start,None),islice(it(5),start)))\n",
    "# ity=cycle(chain(islice(it(12),start,None),islice(it(12),start)))\n",
    "# for i in range(16):\n",
    "#     print(next(itx),next(ity))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "advised-aerospace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:41:28.49 LOG:\n",
      "12:41:28.50 .... ds_length = 5\n",
      "12:41:28.50 .... start = 1\n",
      "12:41:28.50 LOG:\n",
      "12:41:28.50 .... ds_length = 12\n",
      "12:41:28.50 .... start = 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 6\n",
      "2 7\n",
      "3 8\n",
      "4 9\n",
      "1 10\n",
      "2 11\n"
     ]
    }
   ],
   "source": [
    "worker_id=1\n",
    "iterables=[]\n",
    "for ds_length in ds_lengths:\n",
    "    dataset=it(ds_length)\n",
    "    start = (worker_id*per_worker)%ds_length\n",
    "    pp(ds_length,start)\n",
    "    iterables.append(cycle(chain(islice(iter(dataset),start,None),islice(iter(dataset),start))))\n",
    "for i in range(per_worker):\n",
    "    print(next(iterables[0]),next(iterables[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "paperback-legend",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:41:37.32 LOG:\n",
      "12:41:37.32 .... ds_length = 5\n",
      "12:41:37.32 .... start = 0\n",
      "12:41:37.32 LOG:\n",
      "12:41:37.32 .... ds_length = 12\n",
      "12:41:37.32 .... start = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 1\n",
      "2 2\n",
      "3 3\n",
      "4 4\n",
      "0 5\n"
     ]
    }
   ],
   "source": [
    "worker_id=0\n",
    "iterables=[]\n",
    "for ds_length in ds_lengths:\n",
    "    dataset=it(ds_length)\n",
    "    start = (worker_id*per_worker)%ds_length\n",
    "    pp(ds_length,start)\n",
    "    iterables.append(cycle(chain(islice(iter(dataset),start,None),islice(iter(dataset),start))))\n",
    "for i in range(per_worker):\n",
    "    print(next(iterables[0]),next(iterables[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
