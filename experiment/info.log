[INFO] - Global seed set to 42
[INFO] - GPU available: True, used: True
[INFO] - TPU available: None, using: 0 TPU cores
[INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
[INFO] - Using native 16bit precision.
[INFO] - shuffling train set
[INFO] - Global seed set to 42
[INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[INFO] - Optimizer config = AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    weight_decay: 0.0
)
[INFO] - Scheduler "<core.optim.lr_scheduler.CosineAnnealing object at 0x7fd213392640>" 
will be used during training (effective maximum steps = 1720) - 
Parameters : 
(warmup_steps: null
warmup_ratio: 0.1
min_lr: 1.0e-08
last_epoch: -1
max_steps: 1720
)
[INFO] - 
  | Name                       | Type                 | Params
--------------------------------------------------------------------
0 | transformer                | ElectraModel         | 108 M 
1 | punct_classifier           | TokenClassifier      | 594 K 
2 | domain_classifier          | SequenceClassifier   | 2.3 K 
3 | punctuation_loss           | FocalDiceLoss        | 0     
4 | domain_loss                | FocalDiceLoss        | 0     
5 | agg_loss                   | AggregatorLoss       | 0     
6 | punct_class_report         | ClassificationReport | 0     
7 | chunked_punct_class_report | ClassificationReport | 0     
8 | domain_class_report        | ClassificationReport | 0     
--------------------------------------------------------------------
596 K     Trainable params
108 M     Non-trainable params
109 M     Total params
[INFO] - Punctuation report: 
label                                                precision    recall       f1           support   
 (label_id: 0)                                          95.48       0.43       0.86      48954
# (label_id: 1)                                          1.39      13.15       2.51       2045
, (label_id: 2)                                          1.44       0.52       0.76       1741
. (label_id: 3)                                          2.75       2.67       2.70       1313
? (label_id: 4)                                          0.15      48.04       0.30        102
-------------------
micro avg                                                1.06       1.06       1.06      54155
macro avg                                               20.24      12.96       1.43      54155
weighted avg                                            86.47       1.06       0.96      54155

-------------------
                       #           ,           .           ?
      211.00         3.00         6.00         1.00         0.00
    17669.00       269.00       757.00       608.00        50.00
      552.00        60.00         9.00         4.00         1.00
     1166.00         9.00        63.00        35.00         2.00
    29356.00      1704.00       906.00       665.00        49.00
-------------------

[INFO] - Domain report: 
label                                                precision    recall       f1           support   
0 (label_id: 0)                                         49.89      88.57      63.82        245
1 (label_id: 1)                                         49.09      11.02      18.00        245
-------------------
micro avg                                               49.80      49.80      49.80        490
macro avg                                               49.49      49.80      40.91        490
weighted avg                                            49.49      49.80      40.91        490

-------------------
           0           1
      217.00       218.00
       28.00        27.00
-------------------

[INFO] - Punctuation report: 
label                                                precision    recall       f1           support   
 (label_id: 0)                                          99.35      92.55      95.83    1280978
# (label_id: 1)                                         70.56      97.58      81.90      60761
, (label_id: 2)                                         39.81      58.41      47.35      48434
. (label_id: 3)                                         44.05      74.83      55.46      38698
? (label_id: 4)                                         10.15      58.65      17.31       3093
-------------------
micro avg                                               91.05      91.05      91.05    1431964
macro avg                                               52.79      76.40      59.57    1431964
weighted avg                                            94.43      91.05      92.33    1431964

-------------------
                       #           ,           .           ?
  1185490.00      1056.00      5476.00      1138.00       119.00
    24611.00     59288.00        84.00        39.00         0.00
    38833.00        92.00     28289.00      3586.00       256.00
    23228.00       292.00     12353.00     28959.00       904.00
     8816.00        33.00      2232.00      4976.00      1814.00
-------------------

[INFO] - Domain report: 
label                                                precision    recall       f1           support   
0 (label_id: 0)                                         95.82      98.05      96.92       6503
1 (label_id: 1)                                         98.00      95.73      96.85       6503
-------------------
micro avg                                               96.89      96.89      96.89      13006
macro avg                                               96.91      96.89      96.89      13006
weighted avg                                            96.91      96.89      96.89      13006

-------------------
           0           1
     6376.00       278.00
      127.00      6225.00
-------------------

[INFO] - Epoch 0, global step 214: val_loss reached 0.36105 (best 0.36105), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-17_09-26-21/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.36-epoch=0.ckpt" as top 3
[INFO] - Punctuation report: 
label                                                precision    recall       f1           support   
 (label_id: 0)                                          99.45      93.33      96.29    1280170
# (label_id: 1)                                         73.84      98.14      84.27      60778
, (label_id: 2)                                         40.29      71.74      51.60      48526
. (label_id: 3)                                         55.83      75.44      64.17      38783
? (label_id: 4)                                         17.37      58.16      26.75       3076
-------------------
micro avg                                               92.24      92.24      92.24    1431333
macro avg                                               57.35      79.36      64.62    1431333
weighted avg                                            94.99      92.24      93.25    1431333

-------------------
                       #           ,           .           ?
  1194785.00       857.00      4625.00      1078.00       105.00
    21051.00     59645.00        53.00        24.00         0.00
    44753.00        89.00     34811.00      6449.00       306.00
    14527.00       178.00      7565.00     29256.00       876.00
     5054.00         9.00      1472.00      1976.00      1789.00
-------------------

[INFO] - Domain report: 
label                                                precision    recall       f1           support   
0 (label_id: 0)                                         95.85      98.32      97.07       6503
1 (label_id: 1)                                         98.28      95.74      96.99       6503
-------------------
micro avg                                               97.03      97.03      97.03      13006
macro avg                                               97.06      97.03      97.03      13006
weighted avg                                            97.06      97.03      97.03      13006

-------------------
           0           1
     6394.00       277.00
      109.00      6226.00
-------------------

[INFO] - Epoch 1, global step 429: val_loss reached 0.35055 (best 0.35055), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-17_09-26-21/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.35-epoch=1.ckpt" as top 3
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            [INFO] - Punctuation report: 
label                                                precision    recall       f1           support   
 (label_id: 0)                                          99.55      94.12      96.76    1280281
# (label_id: 1)                                         77.30      98.99      86.81      60721
, (label_id: 2)                                         43.70      73.47      54.80      48388
. (label_id: 3)                                         58.56      78.26      66.99      38702
? (label_id: 4)                                         20.08      64.62      30.63       3095
-------------------
micro avg                                               93.14      93.14      93.14    1431187
macro avg                                               59.84      81.89      67.20    1431187
weighted avg                                            95.44      93.14      93.97    1431187

-------------------
                       #           ,           .           ?
  1204986.00       471.00      3967.00       881.00        82.00
    17595.00     60110.00        30.00        25.00         0.00
    39994.00        33.00     35553.00      5514.00       265.00
    12941.00        97.00      7645.00     30288.00       748.00
     4765.00        10.00      1193.00      1994.00      2000.00
-------------------

[INFO] - Domain report: 
label                                                precision    recall       f1           support   
0 (label_id: 0)                                         96.49      98.97      97.72       6503
1 (label_id: 1)                                         98.94      96.40      97.66       6503
-------------------
micro avg                                               97.69      97.69      97.69      13006
macro avg                                               97.72      97.69      97.69      13006
weighted avg                                            97.72      97.69      97.69      13006

-------------------
           0           1
     6436.00       234.00
       67.00      6269.00
-------------------

[INFO] - Epoch 6, global step 3002: val_loss reached 0.34321 (best 0.34321), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-17_08-20-39/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.34-epoch=6.ckpt" as top 3
[INFO] - Punctuation report: 
label                                                precision    recall       f1           support   
 (label_id: 0)                                          99.55      94.17      96.78    1282299
# (label_id: 1)                                         77.45      99.00      86.91      60816
, (label_id: 2)                                         43.89      73.27      54.90      48506
. (label_id: 3)                                         58.65      78.63      67.19      38779
? (label_id: 4)                                         20.31      64.88      30.93       3089
-------------------
micro avg                                               93.18      93.18      93.18    1433489
macro avg                                               59.97      81.99      67.34    1433489
weighted avg                                            95.45      93.18      94.01    1433489

-------------------
                       #           ,           .           ?
  1207501.00       485.00      3963.00       895.00        82.00
    17472.00     60205.00        31.00        26.00         0.00
    39775.00        27.00     35542.00      5370.00       257.00
    12884.00        89.00      7777.00     30493.00       746.00
     4667.00        10.00      1193.00      1995.00      2004.00
-------------------

[INFO] - Domain report: 
label                                                precision    recall       f1           support   
0 (label_id: 0)                                         96.64      99.00      97.80       6503
1 (label_id: 1)                                         98.98      96.56      97.75       6503
-------------------
micro avg                                               97.78      97.78      97.78      13006
macro avg                                               97.81      97.78      97.78      13006
weighted avg                                            97.81      97.78      97.78      13006

-------------------
           0           1
     6438.00       224.00
       65.00      6279.00
-------------------

[INFO] - Epoch 7, global step 3431: val_loss reached 0.34315 (best 0.34315), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-17_08-20-39/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.34-epoch=7.ckpt" as top 3
[INFO] - Saving latest checkpoint...
[INFO] - Global seed set to 42
[INFO] - Optimizer config = AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0.0
)
[INFO] - Scheduler "<core.optim.lr_scheduler.CosineAnnealing object at 0x7efed579fc70>" 
will be used during training (effective maximum steps = 3432) - 
Parameters : 
(warmup_steps: null
warmup_ratio: 0.1
min_lr: 1.0e-08
last_epoch: -1
max_steps: 3432
)
[INFO] - 
  | Name                       | Type                 | Params
--------------------------------------------------------------------
0 | transformer                | ElectraModel         | 108 M 
1 | punct_classifier           | TokenClassifier      | 594 K 
2 | domain_classifier          | SequenceClassifier   | 2.3 K 
3 | punctuation_loss           | FocalDiceLoss        | 0     
4 | domain_loss                | FocalDiceLoss        | 0     
5 | agg_loss                   | AggregatorLoss       | 0     
6 | punct_class_report         | ClassificationReport | 0     
7 | chunked_punct_class_report | ClassificationReport | 0     
8 | domain_class_report        | ClassificationReport | 0     
--------------------------------------------------------------------
7.7 M     Trainable params
101 M     Non-trainable params
109 M     Total params
[INFO] - Punctuation report: 
label                                                precision    recall       f1           support   
 (label_id: 0)                                          99.73      93.96      96.76      48954
# (label_id: 1)                                         74.18      99.46      84.98       2045
, (label_id: 2)                                         43.49      76.51      55.45       1741
. (label_id: 3)                                         55.67      80.73      65.90       1313
? (label_id: 4)                                         16.41      51.96      24.94        102
-------------------
micro avg                                               93.21      93.21      93.21      54155
macro avg                                               57.89      80.52      65.61      54155
weighted avg                                            95.73      93.21      94.10      54155

-------------------
                       #           ,           .           ?
    45997.00         7.00        98.00        19.00         2.00
      707.00      2034.00         1.00         0.00         0.00
     1563.00         0.00      1332.00       164.00         4.00
      521.00         4.00       276.00      1060.00        43.00
      166.00         0.00        34.00        70.00        53.00
-------------------

[INFO] - Domain report: 
label                                                precision    recall       f1           support   
0 (label_id: 0)                                         96.03      98.78      97.38        245
1 (label_id: 1)                                         98.74      95.92      97.31        245
-------------------
micro avg                                               97.35      97.35      97.35        490
macro avg                                               97.39      97.35      97.35        490
weighted avg                                            97.39      97.35      97.35        490

-------------------
           0           1
      242.00        10.00
        3.00       235.00
-------------------

