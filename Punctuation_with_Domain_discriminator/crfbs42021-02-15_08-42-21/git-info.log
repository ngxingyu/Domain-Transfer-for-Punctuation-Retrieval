commit hash: a6aafb0c8591a142e129939facdd94ca4eb7716d
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_12-57-51/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-09_12-57-51/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_12-57-51/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_12-57-51/events.out.tfevents.1612847167.Titan.1188.0 b/Punctuation_with_Domain_discriminator/2021-02-09_12-57-51/events.out.tfevents.1612847167.Titan.1188.0
deleted file mode 100644
index bb015d7..0000000
Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_12-57-51/events.out.tfevents.1612847167.Titan.1188.0 and /dev/null differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_12-57-51/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-09_12-57-51/git-info.log
deleted file mode 100644
index de2bc34..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_12-57-51/git-info.log
+++ /dev/null
@@ -1,388 +0,0 @@
-commit hash: 4cccf1ef224a31f16ab4158a6dbd1472d2705371
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/events.out.tfevents.1612840713.Titan.20119.0 b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/events.out.tfevents.1612840713.Titan.20119.0
-index a4eb1d2..b90a1e5 100644
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/events.out.tfevents.1612840713.Titan.20119.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/events.out.tfevents.1612840713.Titan.20119.0 differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/lightning_logs.txt
-index c7d0c2d..f654115 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/lightning_logs.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/lightning_logs.txt
-@@ -37,3 +37,19 @@ Global seed set to 42
- 299 K     Trainable params
- 13.2 M    Non-trainable params
- 13.5 M    Total params
-+Global seed set to 42
-+
-+  | Name                | Type                 | Params
-+-------------------------------------------------------------
-+0 | transformer         | ElectraModel         | 13.5 M
-+1 | punct_classifier    | TokenClassifier      | 2.6 K 
-+2 | domain_classifier   | SequenceClassifier   | 1.0 K 
-+3 | punctuation_loss    | FocalDiceLoss        | 0     
-+4 | domain_loss         | CrossEntropyLoss     | 0     
-+5 | agg_loss            | AggregatorLoss       | 0     
-+6 | punct_class_report  | ClassificationReport | 0     
-+7 | domain_class_report | ClassificationReport | 0     
-+-------------------------------------------------------------
-+826 K     Trainable params
-+12.7 M    Non-trainable params
-+13.5 M    Total params
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_error_log.txt
-index 0c8b389..f0409dd 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_error_log.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_error_log.txt
-@@ -8,3 +8,6 @@
- [NeMo W 2021-02-09 11:24:13 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-     
-+[NeMo W 2021-02-09 12:57:04 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-+      warnings.warn(*args, **kwargs)
-+    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_log_globalrank-0_localrank-0.txt
-index 84d61e5..4c66514 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_log_globalrank-0_localrank-0.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_log_globalrank-0_localrank-0.txt
-@@ -10,3 +10,6 @@
- [NeMo W 2021-02-09 11:24:13 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-     
-+[NeMo W 2021-02-09 12:57:04 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-+      warnings.warn(*args, **kwargs)
-+    
-diff --git a/experiment/Untitled.ipynb b/experiment/Untitled.ipynb
-index 1a5526d..cd5247e 100644
---- a/experiment/Untitled.ipynb
-+++ b/experiment/Untitled.ipynb
-@@ -3,33 +3,33 @@
-   {
-    "cell_type": "code",
-    "execution_count": 1,
--   "id": "dense-meaning",
-+   "id": "modern-amplifier",
-    "metadata": {},
-    "outputs": [
-     {
-      "name": "stderr",
-      "output_type": "stream",
-      "text": [
--      "12:16:24.02 LOG:\n"
-+      "12:17:39.23 LOG:\n"
-      ]
-     },
-     {
-      "name": "stdout",
-      "output_type": "stream",
-      "text": [
--      "shuffling <data.punctuation_dataset_multi.PunctuationDomainDataset object at 0x7f86e5bc2220>\n"
-+      "shuffling <data.punctuation_dataset_multi.PunctuationDomainDataset object at 0x7f383a13c0a0>\n"
-      ]
-     },
-     {
-      "name": "stderr",
-      "output_type": "stream",
-      "text": [
--      "12:16:24.11 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
--      "12:16:24.17 LOG:\n",
--      "12:16:24.48 .... f\"1st {n} encoder layers of transformer frozen\" = '1st 11 encoder layers of transformer frozen'\n",
-+      "12:17:39.32 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
-+      "12:17:39.38 LOG:\n",
-+      "12:17:39.75 .... f\"1st {n} encoder layers of transformer frozen\" = '1st 11 encoder layers of transformer frozen'\n",
-       "GPU available: True, used: False\n",
-       "TPU available: None, using: 0 TPU cores\n",
--      "[NeMo W 2021-02-09 12:16:24 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
-+      "[NeMo W 2021-02-09 12:17:39 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
-       "      warnings.warn(*args, **kwargs)\n",
-       "    \n"
-      ]
-@@ -74,157 +74,26 @@
-   },
-   {
-    "cell_type": "code",
-+   "execution_count": null,
-+   "id": "minus-mississippi",
-+   "metadata": {},
-+   "outputs": [],
-+   "source": [
-+    "import regex as re\n",
-+    "re.sub('(\"[CLS]\"|)','','[CLS] we, bought: four- shirts, one, pen, and, a, mug, from, the, nvidia- gear, store, in, santa- clara, [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]')"
-+   ]
-+  },
-+  {
-+   "cell_type": "code",
-    "execution_count": 2,
--   "id": "potential-adrian",
-+   "id": "hairy-proxy",
-    "metadata": {},
-    "outputs": [
-     {
--     "name": "stderr",
--     "output_type": "stream",
--     "text": [
--      "12:16:24.62 LOG:\n",
--      "12:16:24.69 .... chunk_to_len_batch(max_seq_length, tokenizer,chunked['texts'],chunked['tags'],True) = {'attention_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
--      "12:16:24.69                                                                                                      True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
--      "12:16:24.69                                                                                                      True, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False],\n",
--      "12:16:24.69                                                                                                    [ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False],\n",
--      "12:16:24.69                                                                                                    [ True,  True,  True,  True,  True, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False]]),\n",
--      "12:16:24.69                                                                                             'input_ids': tensor([[  101,  2057,  4149,  2176, 11344,  2028,  7279,  1998,  1037, 14757,\n",
--      "12:16:24.69                                                                                                      2013,  1996,  1050, 17258,  2401,  6718,  3573,  1999,  4203, 10254,\n",
--      "12:16:24.69                                                                                                       102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0],\n",
--      "12:16:24.69                                                                                                    [  101,  2054,  2064,  1045,  2079,  2005,  2017,  2651,   102,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0],\n",
--      "12:16:24.69                                                                                                    [  101,  2129,  2024,  2017,   102,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0]]),\n",
--      "12:16:24.69                                                                                             'labels': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0],\n",
--      "12:16:24.69                                                                                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0],\n",
--      "12:16:24.69                                                                                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0]]),\n",
--      "12:16:24.69                                                                                             'subtoken_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
--      "12:16:24.69                                                                                                      True,  True, False, False,  True,  True,  True,  True,  True,  True,\n",
--      "12:16:24.69                                                                                                      True, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False],\n",
--      "12:16:24.69                                                                                                    [ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False],\n",
--      "12:16:24.69                                                                                                    [ True,  True,  True,  True,  True, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False]])}\n"
--     ]
--    },
--    {
-      "data": {
-       "text/plain": [
--       "['[CLS] we, bought, four- shirts: one, pen, and, a, mug, from, the, nvidia- gear, store, in, santa- clara, [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
--       " '[CLS] what? can— i? do? for? you? today? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
-+       "['[CLS] we, bought: four- shirts, one, pen, and, a, mug, from, the, nvidia- gear, store, in, santa- clara, [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
-+       " '[CLS] what? can, i? do? for? you? today? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
-        " '[CLS] how? are? you? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']"
-       ]
-      },
-@@ -246,7 +115,7 @@
-   {
-    "cell_type": "code",
-    "execution_count": null,
--   "id": "amateur-production",
-+   "id": "employed-station",
-    "metadata": {},
-    "outputs": [],
-    "source": []
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index efa7a5d..13df7bd 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -5,7 +5,7 @@ trainer:
-     max_epochs: 10
-     max_steps: null # precedence over max_epochs
-     accumulate_grad_batches: 4 # accumulates grads every k batches
--    gradient_clip_val: 0
-+    gradient_clip_val: 4
-     amp_level: O1 # O1/O2 for mixed precision
-     precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-     accelerator: ddp
-@@ -75,7 +75,7 @@ model:
-         ignore_start_end: false
-         use_cache: false
-         # shared among dataloaders
--        num_workers:  4
-+        num_workers:  6
-         pin_memory: true
-         drop_last: false
-         num_labels: 10
-diff --git a/experiment/info.log b/experiment/info.log
-index 69e9a76..e69de29 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,85 +0,0 @@
--[INFO] - shuffling train set
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f717ffe5b80>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          89.19      22.74      36.24       3012
--! (label_id: 1)                                          0.00       0.00       0.00          1
--, (label_id: 2)                                          7.31      36.21      12.16        243
--- (label_id: 3)                                          2.27      21.43       4.11         28
--. (label_id: 4)                                          1.68       1.65       1.66        182
--: (label_id: 5)                                          0.00       0.00       0.00          5
--; (label_id: 6)                                          0.00       0.00       0.00          3
--? (label_id: 7)                                          0.24      22.22       0.48          9
--— (label_id: 8)                                          0.00       0.00       0.00         10
--… (label_id: 9)                                          0.00       0.00       0.00          1
---------------------
--micro avg                                               22.44      22.44      22.44       3494
--macro avg                                               10.07      10.43       5.47       3494
--weighted avg                                            77.50      22.44      32.21       3494
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                         50.00     100.00      66.67         34
--1 (label_id: 1)                                          0.00       0.00       0.00         34
---------------------
--micro avg                                               50.00      50.00      50.00         68
--macro avg                                               25.00      50.00      33.33         68
--weighted avg                                            25.00      50.00      33.33         68
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.007943282347242822
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f717f560fa0>" 
--will be used during training (effective maximum steps = 53050) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 53050
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          89.19      22.74      36.24       3012
--! (label_id: 1)                                          0.00       0.00       0.00          1
--, (label_id: 2)                                          7.31      36.21      12.16        243
--- (label_id: 3)                                          2.27      21.43       4.11         28
--. (label_id: 4)                                          1.68       1.65       1.66        182
--: (label_id: 5)                                          0.00       0.00       0.00          5
--; (label_id: 6)                                          0.00       0.00       0.00          3
--? (label_id: 7)                                          0.24      22.22       0.48          9
--— (label_id: 8)                                          0.00       0.00       0.00         10
--… (label_id: 9)                                          0.00       0.00       0.00          1
---------------------
--micro avg                                               22.44      22.44      22.44       3494
--macro avg                                               10.07      10.43       5.47       3494
--weighted avg                                            77.50      22.44      32.21       3494
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                         50.00     100.00      66.67         34
--1 (label_id: 1)                                          0.00       0.00       0.00         34
---------------------
--micro avg                                               50.00      50.00      50.00         68
--macro avg                                               25.00      50.00      33.33         68
--weighted avg                                            25.00      50.00      33.33         68
--
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_12-57-51/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-09_12-57-51/hparams.yaml
deleted file mode 100644
index 9fae907..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_12-57-51/hparams.yaml
+++ /dev/null
@@ -1,110 +0,0 @@
-seed: 42
-trainer:
-  gpus: 1
-  num_nodes: 1
-  max_epochs: 10
-  max_steps: null
-  accumulate_grad_batches: 4
-  gradient_clip_val: 4
-  amp_level: O1
-  precision: 16
-  accelerator: ddp
-  checkpoint_callback: false
-  logger: false
-  log_every_n_steps: 1
-  val_check_interval: 1.0
-  resume_from_checkpoint: null
-exp_manager:
-  exp_dir: /home/nxingyu/project/
-  name: Punctuation_with_Domain_discriminator
-  create_tensorboard_logger: true
-  create_checkpoint_callback: true
-base_path: /home/nxingyu/data
-tmp_path: /home/nxingyu/data/tmp
-model:
-  nemo_path: null
-  transformer_path: google/electra-small-discriminator
-  unfrozen: 0
-  maximum_unfrozen: 1
-  unfreeze_step: 1
-  punct_label_ids:
-  - ''
-  - '!'
-  - ','
-  - '-'
-  - .
-  - ':'
-  - ;
-  - '?'
-  - —
-  - …
-  punct_class_weights: false
-  dataset:
-    data_dir: /home/nxingyu/data
-    labelled:
-    - /home/nxingyu/data/ted_talks_processed
-    unlabelled:
-    - /home/nxingyu/data/open_subtitles_processed
-    max_seq_length: 128
-    pad_label: ''
-    ignore_extra_tokens: false
-    ignore_start_end: false
-    use_cache: false
-    num_workers: 6
-    pin_memory: true
-    drop_last: false
-    num_labels: 10
-    num_domains: 2
-    test_unlabelled: true
-    train_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 4
-    validation_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 4
-  tokenizer:
-    tokenizer_name: google/electra-small-discriminator
-    vocab_file: null
-    tokenizer_model: null
-    special_tokens: null
-  language_model:
-    pretrained_model_name: google/electra-small-discriminator
-    lm_checkpoint: null
-    config_file: null
-    config: null
-  punct_head:
-    punct_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: dice
-  domain_head:
-    domain_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: cel
-    gamma: 0.1
-    pooling: mean_max
-    idx_conditioned_on: 0
-  dice_loss:
-    epsilon: 0.01
-    alpha: 3
-    macro_average: true
-  focal_loss:
-    gamma: 1
-  optim:
-    name: adamw
-    lr: 0.001
-    weight_decay: 0.0
-    sched:
-      name: WarmupAnnealing
-      warmup_steps: null
-      warmup_ratio: 0.1
-      last_epoch: -1
-      monitor: val_loss
-      reduce_on_plateau: false
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_12-57-51/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_12-57-51/lightning_logs.txt
deleted file mode 100644
index d5716ca..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_12-57-51/lightning_logs.txt
+++ /dev/null
@@ -1,39 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-Using native 16bit precision.
-Global seed set to 42
-initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 1.0 K 
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-299 K     Trainable params
-13.2 M    Non-trainable params
-13.5 M    Total params
-Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_12-57-51/lr_find_temp_model.ckpt
-Global seed set to 42
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 1.0 K 
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-299 K     Trainable params
-13.2 M    Non-trainable params
-13.5 M    Total params
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_12-57-51/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_12-57-51/nemo_error_log.txt
deleted file mode 100644
index 3c5b2ec..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_12-57-51/nemo_error_log.txt
+++ /dev/null
@@ -1,13 +0,0 @@
-[NeMo W 2021-02-09 12:57:51 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-09 12:58:25 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-09 13:06:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-09 13:12:10 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-    
-[NeMo W 2021-02-09 13:54:27 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_12-57-51/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_12-57-51/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 40c8c2e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_12-57-51/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,15 +0,0 @@
-[NeMo I 2021-02-09 12:57:51 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_12-57-51
-[NeMo I 2021-02-09 12:57:51 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-09 12:57:51 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-09 12:58:25 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-09 13:06:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-09 13:12:10 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-    
-[NeMo W 2021-02-09 13:54:27 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_13-00-26/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-09_13-00-26/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_13-00-26/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_13-00-26/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-09_13-00-26/git-info.log
deleted file mode 100644
index bf69500..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_13-00-26/git-info.log
+++ /dev/null
@@ -1,1224 +0,0 @@
-commit hash: 4cccf1ef224a31f16ab4158a6dbd1472d2705371
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/cmd-args.log
-deleted file mode 100644
-index 11a5d8e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/cmd-args.log
-+++ /dev/null
-@@ -1 +0,0 @@
--main.py
-\ No newline at end of file
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/events.out.tfevents.1612840713.Titan.20119.0 b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/events.out.tfevents.1612840713.Titan.20119.0
-deleted file mode 100644
-index a4eb1d2..0000000
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/events.out.tfevents.1612840713.Titan.20119.0 and /dev/null differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/git-info.log
-deleted file mode 100644
-index 5420ee2..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/git-info.log
-+++ /dev/null
-@@ -1,645 +0,0 @@
--commit hash: 089ad1caa03e468560d6d322ace7a5164a8178f3
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt
--index 2a26724..5be2535 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt
--@@ -20,3 +20,21 @@ initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-- 299 K     Trainable params
-- 13.2 M    Non-trainable params
-- 13.5 M    Total params
--+Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lr_find_temp_model.ckpt
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | ElectraModel         | 13.5 M
--+1 | punct_classifier    | TokenClassifier      | 2.6 K 
--+2 | domain_classifier   | SequenceClassifier   | 1.0 K 
--+3 | punctuation_loss    | FocalDiceLoss        | 0     
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+299 K     Trainable params
--+13.2 M    Non-trainable params
--+13.5 M    Total params
--+Epoch 0, global step 5305: val_loss reached 10.18683 (best 10.18683), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/checkpoints/Punctuation_with_Domain_discriminator---val_loss=10.19-epoch=0.ckpt" as top 3
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt
--index 0f1c742..2f2fa91 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt
--@@ -2,3 +2,18 @@
-- [NeMo W 2021-02-09 08:27:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--       warnings.warn(*args, **kwargs)
--     
--+[NeMo W 2021-02-09 08:34:47 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--+      warnings.warn(*args, **kwargs)
--+    
--+[NeMo W 2021-02-09 08:40:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--+      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--+    
--+[NeMo W 2021-02-09 10:21:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fd0> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--+[NeMo W 2021-02-09 10:36:30 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fa0> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--+[NeMo W 2021-02-09 11:07:22 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
--+      warnings.warn(*args, **kwargs)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt
--index e609b5b..2546dc9 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt
--@@ -4,3 +4,18 @@
-- [NeMo W 2021-02-09 08:27:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--       warnings.warn(*args, **kwargs)
--     
--+[NeMo W 2021-02-09 08:34:47 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--+      warnings.warn(*args, **kwargs)
--+    
--+[NeMo W 2021-02-09 08:40:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--+      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--+    
--+[NeMo W 2021-02-09 10:21:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fd0> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--+[NeMo W 2021-02-09 10:36:30 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fa0> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--+[NeMo W 2021-02-09 11:07:22 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
--+      warnings.warn(*args, **kwargs)
--+    
--diff --git a/experiment/Nemo2Lightning.ipynb b/experiment/Nemo2Lightning.ipynb
--index d2ec988..0dbd499 100644
----- a/experiment/Nemo2Lightning.ipynb
--+++ b/experiment/Nemo2Lightning.ipynb
--@@ -2,7 +2,7 @@
--  "cells": [
--   {
--    "cell_type": "code",
---   "execution_count": 2,
--+   "execution_count": 1,
--    "metadata": {},
--    "outputs": [
--     {
--@@ -11,7 +11,7 @@
--      "text": [
--       "Using device: cuda\n",
--       "\n",
---      "Tesla T4\n",
--+      "GeForce GTX 1080 Ti\n",
--       "Memory Usage:\n",
--       "Allocated: 0.0 GB\n",
--       "Cached:    0.0 GB\n"
--@@ -33,16 +33,16 @@
--   },
--   {
--    "cell_type": "code",
---   "execution_count": 1,
--+   "execution_count": 2,
--    "metadata": {},
--    "outputs": [
--     {
--      "data": {
--       "text/plain": [
---       "{'seed': 42, 'trainer': {'gpus': 1, 'num_nodes': 1, 'max_epochs': 6, 'max_steps': None, 'accumulate_grad_batches': 8, 'gradient_clip_val': 0, 'amp_level': 'O1', 'precision': 16, 'accelerator': 'ddp', 'checkpoint_callback': False, 'logger': False, 'log_every_n_steps': 1, 'val_check_interval': 1.0, 'resume_from_checkpoint': None}, 'exp_manager': {'exp_dir': '/home/nxingyu2/project/', 'name': 'Punctuation_with_Domain_discriminator', 'create_tensorboard_logger': True, 'create_checkpoint_callback': True}, 'base_path': '/home/nxingyu2/data', 'tmp_path': '/home/nxingyu2/data/tmp', 'model': {'nemo_path': None, 'transformer_path': 'google/electra-base-discriminator', 'initial_unfrozen': 1, 'punct_label_ids': ['', '!', ',', '-', '.', ':', ';', '?', '—', '…'], 'dataset': {'data_dir': '/home/nxingyu2/data', 'labelled': ['${base_path}/open_subtitles_processed'], 'unlabelled': ['${base_path}/ted_talks_processed'], 'max_seq_length': 128, 'pad_label': '', 'ignore_extra_tokens': False, 'ignore_start_end': False, 'use_cache': False, 'num_workers': 0, 'pin_memory': True, 'drop_last': False, 'num_labels': 10, 'num_domains': 1, 'test_unlabelled': True, 'train_ds': {'shuffle': True, 'num_samples': -1, 'batch_size': 8}, 'validation_ds': {'shuffle': True, 'num_samples': -1, 'batch_size': 2}}, 'tokenizer': {'tokenizer_name': '${model.language_model.pretrained_model_name}', 'vocab_file': None, 'tokenizer_model': None, 'special_tokens': None}, 'language_model': {'pretrained_model_name': '${model.transformer_path}', 'lm_checkpoint': None, 'config_file': None, 'config': None}, 'punct_head': {'punct_num_fc_layers': 1, 'fc_dropout': 0.1, 'activation': 'relu', 'log_softmax': False, 'use_transformer_init': True, 'loss': 'crf'}, 'domain_head': {'domain_num_fc_layers': 1, 'fc_dropout': 0.1, 'activation': 'relu', 'log_softmax': False, 'use_transformer_init': True, 'loss': 'cel', 'gamma': 0.1}, 'dice_loss': {'epsilon': 0.01, 'alpha': 3, 'macro_average': True}, 'focal_loss': {'gamma': 5}, 'optim': {'name': 'adamw', 'lr': 0.001, 'weight_decay': 0.0, 'sched': {'name': 'WarmupAnnealing', 'warmup_steps': None, 'warmup_ratio': 0.1, 'last_epoch': -1, 'monitor': 'val_loss', 'reduce_on_plateau': False}}}}"
--+       "{'seed': 42, 'trainer': {'gpus': 1, 'num_nodes': 1, 'max_epochs': 10, 'max_steps': None, 'accumulate_grad_batches': 4, 'gradient_clip_val': 0, 'amp_level': 'O1', 'precision': 16, 'accelerator': 'ddp', 'checkpoint_callback': False, 'logger': False, 'log_every_n_steps': 1, 'val_check_interval': 1.0, 'resume_from_checkpoint': None}, 'exp_manager': {'exp_dir': '/home/nxingyu/project/', 'name': 'Punctuation_with_Domain_discriminator', 'create_tensorboard_logger': True, 'create_checkpoint_callback': True}, 'base_path': '/home/nxingyu/data', 'tmp_path': '/home/nxingyu/data/tmp', 'model': {'nemo_path': None, 'transformer_path': 'google/electra-small-discriminator', 'unfrozen': 0, 'maximum_unfrozen': 1, 'unfreeze_step': 1, 'punct_label_ids': ['', '!', ',', '-', '.', ':', ';', '?', '—', '…'], 'punct_class_weights': False, 'dataset': {'data_dir': '/home/nxingyu/data', 'labelled': ['${base_path}/ted_talks_processed'], 'unlabelled': ['${base_path}/open_subtitles_processed'], 'max_seq_length': 128, 'pad_label': '', 'ignore_extra_tokens': False, 'ignore_start_end': False, 'use_cache': False, 'num_workers': 4, 'pin_memory': True, 'drop_last': False, 'num_labels': 10, 'num_domains': 2, 'test_unlabelled': True, 'train_ds': {'shuffle': True, 'num_samples': -1, 'batch_size': 4}, 'validation_ds': {'shuffle': True, 'num_samples': -1, 'batch_size': 4}}, 'tokenizer': {'tokenizer_name': '${model.language_model.pretrained_model_name}', 'vocab_file': None, 'tokenizer_model': None, 'special_tokens': None}, 'language_model': {'pretrained_model_name': '${model.transformer_path}', 'lm_checkpoint': None, 'config_file': None, 'config': None}, 'punct_head': {'punct_num_fc_layers': 1, 'fc_dropout': 0.1, 'activation': 'relu', 'log_softmax': False, 'use_transformer_init': True, 'loss': 'dice'}, 'domain_head': {'domain_num_fc_layers': 1, 'fc_dropout': 0.1, 'activation': 'relu', 'log_softmax': False, 'use_transformer_init': True, 'loss': 'cel', 'gamma': 0, 'pooling': 'mean_max', 'idx_conditioned_on': 0}, 'dice_loss': {'epsilon': 0.01, 'alpha': 3, 'macro_average': True}, 'focal_loss': {'gamma': 1}, 'optim': {'name': 'adamw', 'lr': 0.001, 'weight_decay': 0.0, 'sched': {'name': 'WarmupAnnealing', 'warmup_steps': None, 'warmup_ratio': 0.1, 'last_epoch': -1, 'monitor': 'val_loss', 'reduce_on_plateau': False}}}}"
--       ]
--      },
---     "execution_count": 1,
--+     "execution_count": 2,
--      "metadata": {},
--      "output_type": "execute_result"
--     }
--@@ -74,79 +74,79 @@
--   },
--   {
--    "cell_type": "code",
---   "execution_count": 13,
--+   "execution_count": 3,
--    "metadata": {},
--    "outputs": [
--     {
--      "name": "stderr",
--      "output_type": "stream",
--      "text": [
---      "09:01:12.28 LOG:\n",
---      "09:01:12.30 .... 'cel none' = 'cel none'\n",
---      "09:01:12.31 .... output = tensor([1.5514, 1.5514, 0.5514, 0.5514], grad_fn=<NllLossBackward>)\n",
---      "09:01:12.31 LOG:\n",
---      "09:01:12.31 .... 'cel mean' = 'cel mean'\n",
---      "09:01:12.31 .... output = tensor(1.0514, grad_fn=<NllLossBackward>)\n",
---      "09:01:12.31 LOG:\n",
---      "09:01:12.31 .... 'cel sum' = 'cel sum'\n",
---      "09:01:12.31 .... output = tensor(4.2058, grad_fn=<NllLossBackward>)\n",
---      "09:01:12.31 LOG:\n",
---      "09:01:12.32 .... 'focal sum' = 'focal sum'\n",
---      "09:01:12.32 .... loss(inp, tar) = tensor(6.7352, grad_fn=<SumBackward0>)\n",
---      "09:01:12.32 LOG:\n",
---      "09:01:12.32 .... 'focal mean' = 'focal mean'\n",
---      "09:01:12.32 .... loss(inp, tar) = tensor(0.4210, grad_fn=<MeanBackward0>)\n",
---      "09:01:12.32 LOG:\n",
---      "09:01:12.33 .... 'focal none' = 'focal none'\n",
---      "09:01:12.33 .... loss(inp, tar) = tensor([0.9635, 0.9635, 0.0991, 0.0991], grad_fn=<MulBackward0>)\n",
---      "09:01:12.33 LOG:\n",
---      "09:01:12.33 .... 'focal none' = 'focal none'\n",
---      "09:01:12.33 .... loss(inp, tar) = tensor([0.4716, 0.4716, 0.0075, 0.0075], grad_fn=<MulBackward0>)\n",
---      "09:01:12.33 LOG:\n",
---      "09:01:12.34 .... 'crf,none' = 'crf,none'\n",
---      "09:01:12.34 .... output = tensor([4.2927], grad_fn=<NegBackward>)\n",
---      "09:01:12.34 LOG:\n",
---      "09:01:12.34 .... 'crf,mean' = 'crf,mean'\n",
---      "09:01:12.34 .... output = tensor(4.3138, grad_fn=<NegBackward>)\n",
---      "09:01:12.34 LOG:\n",
---      "09:01:12.35 .... 'crf,sum' = 'crf,sum'\n",
---      "09:01:12.35 .... output = tensor(4.2588, grad_fn=<NegBackward>)\n",
---      "09:01:12.35 LOG:\n",
---      "09:01:12.35 .... 'crf,token_mean' = 'crf,token_mean'\n",
---      "09:01:12.35 .... output = tensor(1.0429, grad_fn=<DivBackward0>)\n",
---      "09:01:12.35 LOG:\n",
---      "09:01:12.35 .... 'dice none,micro' = 'dice none,micro'\n",
---      "09:01:12.36 .... output = tensor(0.1876, grad_fn=<PowBackward0>)\n",
---      "09:01:12.36 LOG:\n",
---      "09:01:12.36 .... 'dice mean,micro' = 'dice mean,micro'\n",
---      "09:01:12.36 .... output = tensor(0.0625, grad_fn=<DivBackward0>)\n",
---      "09:01:12.36 LOG:\n",
---      "09:01:12.36 .... 'dice sum,micro' = 'dice sum,micro'\n",
---      "09:01:12.36 .... output = tensor(0.1876, grad_fn=<SumBackward0>)\n",
---      "09:01:12.37 LOG:\n",
---      "09:01:12.37 .... 'dice sum,micro' = 'dice sum,micro'\n",
---      "09:01:12.37 .... output = tensor(0.0812, grad_fn=<SumBackward0>)\n",
---      "09:01:12.37 LOG:\n",
---      "09:01:12.37 .... 'dice none,macro' = 'dice none,macro'\n",
---      "09:01:12.37 .... loss(inp, tar) = tensor([5.9547, 4.4872, 2.0551], grad_fn=<MulBackward0>)\n",
---      "09:01:12.37 LOG:\n",
---      "09:01:12.38 .... 'dice mean,macro' = 'dice mean,macro'\n",
---      "09:01:12.38 .... loss(inp, tar) = tensor(0.2112, grad_fn=<DivBackward0>)\n",
---      "09:01:12.38 LOG:\n",
---      "09:01:12.38 .... 'dice sum,macro' = 'dice sum,macro'\n",
---      "09:01:12.38 .... loss(inp, tar) = tensor(0.6335, grad_fn=<SumBackward0>)\n",
---      "09:01:12.38 LOG:\n",
---      "09:01:12.38 .... 'dice none,macro' = 'dice none,macro'\n",
---      "09:01:12.39 .... loss(inp, tar) = tensor([0.3340, 0.6546, 0.3057], grad_fn=<MulBackward0>)\n",
---      "09:01:12.39 LOG:\n",
---      "09:01:12.39 .... 'dice none,macro' = 'dice none,macro'\n",
---      "09:01:12.39 .... loss(inp, tar) = tensor([0.0373, 0.2805, 0.0286], grad_fn=<MulBackward0>)\n",
---      "09:01:12.39 LOG:\n",
---      "09:01:12.39 .... 'dice sum,macro' = 'dice sum,macro'\n",
---      "09:01:12.40 .... output = tensor([0.5989, 0.7696, 0.2411], grad_fn=<MulBackward0>)\n",
---      "09:01:12.40 LOG:\n",
---      "09:01:12.40 .... 'dice sum,macro' = 'dice sum,macro'\n",
---      "09:01:12.40 .... output = tensor([0.2148, 0.4559, 0.0140], grad_fn=<MulBackward0>)\n"
--+      "10:11:13.98 LOG:\n",
--+      "10:11:14.02 .... 'cel none' = 'cel none'\n",
--+      "10:11:14.02 .... output = tensor([1.5514, 1.5514, 0.5514, 0.5514], grad_fn=<NllLossBackward>)\n",
--+      "10:11:14.02 LOG:\n",
--+      "10:11:14.02 .... 'cel mean' = 'cel mean'\n",
--+      "10:11:14.02 .... output = tensor(1.0514, grad_fn=<NllLossBackward>)\n",
--+      "10:11:14.03 LOG:\n",
--+      "10:11:14.03 .... 'cel sum' = 'cel sum'\n",
--+      "10:11:14.03 .... output = tensor(4.2058, grad_fn=<NllLossBackward>)\n",
--+      "10:11:14.08 LOG:\n",
--+      "10:11:14.08 .... 'focal sum' = 'focal sum'\n",
--+      "10:11:14.08 .... loss(inp, tar) = tensor(6.7352, grad_fn=<SumBackward0>)\n",
--+      "10:11:14.08 LOG:\n",
--+      "10:11:14.08 .... 'focal mean' = 'focal mean'\n",
--+      "10:11:14.09 .... loss(inp, tar) = tensor(0.4210, grad_fn=<MeanBackward0>)\n",
--+      "10:11:14.09 LOG:\n",
--+      "10:11:14.09 .... 'focal none' = 'focal none'\n",
--+      "10:11:14.10 .... loss(inp, tar) = tensor([0.9635, 0.9635, 0.0991, 0.0991], grad_fn=<MulBackward0>)\n",
--+      "10:11:14.10 LOG:\n",
--+      "10:11:14.10 .... 'focal none' = 'focal none'\n",
--+      "10:11:14.10 .... loss(inp, tar) = tensor([0.4716, 0.4716, 0.0075, 0.0075], grad_fn=<MulBackward0>)\n",
--+      "10:11:14.11 LOG:\n",
--+      "10:11:14.12 .... 'crf,none' = 'crf,none'\n",
--+      "10:11:14.12 .... output = tensor([4.1689], grad_fn=<NegBackward>)\n",
--+      "10:11:14.12 LOG:\n",
--+      "10:11:14.12 .... 'crf,mean' = 'crf,mean'\n",
--+      "10:11:14.12 .... output = tensor(4.0544, grad_fn=<NegBackward>)\n",
--+      "10:11:14.12 LOG:\n",
--+      "10:11:14.13 .... 'crf,sum' = 'crf,sum'\n",
--+      "10:11:14.13 .... output = tensor(4.1272, grad_fn=<NegBackward>)\n",
--+      "10:11:14.13 LOG:\n",
--+      "10:11:14.13 .... 'crf,token_mean' = 'crf,token_mean'\n",
--+      "10:11:14.13 .... output = tensor(1.0815, grad_fn=<DivBackward0>)\n",
--+      "10:11:14.13 LOG:\n",
--+      "10:11:14.14 .... 'dice none,micro' = 'dice none,micro'\n",
--+      "10:11:14.14 .... output = tensor(0.4331, grad_fn=<PowBackward0>)\n",
--+      "10:11:14.14 LOG:\n",
--+      "10:11:14.14 .... 'dice mean,micro' = 'dice mean,micro'\n",
--+      "10:11:14.14 .... output = tensor(0.1444, grad_fn=<DivBackward0>)\n",
--+      "10:11:14.14 LOG:\n",
--+      "10:11:14.14 .... 'dice sum,micro' = 'dice sum,micro'\n",
--+      "10:11:14.15 .... output = tensor(0.4331, grad_fn=<SumBackward0>)\n",
--+      "10:11:14.15 LOG:\n",
--+      "10:11:14.15 .... 'dice sum,micro' = 'dice sum,micro'\n",
--+      "10:11:14.15 .... output = tensor(0.0812, grad_fn=<SumBackward0>)\n",
--+      "10:11:14.15 LOG:\n",
--+      "10:11:14.15 .... 'dice none,macro' = 'dice none,macro'\n",
--+      "10:11:14.16 .... loss(inp, tar) = tensor([5.9547, 4.4872, 2.0551], grad_fn=<MulBackward0>)\n",
--+      "10:11:14.16 LOG:\n",
--+      "10:11:14.16 .... 'dice mean,macro' = 'dice mean,macro'\n",
--+      "10:11:14.16 .... loss(inp, tar) = tensor(0.4314, grad_fn=<DivBackward0>)\n",
--+      "10:11:14.16 LOG:\n",
--+      "10:11:14.16 .... 'dice sum,macro' = 'dice sum,macro'\n",
--+      "10:11:14.16 .... loss(inp, tar) = tensor(1.2943, grad_fn=<SumBackward0>)\n",
--+      "10:11:14.17 LOG:\n",
--+      "10:11:14.17 .... 'dice none,macro' = 'dice none,macro'\n",
--+      "10:11:14.17 .... loss(inp, tar) = tensor([0.3340, 0.6546, 0.3057], grad_fn=<MulBackward0>)\n",
--+      "10:11:14.17 LOG:\n",
--+      "10:11:14.17 .... 'dice none,macro' = 'dice none,macro'\n",
--+      "10:11:14.17 .... loss(inp, tar) = tensor([0.0373, 0.2805, 0.0286], grad_fn=<MulBackward0>)\n",
--+      "10:11:14.18 LOG:\n",
--+      "10:11:14.18 .... 'dice sum,macro' = 'dice sum,macro'\n",
--+      "10:11:14.18 .... output = tensor([0.5989, 0.7696, 0.2411], grad_fn=<MulBackward0>)\n",
--+      "10:11:14.18 LOG:\n",
--+      "10:11:14.18 .... 'dice sum,macro' = 'dice sum,macro'\n",
--+      "10:11:14.18 .... output = tensor([0.2148, 0.4559, 0.0140], grad_fn=<MulBackward0>)\n"
--      ]
--     },
--     {
--@@ -155,7 +155,7 @@
--        "('dice sum,macro', tensor([0.2148, 0.4559, 0.0140], grad_fn=<MulBackward0>))"
--       ]
--      },
---     "execution_count": 13,
--+     "execution_count": 3,
--      "metadata": {},
--      "output_type": "execute_result"
--     }
--@@ -286,32 +286,25 @@
--   },
--   {
--    "cell_type": "code",
---   "execution_count": 2,
--+   "execution_count": 4,
--    "metadata": {},
--    "outputs": [
--     {
---     "name": "stderr",
---     "output_type": "stream",
---     "text": [
---      "10:05:46.40 LOG:\n",
---      "10:05:46.46 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
---      "10:05:46.66 LOG:\n",
---      "10:05:46.66 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
---      "10:06:04.19 LOG:\n",
---      "10:06:04.20 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
---      "10:06:04.34 LOG:\n",
---      "10:06:04.34 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n"
--+     "ename": "KeyboardInterrupt",
--+     "evalue": "",
--+     "output_type": "error",
--+     "traceback": [
--+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
--+      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
--+      "\u001b[0;32m<ipython-input-4-b22a0d7714b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mdata_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
--+      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_prepared_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
--+      "\u001b[0;32m~/project/experiment/data/punctuation_datamodule.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'fit'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             self.train_dataset = PunctuationDomainDatasets(split='train',\n\u001b[0m\u001b[1;32m     64\u001b[0m                     \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                     \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
--+      "\u001b[0;32m~/project/experiment/data/punctuation_dataset_multi.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, split, num_samples, max_seq_length, punct_label_ids, labelled, unlabelled, tokenizer, randomize, data_id, tmp_path)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munlabelled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             dataset=PunctuationDomainDataset(\n\u001b[0m\u001b[1;32m    187\u001b[0m                     \u001b[0mcsv_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'{path}.{split}.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                     \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
--+      "\u001b[0;32m~/project/experiment/data/punctuation_dataset_multi.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, csv_file, tokenizer, num_samples, max_seq_length, degree, punct_label_ids, domain, labelled, randomize, target_file, tmp_path, start, end)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv_file\u001b[0m \u001b[0;34m=\u001b[0m   \u001b[0mcsv_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m   \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m=\u001b[0m  \u001b[0mdomain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunct_label_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpunct_label_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
--+      "\u001b[0;32m~/project/experiment/data/punctuation_dataset_multi.py\u001b[0m in \u001b[0;36mset_num_samples\u001b[0;34m(self, csv_file, num_samples)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-l'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTDOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_samples\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
--+      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1009\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stdin_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m                 \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
--+      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
--      ]
---    },
---    {
---     "data": {
---      "text/plain": [
---       "10609"
---      ]
---     },
---     "execution_count": 2,
---     "metadata": {},
---     "output_type": "execute_result"
--     }
--    ],
--    "source": [
--@@ -343,20 +336,9 @@
--   },
--   {
--    "cell_type": "code",
---   "execution_count": 4,
--+   "execution_count": null,
--    "metadata": {},
---   "outputs": [
---    {
---     "data": {
---      "text/plain": [
---       "10609"
---      ]
---     },
---     "execution_count": 4,
---     "metadata": {},
---     "output_type": "execute_result"
---    }
---   ],
--+   "outputs": [],
--    "source": [
--     "# it=dm.train_dataset\n",
--     "# ni=next(it)\n",
--diff --git a/experiment/core/losses/linear_chain_crf.py b/experiment/core/losses/linear_chain_crf.py
--index ed813a9..8dc59cc 100644
----- a/experiment/core/losses/linear_chain_crf.py
--+++ b/experiment/core/losses/linear_chain_crf.py
--@@ -92,6 +92,17 @@ class LinearChainCRF(torch.nn.Module):
--             mask = logits.new_ones(logits.shape[:2], dtype=torch.bool)
--         return self._viterbi_decode(logits,mask)
-- 
--+    @jit.export
--+    def predict(self, logits: Tensor, mask: Optional[Tensor] = None) -> LongTensor:
--+        self._validate(logits, mask=mask)
--+
--+        if mask is None:
--+            mask = logits.new_ones(logits.shape[:2], dtype=torch.bool)
--+        out=[]
--+        for p,m in iter(zip(logits,mask)):
--+            out.append(pad_to_len(logits.shape[1],self._viterbi_decode(p.unsqueeze(0),m.unsqueeze(0))))
--+        return torch.tensor(out)
--+        
--     def _viterbi_decode(self, logits: Tensor, mask: Tensor) -> LongTensor:
--         """
--         decode labels using viterbi algorithm
--diff --git a/experiment/core/utils.py b/experiment/core/utils.py
--index 058cc87..4be7503 100644
----- a/experiment/core/utils.py
--+++ b/experiment/core/utils.py
--@@ -3,6 +3,7 @@ import torch
-- from torch import nn
-- import regex as re
-- import snoop
--+from copy import deepcopy
-- 
-- __all__ = ['chunk_examples_with_degree', 'chunk_to_len_batch', 'view_aligned']
-- 
--@@ -26,14 +27,15 @@ def position_to_mask(max_seq_length:int,indices:list):
--         o[np.array(indices)%(max_seq_length-2)+1]=1
--     except:
--         pp('position_to_mask',np.array(indices)%(max_seq_length-2)+1)
---        # o[(np.array(indices)%(max_seq_length-2)+1).astype(int)]=1
--+        o[(np.array(indices)%(max_seq_length-2)+1).astype(int)]=1
--     return o
-- 
-- def align_labels_to_mask(mask,labels):
--     '''[0,1,0],[2] -> [0,2,0]'''
--     assert(sum(mask)==len(labels))
---    mask[mask>0]=torch.tensor(labels)
---    return mask.tolist()
--+    m1=mask.copy()
--+    m1[mask>0]=torch.tensor(labels)
--+    return m1.tolist()
-- 
-- def view_aligned(texts,tags,tokenizer,labels_to_ids):
--         return [re.sub(' ##','',' '.join(
--@@ -101,7 +103,9 @@ def chunk_to_len(max_seq_length,tokenizer,tokens,labels=None):
--     split_token_end_idxs=np.array_split(token_end_idxs,breakpoints)
--     split_subwords=np.array_split(subwords,np.arange(max_seq_length-2,len(subwords),max_seq_length-2))
--     ids=[pad_to_len(max_seq_length,tokenizer.convert_tokens_to_ids(['[CLS]']+list(_)+['[SEP]'])) for _ in split_subwords]
---    masks=[position_to_mask(max_seq_length,_) for _ in split_token_end_idxs]
--+    masks=[]
--+    for _ in split_token_end_idxs:
--+        masks.append(position_to_mask(max_seq_length,_).copy())
--     padded_labels=None
--     if labels!=None:
--         split_labels=np.array_split(labels,breakpoints)
--@@ -121,7 +125,7 @@ def chunk_to_len_batch(max_seq_length,tokenizer,tokens,labels=None,labelled=True
--     output = {'input_ids': torch.as_tensor(batch_ids, dtype=torch.long),
--               'attention_mask': torch.as_tensor(batch_ids, dtype=torch.bool),
--               'subtoken_mask': torch.as_tensor(batch_masks,dtype=torch.bool)}
---    output['subtoken_mask']|=(output['input_ids']==101)|(output['input_ids']==102)
--+    output['subtoken_mask']|=((output['input_ids']==101)|(output['input_ids']==102))
--     output['subtoken_mask']&=labelled
--     output['labels']=torch.as_tensor(batch_labels,dtype=torch.long) if labelled==True else torch.zeros_like(output['input_ids'],dtype=torch.long)
--     return output
--diff --git a/experiment/data/punctuation_dataset.py b/experiment/data/punctuation_dataset.py
--index bfd015c..c3d9fb6 100644
----- a/experiment/data/punctuation_dataset.py
--+++ b/experiment/data/punctuation_dataset.py
--@@ -10,6 +10,7 @@ import torch
-- import subprocess
-- from time import time
-- from itertools import cycle
--+import math
-- 
-- class PunctuationDomainDataset(IterableDataset):
-- 
--@@ -242,18 +243,23 @@ class PunctuationInferenceDataset(Dataset):
--             "labels": NeuralType(('B', 'T'), ChannelType()),
--         }
-- 
---    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], degree:int = 0, ):
--+    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], num_samples:int=256, degree:int = 0):
--         """ Initializes BertPunctuationInferDataset. """
--+        self.degree=degree
--+        self.punct_label_ids=punct_label_ids
--         chunked=chunk_examples_with_degree(self.degree, self.punct_label_ids)(queries)
--         features = chunk_to_len_batch(max_seq_length=max_seq_length, tokenizer=tokenizer,tokens=chunked['texts'],labelled=False)
--         self.all_input_ids = features['input_ids']
--         self.all_attention_mask = features['attention_mask']
--         self.all_subtoken_mask = features['subtoken_mask']
--+        self.num_samples=num_samples
-- 
--     def __len__(self):
---        return len(self.all_input_ids)
--+        return math.ceil(len(self.all_input_ids)/self.num_samples)
-- 
--     def __getitem__(self, idx):
---        return {'input_ids':self.all_input_ids[idx],
---                'attention_mask':self.all_attention_mask[idx],
---                'subtoken_mask':self.all_subtoken_mask[idx]}
--+        lower=idx*self.num_samples
--+        upper=min(len(self.all_input_ids),(idx+1)*self.num_samples+1)
--+        return {'input_ids':self.all_input_ids[lower:upper],
--+                'attention_mask':self.all_attention_mask[lower:upper],
--+                'subtoken_mask':self.all_subtoken_mask[lower:upper]}
--diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
--index 4027bb2..b3fe282 100644
----- a/experiment/data/punctuation_dataset_multi.py
--+++ b/experiment/data/punctuation_dataset_multi.py
--@@ -261,18 +261,23 @@ class PunctuationInferenceDataset(Dataset):
--             "labels": NeuralType(('B', 'T'), ChannelType()),
--         }
-- 
---    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], degree:int = 0, ):
--+    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], num_samples:int=256, degree:int = 0):
--         """ Initializes BertPunctuationInferDataset. """
--+        self.degree=degree
--+        self.punct_label_ids=punct_label_ids
--         chunked=chunk_examples_with_degree(self.degree, self.punct_label_ids)(queries)
--         features = chunk_to_len_batch(max_seq_length=max_seq_length, tokenizer=tokenizer,tokens=chunked['texts'],labelled=False)
--         self.all_input_ids = features['input_ids']
--         self.all_attention_mask = features['attention_mask']
--         self.all_subtoken_mask = features['subtoken_mask']
--+        self.num_samples=num_samples
-- 
--     def __len__(self):
---        return len(self.all_input_ids)
--+        return math.ceil(len(self.all_input_ids)/self.num_samples)
-- 
--     def __getitem__(self, idx):
---        return {'input_ids':self.all_input_ids[idx],
---                'attention_mask':self.all_attention_mask[idx],
---                'subtoken_mask':self.all_subtoken_mask[idx]}
--+        lower=idx*self.num_samples
--+        upper=min(len(self.all_input_ids),(idx+1)*self.num_samples+1)
--+        return {'input_ids':self.all_input_ids[lower:upper],
--+                'attention_mask':self.all_attention_mask[lower:upper],
--+                'subtoken_mask':self.all_subtoken_mask[lower:upper]}
--diff --git a/experiment/info.log b/experiment/info.log
--index d9d501b..e69de29 100644
----- a/experiment/info.log
--+++ b/experiment/info.log
--@@ -1,17 +0,0 @@
---[INFO] - shuffling train set
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.001
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f412fde0d90>" 
---will be used during training (effective maximum steps = 80) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---last_epoch: -1
---max_steps: 80
---)
--diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
--index c5db7b3..aa05eac 100644
----- a/experiment/models/punctuation_domain_model.py
--+++ b/experiment/models/punctuation_domain_model.py
--@@ -15,9 +15,9 @@ from core.losses import (AggregatorLoss, CrossEntropyLoss, FocalDiceLoss, FocalL
-- from pytorch_lightning.utilities import rank_zero_only
-- from core.optim import get_optimizer, parse_optimizer_args, prepare_lr_scheduler
-- from omegaconf import DictConfig, OmegaConf, open_dict
---from transformers import AutoModel
--+from transformers import AutoModel, AutoTokenizer
-- import torch.utils.data.dataloader as dataloader
---from data import PunctuationDataModule
--+from data import PunctuationDataModule, PunctuationInferenceDataset
-- from os import path
-- import tempfile
-- from core.common import Serialization, FileIO
--@@ -51,6 +51,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
--         self._trainer = trainer
-- 
--         self.transformer = AutoModel.from_pretrained(self.hparams.model.transformer_path)
--+        self.tokenizer=AutoTokenizer.from_pretrained(self._cfg.model.transformer_path)
--         self.ids_to_labels = {_[0]: _[1]
--                               for _ in enumerate(self.hparams.model.punct_label_ids)}
--         self.labels_to_ids = {_[1]: _[0]
--@@ -707,4 +708,23 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
--             if 'PL_TRAINER_GPUS' in os.environ:
--                 os.environ.pop('PL_TRAINER_GPUS')
-- 
---        super().teardown(stage)
--\ No newline at end of file
--+        super().teardown(stage)
--+
--+    def add_punctuation(self, queries):
--+        infer_ds=PunctuationInferenceDataset(
--+            tokenizer= self._cfg.model.transformer_path,
--+            queries=queries, 
--+            max_seq_length=self.hparams.model.dataset.max_seq_length,
--+            punct_label_ids=self._cfg.model.punct_label_ids)
--+        attention_mask = batch['attention_mask']
--+        subtoken_mask = batch['subtoken_mask']
--+        punct_labels = batch['labels']
--+        domain_labels = batch['domain']
--+        input_ids = batch['input_ids']
--+
--+        labelled_mask=(subtoken_mask[:,0]>0)
--+        test_loss, punct_logits, domain_logits = self._make_step(batch)
--+        # attention_mask = attention_mask > 0.5
--+        punct_preds = self.punctuation_loss.predict(punct_logits[labelled_mask], subtoken_mask[labelled_mask]) \
--+            if self.hparams.model.punct_head.loss == 'crf' else torch.argmax(punct_logits[labelled_mask], axis=-1)[subtoken_mask[labelled_mask]]
--+        return view_aligned(input_ids,punct_preds, self.tokenizer,self.ids_to_labels)
--\ No newline at end of file
--diff --git a/experiment/utils/__init__.py b/experiment/utils/__init__.py
--deleted file mode 100644
--index 9a292b8..0000000
----- a/experiment/utils/__init__.py
--+++ /dev/null
--@@ -1,2 +0,0 @@
---from utils.logging import Logger as _Logger
---logging = _Logger()
--diff --git a/experiment/utils/logging.py b/experiment/utils/logging.py
--deleted file mode 100644
--index 15511fd..0000000
----- a/experiment/utils/logging.py
--+++ /dev/null
--@@ -1,69 +0,0 @@
---import os.path
---import logging
---import traceback
---
---from logging import DEBUG, WARNING, ERROR, INFO
---__all__ = ['Logger']
---
---class Logger(object):
---
---    show_source_location = True
---    # Formats the message as needed and calls the correct logging method
---    # to actually handle it
---    def _raw_log(self, logfn, message, exc_info):
---        cname = ''
---        loc = ''
---        fn = ''
---        tb = traceback.extract_stack()
---        if len(tb) > 2:
---            if self.show_source_location:
---                loc = '(%s:%d):' % (os.path.basename(tb[-3][0]), tb[-3][1])
---            fn = tb[-3][2]
---            if fn != '<module>':
---                if self.__class__.__name__ != Logger.__name__:
---                    fn = self.__class__.__name__ + '.' + fn
---                fn += '()'
---
---        logfn(loc + cname + fn + ': ' + message, exc_info=exc_info)
---
---    def info(self, message, exc_info=False):
---        """
---        Log a info-level message. If exc_info is True, if an exception
---        was caught, show the exception information (message and stack trace).
---        """
---        self._raw_log(logging.info, message, exc_info)
---
---    def debug(self, message, exc_info=False):
---        """
---        Log a debug-level message. If exc_info is True, if an exception
---        was caught, show the exception information (message and stack trace).
---        """
---        self._raw_log(logging.debug, message, exc_info)
---
---    def warning(self, message, exc_info=False):
---        """
---        Log a warning-level message. If exc_info is True, if an exception
---        was caught, show the exception information (message and stack trace).
---        """
---        self._raw_log(logging.warning, message, exc_info)
---
---    def error(self, message, exc_info=False):
---        """
---        Log an error-level message. If exc_info is True, if an exception
---        was caught, show the exception information (message and stack trace).
---        """
---        self._raw_log(logging.error, message, exc_info)
---
---    @staticmethod
---    def basicConfig(level=DEBUG):
---        """
---        Apply a basic logging configuration which outputs the log to the
---        console (stderr). Optionally, the minimum log level can be set, one
---        of DEBUG, WARNING, ERROR (or any of the levels from the logging
---        module). If not set, DEBUG log level is used as minimum.
---        """
---        logging.basicConfig(level=level,
---                format='%(asctime)s %(levelname)s %(message)s',
---                datefmt='%Y-%m-%d %H:%M:%S')
---
---        logger = Logger()
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/hparams.yaml
-deleted file mode 100644
-index cbac11e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/hparams.yaml
-+++ /dev/null
-@@ -1,110 +0,0 @@
--seed: 42
--trainer:
--  gpus: 1
--  num_nodes: 1
--  max_epochs: 10
--  max_steps: null
--  accumulate_grad_batches: 4
--  gradient_clip_val: 0
--  amp_level: O1
--  precision: 16
--  accelerator: ddp
--  checkpoint_callback: false
--  logger: false
--  log_every_n_steps: 1
--  val_check_interval: 1.0
--  resume_from_checkpoint: null
--exp_manager:
--  exp_dir: /home/nxingyu/project/
--  name: Punctuation_with_Domain_discriminator
--  create_tensorboard_logger: true
--  create_checkpoint_callback: true
--base_path: /home/nxingyu/data
--tmp_path: /home/nxingyu/data/tmp
--model:
--  nemo_path: null
--  transformer_path: google/electra-small-discriminator
--  unfrozen: 0
--  maximum_unfrozen: 1
--  unfreeze_step: 1
--  punct_label_ids:
--  - ''
--  - '!'
--  - ','
--  - '-'
--  - .
--  - ':'
--  - ;
--  - '?'
--  - —
--  - …
--  punct_class_weights: false
--  dataset:
--    data_dir: /home/nxingyu/data
--    labelled:
--    - /home/nxingyu/data/ted_talks_processed
--    unlabelled:
--    - /home/nxingyu/data/open_subtitles_processed
--    max_seq_length: 128
--    pad_label: ''
--    ignore_extra_tokens: false
--    ignore_start_end: false
--    use_cache: false
--    num_workers: 4
--    pin_memory: true
--    drop_last: false
--    num_labels: 10
--    num_domains: 2
--    test_unlabelled: true
--    train_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--    validation_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--  tokenizer:
--    tokenizer_name: google/electra-small-discriminator
--    vocab_file: null
--    tokenizer_model: null
--    special_tokens: null
--  language_model:
--    pretrained_model_name: google/electra-small-discriminator
--    lm_checkpoint: null
--    config_file: null
--    config: null
--  punct_head:
--    punct_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: dice
--  domain_head:
--    domain_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: cel
--    gamma: 0
--    pooling: mean_max
--    idx_conditioned_on: 0
--  dice_loss:
--    epsilon: 0.01
--    alpha: 3
--    macro_average: true
--  focal_loss:
--    gamma: 1
--  optim:
--    name: adamw
--    lr: 0.001
--    weight_decay: 0.0
--    sched:
--      name: WarmupAnnealing
--      warmup_steps: null
--      warmup_ratio: 0.1
--      last_epoch: -1
--      monitor: val_loss
--      reduce_on_plateau: false
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/lightning_logs.txt
-deleted file mode 100644
-index c7d0c2d..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/lightning_logs.txt
-+++ /dev/null
-@@ -1,39 +0,0 @@
--Global seed set to 42
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
--Using native 16bit precision.
--Global seed set to 42
--initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 1.0 K 
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--299 K     Trainable params
--13.2 M    Non-trainable params
--13.5 M    Total params
--Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/lr_find_temp_model.ckpt
--Global seed set to 42
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 1.0 K 
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--299 K     Trainable params
--13.2 M    Non-trainable params
--13.5 M    Total params
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_error_log.txt
-deleted file mode 100644
-index 0c8b389..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_error_log.txt
-+++ /dev/null
-@@ -1,10 +0,0 @@
--[NeMo W 2021-02-09 11:10:37 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 11:11:08 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 11:18:33 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 11:24:13 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_log_globalrank-0_localrank-0.txt
-deleted file mode 100644
-index 84d61e5..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_log_globalrank-0_localrank-0.txt
-+++ /dev/null
-@@ -1,12 +0,0 @@
--[NeMo I 2021-02-09 11:10:37 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37
--[NeMo I 2021-02-09 11:10:37 exp_manager:519] TensorboardLogger has been set up
--[NeMo W 2021-02-09 11:10:37 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 11:11:08 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 11:18:33 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 11:24:13 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
-diff --git a/experiment/Untitled.ipynb b/experiment/Untitled.ipynb
-index 1a5526d..cd5247e 100644
---- a/experiment/Untitled.ipynb
-+++ b/experiment/Untitled.ipynb
-@@ -3,33 +3,33 @@
-   {
-    "cell_type": "code",
-    "execution_count": 1,
--   "id": "dense-meaning",
-+   "id": "modern-amplifier",
-    "metadata": {},
-    "outputs": [
-     {
-      "name": "stderr",
-      "output_type": "stream",
-      "text": [
--      "12:16:24.02 LOG:\n"
-+      "12:17:39.23 LOG:\n"
-      ]
-     },
-     {
-      "name": "stdout",
-      "output_type": "stream",
-      "text": [
--      "shuffling <data.punctuation_dataset_multi.PunctuationDomainDataset object at 0x7f86e5bc2220>\n"
-+      "shuffling <data.punctuation_dataset_multi.PunctuationDomainDataset object at 0x7f383a13c0a0>\n"
-      ]
-     },
-     {
-      "name": "stderr",
-      "output_type": "stream",
-      "text": [
--      "12:16:24.11 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
--      "12:16:24.17 LOG:\n",
--      "12:16:24.48 .... f\"1st {n} encoder layers of transformer frozen\" = '1st 11 encoder layers of transformer frozen'\n",
-+      "12:17:39.32 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
-+      "12:17:39.38 LOG:\n",
-+      "12:17:39.75 .... f\"1st {n} encoder layers of transformer frozen\" = '1st 11 encoder layers of transformer frozen'\n",
-       "GPU available: True, used: False\n",
-       "TPU available: None, using: 0 TPU cores\n",
--      "[NeMo W 2021-02-09 12:16:24 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
-+      "[NeMo W 2021-02-09 12:17:39 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
-       "      warnings.warn(*args, **kwargs)\n",
-       "    \n"
-      ]
-@@ -74,157 +74,26 @@
-   },
-   {
-    "cell_type": "code",
-+   "execution_count": null,
-+   "id": "minus-mississippi",
-+   "metadata": {},
-+   "outputs": [],
-+   "source": [
-+    "import regex as re\n",
-+    "re.sub('(\"[CLS]\"|)','','[CLS] we, bought: four- shirts, one, pen, and, a, mug, from, the, nvidia- gear, store, in, santa- clara, [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]')"
-+   ]
-+  },
-+  {
-+   "cell_type": "code",
-    "execution_count": 2,
--   "id": "potential-adrian",
-+   "id": "hairy-proxy",
-    "metadata": {},
-    "outputs": [
-     {
--     "name": "stderr",
--     "output_type": "stream",
--     "text": [
--      "12:16:24.62 LOG:\n",
--      "12:16:24.69 .... chunk_to_len_batch(max_seq_length, tokenizer,chunked['texts'],chunked['tags'],True) = {'attention_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
--      "12:16:24.69                                                                                                      True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
--      "12:16:24.69                                                                                                      True, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False],\n",
--      "12:16:24.69                                                                                                    [ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False],\n",
--      "12:16:24.69                                                                                                    [ True,  True,  True,  True,  True, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False]]),\n",
--      "12:16:24.69                                                                                             'input_ids': tensor([[  101,  2057,  4149,  2176, 11344,  2028,  7279,  1998,  1037, 14757,\n",
--      "12:16:24.69                                                                                                      2013,  1996,  1050, 17258,  2401,  6718,  3573,  1999,  4203, 10254,\n",
--      "12:16:24.69                                                                                                       102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0],\n",
--      "12:16:24.69                                                                                                    [  101,  2054,  2064,  1045,  2079,  2005,  2017,  2651,   102,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0],\n",
--      "12:16:24.69                                                                                                    [  101,  2129,  2024,  2017,   102,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
--      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0]]),\n",
--      "12:16:24.69                                                                                             'labels': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0],\n",
--      "12:16:24.69                                                                                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0],\n",
--      "12:16:24.69                                                                                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0]]),\n",
--      "12:16:24.69                                                                                             'subtoken_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
--      "12:16:24.69                                                                                                      True,  True, False, False,  True,  True,  True,  True,  True,  True,\n",
--      "12:16:24.69                                                                                                      True, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False],\n",
--      "12:16:24.69                                                                                                    [ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False],\n",
--      "12:16:24.69                                                                                                    [ True,  True,  True,  True,  True, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
--      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False]])}\n"
--     ]
--    },
--    {
-      "data": {
-       "text/plain": [
--       "['[CLS] we, bought, four- shirts: one, pen, and, a, mug, from, the, nvidia- gear, store, in, santa- clara, [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
--       " '[CLS] what? can— i? do? for? you? today? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
-+       "['[CLS] we, bought: four- shirts, one, pen, and, a, mug, from, the, nvidia- gear, store, in, santa- clara, [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
-+       " '[CLS] what? can, i? do? for? you? today? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
-        " '[CLS] how? are? you? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']"
-       ]
-      },
-@@ -246,7 +115,7 @@
-   {
-    "cell_type": "code",
-    "execution_count": null,
--   "id": "amateur-production",
-+   "id": "employed-station",
-    "metadata": {},
-    "outputs": [],
-    "source": []
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index efa7a5d..c236f27 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -5,7 +5,7 @@ trainer:
-     max_epochs: 10
-     max_steps: null # precedence over max_epochs
-     accumulate_grad_batches: 4 # accumulates grads every k batches
--    gradient_clip_val: 0
-+    gradient_clip_val: 4
-     amp_level: O1 # O1/O2 for mixed precision
-     precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-     accelerator: ddp
-@@ -67,7 +67,7 @@ model:
-             # - ${base_path}/open_subtitles_processed #  
-         unlabelled:
-             # - ${base_path}/ted_talks_processed #
--            - ${base_path}/open_subtitles_processed #  
-+            # - ${base_path}/open_subtitles_processed #  
-             # parameters for dataset preprocessing
-         max_seq_length: 128
-         pad_label: ''
-@@ -75,11 +75,11 @@ model:
-         ignore_start_end: false
-         use_cache: false
-         # shared among dataloaders
--        num_workers:  4
-+        num_workers:  6
-         pin_memory: true
-         drop_last: false
-         num_labels: 10
--        num_domains: 2
-+        num_domains: 1
-         test_unlabelled: true
- 
-         train_ds:
-diff --git a/experiment/info.log b/experiment/info.log
-index 69e9a76..e69de29 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,85 +0,0 @@
--[INFO] - shuffling train set
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f717ffe5b80>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          89.19      22.74      36.24       3012
--! (label_id: 1)                                          0.00       0.00       0.00          1
--, (label_id: 2)                                          7.31      36.21      12.16        243
--- (label_id: 3)                                          2.27      21.43       4.11         28
--. (label_id: 4)                                          1.68       1.65       1.66        182
--: (label_id: 5)                                          0.00       0.00       0.00          5
--; (label_id: 6)                                          0.00       0.00       0.00          3
--? (label_id: 7)                                          0.24      22.22       0.48          9
--— (label_id: 8)                                          0.00       0.00       0.00         10
--… (label_id: 9)                                          0.00       0.00       0.00          1
---------------------
--micro avg                                               22.44      22.44      22.44       3494
--macro avg                                               10.07      10.43       5.47       3494
--weighted avg                                            77.50      22.44      32.21       3494
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                         50.00     100.00      66.67         34
--1 (label_id: 1)                                          0.00       0.00       0.00         34
---------------------
--micro avg                                               50.00      50.00      50.00         68
--macro avg                                               25.00      50.00      33.33         68
--weighted avg                                            25.00      50.00      33.33         68
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.007943282347242822
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f717f560fa0>" 
--will be used during training (effective maximum steps = 53050) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 53050
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          89.19      22.74      36.24       3012
--! (label_id: 1)                                          0.00       0.00       0.00          1
--, (label_id: 2)                                          7.31      36.21      12.16        243
--- (label_id: 3)                                          2.27      21.43       4.11         28
--. (label_id: 4)                                          1.68       1.65       1.66        182
--: (label_id: 5)                                          0.00       0.00       0.00          5
--; (label_id: 6)                                          0.00       0.00       0.00          3
--? (label_id: 7)                                          0.24      22.22       0.48          9
--— (label_id: 8)                                          0.00       0.00       0.00         10
--… (label_id: 9)                                          0.00       0.00       0.00          1
---------------------
--micro avg                                               22.44      22.44      22.44       3494
--macro avg                                               10.07      10.43       5.47       3494
--weighted avg                                            77.50      22.44      32.21       3494
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                         50.00     100.00      66.67         34
--1 (label_id: 1)                                          0.00       0.00       0.00         34
---------------------
--micro avg                                               50.00      50.00      50.00         68
--macro avg                                               25.00      50.00      33.33         68
--weighted avg                                            25.00      50.00      33.33         68
--
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index dab395e..a4a7022 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -157,6 +157,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         passed in as `batch`.
-         """
-         p=(self.current_epoch*self.train_size+batch_idx)/(self.train_size*self.hparams.trainer.max_epochs)
-+        if (self.batch_idx%1000==0):
-+            print('gamma:',p)
-         self.grad_reverse.scale=2/(1+math.exp(-10*p))-1
-         loss, _, _ = self._make_step(batch)
-         lr = self._optimizer.param_groups[0]['lr']
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_13-00-26/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_13-00-26/lightning_logs.txt
deleted file mode 100644
index 1562e1d..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_13-00-26/lightning_logs.txt
+++ /dev/null
@@ -1,22 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
-Using native 16bit precision.
-Global seed set to 42
-initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 513   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-299 K     Trainable params
-13.2 M    Non-trainable params
-13.5 M    Total params
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_13-00-26/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_13-00-26/nemo_error_log.txt
deleted file mode 100644
index 76cf607..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_13-00-26/nemo_error_log.txt
+++ /dev/null
@@ -1,4 +0,0 @@
-[NeMo W 2021-02-09 13:00:26 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-09 13:00:37 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_13-00-26/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_13-00-26/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index f04cec0..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_13-00-26/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,6 +0,0 @@
-[NeMo I 2021-02-09 13:00:26 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_13-00-26
-[NeMo I 2021-02-09 13:00:26 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-09 13:00:26 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-09 13:00:37 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
diff --git a/experiment/config.yaml b/experiment/config.yaml
index 3cf8809..4f37ddd 100644
--- a/experiment/config.yaml
+++ b/experiment/config.yaml
@@ -32,12 +32,12 @@ trainer:
     # resume_from_checkpoint: null
 
 exp_manager:
-    exp_dir: /home/nxingyu2/project/ # /root/project # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
+    exp_dir: /home/nxingyu/project/ # /root/project # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
     name: Punctuation_with_Domain_discriminator  # The name of your model
     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
     create_checkpoint_callback: true 
-base_path: /home/nxingyu2/data # /root/data # 
-tmp_path: /home/nxingyu2/data/tmp # /tmp # 
+base_path: /home/nxingyu/data # /root/data # 
+tmp_path: /home/nxingyu/data/tmp # /tmp # 
 
 model:
     nemo_path: null
@@ -61,13 +61,13 @@ model:
     punct_class_weights: false
     
     dataset:
-        data_dir: /home/nxingyu2/data # /root/data # 
+        data_dir: /home/nxingyu/data # /root/data # 
         labelled:
-            # - ${base_path}/ted_talks_processed #
-            - ${base_path}/open_subtitles_processed #  
-        unlabelled:
             - ${base_path}/ted_talks_processed #
             # - ${base_path}/open_subtitles_processed #  
+        unlabelled:
+            # - ${base_path}/ted_talks_processed #
+            # - ${base_path}/open_subtitles_processed #  
             # parameters for dataset preprocessing
         max_seq_length: 128
         pad_label: ''
@@ -75,11 +75,11 @@ model:
         ignore_start_end: false
         use_cache: false
         # shared among dataloaders
-        num_workers:  4
+        num_workers:  6
         pin_memory: true
         drop_last: true
         num_labels: 10
-        num_domains: 2
+        num_domains: 1
         test_unlabelled: true
         attach_label_to_end: none # false if attach to start none if dont mask
 
@@ -115,7 +115,7 @@ model:
         activation: 'relu'
         log_softmax: false
         use_transformer_init: true
-        loss: 'dice'
+        loss: 'crf'
 
     domain_head:
         domain_num_fc_layers: 1
@@ -124,7 +124,7 @@ model:
         log_softmax: false
         use_transformer_init: true
         loss: 'cel'
-        gamma: 0.1 #0.1 # coefficient of gradient reversal
+        gamma: 0.01 #0.1 # coefficient of gradient reversal
         pooling: 'mean_max' # 'mean' mean_max
         idx_conditioned_on: 0
     
diff --git a/experiment/core/classification_report.py b/experiment/core/classification_report.py
index 1a710a2..5321b5b 100644
--- a/experiment/core/classification_report.py
+++ b/experiment/core/classification_report.py
@@ -90,6 +90,7 @@ class ClassificationReport(Metric):
         FN = []
         FP = []
         CM = torch.zeros((self.num_classes,self.num_classes),dtype=torch.long).to(predictions.device)
+        predictions=predictions.long()
         CM.index_add_(0, predictions, one_hot(labels,num_classes=self.num_classes))
         for label_id in range(self.num_classes):
             current_label = labels == label_id
diff --git a/experiment/info.log b/experiment/info.log
index 99b19eb..e69de29 100644
--- a/experiment/info.log
+++ b/experiment/info.log
@@ -1,1129 +0,0 @@
-[INFO] - GPU available: True, used: True
-[INFO] - TPU available: None, using: 0 TPU cores
-[INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
-[INFO] - Using native 16bit precision.
-[INFO] - shuffling train set
-[INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-[INFO] - Optimizer config = AdamW (
-Parameter Group 0
-    amsgrad: False
-    betas: (0.9, 0.999)
-    eps: 1e-08
-    lr: 0.01
-    weight_decay: 0.0
-)
-[INFO] - Scheduler "<core.optim.lr_scheduler.CosineAnnealing object at 0x7f98a67675e0>" 
-will be used during training (effective maximum steps = 78825) - 
-Parameters : 
-(warmup_steps: null
-warmup_ratio: 0.1
-min_lr: 1.0e-10
-last_epoch: -1
-max_steps: 78825
-)
-[INFO] - 
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 108 M 
-1 | punct_classifier    | TokenClassifier      | 7.7 K 
-2 | domain_classifier   | SequenceClassifier   | 3.1 K 
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-2.4 M     Trainable params
-106 M     Non-trainable params
-108 M     Total params
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.31       1.05       2.09      17162
-! (label_id: 1)                                          0.28      10.34       0.54         58
-, (label_id: 2)                                          3.49      10.34       5.22        532
-- (label_id: 3)                                          0.00       0.00       0.00         27
-. (label_id: 4)                                          7.95       2.53       3.84       1028
-: (label_id: 5)                                          0.00       0.00       0.00          0
-; (label_id: 6)                                          0.03      50.00       0.06          4
-? (label_id: 7)                                          0.37       2.07       0.63        145
-— (label_id: 8)                                          0.00       0.00       0.00          0
-… (label_id: 9)                                          0.52      36.00       1.03         75
--------------------
-micro avg                                                1.58       1.58       1.58      19031
-macro avg                                               13.74      14.04       1.68      19031
-weighted avg                                            88.29       1.58       2.25      19031
-
--------------------
-181.00  0.00  0.00  1.00  4.00  0.00  0.00  0.00  0.00  0.00
-2048.00  6.00 37.00  4.00 59.00  0.00  1.00  9.00  0.00 10.00
-1435.00 10.00 55.00  0.00 68.00  0.00  0.00  5.00  0.00  1.00
-80.00  0.00  3.00  0.00  2.00  0.00  0.00  0.00  0.00  1.00
-273.00  1.00 17.00  0.00 26.00  0.00  0.00  8.00  0.00  2.00
-2232.00  7.00 48.00  6.00 81.00  0.00  1.00 12.00  0.00  1.00
-5515.00 19.00 217.00 10.00 431.00  0.00  2.00 72.00  0.00 30.00
-754.00  0.00 13.00  2.00 28.00  0.00  0.00  3.00  0.00  3.00
-15.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-4629.00 15.00 142.00  4.00 329.00  0.00  0.00 36.00  0.00 27.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                          0.00       0.00       0.00         80
-1 (label_id: 1)                                         50.00     100.00      66.67         80
--------------------
-micro avg                                               50.00      50.00      50.00        160
-macro avg                                               25.00      50.00      33.33        160
-weighted avg                                            25.00      50.00      33.33        160
-
--------------------
- 0.00  0.00
-80.00 80.00
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          95.76      96.99      96.37   15595410
-! (label_id: 1)                                         20.62      33.51      25.53     132074
-, (label_id: 2)                                         36.23      32.82      34.44     481066
-- (label_id: 3)                                         35.39      11.59      17.46      44642
-. (label_id: 4)                                         46.75      40.61      43.47     922110
-: (label_id: 5)                                          0.00       0.00       0.00       2427
-; (label_id: 6)                                          0.00       0.00       0.00        951
-? (label_id: 7)                                         35.71      35.33      35.52     244079
-— (label_id: 8)                                          0.00       0.00       0.00         81
-… (label_id: 9)                                          0.00       0.00       0.00      80692
--------------------
-micro avg                                               90.23      90.23      90.23   17503532
-macro avg                                               27.05      25.08      25.28   17503532
-weighted avg                                            89.52      90.23      89.83   17503532
-
--------------------
-15125737.00 27277.00 191637.00 26435.00 313125.00 1788.00 462.00 70019.00 27.00 39497.00
-27847.00 44260.00 41574.00 3953.00 74077.00 65.00 54.00 15328.00  8.00 7456.00
-113780.00 20070.00 157886.00 3314.00 106258.00 113.00 35.00 20225.00  8.00 14080.00
-7777.00 83.00 168.00 5172.00 1025.00 27.00  1.00 119.00  1.00 242.00
-254717.00 33525.00 71011.00 2723.00 374479.00 316.00 345.00 52153.00 24.00 11717.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-65552.00 6859.00 18790.00 3045.00 53146.00 118.00 54.00 86235.00 13.00 7700.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         50.00     100.00      66.67      72939
-1 (label_id: 1)                                          0.00       0.00       0.00      72939
--------------------
-micro avg                                               50.00      50.00      50.00     145878
-macro avg                                               25.00      50.00      33.33     145878
-weighted avg                                            25.00      50.00      33.33     145878
-
--------------------
-72939.00 72939.00
- 0.00  0.00
-[INFO] - Epoch 0, global step 5254: val_loss reached 115.21539 (best 115.21539), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=115.22-epoch=0.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          96.94      95.95      96.44   15594360
-! (label_id: 1)                                         24.17      34.29      28.35     131487
-, (label_id: 2)                                         37.23      37.73      37.48     481266
-- (label_id: 3)                                         28.21      25.70      26.90      44079
-. (label_id: 4)                                         42.22      54.07      47.41     922224
-: (label_id: 5)                                          0.00       0.00       0.00       2409
-; (label_id: 6)                                          0.00       0.00       0.00        979
-? (label_id: 7)                                         68.33      28.19      39.92     244473
-— (label_id: 8)                                          0.00       0.00       0.00         80
-… (label_id: 9)                                         21.74      19.09      20.33      80827
--------------------
-micro avg                                               90.18      90.18      90.18   17502184
-macro avg                                               31.88      29.50      29.68   17502184
-weighted avg                                            90.93      90.18      90.39   17502184
-
--------------------
-14962490.00 17786.00 153114.00 18055.00 201695.00 1774.00 332.00 47661.00 15.00 31863.00
-17955.00 45084.00 32767.00 2197.00 69259.00 22.00 53.00 14020.00  8.00 5190.00
-109565.00 24617.00 181579.00 1899.00 129163.00 60.00 43.00 29063.00  9.00 11679.00
-25530.00 182.00 347.00 11329.00 2400.00 39.00  3.00 132.00  1.00 191.00
-446731.00 39136.00 98104.00 3537.00 498618.00 463.00 535.00 78504.00 27.00 15484.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-12991.00 2301.00 7030.00 316.00 8301.00 17.00  4.00 68925.00  5.00 988.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-19098.00 2381.00 8325.00 6746.00 12788.00 34.00  9.00 6168.00 15.00 15432.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         75.40      77.49      76.43      72939
-1 (label_id: 1)                                         76.85      74.72      75.77      72939
--------------------
-micro avg                                               76.11      76.11      76.11     145878
-macro avg                                               76.13      76.11      76.10     145878
-weighted avg                                            76.13      76.11      76.10     145878
-
--------------------
-56524.00 18438.00
-16415.00 54501.00
-[INFO] - Epoch 1, global step 10509: val_loss reached 22.29331 (best 22.29331), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=22.29-epoch=1.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.37      95.54      96.45   15597741
-! (label_id: 1)                                         26.42      30.96      28.51     131817
-, (label_id: 2)                                         42.10      32.84      36.90     481164
-- (label_id: 3)                                         23.37      34.25      27.79      44240
-. (label_id: 4)                                         41.14      65.17      50.44     922389
-: (label_id: 5)                                          0.00       0.00       0.00       2305
-; (label_id: 6)                                          0.00       0.00       0.00        978
-? (label_id: 7)                                         65.90      30.99      42.16     244399
-— (label_id: 8)                                          0.00       0.00       0.00         85
-… (label_id: 9)                                         35.29      13.04      19.05      80604
--------------------
-micro avg                                               90.28      90.28      90.28   17505722
-macro avg                                               33.16      30.28      30.13   17505722
-weighted avg                                            91.42      90.28      90.57   17505722
-
--------------------
-14902258.00 14322.00 140661.00 15231.00 161865.00 1308.00 294.00 37909.00 12.00 31475.00
-15291.00 40806.00 26976.00 1918.00 54217.00 34.00 54.00 10322.00  4.00 4836.00
-75146.00 20443.00 158025.00 1326.00 90923.00 37.00 18.00 19692.00  6.00 9759.00
-42984.00 618.00 1546.00 15154.00 3313.00 302.00  6.00 392.00  1.00 516.00
-537671.00 52833.00 141835.00 5214.00 601078.00 599.00 588.00 99145.00 46.00 21990.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-17390.00 2462.00 8431.00 486.00 8863.00 16.00 16.00 75742.00  8.00 1515.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-7001.00 333.00 3690.00 4911.00 2130.00  9.00  2.00 1197.00  8.00 10513.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         82.99      74.51      78.52      72939
-1 (label_id: 1)                                         76.87      84.72      80.61      72939
--------------------
-micro avg                                               79.62      79.62      79.62     145878
-macro avg                                               79.93      79.62      79.56     145878
-weighted avg                                            79.93      79.62      79.56     145878
-
--------------------
-54348.00 11142.00
-18591.00 61797.00
-[INFO] - Epoch 2, global step 15764: val_loss reached 13.03634 (best 13.03634), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=13.04-epoch=2.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.04      95.93      96.48   15593759
-! (label_id: 1)                                         26.18      30.27      28.08     131176
-, (label_id: 2)                                         37.21      38.66      37.92     481613
-- (label_id: 3)                                         26.28      31.16      28.51      44269
-. (label_id: 4)                                         43.46      57.00      49.32     922442
-: (label_id: 5)                                          0.00       0.00       0.00       2480
-; (label_id: 6)                                          0.00       0.00       0.00        986
-? (label_id: 7)                                         62.06      34.80      44.59     243336
-— (label_id: 8)                                          0.00       0.00       0.00         81
-… (label_id: 9)                                         34.95      14.72      20.71      80532
--------------------
-micro avg                                               90.40      90.40      90.40   17500674
-macro avg                                               32.72      30.25      30.56   17500674
-weighted avg                                            91.06      90.40      90.61   17500674
-
--------------------
-14959341.00 16968.00 151956.00 17591.00 191782.00 1556.00 334.00 43138.00 16.00 33463.00
-16095.00 39711.00 25446.00 1535.00 54595.00 16.00 47.00 9960.00  4.00 4278.00
-119910.00 25693.00 186170.00 1855.00 129885.00 76.00 63.00 25617.00 14.00 10999.00
-31612.00 798.00 1312.00 13793.00 3506.00 244.00  7.00 536.00  2.00 673.00
-438663.00 44152.00 100518.00 4189.00 525774.00 547.00 517.00 77855.00 28.00 17465.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-22648.00 3172.00 9643.00 565.00 13889.00 33.00 15.00 84680.00 10.00 1803.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-5490.00 682.00 6568.00 4741.00 3011.00  8.00  3.00 1550.00  7.00 11851.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         58.88      74.19      65.65      72939
-1 (label_id: 1)                                         65.11      48.18      55.38      72939
--------------------
-micro avg                                               61.18      61.18      61.18     145878
-macro avg                                               62.00      61.18      60.52     145878
-weighted avg                                            62.00      61.18      60.52     145878
-
--------------------
-54111.00 37796.00
-18828.00 35143.00
-[INFO] - Epoch 3, global step 21019: val_loss reached 15.18966 (best 13.03634), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=15.19-epoch=3.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.05      94.34      96.16   15589848
-! (label_id: 1)                                         23.17      30.69      26.40     132067
-, (label_id: 2)                                         32.70      44.75      37.79     480971
-- (label_id: 3)                                         23.63      34.60      28.08      44614
-. (label_id: 4)                                         40.91      60.81      48.91     922579
-: (label_id: 5)                                          0.00       0.00       0.00       2446
-; (label_id: 6)                                          0.00       0.00       0.00        960
-? (label_id: 7)                                         58.17      37.93      45.92     244526
-— (label_id: 8)                                          0.00       0.00       0.00         69
-… (label_id: 9)                                         23.68      20.53      21.99      81023
--------------------
-micro avg                                               89.43      89.43      89.43   17499104
-macro avg                                               30.03      32.37      30.53   17499104
-weighted avg                                            91.57      89.43      90.30   17499104
-
--------------------
-14707316.00 9041.00 101568.00 12781.00 117293.00 1108.00 209.00 24817.00  6.00 25145.00
-38842.00 40529.00 23856.00 1966.00 55108.00 28.00 80.00 10252.00  2.00 4269.00
-203127.00 32376.00 215253.00 3074.00 157418.00 184.00 77.00 32763.00 14.00 13995.00
-43782.00 413.00 936.00 15436.00 3542.00 361.00  4.00 407.00  1.00 448.00
-545637.00 44216.00 117425.00 4413.00 561060.00 699.00 560.00 78804.00 30.00 18732.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-31800.00 3685.00 10871.00 538.00 17945.00 38.00 27.00 92754.00  4.00 1803.00
- 5.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-19339.00 1807.00 11062.00 6406.00 10213.00 28.00  3.00 4729.00 12.00 16631.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         71.14      72.42      71.77      72939
-1 (label_id: 1)                                         71.91      70.61      71.26      72939
--------------------
-micro avg                                               71.52      71.52      71.52     145878
-macro avg                                               71.53      71.52      71.52     145878
-weighted avg                                            71.53      71.52      71.52     145878
-
--------------------
-52824.00 21434.00
-20115.00 51505.00
-[INFO] - Epoch 4, global step 26274: val_loss reached 10.95131 (best 10.95131), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=10.95-epoch=4.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          91.79      99.56      95.52   15593568
-! (label_id: 1)                                         37.06      10.66      16.56     131858
-, (label_id: 2)                                         45.37      20.89      28.61     480872
-- (label_id: 3)                                         55.97       0.17       0.34      44566
-. (label_id: 4)                                         59.86      17.09      26.59     923462
-: (label_id: 5)                                          0.00       0.00       0.00       2382
-; (label_id: 6)                                          0.00       0.00       0.00       1005
-? (label_id: 7)                                         80.43      14.19      24.13     243779
-— (label_id: 8)                                          0.00       0.00       0.00         77
-… (label_id: 9)                                         40.56      11.27      17.64      80824
--------------------
-micro avg                                               90.51      90.51      90.51   17502392
-macro avg                                               41.10      17.38      20.94   17502392
-weighted avg                                            87.91      90.51      87.83   17502392
-
--------------------
-15525010.00 87224.00 354030.00 38482.00 675089.00 2252.00 862.00 169600.00 58.00 61242.00
-1157.00 14059.00 4932.00 372.00 13830.00  2.00  4.00 2357.00  1.00 1224.00
-11569.00 15588.00 100440.00 615.00 72381.00  9.00 14.00 15521.00  5.00 5224.00
-56.00  1.00  0.00 75.00  1.00  0.00  0.00  0.00  0.00  1.00
-51578.00 13575.00 15747.00 771.00 157803.00 113.00 119.00 20412.00  7.00 3515.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-1873.00 993.00 2403.00 184.00 2443.00  3.00  4.00 34595.00  3.00 510.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-2325.00 418.00 3320.00 4067.00 1915.00  3.00  2.00 1294.00  3.00 9108.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         83.13      35.47      49.73      72939
-1 (label_id: 1)                                         58.98      92.80      72.13      72939
--------------------
-micro avg                                               64.14      64.14      64.14     145878
-macro avg                                               71.06      64.14      60.93     145878
-weighted avg                                            71.06      64.14      60.93     145878
-
--------------------
-25873.00 5252.00
-47066.00 67687.00
-[INFO] - Epoch 5, global step 31529: val_loss reached 11.56726 (best 10.95131), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=11.57-epoch=5.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.10      96.73      96.91   15594783
-! (label_id: 1)                                         26.03      34.01      29.49     130591
-, (label_id: 2)                                         36.51      44.31      40.03     481352
-- (label_id: 3)                                         28.42      32.41      30.29      44046
-. (label_id: 4)                                         49.66      51.07      50.36     921748
-: (label_id: 5)                                          0.00       0.00       0.00       2368
-; (label_id: 6)                                          0.00       0.00       0.00        969
-? (label_id: 7)                                         62.15      39.66      48.42     244359
-— (label_id: 8)                                          0.00       0.00       0.00         77
-… (label_id: 9)                                         27.15      19.08      22.41      80560
--------------------
-micro avg                                               91.08      91.08      91.08   17500852
-macro avg                                               32.70      31.73      31.79   17500852
-weighted avg                                            91.40      91.08      91.19   17500852
-
--------------------
-15084688.00 15259.00 147298.00 16422.00 197293.00 1535.00 340.00 41006.00 12.00 31565.00
-15860.00 44415.00 26169.00 1407.00 66997.00 25.00 42.00 10913.00  1.00 4805.00
-140870.00 27725.00 213270.00 2277.00 156344.00 129.00 106.00 30845.00 13.00 12569.00
-28941.00 863.00 1674.00 14277.00 3377.00 161.00  2.00 453.00  1.00 482.00
-289083.00 36831.00 72507.00 3113.00 470746.00 467.00 451.00 60772.00 31.00 13876.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 2.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-22093.00 4143.00 10862.00 528.00 19433.00 31.00 23.00 96911.00  9.00 1893.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-13246.00 1355.00 9572.00 6022.00 7558.00 20.00  5.00 3459.00 10.00 15370.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         81.20      84.83      82.97      72939
-1 (label_id: 1)                                         84.12      80.36      82.20      72939
--------------------
-micro avg                                               82.59      82.59      82.59     145878
-macro avg                                               82.66      82.59      82.58     145878
-weighted avg                                            82.66      82.59      82.58     145878
-
--------------------
-61871.00 14324.00
-11068.00 58615.00
-[INFO] - Epoch 6, global step 36784: val_loss reached 4.74845 (best 4.74845), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=4.75-epoch=6.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.41      96.28      96.84   15588559
-! (label_id: 1)                                         27.28      33.75      30.17     131502
-, (label_id: 2)                                         41.37      38.90      40.10     481656
-- (label_id: 3)                                         28.61      33.05      30.67      44145
-. (label_id: 4)                                         44.95      60.42      51.55     922262
-: (label_id: 5)                                          0.00       0.00       0.00       2425
-; (label_id: 6)                                          0.00       0.00       0.00        967
-? (label_id: 7)                                         64.43      36.88      46.91     244604
-— (label_id: 8)                                          0.00       0.00       0.00         55
-… (label_id: 9)                                         32.17      17.56      22.72      80592
--------------------
-micro avg                                               90.96      90.96      90.96   17496768
-macro avg                                               33.62      31.68      31.90   17496768
-weighted avg                                            91.62      90.96      91.16   17496768
-
--------------------
-15007911.00 13555.00 140211.00 15922.00 160726.00 1486.00 277.00 36208.00 10.00 30305.00
-14660.00 44381.00 25986.00 1612.00 60515.00 20.00 53.00 10649.00  3.00 4792.00
-83905.00 24199.00 187365.00 1548.00 121409.00 69.00 46.00 24100.00 10.00 10233.00
-29750.00 794.00 1564.00 14592.00 3181.00 109.00  2.00 460.00  1.00 549.00
-424767.00 44224.00 108211.00 4465.00 557202.00 701.00 560.00 80545.00 20.00 18891.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-19879.00 3483.00 9949.00 496.00 14274.00 26.00 25.00 90204.00  4.00 1667.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-7687.00 866.00 8370.00 5510.00 4955.00 14.00  4.00 2438.00  7.00 14155.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         53.81      98.12      69.50      72939
-1 (label_id: 1)                                         89.34      15.77      26.80      72939
--------------------
-micro avg                                               56.94      56.94      56.94     145878
-macro avg                                               71.57      56.94      48.15     145878
-weighted avg                                            71.57      56.94      48.15     145878
-
--------------------
-71567.00 61440.00
-1372.00 11499.00
-[INFO] - Epoch 7, global step 42039: val_loss reached 10.55844 (best 4.74845), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=10.56-epoch=7.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.07      97.04      97.05   15595430
-! (label_id: 1)                                         22.99      42.55      29.85     131969
-, (label_id: 2)                                         40.06      40.07      40.06     481215
-- (label_id: 3)                                         29.72      30.99      30.34      43898
-. (label_id: 4)                                         49.47      50.39      49.93     923479
-: (label_id: 5)                                          0.00       0.00       0.00       2496
-; (label_id: 6)                                          0.00       0.00       0.00        924
-? (label_id: 7)                                         63.43      39.36      48.58     245290
-— (label_id: 8)                                          0.00       0.00       0.00         89
-… (label_id: 9)                                         28.94      18.33      22.44      81055
--------------------
-micro avg                                               91.24      91.24      91.24   17505844
-macro avg                                               33.17      31.87      31.83   17505844
-weighted avg                                            91.46      91.24      91.28   17505844
-
--------------------
-15133102.00 14587.00 153243.00 17067.00 197324.00 1676.00 306.00 40980.00 10.00 32086.00
-24858.00 56147.00 40060.00 2436.00 95997.00 41.00 97.00 17489.00 15.00 7055.00
-90755.00 22972.00 192829.00 1638.00 136815.00 80.00 64.00 25504.00 12.00 10702.00
-24573.00 724.00 1615.00 13602.00 3707.00 207.00  7.00 610.00  3.00 722.00
-290244.00 32918.00 73254.00 3087.00 465317.00 454.00 428.00 61033.00 29.00 13808.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-20353.00 3805.00 11145.00 521.00 17964.00 24.00 19.00 96545.00  7.00 1824.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-11545.00 816.00 9069.00 5547.00 6355.00 14.00  3.00 3129.00 13.00 14858.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         99.83      22.31      36.47      72939
-1 (label_id: 1)                                         56.27      99.96      72.01      72939
--------------------
-micro avg                                               61.14      61.14      61.14     145878
-macro avg                                               78.05      61.14      54.24     145878
-weighted avg                                            78.05      61.14      54.24     145878
-
--------------------
-16271.00 27.00
-56668.00 72912.00
-[INFO] - Epoch 8, global step 47294: val_loss reached 10.89379 (best 4.74845), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=10.89-epoch=8.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.22      94.84      96.50   15591410
-! (label_id: 1)                                         23.95      40.67      30.15     131931
-, (label_id: 2)                                         43.01      35.73      39.03     481727
-- (label_id: 3)                                         25.51      36.79      30.13      44198
-. (label_id: 4)                                         40.69      66.32      50.44     922501
-: (label_id: 5)                                          0.00       0.00       0.00       2371
-; (label_id: 6)                                          0.00       0.00       0.00       1036
-? (label_id: 7)                                         54.96      43.33      48.46     244078
-— (label_id: 8)                                          0.00       0.00       0.00         71
-… (label_id: 9)                                         26.00      19.54      22.31      80978
--------------------
-micro avg                                               90.07      90.07      90.07   17500300
-macro avg                                               31.23      33.72      31.70   17500300
-weighted avg                                            91.96      90.07      90.79   17500300
-
--------------------
-14786965.00 7765.00 99954.00 12722.00 102452.00 1242.00 235.00 20446.00  7.00 23814.00
-25572.00 53650.00 38795.00 2061.00 83538.00 20.00 95.00 13868.00  8.00 6361.00
-90675.00 17493.00 172132.00 1312.00 92736.00 51.00 34.00 17200.00  6.00 8603.00
-37820.00 952.00 2095.00 16262.00 4703.00 262.00  9.00 705.00  1.00 949.00
-588722.00 46965.00 142518.00 5510.00 611801.00 729.00 631.00 83393.00 34.00 23114.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-42928.00 4227.00 15188.00 648.00 21291.00 43.00 29.00 105763.00  9.00 2312.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-18728.00 879.00 11045.00 5683.00 5980.00 24.00  3.00 2703.00  6.00 15825.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         51.46      93.49      66.38      72939
-1 (label_id: 1)                                         64.46      11.82      19.97      72939
--------------------
-micro avg                                               52.65      52.65      52.65     145878
-macro avg                                               57.96      52.65      43.18     145878
-weighted avg                                            57.96      52.65      43.18     145878
-
--------------------
-68188.00 64321.00
-4751.00 8618.00
-[INFO] - Epoch 9, step 52549: val_loss was not in top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.22      94.83      96.50   15591717
-! (label_id: 1)                                         24.08      39.63      29.96     130786
-, (label_id: 2)                                         39.10      39.33      39.21     482370
-- (label_id: 3)                                         23.57      39.68      29.58      44598
-. (label_id: 4)                                         42.09      65.21      51.16     923093
-: (label_id: 5)                                          0.00       0.00       0.00       2420
-; (label_id: 6)                                          0.00       0.00       0.00        948
-? (label_id: 7)                                         59.51      41.49      48.89     244470
-— (label_id: 8)                                          0.00       0.00       0.00         81
-… (label_id: 9)                                         23.73      21.11      22.34      80366
--------------------
-micro avg                                               90.08      90.08      90.08   17500848
-macro avg                                               31.03      34.13      31.76   17500848
-weighted avg                                            91.99      90.08      90.83   17500848
-
--------------------
-14785707.00 7179.00 98657.00 11323.00 103714.00 1225.00 158.00 21469.00 10.00 23838.00
-27076.00 51833.00 37088.00 2082.00 77749.00 26.00 87.00 13401.00  5.00 5937.00
-134239.00 19324.00 189708.00 1697.00 109257.00 79.00 56.00 20986.00  7.00 9817.00
-48353.00 829.00 2102.00 17698.00 4420.00 342.00  8.00 574.00  1.00 756.00
-541104.00 46631.00 129957.00 5125.00 601949.00 696.00 612.00 82816.00 36.00 21160.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-32932.00 3787.00 12230.00 539.00 17582.00 25.00 22.00 101439.00  9.00 1893.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-22306.00 1203.00 12628.00 6134.00 8422.00 27.00  5.00 3785.00 13.00 16965.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         69.41      80.60      74.59      72939
-1 (label_id: 1)                                         76.87      64.48      70.13      72939
--------------------
-micro avg                                               72.54      72.54      72.54     145878
-macro avg                                               73.14      72.54      72.36     145878
-weighted avg                                            73.14      72.54      72.36     145878
-
--------------------
-58792.00 25911.00
-14147.00 47028.00
-[INFO] - Epoch 10, global step 57804: val_loss reached 1.42583 (best 1.42583), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=1.43-epoch=10.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.35      96.92      97.14   15594830
-! (label_id: 1)                                         25.74      37.59      30.55     131420
-, (label_id: 2)                                         40.87      40.88      40.87     481980
-- (label_id: 3)                                         33.55      29.31      31.29      44213
-. (label_id: 4)                                         50.05      56.15      52.93     923128
-: (label_id: 5)                                          0.00       0.00       0.00       2372
-; (label_id: 6)                                          0.00       0.00       0.00        961
-? (label_id: 7)                                         59.16      43.67      50.25     244547
-— (label_id: 8)                                          0.00       0.00       0.00        104
-… (label_id: 9)                                         30.28      18.55      23.01      80900
--------------------
-micro avg                                               91.49      91.49      91.49   17504456
-macro avg                                               33.70      32.31      32.60   17504456
-weighted avg                                            91.74      91.49      91.57   17504456
-
--------------------
-15115059.00 12216.00 141023.00 17437.00 173399.00 1666.00 288.00 34646.00 16.00 30361.00
-18940.00 49396.00 30708.00 1786.00 73161.00 30.00 71.00 12294.00  5.00 5543.00
-99964.00 23294.00 197010.00 1884.00 125870.00 66.00 62.00 23236.00 14.00 10679.00
-20270.00 578.00 1102.00 12960.00 2738.00 91.00  3.00 359.00  1.00 525.00
-302097.00 40388.00 88944.00 3818.00 518342.00 479.00 503.00 64641.00 39.00 16385.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-28403.00 4694.00 13696.00 682.00 23771.00 29.00 31.00 106791.00 16.00 2399.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-10097.00 854.00 9497.00 5646.00 5847.00 11.00  3.00 2580.00 13.00 15008.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         41.85      29.38      34.53      72939
-1 (label_id: 1)                                         45.59      59.17      51.50      72939
--------------------
-micro avg                                               44.28      44.28      44.28     145878
-macro avg                                               43.72      44.28      43.01     145878
-weighted avg                                            43.72      44.28      43.01     145878
-
--------------------
-21432.00 29781.00
-51507.00 43158.00
-[INFO] - Epoch 11, global step 63059: val_loss reached 2.41300 (best 1.42583), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=2.41-epoch=11.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.07      97.32      97.19   15591873
-! (label_id: 1)                                         26.18      38.00      31.00     131789
-, (label_id: 2)                                         42.82      39.03      40.84     481674
-- (label_id: 3)                                         32.45      30.83      31.62      44417
-. (label_id: 4)                                         51.81      53.46      52.62     923721
-: (label_id: 5)                                          0.00       0.00       0.00       2422
-; (label_id: 6)                                          0.00       0.00       0.00       1006
-? (label_id: 7)                                         57.38      45.38      50.68     243807
-— (label_id: 8)                                          0.00       0.00       0.00         61
-… (label_id: 9)                                         29.65      19.02      23.18      80523
--------------------
-micro avg                                               91.68      91.68      91.68   17501292
-macro avg                                               33.74      32.30      32.71   17501292
-weighted avg                                            91.61      91.68      91.62   17501292
-
--------------------
-15173509.00 14387.00 154434.00 17388.00 199049.00 1721.00 381.00 38658.00 13.00 31666.00
-17137.00 50083.00 30642.00 1569.00 74329.00 20.00 56.00 12095.00  4.00 5344.00
-79851.00 21766.00 188006.00 1675.00 117410.00 58.00 68.00 20739.00  5.00 9469.00
-22355.00 678.00 1373.00 13693.00 3044.00 105.00  2.00 411.00  0.00 541.00
-260179.00 38582.00 81747.00 3604.00 493808.00 458.00 458.00 58609.00 27.00 15591.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-29082.00 5211.00 14735.00 714.00 29776.00 37.00 36.00 110642.00  6.00 2593.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-9760.00 1082.00 10737.00 5774.00 6305.00 23.00  5.00 2653.00  6.00 15319.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         86.91      21.94      35.03      72939
-1 (label_id: 1)                                         55.33      96.70      70.39      72939
--------------------
-micro avg                                               59.32      59.32      59.32     145878
-macro avg                                               71.12      59.32      52.71     145878
-weighted avg                                            71.12      59.32      52.71     145878
-
--------------------
-16000.00 2409.00
-56939.00 70530.00
-[INFO] - Epoch 12, global step 68314: val_loss reached 1.46759 (best 1.42583), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=1.47-epoch=12.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.34      97.00      97.17   15591908
-! (label_id: 1)                                         25.47      39.27      30.90     130685
-, (label_id: 2)                                         42.33      39.47      40.85     481540
-- (label_id: 3)                                         31.44      33.03      32.21      44397
-. (label_id: 4)                                         50.34      55.64      52.86     922617
-: (label_id: 5)                                          0.00       0.00       0.00       2534
-; (label_id: 6)                                          0.00       0.00       0.00        930
-? (label_id: 7)                                         57.08      45.35      50.54     243862
-— (label_id: 8)                                          0.00       0.00       0.00         65
-… (label_id: 9)                                         29.46      18.87      23.00      80856
--------------------
-micro avg                                               91.54      91.54      91.54   17499394
-macro avg                                               33.35      32.86      32.75   17499394
-weighted avg                                            91.75      91.54      91.61   17499394
-
--------------------
-15123679.00 12214.00 143310.00 16008.00 175119.00 1727.00 304.00 34081.00 14.00 30410.00
-19346.00 51319.00 31852.00 1697.00 78658.00 25.00 57.00 12781.00  4.00 5720.00
-85662.00 21875.00 190061.00 1723.00 118315.00 75.00 72.00 21412.00  5.00 9816.00
-25730.00 646.00 1407.00 14664.00 3050.00 126.00  3.00 428.00  0.00 594.00
-295302.00 38618.00 89306.00 3824.00 513363.00 517.00 469.00 61994.00 24.00 16457.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-31992.00 4982.00 14835.00 735.00 27929.00 40.00 24.00 110587.00 10.00 2603.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-10197.00 1031.00 10769.00 5746.00 6183.00 24.00  1.00 2579.00  8.00 15256.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         71.94       2.45       4.73      72939
-1 (label_id: 1)                                         50.38      99.05      66.79      72939
--------------------
-micro avg                                               50.75      50.75      50.75     145878
-macro avg                                               61.16      50.75      35.76     145878
-weighted avg                                            61.16      50.75      35.76     145878
-
--------------------
-1784.00 696.00
-71155.00 72243.00
-[INFO] - Epoch 13, step 73569: val_loss was not in top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.09      97.36      97.22   15597226
-! (label_id: 1)                                         25.63      39.75      31.16     132590
-, (label_id: 2)                                         42.62      39.40      40.95     481644
-- (label_id: 3)                                         34.03      30.76      32.31      44322
-. (label_id: 4)                                         51.84      53.45      52.63     923257
-: (label_id: 5)                                          0.00       0.00       0.00       2342
-; (label_id: 6)                                          0.00       0.00       0.00        985
-? (label_id: 7)                                         60.32      43.10      50.28     244099
-— (label_id: 8)                                          0.00       0.00       0.00         88
-… (label_id: 9)                                         30.58      19.02      23.45      80482
--------------------
-micro avg                                               91.71      91.71      91.71   17507036
-macro avg                                               34.21      32.28      32.80   17507036
-weighted avg                                            91.67      91.71      91.65   17507036
-
--------------------
-15184971.00 14014.00 154132.00 17469.00 196410.00 1641.00 357.00 39030.00 16.00 31639.00
-18872.00 52702.00 31873.00 1745.00 81378.00 29.00 62.00 13343.00  9.00 5620.00
-78356.00 22507.00 189766.00 1729.00 120592.00 64.00 67.00 22358.00  9.00 9828.00
-21010.00 555.00 1210.00 13634.00 2673.00 86.00  3.00 371.00  3.00 525.00
-258970.00 37317.00 81280.00 3439.00 493464.00 478.00 465.00 61116.00 29.00 15341.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-25956.00 4505.00 13012.00 614.00 22839.00 26.00 27.00 105203.00 10.00 2219.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-9091.00 990.00 10371.00 5692.00 5901.00 18.00  4.00 2678.00 12.00 15310.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         63.84       1.68       3.27      72939
-1 (label_id: 1)                                         50.18      99.05      66.62      72939
--------------------
-micro avg                                               50.36      50.36      50.36     145878
-macro avg                                               57.01      50.36      34.94     145878
-weighted avg                                            57.01      50.36      34.94     145878
-
--------------------
-1225.00 694.00
-71714.00 72245.00
-[INFO] - Epoch 14, step 78824: val_loss was not in top 3
-[INFO] - Saving latest checkpoint...
-[INFO] - Optimizer config = AdamW (
-Parameter Group 0
-    amsgrad: False
-    betas: (0.9, 0.999)
-    eps: 1e-08
-    lr: 1e-05
-    weight_decay: 0.0
-)
-[INFO] - Scheduler "<core.optim.lr_scheduler.CosineAnnealing object at 0x7f986558b6a0>" 
-will be used during training (effective maximum steps = 78825) - 
-Parameters : 
-(warmup_steps: null
-warmup_ratio: 0.1
-min_lr: 1.0e-10
-last_epoch: -1
-max_steps: 78825
-)
-[INFO] - 
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 108 M 
-1 | punct_classifier    | TokenClassifier      | 7.7 K 
-2 | domain_classifier   | SequenceClassifier   | 3.1 K 
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-7.1 M     Trainable params
-101 M     Non-trainable params
-108 M     Total params
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          96.71      97.78      97.24      17162
-! (label_id: 1)                                         16.79      37.93      23.28         58
-, (label_id: 2)                                         48.28      34.21      40.04        532
-- (label_id: 3)                                         27.42      62.96      38.20         27
-. (label_id: 4)                                         58.51      54.86      56.63       1028
-: (label_id: 5)                                          0.00       0.00       0.00          0
-; (label_id: 6)                                          0.00       0.00       0.00          4
-? (label_id: 7)                                         67.29      49.66      57.14        145
-— (label_id: 8)                                          0.00       0.00       0.00          0
-… (label_id: 9)                                         48.72      25.33      33.33         75
--------------------
-micro avg                                               92.78      92.78      92.78      19031
-macro avg                                               45.46      45.34      43.23      19031
-weighted avg                                            92.52      92.78      92.56      19031
-
--------------------
-16781.00 13.00 226.00 10.00 266.00  0.00  1.00 24.00  0.00 30.00
-18.00 22.00 18.00  0.00 68.00  0.00  0.00  3.00  0.00  2.00
-60.00  6.00 182.00  0.00 110.00  0.00  0.00 12.00  0.00  7.00
-40.00  0.00  2.00 17.00  2.00  0.00  0.00  0.00  0.00  1.00
-242.00 16.00 90.00  0.00 564.00  0.00  3.00 33.00  0.00 16.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-17.00  1.00  6.00  0.00 11.00  0.00  0.00 72.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 4.00  0.00  8.00  0.00  7.00  0.00  0.00  1.00  0.00 19.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         50.00       1.25       2.44         80
-1 (label_id: 1)                                         50.00      98.75      66.39         80
--------------------
-micro avg                                               50.00      50.00      50.00        160
-macro avg                                               50.00      50.00      34.41        160
-weighted avg                                            50.00      50.00      34.41        160
-
--------------------
- 1.00  1.00
-79.00 79.00
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.39      97.17      97.28   15595410
-! (label_id: 1)                                         28.19      34.91      31.19     132074
-, (label_id: 2)                                         41.36      43.02      42.17     481066
-- (label_id: 3)                                         33.96      33.33      33.64      44642
-. (label_id: 4)                                         52.15      56.35      54.17     922110
-: (label_id: 5)                                          0.00       0.00       0.00       2427
-; (label_id: 6)                                          0.00       0.00       0.00        951
-? (label_id: 7)                                         60.50      47.70      53.34     244079
-— (label_id: 8)                                          0.00       0.00       0.00         81
-… (label_id: 9)                                         31.48      18.31      23.15      80692
--------------------
-micro avg                                               91.82      91.82      91.82   17503532
-macro avg                                               34.50      33.08      33.49   17503532
-weighted avg                                            91.94      91.82      91.86   17503532
-
--------------------
-15153604.00 13605.00 143075.00 16589.00 167391.00 1700.00 303.00 33023.00 15.00 30861.00
-12592.00 46106.00 24459.00 1227.00 64687.00 23.00 49.00 9648.00  5.00 4766.00
-89886.00 26855.00 206948.00 2016.00 138223.00 80.00 78.00 24988.00  9.00 11297.00
-22160.00 907.00 1680.00 14879.00 3008.00 91.00  3.00 449.00  2.00 638.00
-279142.00 38770.00 81261.00 3511.00 519575.00 468.00 483.00 57239.00 31.00 15764.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-29800.00 5008.00 13998.00 771.00 23761.00 33.00 32.00 116430.00 11.00 2595.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-8226.00 823.00 9645.00 5649.00 5465.00 32.00  3.00 2302.00  8.00 14771.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                          5.26       0.45       0.82      72939
-1 (label_id: 1)                                         48.02      91.96      63.09      72939
--------------------
-micro avg                                               46.20      46.20      46.20     145878
-macro avg                                               26.64      46.20      31.96     145878
-weighted avg                                            26.64      46.20      31.96     145878
-
--------------------
-326.00 5866.00
-72613.00 67073.00
-[INFO] - Epoch 0, step 84079: val_loss was not in top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.92      95.26      96.57   15594360
-! (label_id: 1)                                         17.91      35.19      23.74     131487
-, (label_id: 2)                                         28.65      42.70      34.29     481266
-- (label_id: 3)                                         14.54      30.94      19.78      44079
-. (label_id: 4)                                         55.51      53.23      54.34     922224
-: (label_id: 5)                                          0.00       0.00       0.00       2409
-; (label_id: 6)                                          0.00       0.00       0.00        979
-? (label_id: 7)                                         43.61      54.75      48.55     244473
-— (label_id: 8)                                          0.00       0.00       0.00         80
-… (label_id: 9)                                         20.27      17.51      18.79      80827
--------------------
-micro avg                                               90.05      90.05      90.05   17502184
-macro avg                                               27.84      32.96      29.61   17502184
-weighted avg                                            91.83      90.05      90.85   17502184
-
--------------------
-14855928.00 10170.00 111950.00 14205.00 130062.00 1066.00 295.00 22796.00  7.00 25164.00
-82603.00 46272.00 34372.00 2186.00 76741.00 25.00 33.00 10866.00 15.00 5191.00
-273561.00 30550.00 205493.00 3107.00 157176.00 296.00 66.00 31834.00  8.00 15231.00
-70375.00 707.00 2806.00 13640.00 4828.00 314.00  8.00 610.00  1.00 535.00
-205048.00 33726.00 94648.00 3428.00 490863.00 482.00 529.00 39792.00 25.00 15784.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-84788.00 8065.00 23551.00 1335.00 50355.00 186.00 28.00 133843.00 14.00 4769.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-22057.00 1997.00 8446.00 6178.00 12199.00 40.00 20.00 4732.00 10.00 14153.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                          7.53       7.44       7.49      72939
-1 (label_id: 1)                                          8.60       8.71       8.66      72939
--------------------
-micro avg                                                8.08       8.08       8.08     145878
-macro avg                                                8.07       8.08       8.07     145878
-weighted avg                                             8.07       8.08       8.07     145878
-
--------------------
-5425.00 66584.00
-67514.00 6355.00
-[INFO] - Epoch 1, step 89334: val_loss was not in top 3
-                     67.52      64.69      66.08     922389
-: (label_id: 5)                                         92.37      36.75      52.58       2305
-; (label_id: 6)                                          0.00       0.00       0.00        978
-? (label_id: 7)                                         68.69      63.73      66.12     244399
-— (label_id: 8)                                          0.00       0.00       0.00         85
-… (label_id: 9)                                         27.52      23.50      25.35      80604
--------------------
-micro avg                                               93.89      93.89      93.89   17505722
-macro avg                                               48.11      43.97      44.55   17505722
-weighted avg                                            94.35      93.89      94.08   17505722
-
--------------------
-15298322.00 4673.00 64320.00 10071.00 71003.00 564.00 200.00 16255.00  6.00 20530.00
-14786.00 63450.00 31790.00 1407.00 84046.00 29.00 48.00 11254.00  8.00 5944.00
-112986.00 18924.00 281518.00 2658.00 131486.00 274.00 132.00 20680.00 11.00 15612.00
-15426.00 660.00 1435.00 20503.00 2214.00 98.00  7.00 390.00  2.00 1773.00
-120182.00 37391.00 73297.00 3752.00 596722.00 416.00 549.00 36360.00 29.00 15042.00
-59.00  0.00  4.00  7.00  0.00 847.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-21798.00 5194.00 14541.00 744.00 25888.00 26.00 29.00 155764.00 14.00 2765.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-14182.00 1525.00 14259.00 5098.00 11030.00 51.00 13.00 3696.00 15.00 18938.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         96.78      91.45      94.04      72939
-1 (label_id: 1)                                         91.89      96.96      94.36      72939
--------------------
-micro avg                                               94.20      94.20      94.20     145878
-macro avg                                               94.34      94.20      94.20     145878
-weighted avg                                            94.34      94.20      94.20     145878
-
--------------------
-66701.00 2218.00
-6238.00 70721.00
-[INFO] - Epoch 2, global step 94589: val_loss reached 0.09552 (best 0.09403), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.10-epoch=2.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.83      98.14      98.48   15593759
-! (label_id: 1)                                         30.27      47.67      37.03     131176
-, (label_id: 2)                                         48.68      59.73      53.64     481613
-- (label_id: 3)                                         50.15      46.57      48.29      44269
-. (label_id: 4)                                         68.32      65.12      66.68     922442
-: (label_id: 5)                                         93.77      38.23      54.31       2480
-; (label_id: 6)                                          0.00       0.00       0.00        986
-? (label_id: 7)                                         69.01      64.77      66.83     243336
-— (label_id: 8)                                          0.00       0.00       0.00         81
-… (label_id: 9)                                         27.92      23.88      25.74      80532
--------------------
-micro avg                                               94.01      94.01      94.01   17500674
-macro avg                                               48.69      44.41      45.10   17500674
-weighted avg                                            94.46      94.01      94.20   17500674
-
--------------------
-15303085.00 4770.00 61451.00 9982.00 68400.00 605.00 238.00 15734.00  8.00 20290.00
-14036.00 62538.00 30156.00 1383.00 81967.00 25.00 43.00 10712.00  5.00 5744.00
-112463.00 19206.00 287673.00 2758.00 131998.00 307.00 142.00 20471.00 11.00 15960.00
-14428.00 571.00 1263.00 20614.00 2002.00 91.00  3.00 351.00  4.00 1780.00
-114746.00 37274.00 72307.00 3730.00 600666.00 420.00 520.00 34742.00 27.00 14704.00
-33.00  0.00 20.00 10.00  0.00 948.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-21334.00 5195.00 14445.00 742.00 26155.00 29.00 28.00 157616.00 13.00 2823.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-13634.00 1622.00 14298.00 5050.00 11254.00 55.00 12.00 3710.00 13.00 19231.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         96.43      91.93      94.13      72939
-1 (label_id: 1)                                         92.29      96.59      94.39      72939
--------------------
-micro avg                                               94.26      94.26      94.26     145878
-macro avg                                               94.36      94.26      94.26     145878
-weighted avg                                            94.36      94.26      94.26     145878
-
--------------------
-67056.00 2486.00
-5883.00 70453.00
-[INFO] - Epoch 3, global step 99844: val_loss reached 0.08798 (best 0.08798), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.09-epoch=3.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.86      98.16      98.51   15589848
-! (label_id: 1)                                         30.75      48.52      37.65     132067
-, (label_id: 2)                                         49.25      60.06      54.12     480971
-- (label_id: 3)                                         51.22      47.23      49.15      44614
-. (label_id: 4)                                         68.64      65.71      67.14     922579
-: (label_id: 5)                                         86.91      43.99      58.41       2446
-; (label_id: 6)                                          0.00       0.00       0.00        960
-? (label_id: 7)                                         69.69      65.06      67.30     244526
-— (label_id: 8)                                          0.00       0.00       0.00         69
-… (label_id: 9)                                         27.90      24.39      26.03      81023
--------------------
-micro avg                                               94.08      94.08      94.08   17499104
-macro avg                                               48.32      45.31      45.83   17499104
-weighted avg                                            94.53      94.08      94.27   17499104
-
--------------------
-15303627.00 4606.00 58844.00 9743.00 66384.00 437.00 193.00 15429.00  4.00 20117.00
-14128.00 64084.00 30432.00 1460.00 81802.00 31.00 47.00 10749.00  5.00 5632.00
-110447.00 18570.00 288887.00 2805.00 129310.00 303.00 135.00 20102.00 13.00 16042.00
-14074.00 498.00 1231.00 21071.00 1905.00 74.00  6.00 335.00  2.00 1940.00
-112204.00 37458.00 72958.00 3677.00 606246.00 448.00 531.00 34854.00 23.00 14806.00
-37.00  0.00 102.00 20.00  2.00 1076.00  0.00  0.00  0.00  1.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-21189.00 5122.00 14037.00 763.00 25308.00 30.00 35.00 159098.00  7.00 2721.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-14142.00 1729.00 14480.00 5075.00 11622.00 47.00 13.00 3959.00 15.00 19764.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         96.39      92.17      94.23      72939
-1 (label_id: 1)                                         92.50      96.55      94.48      72939
--------------------
-micro avg                                               94.36      94.36      94.36     145878
-macro avg                                               94.45      94.36      94.36     145878
-weighted avg                                            94.45      94.36      94.36     145878
-
--------------------
-67230.00 2517.00
-5709.00 70422.00
-[INFO] - Epoch 4, global step 105099: val_loss reached 0.07594 (best 0.07594), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.08-epoch=4.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.89      98.19      98.54   15593568
-! (label_id: 1)                                         30.76      48.77      37.72     131858
-, (label_id: 2)                                         49.62      60.58      54.55     480872
-- (label_id: 3)                                         51.19      47.49      49.27      44566
-. (label_id: 4)                                         69.00      65.91      67.42     923462
-: (label_id: 5)                                         92.00      44.42      59.91       2382
-; (label_id: 6)                                          0.00       0.00       0.00       1005
-? (label_id: 7)                                         69.79      65.36      67.50     243779
-— (label_id: 8)                                          0.00       0.00       0.00         77
-… (label_id: 9)                                         27.99      24.33      26.03      80824
--------------------
-micro avg                                               94.14      94.14      94.14   17502392
-macro avg                                               48.92      45.51      46.10   17502392
-weighted avg                                            94.59      94.14      94.33   17502392
-
--------------------
-15311796.00 4537.00 56907.00 9548.00 64422.00 408.00 225.00 15186.00  9.00 19934.00
-13944.00 64302.00 30021.00 1375.00 82795.00 22.00 45.00 10657.00  4.00 5907.00
-109404.00 18757.00 291308.00 2874.00 128459.00 281.00 139.00 20059.00 13.00 15802.00
-14082.00 552.00 1263.00 21166.00 1936.00 80.00  6.00 363.00  2.00 1896.00
-110100.00 36853.00 72578.00 3699.00 608661.00 450.00 550.00 34334.00 30.00 14919.00
-34.00  0.00 41.00 13.00  3.00 1058.00  0.00  0.00  0.00  1.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-20522.00 5150.00 14226.00 783.00 25533.00 35.00 26.00 159334.00 12.00 2698.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-13686.00 1707.00 14528.00 5108.00 11653.00 48.00 14.00 3846.00  7.00 19667.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         96.51      92.42      94.42      72939
-1 (label_id: 1)                                         92.73      96.66      94.65      72939
--------------------
-micro avg                                               94.54      94.54      94.54     145878
-macro avg                                               94.62      94.54      94.53     145878
-weighted avg                                            94.62      94.54      94.53     145878
-
--------------------
-67408.00 2438.00
-5531.00 70501.00
-[INFO] - Epoch 5, global step 110354: val_loss reached 0.07319 (best 0.07319), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.07-epoch=5.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.91      98.22      98.57   15594783
-! (label_id: 1)                                         30.76      48.51      37.65     130591
-, (label_id: 2)                                         49.92      61.25      55.01     481352
-- (label_id: 3)                                         51.90      48.17      49.97      44046
-. (label_id: 4)                                         69.42      66.02      67.68     921748
-: (label_id: 5)                                         87.88      45.31      59.79       2368
-; (label_id: 6)                                          0.00       0.00       0.00        969
-? (label_id: 7)                                         70.27      65.67      67.89     244359
-— (label_id: 8)                                          0.00       0.00       0.00         77
-… (label_id: 9)                                         28.02      24.71      26.26      80560
--------------------
-micro avg                                               94.21      94.21      94.21   17500852
-macro avg                                               48.71      45.79      46.28   17500852
-weighted avg                                            94.65      94.21      94.39   17500852
-
--------------------
-15317660.00 4380.00 55955.00 9354.00 63318.00 398.00 190.00 15031.00  5.00 19747.00
-13987.00 63353.00 29483.00 1344.00 81640.00 24.00 40.00 10358.00  3.00 5736.00
-108617.00 18715.00 294849.00 2685.00 129060.00 253.00 128.00 20353.00  7.00 15992.00
-13603.00 502.00 1330.00 21217.00 1937.00 74.00  5.00 364.00  2.00 1847.00
-106703.00 36774.00 71574.00 3623.00 608548.00 448.00 568.00 33784.00 35.00 14622.00
-59.00  0.00 66.00 21.00  1.00 1073.00  0.00  0.00  0.00  1.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-20223.00 5137.00 13689.00 736.00 25328.00 39.00 27.00 160469.00 13.00 2709.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-13931.00 1730.00 14406.00 5066.00 11916.00 59.00 11.00 4000.00 12.00 19906.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         96.67      92.36      94.47      72939
-1 (label_id: 1)                                         92.69      96.82      94.71      72939
--------------------
-micro avg                                               94.59      94.59      94.59     145878
-macro avg                                               94.68      94.59      94.59     145878
-weighted avg                                            94.68      94.59      94.59     145878
-
--------------------
-67368.00 2322.00
-5571.00 70617.00
-[INFO] - Epoch 6, global step 115609: val_loss reached 0.07592 (best 0.07319), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.08-epoch=6.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.90      98.27      98.58   15588559
-! (label_id: 1)                                         31.29      48.45      38.02     131502
-, (label_id: 2)                                         50.48      60.93      55.21     481656
-- (label_id: 3)                                         51.77      48.60      50.14      44145
-. (label_id: 4)                                         69.51      66.44      67.94     922262
-: (label_id: 5)                                         86.75      45.36      59.57       2425
-; (label_id: 6)                                          0.00       0.00       0.00        967
-? (label_id: 7)                                         70.23      65.94      68.02     244604
-— (label_id: 8)                                          0.00       0.00       0.00         55
-… (label_id: 9)                                         28.76      24.48      26.45      80592
--------------------
-micro avg                                               94.26      94.26      94.26   17496768
-macro avg                                               48.77      45.85      46.39   17496768
-weighted avg                                            94.66      94.26      94.42   17496768
-
--------------------
-15318406.00 4602.00 56883.00 9374.00 64498.00 401.00 203.00 15140.00  5.00 19749.00
-13652.00 63710.00 28641.00 1301.00 80418.00 26.00 45.00 10137.00  8.00 5700.00
-104301.00 18679.00 293467.00 2719.00 125746.00 288.00 127.00 19964.00  7.00 16056.00
-13980.00 503.00 1241.00 21456.00 1878.00 62.00  1.00 389.00  4.00 1928.00
-105129.00 37133.00 73203.00 3651.00 612782.00 453.00 551.00 33916.00 13.00 14709.00
-88.00  0.00 62.00 16.00  1.00 1100.00  0.00  0.00  0.00  1.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-19908.00 5220.00 14193.00 714.00 25537.00 35.00 30.00 161293.00  9.00 2720.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-13095.00 1655.00 13966.00 4914.00 11402.00 60.00 10.00 3765.00  9.00 19729.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         96.67      92.47      94.52      72939
-1 (label_id: 1)                                         92.79      96.81      94.76      72939
--------------------
-micro avg                                               94.64      94.64      94.64     145878
-macro avg                                               94.73      94.64      94.64     145878
-weighted avg                                            94.73      94.64      94.64     145878
-
--------------------
-67449.00 2324.00
-5490.00 70615.00
-[INFO] - Epoch 7, global step 120864: val_loss reached 0.07287 (best 0.07287), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.07-epoch=7.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.91      98.29      98.60   15595430
-! (label_id: 1)                                         31.51      47.61      37.93     131969
-, (label_id: 2)                                         50.55      61.10      55.33     481215
-- (label_id: 3)                                         52.20      48.19      50.11      43898
-. (label_id: 4)                                         69.77      66.68      68.19     923479
-: (label_id: 5)                                         80.38      52.04      63.18       2496
-; (label_id: 6)                                          0.00       0.00       0.00        924
-? (label_id: 7)                                         70.41      66.11      68.20     245290
-— (label_id: 8)                                          0.00       0.00       0.00         89
-… (label_id: 9)                                         28.21      24.68      26.33      81055
--------------------
-micro avg                                               94.29      94.29      94.29   17505844
-macro avg                                               48.19      46.47      46.79   17505844
-weighted avg                                            94.68      94.29      94.46   17505844
-
--------------------
-15329501.00 4701.00 56277.00 9384.00 63640.00 351.00 180.00 14908.00  3.00 19960.00
-13215.00 62837.00 28301.00 1275.00 78245.00 22.00 40.00 9797.00 12.00 5650.00
-103006.00 19029.00 294003.00 2717.00 126463.00 245.00 114.00 19981.00 12.00 16015.00
-13280.00 519.00 1343.00 21154.00 1871.00 51.00  5.00 380.00  4.00 1920.00
-102505.00 37849.00 72926.00 3614.00 615774.00 439.00 545.00 34140.00 27.00 14769.00
-128.00  0.00 154.00 28.00  5.00 1299.00  0.00  0.00  0.00  2.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-19993.00 5204.00 13919.00 738.00 25482.00 31.00 22.00 162167.00 15.00 2734.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-13802.00 1830.00 14292.00 4988.00 11999.00 58.00 18.00 3917.00 16.00 20005.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         96.84      92.67      94.71      72939
-1 (label_id: 1)                                         92.97      96.97      94.93      72939
--------------------
-micro avg                                               94.82      94.82      94.82     145878
-macro avg                                               94.90      94.82      94.82     145878
-weighted avg                                            94.90      94.82      94.82     145878
-
--------------------
-67593.00 2209.00
-5346.00 70730.00
-[INFO] - Epoch 8, global step 126119: val_loss reached 0.06787 (best 0.06787), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.07-epoch=8.ckpt" as top 3
