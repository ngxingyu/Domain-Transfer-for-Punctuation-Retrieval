commit hash: 0c68d6fc9a2d266bb3dbc9ab2483228cbc42e806
diff --git a/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/events.out.tfevents.1613128521.intern-instance.30071.0 b/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/events.out.tfevents.1613128521.intern-instance.30071.0
index 0fce37c..dcb35a4 100644
Binary files a/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/events.out.tfevents.1613128521.intern-instance.30071.0 and b/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/events.out.tfevents.1613128521.intern-instance.30071.0 differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/lightning_logs.txt
index 114baea..f7ccecc 100644
--- a/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/lightning_logs.txt
+++ b/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/lightning_logs.txt
@@ -62,3 +62,9 @@ Epoch 9, step 131374: val_loss was not in top 3
 Epoch 10, step 136629: val_loss was not in top 3
 Epoch 11, step 141884: val_loss was not in top 3
 Epoch 12, step 147139: val_loss was not in top 3
+Epoch 13, step 152394: val_loss was not in top 3
+Epoch 14, step 157649: val_loss was not in top 3
+GPU available: True, used: True
+TPU available: None, using: 0 TPU cores
+Using environment variable NODE_RANK for node rank (0).
+LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
diff --git a/experiment/Inference.ipynb b/experiment/Inference.ipynb
index 0c9d9c1..4f3ed55 100644
--- a/experiment/Inference.ipynb
+++ b/experiment/Inference.ipynb
@@ -2,7 +2,7 @@
  "cells": [
   {
    "cell_type": "code",
-   "execution_count": 26,
+   "execution_count": 2,
    "id": "modern-amplifier",
    "metadata": {},
    "outputs": [
@@ -10,35 +10,18 @@
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "/home/nxingyu/project/experiment\r\n"
+      "/home/nxingyu2/project/experiment\r\n"
      ]
     },
     {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "16:37:06.52 LOG:\n",
-      "16:37:06.52 .... <argument> = 0\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "shuffling <data.punctuation_dataset_multi.PunctuationDomainDataset object at 0x7fb43cb37790>\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "16:37:06.64 LOG:\n",
-      "16:37:06.64 .... <argument> = '1st 12 encoder layers of transformer frozen'\n",
-      "GPU available: True, used: True\n",
-      "TPU available: None, using: 0 TPU cores\n",
-      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
-      "Using native 16bit precision.\n"
-     ]
+     "data": {
+      "text/plain": [
+       "{0: '', 1: '!', 2: ',', 3: '-', 4: '.', 5: ':', 6: '?', 7: '—'}"
+      ]
+     },
+     "execution_count": 2,
+     "metadata": {},
+     "output_type": "execute_result"
     }
    ],
    "source": [
@@ -80,218 +63,15 @@
     "cfg.model.punct_label_ids=OmegaConf.create(sorted(cfg.model.punct_label_ids))\n",
     "labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
     "ids_to_labels = {_[0]:_[1] for _ in enumerate(cfg.model.punct_label_ids)}\n",
-    "model = PunctuationDomainModel.load_from_checkpoint( #TEDend2021-02-11_07-57-33  # TEDstart2021-02-11_07-55-58\n",
-    "    checkpoint_path=f\"/home/nxingyu/project/Punctuation_with_Domain_discriminator/{folder}/checkpoints/Punctuation_with_Domain_discriminator-last.ckpt\")\n",
-    "trainer = pl.Trainer(**cfg.trainer)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 27,
-   "id": "classical-authentication",
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "{0: '', 1: '!', 2: ',', 3: '-', 4: '.', 5: ':', 6: ';', 7: '?', 8: '—', 9: '…'}"
-      ]
-     },
-     "execution_count": 27,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
+    "# model = PunctuationDomainModel.load_from_checkpoint( #TEDend2021-02-11_07-57-33  # TEDstart2021-02-11_07-55-58\n",
+    "#     checkpoint_path=f\"/home/nxingyu/project/Punctuation_with_Domain_discriminator/{folder}/checkpoints/Punctuation_with_Domain_discriminator-last.ckpt\")\n",
+    "# trainer = pl.Trainer(**cfg.trainer)\n",
     "ids_to_labels"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 28,
-   "id": "competitive-dubai",
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "[' hooooooo                                                                                                                           ',\n",
-       " ' what can i do for you? today?                                                                                                                        ',\n",
-       " ' how are you.                                                                                                                            ',\n",
-       " ' in guadeloupe, or martinique, it also brin- gs into ques- tion, budgetary po- licy, because the europe, an union is after all, making a present of ecu1, 9 billion to three multinationals. where are the financial interests of the european union? firstly, development policy in africa? in any case, in the acp countries employ ment policy in madeira, the canaries, guadel- oupe, martinique and crete, regional pol icy, in the ultraperipheral areas— human rights, which mr barthet mayer mentioned earlier, since dollar bananas are after all, slavery, bananas, the product of human exploitation by three multinationals, payments of ec ',\n",
-       " ' u 50 per mon, th! instead of ecu 50 per day.                                                                                                                   ',\n",
-       " ' plans for this weekend include turning wine into water. the small white buoys marked the location of hundreds of crab pots. he said he was not there yesterday, however many people saw him there today. arrived. with a crash of my car through the garage door. the lyrics of the song sounded like fingernails on a chalkboard. the guineafowl flies through the air with all the grace of a turtle. they ran around the corner to find that they had traveled back in time.                                      ',\n",
-       " ' good morning everyone. how have your weekends been? its a really great day. thank you.                                                                                                                ',\n",
-       " ' first of all, i too agree that tourism- related action must include employment training and education. as you know, after the european conference on tourism and employment, in luxemborg, we set up a high- level group whose mission was to examine how best tourism could contribute towards employment. the first stage.                                                                          ',\n",
-       " ' ummm, im not really sure. can you check with mr john instead? thanks!                                                                                                                 ']"
-      ]
-     },
-     "execution_count": 28,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "\n",
-    "queries = [\n",
-    "    'Hooooooo!',\n",
-    "    'what can i do for you today',\n",
-    "    'how are you',\n",
-    "    ' in guadeloupe or marti nique it also brin gs into ques tion budgetary po licy because the europe an union is after all making a present of ecu 1.9 billion to three multinationals where are the financial interests of the european union firstly development policy in africa in any case in the acp countries employ ment policy in madeira the canaries guadel oupe martinique and crete regional pol icy in the ultra-peripheral areas human rights which mr barthet mayer mentioned earlier since dollar bananas are after all slavery bana nas the product of human exploitation by three multinat ionals payments of ecu 50 per mon th instead of ecu 50 per day',\n",
-    "    '''Plans for this weekend include turning wine into water.\n",
-    "The small white buoys marked the location of hundreds of crab pots.\n",
-    "He said he was not there yesterday; however, many people saw him there.\n",
-    "Today arrived with a crash of my car through the garage door.\n",
-    "The lyrics of the song sounded like fingernails on a chalkboard.\n",
-    "The Guinea fowl flies through the air with all the grace of a turtle.\n",
-    "They ran around the corner to find that they had traveled back in time.''',\n",
-    "    'good morning everyone how have your weekends been its a really great day thank you',\n",
-    "    'first of all i too agree that tourism related action must include employment training and education as you know after the european conference on tourism and employment in luxemborg we set up a high-level group whose mission was to examine how best tourism could contribute towards employment the first stage',\n",
-    "    \"Ummm Im not really sure can you check with mr john instead thanks\"\n",
-    "]\n",
-    "inference_results = model.add_punctuation(queries)\n",
-    "inference_results"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 16,
-   "id": "dietary-violin",
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "[' hooooooo.                                                                                                                           ',\n",
-       " ' what can i do for you? today                                                                                                                        ',\n",
-       " ' how are you?                                                                                                                            ',\n",
-       " ' in guadeloupe or marti nique, it also brin gs into ques tion budgetary po- licy, because the europe, an union is after all, making a present of ecu 1, 9 billion to three multinationals. where are the financial interests of the european union? firstly, development policy in africa in any case, in the acp countries, employ ment policy in madeira, the canaries, guadel oupe, martinique and crete, regional pol- icy in the ultra- peripheral areas— human rights, which mr barthet mayer mentioned earlier, since dollar bananas are after all, slavery, bana nas the product of human exploitation by three multinat- ionals— payments of ec ',\n",
-       " ' u 50 per mon th, instead of ecu 50 per day.                                                                                                                   ',\n",
-       " ' plans for this weekend include turning wine into water. the small white buoys marked the location of hundreds of crab pots. he said he was not there yesterday. however many people saw him there today. arrived with a crash of my car through the garage door. the lyrics of the song sounded like fingernails on a chalkboard. the guinea fowl flies through the air with all the grace of a turtle. they ran around the corner to find that they had traveled back in time.                                      ',\n",
-       " ' good morning, everyone. how have your weekends been? its a really great day. thank you.                                                                                                                ',\n",
-       " ' first of all, i too agree that tourism- related action must include employment training and education. as you know, after the european conference on tourism and employment in luxemborg, we set up a high- level group, whose mission was to examine how best tourism could contribute towards employment. the first stage.                                                                          ',\n",
-       " ' ummm, im not really sure. can you check with mr? john instead? thanks.                                                                                                                 ']"
-      ]
-     },
-     "execution_count": 16,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "\n",
-    "queries = [\n",
-    "    'Hooooooo!',\n",
-    "    'what can i do for you today',\n",
-    "    'how are you',\n",
-    "    ' in guadeloupe or marti nique it also brin gs into ques tion budgetary po licy because the europe an union is after all making a present of ecu 1.9 billion to three multinationals where are the financial interests of the european union firstly development policy in africa in any case in the acp countries employ ment policy in madeira the canaries guadel oupe martinique and crete regional pol icy in the ultra-peripheral areas human rights which mr barthet mayer mentioned earlier since dollar bananas are after all slavery bana nas the product of human exploitation by three multinat ionals payments of ecu 50 per mon th instead of ecu 50 per day',\n",
-    "    '''Plans for this weekend include turning wine into water.\n",
-    "The small white buoys marked the location of hundreds of crab pots.\n",
-    "He said he was not there yesterday; however, many people saw him there.\n",
-    "Today arrived with a crash of my car through the garage door.\n",
-    "The lyrics of the song sounded like fingernails on a chalkboard.\n",
-    "The Guinea fowl flies through the air with all the grace of a turtle.\n",
-    "They ran around the corner to find that they had traveled back in time.''',\n",
-    "    'good morning everyone how have your weekends been its a really great day thank you',\n",
-    "    'first of all i too agree that tourism related action must include employment training and education as you know after the european conference on tourism and employment in luxemborg we set up a high-level group whose mission was to examine how best tourism could contribute towards employment the first stage',\n",
-    "    \"Ummm Im not really sure can you check with mr john instead thanks\"\n",
-    "]\n",
-    "inference_results = model.add_punctuation(queries)\n",
-    "inference_results"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 4,
-   "id": "hairy-proxy",
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "[' hooooooo.                                                                                                                           ',\n",
-       " ' what can i do for you? today                                                                                                                        ',\n",
-       " ' how are you?                                                                                                                            ',\n",
-       " ' in guadeloupe or marti nique, it also brin gs into ques tion budgetary po- licy because the europe, an union is after all, making a present of ecu 1, 9 billion to three multinationals. where are the financial interests of the european union? firstly, development policy in africa in any case, in the acp countries, employ ment policy in madeira, the canaries, guadel oupe, martinique and crete, regional pol- icy in the ultra- peripheral areas— human rights, which mr barthet mayer mentioned earlier, since dollar bananas are after all, slavery, bana nas the product of human exploitation by three multinat- ionals— payments of ec ',\n",
-       " ' u 50 per mon th, instead of ecu 50 per day.                                                                                                                   ',\n",
-       " ' plans for this weekend include turning wine into water. the small white buoys marked the location of hundreds of crab pots. he said he was not there yesterday, however many people saw him there today. arrived with a crash of my car through the garage door. the lyrics of the song sounded like fingernails on a chalkboard. the guinea fowl flies through the air with all the grace of a turtle. they ran around the corner to find that they had traveled back in time.                                      ',\n",
-       " ' good morning, everyone. how have your weekends been? its a really great day. thank you.                                                                                                                ',\n",
-       " ' first of all, i too agree that tourism- related action must include employment training and education. as you know, after the european conference on tourism and employment in luxemborg, we set up a high- level group, whose mission was to examine how best tourism could contribute towards employment. the first stage.                                                                          ']"
-      ]
-     },
-     "execution_count": 4,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "\n",
-    "queries = [\n",
-    "    'Hooooooo!',\n",
-    "    'what can i do for you today',\n",
-    "    'how are you',\n",
-    "    ' in guadeloupe or marti nique it also brin gs into ques tion budgetary po licy because the europe an union is after all making a present of ecu 1.9 billion to three multinationals where are the financial interests of the european union firstly development policy in africa in any case in the acp countries employ ment policy in madeira the canaries guadel oupe martinique and crete regional pol icy in the ultra-peripheral areas human rights which mr barthet mayer mentioned earlier since dollar bananas are after all slavery bana nas the product of human exploitation by three multinat ionals payments of ecu 50 per mon th instead of ecu 50 per day',\n",
-    "    '''Plans for this weekend include turning wine into water.\n",
-    "The small white buoys marked the location of hundreds of crab pots.\n",
-    "He said he was not there yesterday; however, many people saw him there.\n",
-    "Today arrived with a crash of my car through the garage door.\n",
-    "The lyrics of the song sounded like fingernails on a chalkboard.\n",
-    "The Guinea fowl flies through the air with all the grace of a turtle.\n",
-    "They ran around the corner to find that they had traveled back in time.''',\n",
-    "    'good morning everyone how have your weekends been its a really great day thank you',\n",
-    "    'first of all i too agree that tourism related action must include employment training and education as you know after the european conference on tourism and employment in luxemborg we set up a high-level group whose mission was to examine how best tourism could contribute towards employment the first stage'\n",
-    "]\n",
-    "inference_results = model.add_punctuation(queries)\n",
-    "inference_results"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 9,
-   "id": "residential-scene",
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "[' hooooooo,                                                                                                                           ',\n",
-       " ' what can i do for you today?                                                                                                                        ',\n",
-       " ' ? how are you?                                                                                                                            ',\n",
-       " ' firstly, development policy in africa. in any case, in the acp, countries, employment policy in madeira, the canaries— guadeloupe, martinique and crete, regional policy in the ultra- peripheral areas— human rights, which mr barthet mayer, mentioned earlier, since dollar bananas are after all, slavery. bananas, the product of human exploitation by three multinationals, payments of ecu 50 per month, instead of ecu 50 per day in guadeloupe or martinique. it also brings into question budgetary policy. because the european union is after all, making a present of ecu 1, 9 billion to three multinationals. where are the financial interests of the european union?         ',\n",
-       " ' plans for this weekend, include turning wine into water. the small white buoys marked the location of hundreds of crab pots. he said he was not there yesterday. however, many people saw him there. today. arrived with a crash of my car through the garage door. the lyrics of the song sounded like fingernails on a chalkboard. the guinea fowl flies through the air with all the grace of a turtle. they ran around the corner, to find that they had traveled back in time,                                      ',\n",
-       " ' good morning. everyone? how have your weekends been? its a really great day? thank you.                                                                                                                ',\n",
-       " ' first of all, i too agree that tourism- related action must include employment training and education. as you know, after the european conference on tourism and employment in luxemborg, we set up a high- level group, whose mission was to examine how best tourism could contribute towards employment— the first stage,                                                                          ']"
-      ]
-     },
-     "execution_count": 9,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "\n",
-    "queries = [\n",
-    "    'Hooooooo!',\n",
-    "    'what can i do for you today',\n",
-    "    'how are you',\n",
-    "    'firstly development policy in africa in any case in the acp countries employment policy in madeira the canaries guadeloupe martinique and crete regional policy in the ultra-peripheral areas human rights which mr barthet-mayer mentioned earlier since dollar bananas are after all slavery bananas the product of human exploitation by three multinationals payments of ecu 50 per month instead of ecu 50 per day in guadeloupe or martinique it also brings into question budgetary policy because the european union is after all making a present of ecu 1.9 billion to three multinationals where are the financial interests of the european union',\n",
-    "    '''Plans for this weekend include turning wine into water.\n",
-    "The small white buoys marked the location of hundreds of crab pots.\n",
-    "He said he was not there yesterday; however, many people saw him there.\n",
-    "Today arrived with a crash of my car through the garage door.\n",
-    "The lyrics of the song sounded like fingernails on a chalkboard.\n",
-    "The Guinea fowl flies through the air with all the grace of a turtle.\n",
-    "They ran around the corner to find that they had traveled back in time.''',\n",
-    "    'good morning everyone how have your weekends been its a really great day thank you',\n",
-    "    'first of all i too agree that tourism related action must include employment training and education as you know after the european conference on tourism and employment in luxemborg we set up a high-level group whose mission was to examine how best tourism could contribute towards employment the first stage'\n",
-    "]\n",
-    "inference_results = model.add_punctuation(queries)\n",
-    "inference_results"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 20,
+   "execution_count": 5,
    "id": "loose-assignment",
    "metadata": {},
    "outputs": [
@@ -299,7 +79,7 @@
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "{'': 0, '!': 1, '#': 2, ',': 3, '-': 4, '.': 5, ':': 6, ';': 7, '?': 8, '—': 9, '…': 10}\n"
+      "{'': 0, '!': 1, ',': 2, '-': 3, '.': 4, ':': 5, '?': 6, '—': 7}\n"
      ]
     },
     {
@@ -461,19 +241,19 @@
        "          False, False, False, False, False, False, False, False, False, False,\n",
        "          False, False, False, False, False, False, False, False, False, False,\n",
        "          False, False, False, False, False, False, False, False]]),\n",
-       " 'labels': tensor([[0, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
+       " 'labels': tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "          0, 0, 0, 0, 0, 0, 0, 0],\n",
-       "         [0, 0, 0, 0, 0, 0, 8, 2, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
+       "         [0, 0, 0, 0, 4, 0, 6, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "          0, 0, 0, 0, 0, 0, 0, 0],\n",
-       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
+       "         [0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
@@ -487,7 +267,7 @@
        "          0, 0, 0, 0, 0, 0, 0, 0]])}"
       ]
      },
-     "execution_count": 20,
+     "execution_count": 5,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -498,17 +278,201 @@
     "\n",
     "queries = [\n",
     "    'Hooooooo!',\n",
-    "    'what can i doo for you? tooday?',\n",
-    "    'how are you',\n",
+    "    'what can i doo; for you? tooday?',\n",
+    "    'how are you…',\n",
     "    'good morning everyone how have your weekends been its a really great day'\n",
     "]\n",
     "cfg.model.punct_label_ids=OmegaConf.create(sorted(cfg.model.punct_label_ids))\n",
     "labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
+    "label_map = {'…':'.',';':'.'}\n",
     "print(labels_to_ids)\n",
-    "ds=PunctuationInferenceDataset(tokenizer=AutoTokenizer.from_pretrained('google/electra-small-discriminator'), queries=queries, max_seq_length=128, punct_label_ids=labels_to_ids, degree=0, attach_label_to_end=None)\n",
+    "ds=PunctuationInferenceDataset(tokenizer=AutoTokenizer.from_pretrained('google/electra-small-discriminator'), queries=queries, max_seq_length=128, punct_label_ids=labels_to_ids, label_map=label_map, degree=0, attach_label_to_end=None)\n",
     "ds[0]"
    ]
   },
+  {
+   "cell_type": "code",
+   "execution_count": 28,
+   "id": "competitive-dubai",
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "[' hooooooo                                                                                                                           ',\n",
+       " ' what can i do for you? today?                                                                                                                        ',\n",
+       " ' how are you.                                                                                                                            ',\n",
+       " ' in guadeloupe, or martinique, it also brin- gs into ques- tion, budgetary po- licy, because the europe, an union is after all, making a present of ecu1, 9 billion to three multinationals. where are the financial interests of the european union? firstly, development policy in africa? in any case, in the acp countries employ ment policy in madeira, the canaries, guadel- oupe, martinique and crete, regional pol icy, in the ultraperipheral areas— human rights, which mr barthet mayer mentioned earlier, since dollar bananas are after all, slavery, bananas, the product of human exploitation by three multinationals, payments of ec ',\n",
+       " ' u 50 per mon, th! instead of ecu 50 per day.                                                                                                                   ',\n",
+       " ' plans for this weekend include turning wine into water. the small white buoys marked the location of hundreds of crab pots. he said he was not there yesterday, however many people saw him there today. arrived. with a crash of my car through the garage door. the lyrics of the song sounded like fingernails on a chalkboard. the guineafowl flies through the air with all the grace of a turtle. they ran around the corner to find that they had traveled back in time.                                      ',\n",
+       " ' good morning everyone. how have your weekends been? its a really great day. thank you.                                                                                                                ',\n",
+       " ' first of all, i too agree that tourism- related action must include employment training and education. as you know, after the european conference on tourism and employment, in luxemborg, we set up a high- level group whose mission was to examine how best tourism could contribute towards employment. the first stage.                                                                          ',\n",
+       " ' ummm, im not really sure. can you check with mr john instead? thanks!                                                                                                                 ']"
+      ]
+     },
+     "execution_count": 28,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "\n",
+    "queries = [\n",
+    "    'Hooooooo!',\n",
+    "    'what can i do for you today',\n",
+    "    'how are you',\n",
+    "    ' in guadeloupe or marti nique it also brin gs into ques tion budgetary po licy because the europe an union is after all making a present of ecu 1.9 billion to three multinationals where are the financial interests of the european union firstly development policy in africa in any case in the acp countries employ ment policy in madeira the canaries guadel oupe martinique and crete regional pol icy in the ultra-peripheral areas human rights which mr barthet mayer mentioned earlier since dollar bananas are after all slavery bana nas the product of human exploitation by three multinat ionals payments of ecu 50 per mon th instead of ecu 50 per day',\n",
+    "    '''Plans for this weekend include turning wine into water.\n",
+    "The small white buoys marked the location of hundreds of crab pots.\n",
+    "He said he was not there yesterday; however, many people saw him there.\n",
+    "Today arrived with a crash of my car through the garage door.\n",
+    "The lyrics of the song sounded like fingernails on a chalkboard.\n",
+    "The Guinea fowl flies through the air with all the grace of a turtle.\n",
+    "They ran around the corner to find that they had traveled back in time.''',\n",
+    "    'good morning everyone how have your weekends been its a really great day thank you',\n",
+    "    'first of all i too agree that tourism related action must include employment training and education as you know after the european conference on tourism and employment in luxemborg we set up a high-level group whose mission was to examine how best tourism could contribute towards employment the first stage',\n",
+    "    \"Ummm Im not really sure can you check with mr john instead thanks\"\n",
+    "]\n",
+    "inference_results = model.add_punctuation(queries)\n",
+    "inference_results"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 16,
+   "id": "dietary-violin",
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "[' hooooooo.                                                                                                                           ',\n",
+       " ' what can i do for you? today                                                                                                                        ',\n",
+       " ' how are you?                                                                                                                            ',\n",
+       " ' in guadeloupe or marti nique, it also brin gs into ques tion budgetary po- licy, because the europe, an union is after all, making a present of ecu 1, 9 billion to three multinationals. where are the financial interests of the european union? firstly, development policy in africa in any case, in the acp countries, employ ment policy in madeira, the canaries, guadel oupe, martinique and crete, regional pol- icy in the ultra- peripheral areas— human rights, which mr barthet mayer mentioned earlier, since dollar bananas are after all, slavery, bana nas the product of human exploitation by three multinat- ionals— payments of ec ',\n",
+       " ' u 50 per mon th, instead of ecu 50 per day.                                                                                                                   ',\n",
+       " ' plans for this weekend include turning wine into water. the small white buoys marked the location of hundreds of crab pots. he said he was not there yesterday. however many people saw him there today. arrived with a crash of my car through the garage door. the lyrics of the song sounded like fingernails on a chalkboard. the guinea fowl flies through the air with all the grace of a turtle. they ran around the corner to find that they had traveled back in time.                                      ',\n",
+       " ' good morning, everyone. how have your weekends been? its a really great day. thank you.                                                                                                                ',\n",
+       " ' first of all, i too agree that tourism- related action must include employment training and education. as you know, after the european conference on tourism and employment in luxemborg, we set up a high- level group, whose mission was to examine how best tourism could contribute towards employment. the first stage.                                                                          ',\n",
+       " ' ummm, im not really sure. can you check with mr? john instead? thanks.                                                                                                                 ']"
+      ]
+     },
+     "execution_count": 16,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "\n",
+    "queries = [\n",
+    "    'Hooooooo!',\n",
+    "    'what can i do for you today',\n",
+    "    'how are you',\n",
+    "    ' in guadeloupe or marti nique it also brin gs into ques tion budgetary po licy because the europe an union is after all making a present of ecu 1.9 billion to three multinationals where are the financial interests of the european union firstly development policy in africa in any case in the acp countries employ ment policy in madeira the canaries guadel oupe martinique and crete regional pol icy in the ultra-peripheral areas human rights which mr barthet mayer mentioned earlier since dollar bananas are after all slavery bana nas the product of human exploitation by three multinat ionals payments of ecu 50 per mon th instead of ecu 50 per day',\n",
+    "    '''Plans for this weekend include turning wine into water.\n",
+    "The small white buoys marked the location of hundreds of crab pots.\n",
+    "He said he was not there yesterday; however, many people saw him there.\n",
+    "Today arrived with a crash of my car through the garage door.\n",
+    "The lyrics of the song sounded like fingernails on a chalkboard.\n",
+    "The Guinea fowl flies through the air with all the grace of a turtle.\n",
+    "They ran around the corner to find that they had traveled back in time.''',\n",
+    "    'good morning everyone how have your weekends been its a really great day thank you',\n",
+    "    'first of all i too agree that tourism related action must include employment training and education as you know after the european conference on tourism and employment in luxemborg we set up a high-level group whose mission was to examine how best tourism could contribute towards employment the first stage',\n",
+    "    \"Ummm Im not really sure can you check with mr john instead thanks\"\n",
+    "]\n",
+    "inference_results = model.add_punctuation(queries)\n",
+    "inference_results"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 4,
+   "id": "hairy-proxy",
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "[' hooooooo.                                                                                                                           ',\n",
+       " ' what can i do for you? today                                                                                                                        ',\n",
+       " ' how are you?                                                                                                                            ',\n",
+       " ' in guadeloupe or marti nique, it also brin gs into ques tion budgetary po- licy because the europe, an union is after all, making a present of ecu 1, 9 billion to three multinationals. where are the financial interests of the european union? firstly, development policy in africa in any case, in the acp countries, employ ment policy in madeira, the canaries, guadel oupe, martinique and crete, regional pol- icy in the ultra- peripheral areas— human rights, which mr barthet mayer mentioned earlier, since dollar bananas are after all, slavery, bana nas the product of human exploitation by three multinat- ionals— payments of ec ',\n",
+       " ' u 50 per mon th, instead of ecu 50 per day.                                                                                                                   ',\n",
+       " ' plans for this weekend include turning wine into water. the small white buoys marked the location of hundreds of crab pots. he said he was not there yesterday, however many people saw him there today. arrived with a crash of my car through the garage door. the lyrics of the song sounded like fingernails on a chalkboard. the guinea fowl flies through the air with all the grace of a turtle. they ran around the corner to find that they had traveled back in time.                                      ',\n",
+       " ' good morning, everyone. how have your weekends been? its a really great day. thank you.                                                                                                                ',\n",
+       " ' first of all, i too agree that tourism- related action must include employment training and education. as you know, after the european conference on tourism and employment in luxemborg, we set up a high- level group, whose mission was to examine how best tourism could contribute towards employment. the first stage.                                                                          ']"
+      ]
+     },
+     "execution_count": 4,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "\n",
+    "queries = [\n",
+    "    'Hooooooo!',\n",
+    "    'what can i do for you today',\n",
+    "    'how are you',\n",
+    "    ' in guadeloupe or marti nique it also brin gs into ques tion budgetary po licy because the europe an union is after all making a present of ecu 1.9 billion to three multinationals where are the financial interests of the european union firstly development policy in africa in any case in the acp countries employ ment policy in madeira the canaries guadel oupe martinique and crete regional pol icy in the ultra-peripheral areas human rights which mr barthet mayer mentioned earlier since dollar bananas are after all slavery bana nas the product of human exploitation by three multinat ionals payments of ecu 50 per mon th instead of ecu 50 per day',\n",
+    "    '''Plans for this weekend include turning wine into water.\n",
+    "The small white buoys marked the location of hundreds of crab pots.\n",
+    "He said he was not there yesterday; however, many people saw him there.\n",
+    "Today arrived with a crash of my car through the garage door.\n",
+    "The lyrics of the song sounded like fingernails on a chalkboard.\n",
+    "The Guinea fowl flies through the air with all the grace of a turtle.\n",
+    "They ran around the corner to find that they had traveled back in time.''',\n",
+    "    'good morning everyone how have your weekends been its a really great day thank you',\n",
+    "    'first of all i too agree that tourism related action must include employment training and education as you know after the european conference on tourism and employment in luxemborg we set up a high-level group whose mission was to examine how best tourism could contribute towards employment the first stage'\n",
+    "]\n",
+    "inference_results = model.add_punctuation(queries)\n",
+    "inference_results"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 9,
+   "id": "residential-scene",
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "[' hooooooo,                                                                                                                           ',\n",
+       " ' what can i do for you today?                                                                                                                        ',\n",
+       " ' ? how are you?                                                                                                                            ',\n",
+       " ' firstly, development policy in africa. in any case, in the acp, countries, employment policy in madeira, the canaries— guadeloupe, martinique and crete, regional policy in the ultra- peripheral areas— human rights, which mr barthet mayer, mentioned earlier, since dollar bananas are after all, slavery. bananas, the product of human exploitation by three multinationals, payments of ecu 50 per month, instead of ecu 50 per day in guadeloupe or martinique. it also brings into question budgetary policy. because the european union is after all, making a present of ecu 1, 9 billion to three multinationals. where are the financial interests of the european union?         ',\n",
+       " ' plans for this weekend, include turning wine into water. the small white buoys marked the location of hundreds of crab pots. he said he was not there yesterday. however, many people saw him there. today. arrived with a crash of my car through the garage door. the lyrics of the song sounded like fingernails on a chalkboard. the guinea fowl flies through the air with all the grace of a turtle. they ran around the corner, to find that they had traveled back in time,                                      ',\n",
+       " ' good morning. everyone? how have your weekends been? its a really great day? thank you.                                                                                                                ',\n",
+       " ' first of all, i too agree that tourism- related action must include employment training and education. as you know, after the european conference on tourism and employment in luxemborg, we set up a high- level group, whose mission was to examine how best tourism could contribute towards employment— the first stage,                                                                          ']"
+      ]
+     },
+     "execution_count": 9,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "\n",
+    "queries = [\n",
+    "    'Hooooooo!',\n",
+    "    'what can i do for you today',\n",
+    "    'how are you',\n",
+    "    'firstly development policy in africa in any case in the acp countries employment policy in madeira the canaries guadeloupe martinique and crete regional policy in the ultra-peripheral areas human rights which mr barthet-mayer mentioned earlier since dollar bananas are after all slavery bananas the product of human exploitation by three multinationals payments of ecu 50 per month instead of ecu 50 per day in guadeloupe or martinique it also brings into question budgetary policy because the european union is after all making a present of ecu 1.9 billion to three multinationals where are the financial interests of the european union',\n",
+    "    '''Plans for this weekend include turning wine into water.\n",
+    "The small white buoys marked the location of hundreds of crab pots.\n",
+    "He said he was not there yesterday; however, many people saw him there.\n",
+    "Today arrived with a crash of my car through the garage door.\n",
+    "The lyrics of the song sounded like fingernails on a chalkboard.\n",
+    "The Guinea fowl flies through the air with all the grace of a turtle.\n",
+    "They ran around the corner to find that they had traveled back in time.''',\n",
+    "    'good morning everyone how have your weekends been its a really great day thank you',\n",
+    "    'first of all i too agree that tourism related action must include employment training and education as you know after the european conference on tourism and employment in luxemborg we set up a high-level group whose mission was to examine how best tourism could contribute towards employment the first stage'\n",
+    "]\n",
+    "inference_results = model.add_punctuation(queries)\n",
+    "inference_results"
+   ]
+  },
   {
    "cell_type": "code",
    "execution_count": 18,
diff --git a/experiment/config.yaml b/experiment/config.yaml
index 6813b0d..a0939a1 100644
--- a/experiment/config.yaml
+++ b/experiment/config.yaml
@@ -2,7 +2,7 @@ seed: 42
 trainer:
     gpus: 1 # the number of gpus, 0 for CPU
     num_nodes: 1
-    max_epochs: 20
+    max_epochs: 15
     max_steps: null # precedence over max_epochs
     accumulate_grad_batches: 1 # accumulates grads every k batches
     gradient_clip_val: 0
@@ -32,18 +32,18 @@ trainer:
     # resume_from_checkpoint: null
 
 exp_manager:
-    exp_dir: /home/nxingyu/project/ # /root/project # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
+    exp_dir: /home/nxingyu2/project/ # /root/project # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
     name: Punctuation_with_Domain_discriminator  # The name of your model
     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
     create_checkpoint_callback: true 
-base_path: /home/nxingyu/data # /root/data # 
-tmp_path: /home/nxingyu/data/tmp # /tmp # 
+base_path: /home/nxingyu2/data # /root/data # 
+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
 
 model:
     nemo_path: null
     transformer_path: google/electra-base-discriminator #google/electra-base-discriminator # roberta-base #google/electra-base-discriminator # distilbert-base-uncased # filename to save the model and associated artifacts to .nemo file
     unfrozen: 0
-    maximum_unfrozen: 1
+    maximum_unfrozen: 2
     unfreeze_step: 1
     punct_label_ids:
         - ""
@@ -52,22 +52,27 @@ model:
         - "-"
         - "."
         - ":"
-        - ";"
         - "?"
         - "—"
-        - "…"
+
+        # - ";"
+        # - "…"
+
+    label_map:
+        "…": "."
+        ";": "."
 
     no_space_label: '#'
 
     punct_class_weights: false #false
     
     dataset:
-        data_dir: /home/nxingyu/data # /root/data # 
+        data_dir: /home/nxingyu2/data # /root/data # 
         labelled:
             # - ${base_path}/ted_talks_processed #
             - ${base_path}/open_subtitles_processed #  
         unlabelled:
-            - ${base_path}/ted_talks_processed #
+            # - ${base_path}/ted_talks_processed #
             # - ${base_path}/open_subtitles_processed #  
             # parameters for dataset preprocessing
         max_seq_length: 128
@@ -76,10 +81,10 @@ model:
         ignore_start_end: false
         use_cache: false
         # shared among dataloaders
-        num_workers:  8
+        num_workers:  4
         pin_memory: false
         drop_last: true
-        num_labels: 11
+        num_labels: 9
         num_domains: 1
         test_unlabelled: true
         attach_label_to_end: none # false if attach to start none if dont mask
@@ -88,7 +93,7 @@ model:
             shuffle: true
             num_samples: -1
             batch_size: 32
-            manual_len: 000 #default 0 84074
+            manual_len: 4000 #default 0 84074
 
         validation_ds:
             # if evaluation data is not in the model.dataset.data_dir as the training data or multiple datasets are used for
@@ -112,7 +117,7 @@ model:
         # unfrozen_layers: 1
     
     punct_head:
-        punct_num_fc_layers: 3
+        punct_num_fc_layers: 2
         fc_dropout: 0.1
         activation: 'gelu'
         log_softmax: false
@@ -133,15 +138,18 @@ model:
     
     dice_loss:
         epsilon: 0.01
-        alpha: 2
+        alpha: 3
         macro_average: true
 
     focal_loss: 
         gamma: 2
 
     frozen_lr:
-        - 1e-2
-        - 1e-3
+        - 2e-2
+        - 1e-4
+        - 5e-6
+        - 5e-7
+        - 1e-7
 
     optim:
         name: adamw
@@ -154,7 +162,7 @@ model:
             # mode: 'triangular2'
             # last_epoch: -1
 
-            name: WarmupAnnealing #CosineAnnealing #WarmupAnnealing #CyclicLR
+            name: CosineAnnealing #CosineAnnealing #WarmupAnnealing #CyclicLR
             # Scheduler params
             warmup_steps: null
             warmup_ratio: 0.1
diff --git a/experiment/core/utils.py b/experiment/core/utils.py
index 3f1631b..6226a01 100644
--- a/experiment/core/utils.py
+++ b/experiment/core/utils.py
@@ -46,14 +46,18 @@ def view_aligned(texts,tags,tokenizer,labels_to_ids):
             )]
         )))) for _ in zip(texts,tags)]
 
-def text2masks(n, labels_to_ids):
+def text2masks(n, labels_to_ids,label_map):
     def text2masks(text):
         '''Converts single paragraph of text into a list of words and corresponding punctuation based on the degree requested.'''
+        labels=''.join(labels_to_ids.keys())
         if n==0: 
-            refilter="(?<=[.?!,;:\-—… ])(?=[^.?!,;:\-—… ])|$"
+            refilter=f"(?<=[{labels} ])(?=[^{labels} ])|$"
         else:
-            refilter="[.?!,;:\-—…]{1,%d}(?= *[^.?!,;:\-—…]+|$)|(?<=[^.?!,;:\-—…]) +(?=[^.?!,;:\-—…])"%(n)
+            refilter=f"[{labels}]{{1,{n}}}(?= *[^{labels}]+|$)|(?<=[^{labels}]) +(?=[^{labels}])"
         text=re.sub(r'^[_\W]*','',text)
+        for k,v in label_map.items():
+            text=re.sub(f"(?<=[{labels} ]){k}+","",text)
+            text=re.sub(f"(?<=[A-Za-z0-9 ]){k}+",v,text)
         word=re.split(refilter,text, flags=re.V1)
         punct=re.findall(refilter,text, flags=re.V1)
         wordlist,punctlist=([] for _ in range(2))
@@ -65,29 +69,29 @@ def text2masks(n, labels_to_ids):
         for i in zip(word,punct): #+[''] to correspond to the last word or '' after the last punctuation.
             w,p=i[0].strip(),i[1].strip()
             if w!='':
-                wordlist.append(re.sub(r'[.?!,;:\-—… ]','',w))
-                punctlist.append(0 if not w[-1] in '.?!,;:-—…' else labels_to_ids[w[-1]])
+                wordlist.append(re.sub(f'[{labels} ]','',w))
+                punctlist.append(0 if not w[-1] in labels else labels_to_ids[w[-1]])
             if p!='':
                 wordlist.append(p)
                 punctlist.append(0)
         return(wordlist,punctlist)
     return text2masks
 
-def chunk_examples_with_degree(n, labels_to_ids):
+def chunk_examples_with_degree(n, labels_to_ids,label_map):
     '''Ensure batched=True if using dataset.map or ensure the examples are wrapped in lists.'''
     def chunk_examples(examples):
         output={}
         output['texts']=[]
         output['tags']=[]
         for sentence in examples:
-            text,tag=text2masks(n, labels_to_ids)(sentence)
+            text,tag=text2masks(n, labels_to_ids, label_map)(sentence)
             output['texts'].append(text)
             output['tags'].append(tag)
             # output['tags'].append([0]+tag if text[0]!='' else tag) # [0]+tag so that in all case, the first tag refers to [CLS]
             # not necessary since all the leading punctuations are stripped
         return output
     return chunk_examples
-assert(chunk_examples_with_degree(0,{'': 0, '!': 1, ',': 2, '-': 3, '.': 4, ':': 5, ';': 6, '?': 7, '—': 8, '…': 9})(['Hello!Bye…'])=={'texts': [['Hello', 'Bye']], 'tags': [[1, 9]]})
+assert(chunk_examples_with_degree(0,{'': 0, '!': 1, ',': 2, '-': 3, '.': 4, ':': 5, '?': 6, '—': 7},{'…':'.',';':'.'})(['Hello!Bye…'])=={'texts': [['Hello', 'Bye']], 'tags': [[1, 4]]})
 
 def subword_tokenize(tokenizer,tokens):
     subwords = list(map(tokenizer.tokenize, tokens))
diff --git a/experiment/data/punctuation_datamodule.py b/experiment/data/punctuation_datamodule.py
index 8561831..4950246 100644
--- a/experiment/data/punctuation_datamodule.py
+++ b/experiment/data/punctuation_datamodule.py
@@ -13,6 +13,7 @@ class PunctuationDataModule(LightningDataModule):
             labelled: List[str], 
             unlabelled: List[str], 
             punct_label_ids: Dict[str,int],
+            label_map:Dict[str,str],
             train_batch_size: int = 16,
             max_seq_length:int = 256,
             val_batch_size:int = 256, 
@@ -35,6 +36,7 @@ class PunctuationDataModule(LightningDataModule):
         self.unlabelled=unlabelled
         self.tokenizer=AutoTokenizer.from_pretrained(tokenizer)
         self.punct_label_ids=pp(punct_label_ids)
+        self.label_map=label_map
         self.num_domains=len(labelled)+len(unlabelled)
         self.train_batch_size=train_batch_size
         self.val_batch_size=val_batch_size
@@ -74,6 +76,7 @@ class PunctuationDataModule(LightningDataModule):
                     num_samples=self.train_batch_size,
                     max_seq_length=self.max_seq_length,
                     punct_label_ids=self.punct_label_ids,
+                    label_map=self.label_map,
                     labelled=self.labelled,
                     unlabelled=self.unlabelled,
                     tokenizer=self.tokenizer,
@@ -86,6 +89,7 @@ class PunctuationDataModule(LightningDataModule):
                     num_samples=self.val_batch_size,
                     max_seq_length=self.max_seq_length,
                     punct_label_ids=self.punct_label_ids,
+                    label_map=self.label_map,
                     labelled=self.labelled,
                     unlabelled=self.unlabelled,
                     tokenizer=self.tokenizer,
@@ -99,6 +103,7 @@ class PunctuationDataModule(LightningDataModule):
                     num_samples=self.val_batch_size,
                     max_seq_length=self.max_seq_length,
                     punct_label_ids=self.punct_label_ids,
+                    label_map=self.label_map,
                     labelled=self.unlabelled,
                     unlabelled=[],
                     tokenizer=self.tokenizer,
@@ -111,6 +116,7 @@ class PunctuationDataModule(LightningDataModule):
                     num_samples=self.val_batch_size,
                     max_seq_length=self.max_seq_length,
                     punct_label_ids=self.punct_label_ids,
+                    label_map=self.label_map,
                     labelled=self.labelled,
                     unlabelled=self.unlabelled,
                     tokenizer=self.tokenizer,
diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
index b875b17..dda9cf5 100644
--- a/experiment/data/punctuation_dataset_multi.py
+++ b/experiment/data/punctuation_dataset_multi.py
@@ -32,6 +32,7 @@ class PunctuationDomainDataset(IterableDataset):
         max_seq_length:int=256,
         degree=0,
         punct_label_ids: Dict[str, int] = None,
+        label_map:Dict[str,str] = None,
         domain=0,
         labelled=True,
         randomize=True,
@@ -41,6 +42,7 @@ class PunctuationDomainDataset(IterableDataset):
         end=-1,
         attach_label_to_end=None,
         no_space_label=None,
+        manual_len=0,
     ):
         if not (os.path.exists(csv_file)):
             raise FileNotFoundError(
@@ -56,9 +58,11 @@ class PunctuationDomainDataset(IterableDataset):
         
         self.csv_file =   csv_file
         self.max_seq_length =   max_seq_length
-        self.set_num_samples(csv_file, num_samples)
+        self.manual_len=manual_len
+        self.set_num_samples(csv_file, num_samples, manual_len)
         self.domain=  domain
         self.punct_label_ids=punct_label_ids
+        self.label_map=label_map
         self.labelled=  labelled
         self.tokenizer= tokenizer
         self.degree=degree
@@ -90,7 +94,7 @@ class PunctuationDomainDataset(IterableDataset):
         b=np.minimum(l,a+self.max_seq_length*n)
         batch=pd.DataFrame({'t':batch,'a':a,'b':b}).apply(lambda row: ' '.join(row.t.split()[row.a:row.b]),axis=1)
 
-        chunked=chunk_examples_with_degree(self.degree, self.punct_label_ids)(batch)
+        chunked=chunk_examples_with_degree(self.degree, self.punct_label_ids, self.label_map)(batch)
         batched=chunk_to_len_batch(self.max_seq_length,self.tokenizer,chunked['texts'],chunked['tags'],self.labelled,no_space_label=self.no_space_label)
         num_samples=batched['labels'].shape[0]
         batched['domain']=self.domain*torch.ones(num_samples,1,dtype=torch.long)
@@ -101,9 +105,11 @@ class PunctuationDomainDataset(IterableDataset):
         else:
             return batched
 
-    def set_num_samples(self,csv_file,num_samples):
+    def set_num_samples(self,csv_file,num_samples, manual_len):
         self.num_samples = num_samples
         self.total_samples=int(subprocess.Popen(['wc', '-l', csv_file], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0])
+        if manual_len>0:
+            self.total_samples=min(manual_len,self.total_samples)
         self.len = int(self.total_samples / self.num_samples)
         
 
@@ -150,6 +156,7 @@ class PunctuationDomainDatasets(IterableDataset):
                  num_samples:int,
                  max_seq_length:int,
                  punct_label_ids: Dict[str, int],
+                 label_map:Dict[str,str],
                  labelled: List[str],
                  unlabelled: List[str],
                  tokenizer,
@@ -167,11 +174,11 @@ class PunctuationDomainDatasets(IterableDataset):
         self.iterables=[]
         self.randomize=randomize
         self.punct_label_ids=punct_label_ids
-
+        self.label_map=label_map
         self.ds_lengths=[]
         for path in labelled+unlabelled:
             self.ds_lengths.append(int(subprocess.Popen(['wc', '-l', f'{path}.{split}.csv'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0]))
-        self.max_length=max(self.ds_lengths) if manual_len==0 else manual_len
+        self.max_length=max(self.ds_lengths) 
         self.per_worker=int(self.max_length/self.num_workers)
         self.len=int(self.per_worker/num_samples) 
         self.class_weights=None
@@ -182,12 +189,15 @@ class PunctuationDomainDatasets(IterableDataset):
             dataset=PunctuationDomainDataset(
                     csv_file=f'{path}.{split}.csv', tokenizer=tokenizer,
                     num_samples=num_samples,max_seq_length=max_seq_length,
-                    punct_label_ids=punct_label_ids,domain=i,labelled=True,
+                    punct_label_ids=punct_label_ids,
+                    label_map=label_map,
+                    domain=i,labelled=True,
                     randomize=randomize,
                     target_file=f'{target}.{split}.{data_id}.csv',
                     tmp_path=tmp_path,
                     attach_label_to_end=attach_label_to_end,
-                    no_space_label=no_space_label,)
+                    no_space_label=no_space_label,
+                    manual_len=manual_len,)
             self.datasets.append(dataset)
             self.iterables.append(cycle(dataset))
             
@@ -196,12 +206,14 @@ class PunctuationDomainDatasets(IterableDataset):
             dataset=PunctuationDomainDataset(
                     csv_file=f'{path}.{split}.csv', tokenizer=tokenizer,
                     num_samples=num_samples,max_seq_length=max_seq_length,
-                    punct_label_ids=punct_label_ids,domain=len(labelled)+i,labelled=False,
+                    punct_label_ids=punct_label_ids,
+                    label_map=label_map,domain=len(labelled)+i,labelled=False,
                     randomize=randomize,
                     target_file=f'{target}.{split}.{data_id}.csv',
                     tmp_path=tmp_path,
                     attach_label_to_end=attach_label_to_end,
-                    no_space_label=no_space_label,)
+                    no_space_label=no_space_label,
+                    manual_len=manual_len,)
             self.datasets.append(dataset)
             self.iterables.append(cycle(dataset))
 
@@ -273,11 +285,12 @@ class PunctuationInferenceDataset(Dataset):
             "labels": NeuralType(('B', 'T'), ChannelType()),
         }
 
-    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], num_samples:int=256, degree:int = 0, attach_label_to_end:bool=None,no_space_label=None,):
+    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], label_map:Dict[str,str], num_samples:int=256, degree:int = 0, attach_label_to_end:bool=None,no_space_label=None,):
         """ Initializes BertPunctuationInferDataset. """
         self.degree=degree
         self.punct_label_ids=punct_label_ids
-        chunked=chunk_examples_with_degree(self.degree, self.punct_label_ids)(queries)
+        self.label_map = label_map
+        chunked=chunk_examples_with_degree(self.degree, self.punct_label_ids, self.label_map)(queries)
         self.features = chunk_to_len_batch(max_seq_length, tokenizer,chunked['texts'],chunked['tags'],attach_label_to_end=attach_label_to_end,no_space_label=no_space_label,)
         self.attach_label_to_end=attach_label_to_end
         # self.all_input_ids = features['input_ids']
diff --git a/experiment/info.log b/experiment/info.log
index 846f10d..104c5dd 100644
--- a/experiment/info.log
+++ b/experiment/info.log
@@ -1,1434 +1,4 @@
-[INFO] - shuffling train set
-[INFO] - Optimizer config = AdamW (
-Parameter Group 0
-    amsgrad: False
-    betas: (0.9, 0.999)
-    eps: 1e-08
-    lr: 0.01
-    weight_decay: 0.0
-)
-[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fbc52a47cd0>" 
-will be used during training (effective maximum steps = 52540) - 
-Parameters : 
-(warmup_steps: null
-warmup_ratio: 0.1
-min_lr: 1.0e-08
-last_epoch: -1
-max_steps: 52540
-)
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          82.55      58.85      68.72      70786
-! (label_id: 1)                                          0.89      31.32       1.74        562
-# (label_id: 2)                                          4.55       0.84       1.42       5242
-, (label_id: 3)                                          0.00       0.00       0.00       2395
-- (label_id: 4)                                          0.00       0.00       0.00        212
-. (label_id: 5)                                          2.29       7.62       3.52       3821
-: (label_id: 6)                                          0.00       0.00       0.00         13
-; (label_id: 7)                                          0.00       0.00       0.00         13
-? (label_id: 8)                                          0.00       0.00       0.00        937
-— (label_id: 9)                                          0.00       0.00       0.00          0
-… (label_id: 10)                                         0.00       0.00       0.00        315
--------------------
-micro avg                                               50.03      50.03      50.03      84296
-macro avg                                                9.03       9.86       7.54      84296
-weighted avg                                            69.71      50.03      57.96      84296
-
--------------------
-41660.00 355.00 3678.00 1494.00 139.00 2338.00  7.00  6.00 583.00  0.00 208.00
-16063.00 176.00 1202.00 672.00 58.00 1150.00  6.00  6.00 276.00  0.00 75.00
-869.00  4.00 44.00 19.00  2.00 24.00  0.00  1.00  4.00  0.00  1.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-29.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-11835.00 17.00 272.00 195.00 13.00 291.00  0.00  0.00 66.00  0.00 26.00
-89.00  6.00  9.00  2.00  0.00  3.00  0.00  0.00  2.00  0.00  1.00
- 8.00  0.00  0.00  7.00  0.00  1.00  0.00  0.00  0.00  0.00  3.00
-31.00  0.00  5.00  1.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-202.00  4.00 32.00  5.00  0.00 14.00  0.00  0.00  6.00  0.00  1.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         48.07      89.75      62.61        361
-1 (label_id: 1)                                         22.92       3.05       5.38        361
--------------------
-micro avg                                               46.40      46.40      46.40        722
-macro avg                                               35.49      46.40      33.99        722
-weighted avg                                            35.49      46.40      33.99        722
-
--------------------
- 0.00  0.00
-80.00 80.00
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          95.76      96.99      96.37   15595410
-! (label_id: 1)                                         20.62      33.51      25.53     132074
-, (label_id: 2)                                         36.23      32.82      34.44     481066
-- (label_id: 3)                                         35.39      11.59      17.46      44642
-. (label_id: 4)                                         46.75      40.61      43.47     922110
-: (label_id: 5)                                          0.00       0.00       0.00       2427
-; (label_id: 6)                                          0.00       0.00       0.00        951
-? (label_id: 7)                                         35.71      35.33      35.52     244079
-— (label_id: 8)                                          0.00       0.00       0.00         81
-… (label_id: 9)                                          0.00       0.00       0.00      80692
--------------------
-micro avg                                               90.23      90.23      90.23   17503532
-macro avg                                               27.05      25.08      25.28   17503532
-weighted avg                                            89.52      90.23      89.83   17503532
-
--------------------
-15125737.00 27277.00 191637.00 26435.00 313125.00 1788.00 462.00 70019.00 27.00 39497.00
-27847.00 44260.00 41574.00 3953.00 74077.00 65.00 54.00 15328.00  8.00 7456.00
-113780.00 20070.00 157886.00 3314.00 106258.00 113.00 35.00 20225.00  8.00 14080.00
-7777.00 83.00 168.00 5172.00 1025.00 27.00  1.00 119.00  1.00 242.00
-254717.00 33525.00 71011.00 2723.00 374479.00 316.00 345.00 52153.00 24.00 11717.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-65552.00 6859.00 18790.00 3045.00 53146.00 118.00 54.00 86235.00 13.00 7700.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         50.00     100.00      66.67      72939
-1 (label_id: 1)                                          0.00       0.00       0.00      72939
--------------------
-micro avg                                               50.00      50.00      50.00     145878
-macro avg                                               25.00      50.00      33.33     145878
-weighted avg                                            25.00      50.00      33.33     145878
-
--------------------
-72939.00 72939.00
- 0.00  0.00
-[INFO] - Epoch 0, global step 5254: val_loss reached 115.21539 (best 115.21539), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=115.22-epoch=0.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          96.94      95.95      96.44   15594360
-! (label_id: 1)                                         24.17      34.29      28.35     131487
-, (label_id: 2)                                         37.23      37.73      37.48     481266
-- (label_id: 3)                                         28.21      25.70      26.90      44079
-. (label_id: 4)                                         42.22      54.07      47.41     922224
-: (label_id: 5)                                          0.00       0.00       0.00       2409
-; (label_id: 6)                                          0.00       0.00       0.00        979
-? (label_id: 7)                                         68.33      28.19      39.92     244473
-— (label_id: 8)                                          0.00       0.00       0.00         80
-… (label_id: 9)                                         21.74      19.09      20.33      80827
--------------------
-micro avg                                               90.18      90.18      90.18   17502184
-macro avg                                               31.88      29.50      29.68   17502184
-weighted avg                                            90.93      90.18      90.39   17502184
-
--------------------
-14962490.00 17786.00 153114.00 18055.00 201695.00 1774.00 332.00 47661.00 15.00 31863.00
-17955.00 45084.00 32767.00 2197.00 69259.00 22.00 53.00 14020.00  8.00 5190.00
-109565.00 24617.00 181579.00 1899.00 129163.00 60.00 43.00 29063.00  9.00 11679.00
-25530.00 182.00 347.00 11329.00 2400.00 39.00  3.00 132.00  1.00 191.00
-446731.00 39136.00 98104.00 3537.00 498618.00 463.00 535.00 78504.00 27.00 15484.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-12991.00 2301.00 7030.00 316.00 8301.00 17.00  4.00 68925.00  5.00 988.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-19098.00 2381.00 8325.00 6746.00 12788.00 34.00  9.00 6168.00 15.00 15432.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         75.40      77.49      76.43      72939
-1 (label_id: 1)                                         76.85      74.72      75.77      72939
--------------------
-micro avg                                               76.11      76.11      76.11     145878
-macro avg                                               76.13      76.11      76.10     145878
-weighted avg                                            76.13      76.11      76.10     145878
-
--------------------
-56524.00 18438.00
-16415.00 54501.00
-[INFO] - Epoch 1, global step 10509: val_loss reached 22.29331 (best 22.29331), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=22.29-epoch=1.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.37      95.54      96.45   15597741
-! (label_id: 1)                                         26.42      30.96      28.51     131817
-, (label_id: 2)                                         42.10      32.84      36.90     481164
-- (label_id: 3)                                         23.37      34.25      27.79      44240
-. (label_id: 4)                                         41.14      65.17      50.44     922389
-: (label_id: 5)                                          0.00       0.00       0.00       2305
-; (label_id: 6)                                          0.00       0.00       0.00        978
-? (label_id: 7)                                         65.90      30.99      42.16     244399
-— (label_id: 8)                                          0.00       0.00       0.00         85
-… (label_id: 9)                                         35.29      13.04      19.05      80604
--------------------
-micro avg                                               90.28      90.28      90.28   17505722
-macro avg                                               33.16      30.28      30.13   17505722
-weighted avg                                            91.42      90.28      90.57   17505722
-
--------------------
-14902258.00 14322.00 140661.00 15231.00 161865.00 1308.00 294.00 37909.00 12.00 31475.00
-15291.00 40806.00 26976.00 1918.00 54217.00 34.00 54.00 10322.00  4.00 4836.00
-75146.00 20443.00 158025.00 1326.00 90923.00 37.00 18.00 19692.00  6.00 9759.00
-42984.00 618.00 1546.00 15154.00 3313.00 302.00  6.00 392.00  1.00 516.00
-537671.00 52833.00 141835.00 5214.00 601078.00 599.00 588.00 99145.00 46.00 21990.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-17390.00 2462.00 8431.00 486.00 8863.00 16.00 16.00 75742.00  8.00 1515.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-7001.00 333.00 3690.00 4911.00 2130.00  9.00  2.00 1197.00  8.00 10513.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         82.99      74.51      78.52      72939
-1 (label_id: 1)                                         76.87      84.72      80.61      72939
--------------------
-micro avg                                               79.62      79.62      79.62     145878
-macro avg                                               79.93      79.62      79.56     145878
-weighted avg                                            79.93      79.62      79.56     145878
-
--------------------
-54348.00 11142.00
-18591.00 61797.00
-[INFO] - Epoch 2, global step 15764: val_loss reached 13.03634 (best 13.03634), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=13.04-epoch=2.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.04      95.93      96.48   15593759
-! (label_id: 1)                                         26.18      30.27      28.08     131176
-, (label_id: 2)                                         37.21      38.66      37.92     481613
-- (label_id: 3)                                         26.28      31.16      28.51      44269
-. (label_id: 4)                                         43.46      57.00      49.32     922442
-: (label_id: 5)                                          0.00       0.00       0.00       2480
-; (label_id: 6)                                          0.00       0.00       0.00        986
-? (label_id: 7)                                         62.06      34.80      44.59     243336
-— (label_id: 8)                                          0.00       0.00       0.00         81
-… (label_id: 9)                                         34.95      14.72      20.71      80532
--------------------
-micro avg                                               90.40      90.40      90.40   17500674
-macro avg                                               32.72      30.25      30.56   17500674
-weighted avg                                            91.06      90.40      90.61   17500674
-
--------------------
-14959341.00 16968.00 151956.00 17591.00 191782.00 1556.00 334.00 43138.00 16.00 33463.00
-16095.00 39711.00 25446.00 1535.00 54595.00 16.00 47.00 9960.00  4.00 4278.00
-119910.00 25693.00 186170.00 1855.00 129885.00 76.00 63.00 25617.00 14.00 10999.00
-31612.00 798.00 1312.00 13793.00 3506.00 244.00  7.00 536.00  2.00 673.00
-438663.00 44152.00 100518.00 4189.00 525774.00 547.00 517.00 77855.00 28.00 17465.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-22648.00 3172.00 9643.00 565.00 13889.00 33.00 15.00 84680.00 10.00 1803.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-5490.00 682.00 6568.00 4741.00 3011.00  8.00  3.00 1550.00  7.00 11851.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         58.88      74.19      65.65      72939
-1 (label_id: 1)                                         65.11      48.18      55.38      72939
--------------------
-micro avg                                               61.18      61.18      61.18     145878
-macro avg                                               62.00      61.18      60.52     145878
-weighted avg                                            62.00      61.18      60.52     145878
-
--------------------
-54111.00 37796.00
-18828.00 35143.00
-[INFO] - Epoch 3, global step 21019: val_loss reached 15.18966 (best 13.03634), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=15.19-epoch=3.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.05      94.34      96.16   15589848
-! (label_id: 1)                                         23.17      30.69      26.40     132067
-, (label_id: 2)                                         32.70      44.75      37.79     480971
-- (label_id: 3)                                         23.63      34.60      28.08      44614
-. (label_id: 4)                                         40.91      60.81      48.91     922579
-: (label_id: 5)                                          0.00       0.00       0.00       2446
-; (label_id: 6)                                          0.00       0.00       0.00        960
-? (label_id: 7)                                         58.17      37.93      45.92     244526
-— (label_id: 8)                                          0.00       0.00       0.00         69
-… (label_id: 9)                                         23.68      20.53      21.99      81023
--------------------
-micro avg                                               89.43      89.43      89.43   17499104
-macro avg                                               30.03      32.37      30.53   17499104
-weighted avg                                            91.57      89.43      90.30   17499104
-
--------------------
-14707316.00 9041.00 101568.00 12781.00 117293.00 1108.00 209.00 24817.00  6.00 25145.00
-38842.00 40529.00 23856.00 1966.00 55108.00 28.00 80.00 10252.00  2.00 4269.00
-203127.00 32376.00 215253.00 3074.00 157418.00 184.00 77.00 32763.00 14.00 13995.00
-43782.00 413.00 936.00 15436.00 3542.00 361.00  4.00 407.00  1.00 448.00
-545637.00 44216.00 117425.00 4413.00 561060.00 699.00 560.00 78804.00 30.00 18732.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-31800.00 3685.00 10871.00 538.00 17945.00 38.00 27.00 92754.00  4.00 1803.00
- 5.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-19339.00 1807.00 11062.00 6406.00 10213.00 28.00  3.00 4729.00 12.00 16631.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         71.14      72.42      71.77      72939
-1 (label_id: 1)                                         71.91      70.61      71.26      72939
--------------------
-micro avg                                               71.52      71.52      71.52     145878
-macro avg                                               71.53      71.52      71.52     145878
-weighted avg                                            71.53      71.52      71.52     145878
-
--------------------
-52824.00 21434.00
-20115.00 51505.00
-[INFO] - Epoch 4, global step 26274: val_loss reached 10.95131 (best 10.95131), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=10.95-epoch=4.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          91.79      99.56      95.52   15593568
-! (label_id: 1)                                         37.06      10.66      16.56     131858
-, (label_id: 2)                                         45.37      20.89      28.61     480872
-- (label_id: 3)                                         55.97       0.17       0.34      44566
-. (label_id: 4)                                         59.86      17.09      26.59     923462
-: (label_id: 5)                                          0.00       0.00       0.00       2382
-; (label_id: 6)                                          0.00       0.00       0.00       1005
-? (label_id: 7)                                         80.43      14.19      24.13     243779
-— (label_id: 8)                                          0.00       0.00       0.00         77
-… (label_id: 9)                                         40.56      11.27      17.64      80824
--------------------
-micro avg                                               90.51      90.51      90.51   17502392
-macro avg                                               41.10      17.38      20.94   17502392
-weighted avg                                            87.91      90.51      87.83   17502392
-
--------------------
-15525010.00 87224.00 354030.00 38482.00 675089.00 2252.00 862.00 169600.00 58.00 61242.00
-1157.00 14059.00 4932.00 372.00 13830.00  2.00  4.00 2357.00  1.00 1224.00
-11569.00 15588.00 100440.00 615.00 72381.00  9.00 14.00 15521.00  5.00 5224.00
-56.00  1.00  0.00 75.00  1.00  0.00  0.00  0.00  0.00  1.00
-51578.00 13575.00 15747.00 771.00 157803.00 113.00 119.00 20412.00  7.00 3515.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-1873.00 993.00 2403.00 184.00 2443.00  3.00  4.00 34595.00  3.00 510.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-2325.00 418.00 3320.00 4067.00 1915.00  3.00  2.00 1294.00  3.00 9108.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         83.13      35.47      49.73      72939
-1 (label_id: 1)                                         58.98      92.80      72.13      72939
--------------------
-micro avg                                               64.14      64.14      64.14     145878
-macro avg                                               71.06      64.14      60.93     145878
-weighted avg                                            71.06      64.14      60.93     145878
-
--------------------
-25873.00 5252.00
-47066.00 67687.00
-[INFO] - Epoch 5, global step 31529: val_loss reached 11.56726 (best 10.95131), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=11.57-epoch=5.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.10      96.73      96.91   15594783
-! (label_id: 1)                                         26.03      34.01      29.49     130591
-, (label_id: 2)                                         36.51      44.31      40.03     481352
-- (label_id: 3)                                         28.42      32.41      30.29      44046
-. (label_id: 4)                                         49.66      51.07      50.36     921748
-: (label_id: 5)                                          0.00       0.00       0.00       2368
-; (label_id: 6)                                          0.00       0.00       0.00        969
-? (label_id: 7)                                         62.15      39.66      48.42     244359
-— (label_id: 8)                                          0.00       0.00       0.00         77
-… (label_id: 9)                                         27.15      19.08      22.41      80560
--------------------
-micro avg                                               91.08      91.08      91.08   17500852
-macro avg                                               32.70      31.73      31.79   17500852
-weighted avg                                            91.40      91.08      91.19   17500852
-
--------------------
-15084688.00 15259.00 147298.00 16422.00 197293.00 1535.00 340.00 41006.00 12.00 31565.00
-15860.00 44415.00 26169.00 1407.00 66997.00 25.00 42.00 10913.00  1.00 4805.00
-140870.00 27725.00 213270.00 2277.00 156344.00 129.00 106.00 30845.00 13.00 12569.00
-28941.00 863.00 1674.00 14277.00 3377.00 161.00  2.00 453.00  1.00 482.00
-289083.00 36831.00 72507.00 3113.00 470746.00 467.00 451.00 60772.00 31.00 13876.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 2.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-22093.00 4143.00 10862.00 528.00 19433.00 31.00 23.00 96911.00  9.00 1893.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-13246.00 1355.00 9572.00 6022.00 7558.00 20.00  5.00 3459.00 10.00 15370.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         81.20      84.83      82.97      72939
-1 (label_id: 1)                                         84.12      80.36      82.20      72939
--------------------
-micro avg                                               82.59      82.59      82.59     145878
-macro avg                                               82.66      82.59      82.58     145878
-weighted avg                                            82.66      82.59      82.58     145878
-
--------------------
-61871.00 14324.00
-11068.00 58615.00
-[INFO] - Epoch 6, global step 36784: val_loss reached 4.74845 (best 4.74845), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=4.75-epoch=6.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.41      96.28      96.84   15588559
-! (label_id: 1)                                         27.28      33.75      30.17     131502
-, (label_id: 2)                                         41.37      38.90      40.10     481656
-- (label_id: 3)                                         28.61      33.05      30.67      44145
-. (label_id: 4)                                         44.95      60.42      51.55     922262
-: (label_id: 5)                                          0.00       0.00       0.00       2425
-; (label_id: 6)                                          0.00       0.00       0.00        967
-? (label_id: 7)                                         64.43      36.88      46.91     244604
-— (label_id: 8)                                          0.00       0.00       0.00         55
-… (label_id: 9)                                         32.17      17.56      22.72      80592
--------------------
-micro avg                                               90.96      90.96      90.96   17496768
-macro avg                                               33.62      31.68      31.90   17496768
-weighted avg                                            91.62      90.96      91.16   17496768
-
--------------------
-15007911.00 13555.00 140211.00 15922.00 160726.00 1486.00 277.00 36208.00 10.00 30305.00
-14660.00 44381.00 25986.00 1612.00 60515.00 20.00 53.00 10649.00  3.00 4792.00
-83905.00 24199.00 187365.00 1548.00 121409.00 69.00 46.00 24100.00 10.00 10233.00
-29750.00 794.00 1564.00 14592.00 3181.00 109.00  2.00 460.00  1.00 549.00
-424767.00 44224.00 108211.00 4465.00 557202.00 701.00 560.00 80545.00 20.00 18891.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-19879.00 3483.00 9949.00 496.00 14274.00 26.00 25.00 90204.00  4.00 1667.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-7687.00 866.00 8370.00 5510.00 4955.00 14.00  4.00 2438.00  7.00 14155.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         53.81      98.12      69.50      72939
-1 (label_id: 1)                                         89.34      15.77      26.80      72939
--------------------
-micro avg                                               56.94      56.94      56.94     145878
-macro avg                                               71.57      56.94      48.15     145878
-weighted avg                                            71.57      56.94      48.15     145878
-
--------------------
-71567.00 61440.00
-1372.00 11499.00
-[INFO] - Epoch 7, global step 42039: val_loss reached 10.55844 (best 4.74845), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=10.56-epoch=7.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.07      97.04      97.05   15595430
-! (label_id: 1)                                         22.99      42.55      29.85     131969
-, (label_id: 2)                                         40.06      40.07      40.06     481215
-- (label_id: 3)                                         29.72      30.99      30.34      43898
-. (label_id: 4)                                         49.47      50.39      49.93     923479
-: (label_id: 5)                                          0.00       0.00       0.00       2496
-; (label_id: 6)                                          0.00       0.00       0.00        924
-? (label_id: 7)                                         63.43      39.36      48.58     245290
-— (label_id: 8)                                          0.00       0.00       0.00         89
-… (label_id: 9)                                         28.94      18.33      22.44      81055
--------------------
-micro avg                                               91.24      91.24      91.24   17505844
-macro avg                                               33.17      31.87      31.83   17505844
-weighted avg                                            91.46      91.24      91.28   17505844
-
--------------------
-15133102.00 14587.00 153243.00 17067.00 197324.00 1676.00 306.00 40980.00 10.00 32086.00
-24858.00 56147.00 40060.00 2436.00 95997.00 41.00 97.00 17489.00 15.00 7055.00
-90755.00 22972.00 192829.00 1638.00 136815.00 80.00 64.00 25504.00 12.00 10702.00
-24573.00 724.00 1615.00 13602.00 3707.00 207.00  7.00 610.00  3.00 722.00
-290244.00 32918.00 73254.00 3087.00 465317.00 454.00 428.00 61033.00 29.00 13808.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-20353.00 3805.00 11145.00 521.00 17964.00 24.00 19.00 96545.00  7.00 1824.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-11545.00 816.00 9069.00 5547.00 6355.00 14.00  3.00 3129.00 13.00 14858.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         99.83      22.31      36.47      72939
-1 (label_id: 1)                                         56.27      99.96      72.01      72939
--------------------
-micro avg                                               61.14      61.14      61.14     145878
-macro avg                                               78.05      61.14      54.24     145878
-weighted avg                                            78.05      61.14      54.24     145878
-
--------------------
-16271.00 27.00
-56668.00 72912.00
-[INFO] - Epoch 8, global step 47294: val_loss reached 10.89379 (best 4.74845), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=10.89-epoch=8.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.22      94.84      96.50   15591410
-! (label_id: 1)                                         23.95      40.67      30.15     131931
-, (label_id: 2)                                         43.01      35.73      39.03     481727
-- (label_id: 3)                                         25.51      36.79      30.13      44198
-. (label_id: 4)                                         40.69      66.32      50.44     922501
-: (label_id: 5)                                          0.00       0.00       0.00       2371
-; (label_id: 6)                                          0.00       0.00       0.00       1036
-? (label_id: 7)                                         54.96      43.33      48.46     244078
-— (label_id: 8)                                          0.00       0.00       0.00         71
-… (label_id: 9)                                         26.00      19.54      22.31      80978
--------------------
-micro avg                                               90.07      90.07      90.07   17500300
-macro avg                                               31.23      33.72      31.70   17500300
-weighted avg                                            91.96      90.07      90.79   17500300
-
--------------------
-14786965.00 7765.00 99954.00 12722.00 102452.00 1242.00 235.00 20446.00  7.00 23814.00
-25572.00 53650.00 38795.00 2061.00 83538.00 20.00 95.00 13868.00  8.00 6361.00
-90675.00 17493.00 172132.00 1312.00 92736.00 51.00 34.00 17200.00  6.00 8603.00
-37820.00 952.00 2095.00 16262.00 4703.00 262.00  9.00 705.00  1.00 949.00
-588722.00 46965.00 142518.00 5510.00 611801.00 729.00 631.00 83393.00 34.00 23114.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-42928.00 4227.00 15188.00 648.00 21291.00 43.00 29.00 105763.00  9.00 2312.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-18728.00 879.00 11045.00 5683.00 5980.00 24.00  3.00 2703.00  6.00 15825.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         51.46      93.49      66.38      72939
-1 (label_id: 1)                                         64.46      11.82      19.97      72939
--------------------
-micro avg                                               52.65      52.65      52.65     145878
-macro avg                                               57.96      52.65      43.18     145878
-weighted avg                                            57.96      52.65      43.18     145878
-
--------------------
-68188.00 64321.00
-4751.00 8618.00
-[INFO] - Epoch 9, step 52549: val_loss was not in top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.22      94.83      96.50   15591717
-! (label_id: 1)                                         24.08      39.63      29.96     130786
-, (label_id: 2)                                         39.10      39.33      39.21     482370
-- (label_id: 3)                                         23.57      39.68      29.58      44598
-. (label_id: 4)                                         42.09      65.21      51.16     923093
-: (label_id: 5)                                          0.00       0.00       0.00       2420
-; (label_id: 6)                                          0.00       0.00       0.00        948
-? (label_id: 7)                                         59.51      41.49      48.89     244470
-— (label_id: 8)                                          0.00       0.00       0.00         81
-… (label_id: 9)                                         23.73      21.11      22.34      80366
--------------------
-micro avg                                               90.08      90.08      90.08   17500848
-macro avg                                               31.03      34.13      31.76   17500848
-weighted avg                                            91.99      90.08      90.83   17500848
-
--------------------
-14785707.00 7179.00 98657.00 11323.00 103714.00 1225.00 158.00 21469.00 10.00 23838.00
-27076.00 51833.00 37088.00 2082.00 77749.00 26.00 87.00 13401.00  5.00 5937.00
-134239.00 19324.00 189708.00 1697.00 109257.00 79.00 56.00 20986.00  7.00 9817.00
-48353.00 829.00 2102.00 17698.00 4420.00 342.00  8.00 574.00  1.00 756.00
-541104.00 46631.00 129957.00 5125.00 601949.00 696.00 612.00 82816.00 36.00 21160.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-32932.00 3787.00 12230.00 539.00 17582.00 25.00 22.00 101439.00  9.00 1893.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-22306.00 1203.00 12628.00 6134.00 8422.00 27.00  5.00 3785.00 13.00 16965.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         69.41      80.60      74.59      72939
-1 (label_id: 1)                                         76.87      64.48      70.13      72939
--------------------
-micro avg                                               72.54      72.54      72.54     145878
-macro avg                                               73.14      72.54      72.36     145878
-weighted avg                                            73.14      72.54      72.36     145878
-
--------------------
-58792.00 25911.00
-14147.00 47028.00
-[INFO] - Epoch 10, global step 57804: val_loss reached 1.42583 (best 1.42583), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=1.43-epoch=10.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.35      96.92      97.14   15594830
-! (label_id: 1)                                         25.74      37.59      30.55     131420
-, (label_id: 2)                                         40.87      40.88      40.87     481980
-- (label_id: 3)                                         33.55      29.31      31.29      44213
-. (label_id: 4)                                         50.05      56.15      52.93     923128
-: (label_id: 5)                                          0.00       0.00       0.00       2372
-; (label_id: 6)                                          0.00       0.00       0.00        961
-? (label_id: 7)                                         59.16      43.67      50.25     244547
-— (label_id: 8)                                          0.00       0.00       0.00        104
-… (label_id: 9)                                         30.28      18.55      23.01      80900
--------------------
-micro avg                                               91.49      91.49      91.49   17504456
-macro avg                                               33.70      32.31      32.60   17504456
-weighted avg                                            91.74      91.49      91.57   17504456
-
--------------------
-15115059.00 12216.00 141023.00 17437.00 173399.00 1666.00 288.00 34646.00 16.00 30361.00
-18940.00 49396.00 30708.00 1786.00 73161.00 30.00 71.00 12294.00  5.00 5543.00
-99964.00 23294.00 197010.00 1884.00 125870.00 66.00 62.00 23236.00 14.00 10679.00
-20270.00 578.00 1102.00 12960.00 2738.00 91.00  3.00 359.00  1.00 525.00
-302097.00 40388.00 88944.00 3818.00 518342.00 479.00 503.00 64641.00 39.00 16385.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-28403.00 4694.00 13696.00 682.00 23771.00 29.00 31.00 106791.00 16.00 2399.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-10097.00 854.00 9497.00 5646.00 5847.00 11.00  3.00 2580.00 13.00 15008.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         41.85      29.38      34.53      72939
-1 (label_id: 1)                                         45.59      59.17      51.50      72939
--------------------
-micro avg                                               44.28      44.28      44.28     145878
-macro avg                                               43.72      44.28      43.01     145878
-weighted avg                                            43.72      44.28      43.01     145878
-
--------------------
-21432.00 29781.00
-51507.00 43158.00
-[INFO] - Epoch 11, global step 63059: val_loss reached 2.41300 (best 1.42583), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=2.41-epoch=11.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.07      97.32      97.19   15591873
-! (label_id: 1)                                         26.18      38.00      31.00     131789
-, (label_id: 2)                                         42.82      39.03      40.84     481674
-- (label_id: 3)                                         32.45      30.83      31.62      44417
-. (label_id: 4)                                         51.81      53.46      52.62     923721
-: (label_id: 5)                                          0.00       0.00       0.00       2422
-; (label_id: 6)                                          0.00       0.00       0.00       1006
-? (label_id: 7)                                         57.38      45.38      50.68     243807
-— (label_id: 8)                                          0.00       0.00       0.00         61
-… (label_id: 9)                                         29.65      19.02      23.18      80523
--------------------
-micro avg                                               91.68      91.68      91.68   17501292
-macro avg                                               33.74      32.30      32.71   17501292
-weighted avg                                            91.61      91.68      91.62   17501292
-
--------------------
-15173509.00 14387.00 154434.00 17388.00 199049.00 1721.00 381.00 38658.00 13.00 31666.00
-17137.00 50083.00 30642.00 1569.00 74329.00 20.00 56.00 12095.00  4.00 5344.00
-79851.00 21766.00 188006.00 1675.00 117410.00 58.00 68.00 20739.00  5.00 9469.00
-22355.00 678.00 1373.00 13693.00 3044.00 105.00  2.00 411.00  0.00 541.00
-260179.00 38582.00 81747.00 3604.00 493808.00 458.00 458.00 58609.00 27.00 15591.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-29082.00 5211.00 14735.00 714.00 29776.00 37.00 36.00 110642.00  6.00 2593.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-9760.00 1082.00 10737.00 5774.00 6305.00 23.00  5.00 2653.00  6.00 15319.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         86.91      21.94      35.03      72939
-1 (label_id: 1)                                         55.33      96.70      70.39      72939
--------------------
-micro avg                                               59.32      59.32      59.32     145878
-macro avg                                               71.12      59.32      52.71     145878
-weighted avg                                            71.12      59.32      52.71     145878
-
--------------------
-16000.00 2409.00
-56939.00 70530.00
-[INFO] - Epoch 12, global step 68314: val_loss reached 1.46759 (best 1.42583), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=1.47-epoch=12.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.34      97.00      97.17   15591908
-! (label_id: 1)                                         25.47      39.27      30.90     130685
-, (label_id: 2)                                         42.33      39.47      40.85     481540
-- (label_id: 3)                                         31.44      33.03      32.21      44397
-. (label_id: 4)                                         50.34      55.64      52.86     922617
-: (label_id: 5)                                          0.00       0.00       0.00       2534
-; (label_id: 6)                                          0.00       0.00       0.00        930
-? (label_id: 7)                                         57.08      45.35      50.54     243862
-— (label_id: 8)                                          0.00       0.00       0.00         65
-… (label_id: 9)                                         29.46      18.87      23.00      80856
--------------------
-micro avg                                               91.54      91.54      91.54   17499394
-macro avg                                               33.35      32.86      32.75   17499394
-weighted avg                                            91.75      91.54      91.61   17499394
-
--------------------
-15123679.00 12214.00 143310.00 16008.00 175119.00 1727.00 304.00 34081.00 14.00 30410.00
-19346.00 51319.00 31852.00 1697.00 78658.00 25.00 57.00 12781.00  4.00 5720.00
-85662.00 21875.00 190061.00 1723.00 118315.00 75.00 72.00 21412.00  5.00 9816.00
-25730.00 646.00 1407.00 14664.00 3050.00 126.00  3.00 428.00  0.00 594.00
-295302.00 38618.00 89306.00 3824.00 513363.00 517.00 469.00 61994.00 24.00 16457.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-31992.00 4982.00 14835.00 735.00 27929.00 40.00 24.00 110587.00 10.00 2603.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-10197.00 1031.00 10769.00 5746.00 6183.00 24.00  1.00 2579.00  8.00 15256.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         71.94       2.45       4.73      72939
-1 (label_id: 1)                                         50.38      99.05      66.79      72939
--------------------
-micro avg                                               50.75      50.75      50.75     145878
-macro avg                                               61.16      50.75      35.76     145878
-weighted avg                                            61.16      50.75      35.76     145878
-
--------------------
-1784.00 696.00
-71155.00 72243.00
-[INFO] - Epoch 13, step 73569: val_loss was not in top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.09      97.36      97.22   15597226
-! (label_id: 1)                                         25.63      39.75      31.16     132590
-, (label_id: 2)                                         42.62      39.40      40.95     481644
-- (label_id: 3)                                         34.03      30.76      32.31      44322
-. (label_id: 4)                                         51.84      53.45      52.63     923257
-: (label_id: 5)                                          0.00       0.00       0.00       2342
-; (label_id: 6)                                          0.00       0.00       0.00        985
-? (label_id: 7)                                         60.32      43.10      50.28     244099
-— (label_id: 8)                                          0.00       0.00       0.00         88
-… (label_id: 9)                                         30.58      19.02      23.45      80482
--------------------
-micro avg                                               91.71      91.71      91.71   17507036
-macro avg                                               34.21      32.28      32.80   17507036
-weighted avg                                            91.67      91.71      91.65   17507036
-
--------------------
-15184971.00 14014.00 154132.00 17469.00 196410.00 1641.00 357.00 39030.00 16.00 31639.00
-18872.00 52702.00 31873.00 1745.00 81378.00 29.00 62.00 13343.00  9.00 5620.00
-78356.00 22507.00 189766.00 1729.00 120592.00 64.00 67.00 22358.00  9.00 9828.00
-21010.00 555.00 1210.00 13634.00 2673.00 86.00  3.00 371.00  3.00 525.00
-258970.00 37317.00 81280.00 3439.00 493464.00 478.00 465.00 61116.00 29.00 15341.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-25956.00 4505.00 13012.00 614.00 22839.00 26.00 27.00 105203.00 10.00 2219.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-9091.00 990.00 10371.00 5692.00 5901.00 18.00  4.00 2678.00 12.00 15310.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         63.84       1.68       3.27      72939
-1 (label_id: 1)                                         50.18      99.05      66.62      72939
--------------------
-micro avg                                               50.36      50.36      50.36     145878
-macro avg                                               57.01      50.36      34.94     145878
-weighted avg                                            57.01      50.36      34.94     145878
-
--------------------
-1225.00 694.00
-71714.00 72245.00
-[INFO] - Epoch 14, step 78824: val_loss was not in top 3
-[INFO] - Saving latest checkpoint...
-[INFO] - Optimizer config = AdamW (
-Parameter Group 0
-    amsgrad: False
-    betas: (0.9, 0.999)
-    eps: 1e-08
-    lr: 1e-05
-    weight_decay: 0.0
-)
-[INFO] - Scheduler "<core.optim.lr_scheduler.CosineAnnealing object at 0x7f986558b6a0>" 
-will be used during training (effective maximum steps = 78825) - 
-Parameters : 
-(warmup_steps: null
-warmup_ratio: 0.1
-min_lr: 1.0e-10
-last_epoch: -1
-max_steps: 78825
-)
-[INFO] - 
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 108 M 
-1 | punct_classifier    | TokenClassifier      | 7.7 K 
-2 | domain_classifier   | SequenceClassifier   | 3.1 K 
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-7.1 M     Trainable params
-101 M     Non-trainable params
-108 M     Total params
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          96.71      97.78      97.24      17162
-! (label_id: 1)                                         16.79      37.93      23.28         58
-, (label_id: 2)                                         48.28      34.21      40.04        532
-- (label_id: 3)                                         27.42      62.96      38.20         27
-. (label_id: 4)                                         58.51      54.86      56.63       1028
-: (label_id: 5)                                          0.00       0.00       0.00          0
-; (label_id: 6)                                          0.00       0.00       0.00          4
-? (label_id: 7)                                         67.29      49.66      57.14        145
-— (label_id: 8)                                          0.00       0.00       0.00          0
-… (label_id: 9)                                         48.72      25.33      33.33         75
--------------------
-micro avg                                               92.78      92.78      92.78      19031
-macro avg                                               45.46      45.34      43.23      19031
-weighted avg                                            92.52      92.78      92.56      19031
-
--------------------
-16781.00 13.00 226.00 10.00 266.00  0.00  1.00 24.00  0.00 30.00
-18.00 22.00 18.00  0.00 68.00  0.00  0.00  3.00  0.00  2.00
-60.00  6.00 182.00  0.00 110.00  0.00  0.00 12.00  0.00  7.00
-40.00  0.00  2.00 17.00  2.00  0.00  0.00  0.00  0.00  1.00
-242.00 16.00 90.00  0.00 564.00  0.00  3.00 33.00  0.00 16.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-17.00  1.00  6.00  0.00 11.00  0.00  0.00 72.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 4.00  0.00  8.00  0.00  7.00  0.00  0.00  1.00  0.00 19.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         50.00       1.25       2.44         80
-1 (label_id: 1)                                         50.00      98.75      66.39         80
--------------------
-micro avg                                               50.00      50.00      50.00        160
-macro avg                                               50.00      50.00      34.41        160
-weighted avg                                            50.00      50.00      34.41        160
-
--------------------
- 1.00  1.00
-79.00 79.00
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.39      97.17      97.28   15595410
-! (label_id: 1)                                         28.19      34.91      31.19     132074
-, (label_id: 2)                                         41.36      43.02      42.17     481066
-- (label_id: 3)                                         33.96      33.33      33.64      44642
-. (label_id: 4)                                         52.15      56.35      54.17     922110
-: (label_id: 5)                                          0.00       0.00       0.00       2427
-; (label_id: 6)                                          0.00       0.00       0.00        951
-? (label_id: 7)                                         60.50      47.70      53.34     244079
-— (label_id: 8)                                          0.00       0.00       0.00         81
-… (label_id: 9)                                         31.48      18.31      23.15      80692
--------------------
-micro avg                                               91.82      91.82      91.82   17503532
-macro avg                                               34.50      33.08      33.49   17503532
-weighted avg                                            91.94      91.82      91.86   17503532
-
--------------------
-15153604.00 13605.00 143075.00 16589.00 167391.00 1700.00 303.00 33023.00 15.00 30861.00
-12592.00 46106.00 24459.00 1227.00 64687.00 23.00 49.00 9648.00  5.00 4766.00
-89886.00 26855.00 206948.00 2016.00 138223.00 80.00 78.00 24988.00  9.00 11297.00
-22160.00 907.00 1680.00 14879.00 3008.00 91.00  3.00 449.00  2.00 638.00
-279142.00 38770.00 81261.00 3511.00 519575.00 468.00 483.00 57239.00 31.00 15764.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-29800.00 5008.00 13998.00 771.00 23761.00 33.00 32.00 116430.00 11.00 2595.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-8226.00 823.00 9645.00 5649.00 5465.00 32.00  3.00 2302.00  8.00 14771.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                          5.26       0.45       0.82      72939
-1 (label_id: 1)                                         48.02      91.96      63.09      72939
--------------------
-micro avg                                               46.20      46.20      46.20     145878
-macro avg                                               26.64      46.20      31.96     145878
-weighted avg                                            26.64      46.20      31.96     145878
-
--------------------
-326.00 5866.00
-72613.00 67073.00
-[INFO] - Epoch 0, step 84079: val_loss was not in top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.92      95.26      96.57   15594360
-! (label_id: 1)                                         17.91      35.19      23.74     131487
-, (label_id: 2)                                         28.65      42.70      34.29     481266
-- (label_id: 3)                                         14.54      30.94      19.78      44079
-. (label_id: 4)                                         55.51      53.23      54.34     922224
-: (label_id: 5)                                          0.00       0.00       0.00       2409
-; (label_id: 6)                                          0.00       0.00       0.00        979
-? (label_id: 7)                                         43.61      54.75      48.55     244473
-— (label_id: 8)                                          0.00       0.00       0.00         80
-… (label_id: 9)                                         20.27      17.51      18.79      80827
--------------------
-micro avg                                               90.05      90.05      90.05   17502184
-macro avg                                               27.84      32.96      29.61   17502184
-weighted avg                                            91.83      90.05      90.85   17502184
-
--------------------
-14855928.00 10170.00 111950.00 14205.00 130062.00 1066.00 295.00 22796.00  7.00 25164.00
-82603.00 46272.00 34372.00 2186.00 76741.00 25.00 33.00 10866.00 15.00 5191.00
-273561.00 30550.00 205493.00 3107.00 157176.00 296.00 66.00 31834.00  8.00 15231.00
-70375.00 707.00 2806.00 13640.00 4828.00 314.00  8.00 610.00  1.00 535.00
-205048.00 33726.00 94648.00 3428.00 490863.00 482.00 529.00 39792.00 25.00 15784.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-84788.00 8065.00 23551.00 1335.00 50355.00 186.00 28.00 133843.00 14.00 4769.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-22057.00 1997.00 8446.00 6178.00 12199.00 40.00 20.00 4732.00 10.00 14153.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                          7.53       7.44       7.49      72939
-1 (label_id: 1)                                          8.60       8.71       8.66      72939
--------------------
-micro avg                                                8.08       8.08       8.08     145878
-macro avg                                                8.07       8.08       8.07     145878
-weighted avg                                             8.07       8.08       8.07     145878
-
--------------------
-5425.00 66584.00
-67514.00 6355.00
-[INFO] - Epoch 1, step 89334: val_loss was not in top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.02      98.24      97.63   15597741
-! (label_id: 1)                                         20.99      45.31      28.69     131817
-, (label_id: 2)                                         40.14      34.86      37.32     481164
-- (label_id: 3)                                         43.09      17.32      24.71      44240
-. (label_id: 4)                                         61.25      50.89      55.59     922389
-: (label_id: 5)                                          0.00       0.00       0.00       2305
-; (label_id: 6)                                          0.00       0.00       0.00        978
-? (label_id: 7)                                         63.22      46.46      53.56     244399
-— (label_id: 8)                                          0.00       0.00       0.00         85
-… (label_id: 9)                                         27.74      15.32      19.74      80604
--------------------
-micro avg                                               92.28      92.28      92.28   17505722
-macro avg                                               35.34      30.84      31.72   17505722
-weighted avg                                            92.05      92.28      92.06   17505722
-
--------------------
-15323837.00 14695.00 161781.00 21687.00 200957.00 1669.00 378.00 37495.00 14.00 32428.00
-26221.00 59731.00 51699.00 3402.00 115944.00 36.00 72.00 19538.00 15.00 7963.00
-84901.00 20182.00 167757.00 1944.00 107247.00 90.00 32.00 25166.00  6.00 10602.00
-5762.00 283.00 838.00 7661.00 2686.00 49.00  2.00 236.00  0.00 262.00
-123485.00 31817.00 79720.00 3232.00 469360.00 402.00 467.00 43223.00 27.00 14617.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-26782.00 3958.00 12540.00 625.00 19699.00 31.00 20.00 113550.00 12.00 2386.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-6753.00 1151.00 6829.00 5689.00 6496.00 28.00  7.00 5191.00 11.00 12346.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         10.09      10.82      10.45      72939
-1 (label_id: 1)                                          3.88       3.60       3.73      72939
--------------------
-micro avg                                                7.21       7.21       7.21     145878
-macro avg                                                6.99       7.21       7.09     145878
-weighted avg                                             6.99       7.21       7.09     145878
-
--------------------
-7895.00 70316.00
-65044.00 2623.00
-[INFO] - Epoch 2, step 94589: val_loss was not in top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.95      97.52      97.74   15593759
-! (label_id: 1)                                         20.95      45.08      28.61     131176
-, (label_id: 2)                                         38.74      39.92      39.32     481613
-- (label_id: 3)                                         43.67      27.95      34.08      44269
-. (label_id: 4)                                         55.08      56.18      55.63     922442
-: (label_id: 5)                                          0.00       0.00       0.00       2480
-; (label_id: 6)                                          0.00       0.00       0.00        986
-? (label_id: 7)                                         60.81      48.72      54.10     243336
-— (label_id: 8)                                          0.00       0.00       0.00         81
-… (label_id: 9)                                         33.55      13.49      19.25      80532
--------------------
-micro avg                                               92.11      92.11      92.11   17500674
-macro avg                                               35.08      32.89      32.87   17500674
-weighted avg                                            92.52      92.11      92.24   17500674
-
--------------------
-15207599.00 10190.00 102655.00 16019.00 134351.00 1457.00 338.00 26884.00 10.00 26177.00
-33849.00 59132.00 50816.00 3698.00 110059.00 60.00 88.00 16440.00 15.00 8038.00
-102611.00 23445.00 192273.00 2520.00 133218.00 175.00 41.00 29257.00 12.00 12820.00
-10356.00 357.00 1114.00 12371.00 3131.00 240.00  3.00 339.00  1.00 417.00
-200662.00 33019.00 115436.00 4009.00 518213.00 501.00 496.00 49211.00 21.00 19188.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-34219.00 4485.00 13831.00 833.00 19936.00 32.00 18.00 118557.00 15.00 3025.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-4463.00 548.00 5488.00 4819.00 3534.00 15.00  2.00 2648.00  7.00 10867.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         21.11      26.50      23.50      72939
-1 (label_id: 1)                                          1.29       0.96       1.10      72939
--------------------
-micro avg                                               13.73      13.73      13.73     145878
-macro avg                                               11.20      13.73      12.30     145878
-weighted avg                                            11.20      13.73      12.30     145878
-
--------------------
-19328.00 72238.00
-53611.00 701.00
-[INFO] - Epoch 3, step 99844: val_loss was not in top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.70      98.04      97.87   15589848
-! (label_id: 1)                                         24.24      41.95      30.72     132067
-, (label_id: 2)                                         43.34      37.75      40.35     480971
-- (label_id: 3)                                         44.07      30.28      35.90      44614
-. (label_id: 4)                                         57.37      57.22      57.29     922579
-: (label_id: 5)                                          0.00       0.00       0.00       2446
-; (label_id: 6)                                          0.00       0.00       0.00        960
-? (label_id: 7)                                         57.77      53.82      55.72     244526
-— (label_id: 8)                                          0.00       0.00       0.00         69
-… (label_id: 9)                                         36.35      13.58      19.77      81023
--------------------
-micro avg                                               92.60      92.60      92.60   17499104
-macro avg                                               36.08      33.26      33.76   17499104
-weighted avg                                            92.53      92.60      92.51   17499104
-
--------------------
-15283627.00 13754.00 117912.00 16808.00 150143.00 1647.00 296.00 29563.00 10.00 29008.00
-17067.00 55399.00 44496.00 2723.00 89952.00 29.00 58.00 12209.00  7.00 6642.00
-61418.00 19554.00 181551.00 1903.00 120943.00 118.00 46.00 22462.00  8.00 10921.00
-10384.00 521.00 1347.00 13510.00 3730.00 164.00  3.00 455.00  2.00 541.00
-174426.00 36380.00 110943.00 3884.00 527873.00 440.00 530.00 46316.00 29.00 19284.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-40045.00 5969.00 18547.00 964.00 26977.00 39.00 26.00 131602.00  6.00 3627.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-2881.00 490.00 6175.00 4822.00 2961.00  9.00  1.00 1919.00  7.00 11000.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         16.86      20.10      18.34      72939
-1 (label_id: 1)                                          1.13       0.92       1.01      72939
--------------------
-micro avg                                               10.51      10.51      10.51     145878
-macro avg                                                9.00      10.51       9.68     145878
-weighted avg                                             9.00      10.51       9.68     145878
-
--------------------
-14660.00 72271.00
-58279.00 668.00
-[INFO] - Epoch 4, step 105099: val_loss was not in top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.17      97.62      97.90   15593568
-! (label_id: 1)                                         22.12      48.12      30.30     131858
-, (label_id: 2)                                         44.34      37.75      40.78     480872
-- (label_id: 3)                                         43.06      32.84      37.26      44566
-. (label_id: 4)                                         55.06      58.47      56.71     923462
-: (label_id: 5)                                          0.00       0.00       0.00       2382
-; (label_id: 6)                                          0.00       0.00       0.00       1005
-? (label_id: 7)                                         63.02      51.31      56.56     243779
-— (label_id: 8)                                          0.00       0.00       0.00         77
-… (label_id: 9)                                         19.75      21.40      20.54      80824
--------------------
-micro avg                                               92.35      92.35      92.35   17502392
-macro avg                                               34.55      34.75      34.01   17502392
-weighted avg                                            92.84      92.35      92.54   17502392
-
--------------------
-15222205.00 9390.00 92612.00 13702.00 115656.00 1246.00 285.00 26185.00 11.00 24103.00
-28703.00 63449.00 50921.00 3425.00 114943.00 83.00 81.00 17024.00  9.00 8257.00
-64958.00 18494.00 181515.00 1514.00 111089.00 104.00 39.00 22098.00  8.00 9569.00
-13517.00 322.00 898.00 14636.00 3781.00 245.00  6.00 298.00  0.00 285.00
-213763.00 33412.00 125073.00 3673.00 539944.00 537.00 558.00 44686.00 28.00 18980.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-31404.00 4497.00 14406.00 600.00 20095.00 32.00 18.00 125075.00 13.00 2331.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-19018.00 2294.00 15447.00 7016.00 17954.00 135.00 18.00 8413.00  8.00 17299.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         15.20      17.79      16.39      72939
-1 (label_id: 1)                                          0.96       0.80       0.87      72939
--------------------
-micro avg                                                9.29       9.29       9.29     145878
-macro avg                                                8.08       9.29       8.63     145878
-weighted avg                                             8.08       9.29       8.63     145878
-
--------------------
-12973.00 72357.00
-59966.00 582.00
-[INFO] - Epoch 5, step 110354: val_loss was not in top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.84      96.08      97.44   15594783
-! (label_id: 1)                                         18.08      56.67      27.42     130591
-, (label_id: 2)                                         33.80      46.01      38.97     481352
-- (label_id: 3)                                         28.67      42.40      34.21      44046
-. (label_id: 4)                                         55.92      51.64      53.69     921748
-: (label_id: 5)                                          0.00       0.00       0.00       2368
-; (label_id: 6)                                          0.00       0.00       0.00        969
-? (label_id: 7)                                         51.11      60.70      55.49     244359
-— (label_id: 8)                                          0.00       0.00       0.00         77
-… (label_id: 9)                                         22.37      19.44      20.80      80560
--------------------
-micro avg                                               91.07      91.07      91.07   17500852
-macro avg                                               30.88      37.29      32.80   17500852
-weighted avg                                            92.97      91.07      91.89   17500852
-
--------------------
-14983972.00 3927.00 57121.00 8450.00 74979.00 810.00 179.00 12472.00  5.00 17956.00
-60244.00 74006.00 62097.00 3749.00 174614.00 72.00 148.00 23346.00 13.00 10935.00
-225529.00 21210.00 221452.00 2706.00 144463.00 209.00 75.00 24591.00  9.00 14901.00
-36136.00 516.00 2203.00 18674.00 5670.00 759.00  5.00 584.00  1.00 587.00
-194051.00 23490.00 105773.00 3206.00 475991.00 422.00 517.00 31372.00 25.00 16414.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-73783.00 6265.00 20792.00 1118.00 35722.00 52.00 34.00 148316.00 14.00 4106.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-21068.00 1177.00 11914.00 6143.00 10309.00 44.00 11.00 3678.00 10.00 15661.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         15.56      18.31      16.83      72939
-1 (label_id: 1)                                          0.80       0.66       0.72      72939
--------------------
-micro avg                                                9.49       9.49       9.49     145878
-macro avg                                                8.18       9.49       8.77     145878
-weighted avg                                             8.18       9.49       8.77     145878
-
--------------------
-13356.00 72458.00
-59583.00 481.00
-[INFO] - Epoch 6, step 115609: val_loss was not in top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.84      98.56      98.20   15588559
-! (label_id: 1)                                         22.56      49.16      30.92     131502
-, (label_id: 2)                                         47.40      38.09      42.24     481656
-- (label_id: 3)                                         48.52      31.28      38.04      44145
-. (label_id: 4)                                         63.17      54.84      58.71     922262
-: (label_id: 5)                                          0.00       0.00       0.00       2425
-; (label_id: 6)                                          0.00       0.00       0.00        967
-? (label_id: 7)                                         60.41      58.55      59.47     244604
-— (label_id: 8)                                          0.00       0.00       0.00         55
-… (label_id: 9)                                         27.95      18.94      22.58      80592
--------------------
-micro avg                                               93.10      93.10      93.10   17496768
-macro avg                                               36.78      34.94      35.02   17496768
-weighted avg                                            93.07      93.10      93.01   17496768
-
--------------------
-15363700.00 10827.00 115291.00 15209.00 140500.00 1534.00 310.00 27581.00  7.00 27376.00
-24887.00 64647.00 48625.00 3075.00 120595.00 46.00 88.00 16385.00 12.00 8239.00
-46785.00 18261.00 183441.00 1550.00 108830.00 82.00 34.00 18705.00  5.00 9315.00
-9876.00 241.00 663.00 13810.00 3212.00 214.00  3.00 233.00  0.00 212.00
-105320.00 29909.00 103487.00 3280.00 505781.00 468.00 494.00 35314.00 15.00 16603.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-30233.00 6302.00 18348.00 924.00 34376.00 42.00 29.00 143221.00  7.00 3581.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-7758.00 1315.00 11801.00 6297.00 8968.00 39.00  9.00 3165.00  9.00 15266.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         16.55      19.76      18.02      72939
-1 (label_id: 1)                                          0.48       0.38       0.43      72939
--------------------
-micro avg                                               10.07      10.07      10.07     145878
-macro avg                                                8.52      10.07       9.22     145878
-weighted avg                                             8.52      10.07       9.22     145878
-
--------------------
-14414.00 72659.00
-58525.00 280.00
-[INFO] - Epoch 7, step 120864: val_loss was not in top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.99      98.46      98.23   15595430
-! (label_id: 1)                                         23.10      46.84      30.94     131969
-, (label_id: 2)                                         43.19      45.43      44.28     481215
-- (label_id: 3)                                         46.48      33.03      38.62      43898
-. (label_id: 4)                                         63.78      53.14      57.98     923479
-: (label_id: 5)                                          0.00       0.00       0.00       2496
-; (label_id: 6)                                          0.00       0.00       0.00        924
-? (label_id: 7)                                         65.64      54.92      59.81     245290
-— (label_id: 8)                                          0.00       0.00       0.00         89
-… (label_id: 9)                                         27.64      19.01      22.53      81055
--------------------
-micro avg                                               93.06      93.06      93.06   17505844
-macro avg                                               36.78      35.08      35.24   17505844
-weighted avg                                            93.19      93.06      93.06   17505844
-
--------------------
-15355700.00 9450.00 105852.00 14118.00 130830.00 1598.00 285.00 26119.00  6.00 26430.00
-24567.00 61814.00 40826.00 2667.00 113801.00 47.00 66.00 16204.00 15.00 7634.00
-68258.00 24684.00 218626.00 2360.00 152616.00 151.00 92.00 26878.00 13.00 12537.00
-11430.00 362.00 751.00 14498.00 3356.00 179.00  3.00 308.00  2.00 302.00
-101416.00 29130.00 90326.00 3203.00 490780.00 462.00 447.00 37770.00 25.00 15877.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-25576.00 5023.00 13544.00 813.00 22625.00 28.00 24.00 134721.00 13.00 2866.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-8483.00 1506.00 11290.00 6239.00 9471.00 31.00  7.00 3290.00 15.00 15409.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         20.64      25.94      22.99      72939
-1 (label_id: 1)                                          0.34       0.25       0.29      72939
--------------------
-micro avg                                               13.10      13.10      13.10     145878
-macro avg                                               10.49      13.10      11.64     145878
-weighted avg                                            10.49      13.10      11.64     145878
-
--------------------
-18922.00 72757.00
-54017.00 182.00
-[INFO] - Epoch 8, step 126119: val_loss was not in top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.94      98.53      98.23   15591410
-! (label_id: 1)                                         24.06      46.66      31.75     131931
-, (label_id: 2)                                         49.32      37.33      42.50     481727
-- (label_id: 3)                                         43.70      34.83      38.76      44198
-. (label_id: 4)                                         62.53      58.59      60.49     922501
-: (label_id: 5)                                          0.00       0.00       0.00       2371
-; (label_id: 6)                                          0.00       0.00       0.00       1036
-? (label_id: 7)                                         61.09      58.69      59.87     244078
-— (label_id: 8)                                          0.00       0.00       0.00         71
-… (label_id: 9)                                         26.14      19.58      22.39      80978
--------------------
-micro avg                                               93.25      93.25      93.25   17500300
-macro avg                                               36.48      35.42      35.40   17500300
-weighted avg                                            93.18      93.25      93.15   17500300
-
--------------------
-15362008.00 9639.00 110928.00 13743.00 134211.00 1512.00 339.00 25288.00  6.00 27385.00
-23206.00 61564.00 46051.00 2681.00 100949.00 44.00 65.00 14069.00 13.00 7256.00
-39147.00 16589.00 179838.00 1474.00 101530.00 65.00 42.00 17398.00  5.00 8516.00
-14121.00 372.00 925.00 15393.00 3631.00 199.00  3.00 266.00  1.00 314.00
-112016.00 36149.00 112946.00 3724.00 540471.00 482.00 541.00 39710.00 27.00 18309.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-32320.00 5756.00 18159.00 891.00 30674.00 36.00 36.00 143252.00 13.00 3344.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-8592.00 1862.00 12880.00 6292.00 11035.00 33.00 10.00 4095.00  6.00 15854.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         24.33      32.11      27.68      72939
-1 (label_id: 1)                                          0.20       0.14       0.16      72939
--------------------
-micro avg                                               16.12      16.12      16.12     145878
-macro avg                                               12.27      16.12      13.92     145878
-weighted avg                                            12.27      16.12      13.92     145878
-
--------------------
-23420.00 72839.00
-49519.00 100.00
-[INFO] - Epoch 9, step 131374: val_loss was not in top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.44      97.72      98.08   15591717
-! (label_id: 1)                                         20.55      54.09      29.78     130786
-, (label_id: 2)                                         44.01      41.72      42.83     482370
-- (label_id: 3)                                         34.13      40.61      37.09      44598
-. (label_id: 4)                                         59.85      56.93      58.36     923093
-: (label_id: 5)                                          0.00       0.00       0.00       2420
-; (label_id: 6)                                          0.00       0.00       0.00        948
-? (label_id: 7)                                         62.19      57.23      59.61     244470
-— (label_id: 8)                                          0.00       0.00       0.00         81
-… (label_id: 9)                                         24.44      20.13      22.08      80366
--------------------
-micro avg                                               92.61      92.61      92.61   17500848
-macro avg                                               34.36      36.84      34.78   17500848
-weighted avg                                            93.30      92.61      92.89   17500848
-
--------------------
-15235982.00 6213.00 80367.00 10816.00 100648.00 1281.00 233.00 18905.00  3.00 22591.00
-40777.00 70740.00 57630.00 2849.00 141966.00 60.00 105.00 20786.00 14.00 9359.00
-92363.00 16479.00 201233.00 1873.00 114148.00 148.00 56.00 20298.00  8.00 10665.00
-25575.00 810.00 2308.00 18112.00 4874.00 353.00  7.00 470.00  2.00 554.00
-148456.00 29918.00 111335.00 3753.00 525535.00 517.00 513.00 39888.00 30.00 18073.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-35394.00 5004.00 15644.00 793.00 25229.00 31.00 26.00 139916.00 10.00 2949.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-13170.00 1622.00 13853.00 6402.00 10693.00 30.00  8.00 4207.00 14.00 16175.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         27.32      37.57      31.64      72939
-1 (label_id: 1)                                          0.11       0.07       0.09      72939
--------------------
-micro avg                                               18.82      18.82      18.82     145878
-macro avg                                               13.72      18.82      15.86     145878
-weighted avg                                            13.72      18.82      15.86     145878
-
--------------------
-27401.00 72888.00
-45538.00 51.00
-[INFO] - Epoch 10, step 136629: val_loss was not in top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.11      98.35      98.23   15594830
-! (label_id: 1)                                         25.43      42.73      31.89     131420
-, (label_id: 2)                                         44.38      45.34      44.86     481980
-- (label_id: 3)                                         42.86      36.32      39.32      44213
-. (label_id: 4)                                         63.26      57.61      60.30     923128
-: (label_id: 5)                                          0.00       0.00       0.00       2372
-; (label_id: 6)                                          0.00       0.00       0.00        961
-? (label_id: 7)                                         63.10      57.52      60.18     244547
-— (label_id: 8)                                          0.00       0.00       0.00        104
-… (label_id: 9)                                         27.33      19.51      22.77      80900
--------------------
-micro avg                                               93.21      93.21      93.21   17504456
-macro avg                                               36.45      35.74      35.75   17504456
-weighted avg                                            93.27      93.21      93.21   17504456
-
--------------------
-15336956.00 8577.00 99316.00 13261.00 123682.00 1554.00 286.00 23485.00  6.00 25353.00
-20093.00 56153.00 37298.00 2147.00 86370.00 22.00 59.00 12265.00 13.00 6370.00
-72365.00 22320.00 218540.00 2090.00 140328.00 138.00 80.00 24467.00 16.00 12100.00
-14844.00 599.00 1192.00 16058.00 3815.00 171.00  6.00 363.00  0.00 421.00
-111960.00 36885.00 97988.00 3568.00 531808.00 443.00 488.00 39765.00 39.00 17661.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-30204.00 5360.00 15524.00 858.00 27026.00 29.00 34.00 140652.00 15.00 3208.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-8408.00 1526.00 12122.00 6231.00 10099.00 15.00  8.00 3550.00 15.00 15787.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         28.55      39.93      33.30      72939
-1 (label_id: 1)                                          0.12       0.07       0.09      72939
--------------------
-micro avg                                               20.00      20.00      20.00     145878
-macro avg                                               14.34      20.00      16.69     145878
-weighted avg                                            14.34      20.00      16.69     145878
-
--------------------
-29128.00 72887.00
-43811.00 52.00
-[INFO] - Epoch 11, step 141884: val_loss was not in top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.14      98.26      98.20   15591873
-! (label_id: 1)                                         27.00      40.46      32.39     131789
-, (label_id: 2)                                         42.91      47.33      45.01     481674
-- (label_id: 3)                                         41.39      36.84      38.98      44417
-. (label_id: 4)                                         63.16      57.69      60.30     923721
-: (label_id: 5)                                          0.00       0.00       0.00       2422
-; (label_id: 6)                                          0.00       0.00       0.00       1006
-? (label_id: 7)                                         63.45      57.07      60.09     243807
-— (label_id: 8)                                          0.00       0.00       0.00         61
-… (label_id: 9)                                         26.89      19.61      22.68      80523
--------------------
-micro avg                                               93.17      93.17      93.17   17501292
-macro avg                                               36.29      35.73      35.77   17501292
-weighted avg                                            93.26      93.17      93.19   17501292
-
--------------------
-15320596.00 8610.00 96014.00 13004.00 122957.00 1588.00 309.00 23152.00  6.00 24992.00
-15923.00 53328.00 31740.00 1869.00 78003.00 23.00 54.00 10890.00  6.00 5639.00
-86347.00 25199.00 227969.00 2434.00 149713.00 159.00 104.00 26356.00  9.00 13034.00
-16647.00 572.00 1140.00 16365.00 3856.00 171.00  2.00 384.00  0.00 405.00
-113458.00 36968.00 98010.00 3572.00 532918.00 423.00 499.00 40212.00 26.00 17629.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-29792.00 5412.00 14842.00 859.00 26138.00 32.00 28.00 139144.00  7.00 3036.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-9110.00 1700.00 11959.00 6314.00 10136.00 26.00 10.00 3669.00  7.00 15788.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         30.13      43.10      35.46      72939
-1 (label_id: 1)                                          0.06       0.03       0.04      72939
--------------------
-micro avg                                               21.57      21.57      21.57     145878
-macro avg                                               15.09      21.57      17.75     145878
-weighted avg                                            15.09      21.57      17.75     145878
-
--------------------
-31438.00 72914.00
-41501.00 25.00
-[INFO] - Epoch 12, step 147139: val_loss was not in top 3
-                      94.69      94.34      94.49   17501292
-
--------------------
-15334518.00 4758.00 57251.00 9416.00 66429.00 350.00 208.00 15293.00  4.00 19900.00
-12750.00 62757.00 27470.00 1275.00 77140.00 24.00 46.00 9428.00  6.00 5370.00
-98516.00 19069.00 291973.00 2547.00 122298.00 213.00 138.00 19680.00  6.00 15415.00
-13509.00 523.00 1345.00 21526.00 2025.00 46.00  3.00 386.00  3.00 1977.00
-99622.00 37679.00 74500.00 3702.00 618129.00 433.00 559.00 33916.00 22.00 14850.00
-109.00  0.00 74.00 12.00  3.00 1261.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-18822.00 5157.00 13773.00 772.00 25232.00 36.00 32.00 161153.00 12.00 2771.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-14027.00 1846.00 15288.00 5167.00 12465.00 59.00 20.00 3951.00  8.00 20240.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         96.84      92.16      94.44      72939
-1 (label_id: 1)                                         92.52      96.99      94.70      72939
--------------------
-micro avg                                               94.58      94.58      94.58     145878
-macro avg                                               94.68      94.58      94.57     145878
-weighted avg                                            94.68      94.58      94.57     145878
-
--------------------
-67220.00 2194.00
-5719.00 70745.00
-[INFO] - Epoch 12, step 147139: val_loss was not in top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.84      98.40      98.62   15591908
-! (label_id: 1)                                         31.91      47.67      38.23     130685
-, (label_id: 2)                                         51.56      60.20      55.55     481540
-- (label_id: 3)                                         52.01      48.06      49.96      44397
-. (label_id: 4)                                         70.24      66.62      68.38     922617
-: (label_id: 5)                                         84.47      51.30      63.84       2534
-; (label_id: 6)                                          0.00       0.00       0.00        930
-? (label_id: 7)                                         70.99      65.97      68.39     243862
-— (label_id: 8)                                          0.00       0.00       0.00         65
-… (label_id: 9)                                         27.02      25.20      26.08      80856
--------------------
-micro avg                                               94.36      94.36      94.36   17499394
-macro avg                                               48.71      46.34      46.90   17499394
-weighted avg                                            94.69      94.36      94.50   17499394
-
--------------------
-15342330.00 4767.00 59799.00 9588.00 69214.00 356.00 194.00 15719.00  6.00 20399.00
-12509.00 62293.00 26792.00 1229.00 77470.00 25.00 41.00 9373.00  4.00 5481.00
-94804.00 19077.00 289890.00 2522.00 120782.00 245.00 121.00 19445.00  6.00 15299.00
-13537.00 551.00 1397.00 21338.00 1987.00 49.00  6.00 355.00  3.00 1806.00
-96009.00 36888.00 74139.00 3639.00 614654.00 455.00 530.00 33991.00 25.00 14712.00
-114.00  0.00 94.00 24.00  5.00 1300.00  0.00  0.00  0.00  2.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-18414.00 5109.00 13592.00 756.00 25012.00 31.00 25.00 160872.00 10.00 2781.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-14191.00 2000.00 15837.00 5301.00 13493.00 73.00 13.00 4107.00 11.00 20376.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         97.15      91.52      94.25      72939
-1 (label_id: 1)                                         91.99      97.31      94.57      72939
--------------------
-micro avg                                               94.42      94.42      94.42     145878
-macro avg                                               94.57      94.42      94.41     145878
-weighted avg                                            94.57      94.42      94.41     145878
-
--------------------
-66755.00 1961.00
-6184.00 70978.00
-[INFO] - Epoch 13, step 152394: val_loss was not in top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.82      98.43      98.63   15597226
-! (label_id: 1)                                         31.63      48.68      38.35     132590
-, (label_id: 2)                                         51.66      59.85      55.45     481644
-- (label_id: 3)                                         52.58      48.16      50.27      44322
-. (label_id: 4)                                         70.46      66.07      68.19     923257
-: (label_id: 5)                                         83.83      48.93      61.80       2342
-; (label_id: 6)                                          0.00       0.00       0.00        985
-? (label_id: 7)                                         71.22      65.92      68.47     244099
-— (label_id: 8)                                          0.00       0.00       0.00         88
-… (label_id: 9)                                         27.14      25.85      26.48      80482
--------------------
-micro avg                                               94.36      94.36      94.36   17507036
-macro avg                                               48.73      46.19      46.76   17507036
-weighted avg                                            94.68      94.36      94.49   17507036
-
--------------------
-15351999.00 4842.00 61083.00 9659.00 70548.00 378.00 217.00 15763.00  5.00 20193.00
-13202.00 64547.00 27664.00 1251.00 81976.00 29.00 47.00 9879.00 10.00 5468.00
-93082.00 18994.00 288255.00 2478.00 120173.00 212.00 111.00 19571.00 10.00 15132.00
-13266.00 561.00 1346.00 21345.00 1904.00 44.00  6.00 347.00  3.00 1776.00
-93175.00 36504.00 73745.00 3501.00 609979.00 435.00 564.00 33338.00 31.00 14440.00
-115.00  0.00 72.00 28.00  5.00 1146.00  0.00  0.00  0.00  1.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-18176.00 5171.00 13442.00 733.00 24771.00 29.00 25.00 160905.00 11.00 2668.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-14211.00 1971.00 16037.00 5327.00 13901.00 69.00 15.00 4296.00 18.00 20804.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         97.23      91.25      94.14      72939
-1 (label_id: 1)                                         91.75      97.40      94.49      72939
--------------------
-micro avg                                               94.32      94.32      94.32     145878
-macro avg                                               94.49      94.32      94.32     145878
-weighted avg                                            94.49      94.32      94.32     145878
-
--------------------
-66554.00 1898.00
-6385.00 71041.00
-[INFO] - Epoch 14, step 157649: val_loss was not in top 3
 [INFO] - GPU available: True, used: True
 [INFO] - TPU available: None, using: 0 TPU cores
-[INFO] - Using environment variable NODE_RANK for node rank (0).
 [INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          86.89      99.94      92.96     371748
-! (label_id: 1)                                         23.81      12.82      16.67        156
-, (label_id: 2)                                         68.27       2.50       4.81      27252
-- (label_id: 3)                                         55.56       2.06       3.98       1940
-. (label_id: 4)                                         80.15       5.20       9.77      25152
-: (label_id: 5)                                        100.00       1.55       3.05        516
-; (label_id: 6)                                          0.00       0.00       0.00        328
-? (label_id: 7)                                         81.43      11.63      20.36       1960
-— (label_id: 8)                                          0.00       0.00       0.00       1596
-… (label_id: 9)                                         11.11       7.69       9.09        104
--------------------
-micro avg                                               86.78      86.78      86.78     430752
-macro avg                                               50.72      14.34      16.07     430752
-weighted avg                                            84.73      86.78      81.22     430752
-
--------------------
-371528.00 108.00 26340.00 1900.00 23644.00 492.00 312.00 1676.00 1524.00 84.00
- 0.00 20.00 16.00  0.00 44.00  0.00  0.00  4.00  0.00  0.00
-132.00  8.00 680.00  0.00 132.00  0.00  4.00 24.00 12.00  4.00
-28.00  0.00  4.00 40.00  0.00  0.00  0.00  0.00  0.00  0.00
-40.00 20.00 184.00  0.00 1308.00 12.00 12.00 24.00 28.00  4.00
- 0.00  0.00  0.00  0.00  0.00  8.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
- 8.00  0.00 12.00  0.00 20.00  0.00  0.00 228.00  8.00  4.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-12.00  0.00 16.00  0.00  4.00  4.00  0.00  4.00 24.00  8.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00       2.11       4.14       3596
-1 (label_id: 1)                                          0.00       0.00       0.00          0
--------------------
-micro avg                                                2.11       2.11       2.11       3596
-macro avg                                              100.00       2.11       4.14       3596
-weighted avg                                           100.00       2.11       4.14       3596
-
--------------------
-76.00  0.00
-3520.00  0.00
-[INFO] - Internal process exited
+[INFO] - Using native 16bit precision.
diff --git a/experiment/main.py b/experiment/main.py
index 7f97cb4..da25a34 100644
--- a/experiment/main.py
+++ b/experiment/main.py
@@ -35,7 +35,7 @@ def main(cfg: DictConfig)->None:
 
     pp(cfg)
     pl.seed_everything(cfg.seed)
-    trainer = pl.Trainer(**cfg.trainer,track_grad_norm=2)
+    trainer = pl.Trainer(**cfg.trainer) #,track_grad_norm=2
     exp_manager(trainer, cfg.exp_manager)
     model = PunctuationDomainModel(cfg=cfg, trainer=trainer, data_id = data_id)
     
@@ -56,17 +56,17 @@ def main(cfg: DictConfig)->None:
     #         tmp_path=cfg.tmp_path,
     #         test_unlabelled=False,
     #     )
+
     lrs=[1e-2,1e-5] if cfg.model.frozen_lr is None else list(cfg.model.frozen_lr)
     while(model.hparams.model.unfrozen<=cfg.model.maximum_unfrozen and model.hparams.model.unfrozen>=0):
         # trainer.current_epoch=0
-        # lr_finder = trainer.tuner.lr_find(model,datamodule=lr_finder_dm,min_lr=1e-8, max_lr=0.5, num_training=80) #, early_stop_threshold=None
+        # lr_finder = trainer.tuner.lr_find(model,min_lr=1e-8, max_lr=0.5, num_training=80) #, early_stop_threshold=None
         # # Results can be found in
         # pp(lr_finder.results)
         # new_lr = lr_finder.suggestion()
         # model.hparams.model.optim.lr = new_lr
-        # lr_finder_dm.reset()
         # model.dm.reset()
-        model.hparams.model.optim.lr = lrs[model.hparams.model.unfrozen]
+        model.hparams.model.optim.lr = lrs.pop(0)
         trainer.current_epoch=0
         trainer.fit(model)
         try:
diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
index 8cdfb20..cd564f4 100644
--- a/experiment/models/punctuation_domain_model.py
+++ b/experiment/models/punctuation_domain_model.py
@@ -59,6 +59,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
                               for _ in enumerate(sorted(self.hparams.model.punct_label_ids))}
         self.labels_to_ids = {_[1]: _[0]
                               for _ in enumerate(sorted(self.hparams.model.punct_label_ids))}
+        self.label_map=pp({k:v for k,v in self._cfg.model.label_map.items()})
         self.data_id=data_id
         self.setup_datamodule()
 
@@ -513,6 +514,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
             labelled= list(data_config.labelled),
             unlabelled= list(data_config.unlabelled),
             punct_label_ids= labels_to_ids,
+            label_map=self.label_map,
             train_batch_size= data_config.train_ds.batch_size,
             max_seq_length= data_config.max_seq_length,
             val_batch_size= data_config.validation_ds.batch_size,
@@ -743,6 +745,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
             queries=queries, 
             max_seq_length=self.hparams.model.dataset.max_seq_length,
             punct_label_ids=self.labels_to_ids,
+            label_map=self.label_map,
             attach_label_to_end=self._cfg.model.dataset.attach_label_to_end)
         batch=ds[0]
         attention_mask = batch['attention_mask']
