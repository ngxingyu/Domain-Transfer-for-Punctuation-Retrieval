commit hash: 540fe2fe9682cb0d2ee24a738c5e5c4464082a2f
diff --git a/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/events.out.tfevents.1613452235.Titan.28818.0 b/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/events.out.tfevents.1613452235.Titan.28818.0
deleted file mode 100644
index cdb7965..0000000
Binary files a/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/events.out.tfevents.1613452235.Titan.28818.0 and /dev/null differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/git-info.log
deleted file mode 100644
index 9c1fa8b..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/git-info.log
+++ /dev/null
@@ -1,7565 +0,0 @@
-commit hash: c523cbaa0b6d3efca5ac9fbc9e89b8678b7e7788
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/cmd-args.log
-deleted file mode 100644
-index 11a5d8e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/cmd-args.log
-+++ /dev/null
-@@ -1 +0,0 @@
--main.py
-\ No newline at end of file
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0
-deleted file mode 100644
-index 53377de..0000000
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 and /dev/null differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/git-info.log
-deleted file mode 100644
-index 3a42620..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/git-info.log
-+++ /dev/null
-@@ -1,3780 +0,0 @@
--commit hash: 4cccf1ef224a31f16ab4158a6dbd1472d2705371
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/cmd-args.log
--deleted file mode 100644
--index 11a5d8e..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/cmd-args.log
--+++ /dev/null
--@@ -1 +0,0 @@
---main.py
--\ No newline at end of file
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/events.out.tfevents.1612741439.Titan.4686.0 b/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/events.out.tfevents.1612741439.Titan.4686.0
--deleted file mode 100644
--index dc6761d..0000000
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/events.out.tfevents.1612741439.Titan.4686.0 and /dev/null differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/git-info.log
--deleted file mode 100644
--index 83e328a..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/git-info.log
--+++ /dev/null
--@@ -1,406 +0,0 @@
---commit hash: 939a671c8c117db6975316767ced5d95449e2b27
---diff --git a/experiment/config.yaml b/experiment/config.yaml
---index fe58670..1442409 100644
------ a/experiment/config.yaml
---+++ b/experiment/config.yaml
---@@ -1,47 +1,47 @@
--- seed: 42
--- trainer:
----    # gpus: 1 # the number of gpus, 0 for CPU
----    # num_nodes: 1
----    # max_epochs: 2
----    # max_steps: null # precedence over max_epochs
----    # accumulate_grad_batches: 4 # accumulates grads every k batches
----    # gradient_clip_val: 0
----    # amp_level: O1 # O1/O2 for mixed precision
----    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----    # accelerator: ddp
----    # checkpoint_callback: false  # Provided by exp_manager
----    # logger: false #false  # Provided by exp_manager
----    # log_every_n_steps: 1  # Interval of logging.
----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----    # resume_from_checkpoint: null
----
----    gpus: 0 # the number of gpus, 0 for CPU
---+    gpus: 1 # the number of gpus, 0 for CPU
---     num_nodes: 1
----    max_epochs: 8
---+    max_epochs: 2
---     max_steps: null # precedence over max_epochs
----    accumulate_grad_batches: 1 # accumulates grads every k batches
---+    accumulate_grad_batches: 4 # accumulates grads every k batches
---     gradient_clip_val: 0
----    amp_level: O0 # O1/O2 for mixed precision
----    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----    # accelerator: ddp
---+    amp_level: O1 # O1/O2 for mixed precision
---+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
---+    accelerator: ddp
---     checkpoint_callback: false  # Provided by exp_manager
---     logger: false #false  # Provided by exp_manager
---     log_every_n_steps: 1  # Interval of logging.
---     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----    reload_dataloaders_every_epoch: true
---     resume_from_checkpoint: null
--- 
---+    # gpus: 0 # the number of gpus, 0 for CPU
---+    # num_nodes: 1
---+    # max_epochs: 8
---+    # max_steps: null # precedence over max_epochs
---+    # accumulate_grad_batches: 1 # accumulates grads every k batches
---+    # gradient_clip_val: 0
---+    # amp_level: O0 # O1/O2 for mixed precision
---+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
---+    # # accelerator: ddp
---+    # checkpoint_callback: false  # Provided by exp_manager
---+    # logger: false #false  # Provided by exp_manager
---+    # log_every_n_steps: 1  # Interval of logging.
---+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
---+    # reload_dataloaders_every_epoch: true
---+    # resume_from_checkpoint: null
---+
--- exp_manager:
----    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
---+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
---     name: Punctuation_with_Domain_discriminator  # The name of your model
---     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
---     create_checkpoint_callback: true 
----base_path: /home/nxingyu2/data # /root/data # 
----tmp_path: /home/nxingyu2/data/tmp # /tmp # 
---+base_path: /home/nxingyu/data # /root/data # 
---+tmp_path: /home/nxingyu/data/tmp # /tmp # 
--- 
--- model:
---     nemo_path: null
----    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
---+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
---     unfrozen: 0
---     maximum_unfrozen: 2
---     unfreeze_step: 1
---@@ -58,10 +58,10 @@ model:
---         - "—"
---         - "…"
--- 
----    punct_class_weights: true
---+    punct_class_weights: false
---     
---     dataset:
----        data_dir: /home/nxingyu2/data # /root/data # 
---+        data_dir: /home/nxingyu/data # /root/data # 
---         labelled:
---             - ${base_path}/ted_talks_processed #
---         unlabelled:
---@@ -132,7 +132,7 @@ model:
---         gamma: 5
--- 
---     optim:
----        name: novograd
---+        name: adam
---         lr: 1e-3
---         weight_decay: 0.00
---         sched:
---diff --git a/experiment/info.log b/experiment/info.log
---index 2471fe9..e69de29 100644
------ a/experiment/info.log
---+++ b/experiment/info.log
---@@ -1,300 +0,0 @@
----[INFO] - GPU available: True, used: False
----[INFO] - TPU available: None, using: 0 TPU cores
----[INFO] - shuffling train set
----[INFO] - Optimizer config = Novograd (
----Parameter Group 0
----    amsgrad: False
----    betas: (0.95, 0.98)
----    eps: 1e-08
----    grad_averaging: False
----    lr: 0.001
----    weight_decay: 0.0
----)
----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
----will be used during training (effective maximum steps = 80) - 
----Parameters : 
----(warmup_steps: null
----warmup_ratio: 0.1
----last_epoch: -1
----max_steps: 80
----)
----[INFO] - 
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 108 M 
----1 | punct_classifier    | TokenClassifier      | 7.7 K 
----2 | domain_classifier   | SequenceClassifier   | 769   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----8.5 K     Trainable params
----108 M     Non-trainable params
----108 M     Total params
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.00       0.00       0.00        184
----! (label_id: 1)                                          0.00       0.00       0.00          4
----, (label_id: 2)                                          1.23       0.34       0.53        594
----- (label_id: 3)                                          3.06      25.42       5.46         59
----. (label_id: 4)                                         47.22      12.98      20.36        524
----: (label_id: 5)                                          0.00       0.00       0.00         18
----; (label_id: 6)                                          0.00       0.00       0.00         13
----? (label_id: 7)                                          8.45       6.32       7.23         95
----— (label_id: 8)                                          0.00       0.00       0.00         12
----… (label_id: 9)                                          0.00       0.00       0.00          0
-----------------------
----micro avg                                                6.05       6.05       6.05       1503
----macro avg                                                6.66       5.01       3.73       1503
----weighted avg                                            17.61       6.05       7.98       1503
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00         92
-----------------------
----micro avg                                              100.00     100.00     100.00         92
----macro avg                                              100.00     100.00     100.00         92
----weighted avg                                           100.00     100.00     100.00         92
----
----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
----[INFO] - Optimizer config = Novograd (
----Parameter Group 0
----    amsgrad: False
----    betas: (0.95, 0.98)
----    eps: 1e-08
----    grad_averaging: False
----    lr: 1.5848931924611143e-08
----    weight_decay: 0.0
----)
----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
----will be used during training (effective maximum steps = 3192) - 
----Parameters : 
----(warmup_steps: null
----warmup_ratio: 0.1
----last_epoch: -1
----max_steps: 3192
----)
----[INFO] - 
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 108 M 
----1 | punct_classifier    | TokenClassifier      | 7.7 K 
----2 | domain_classifier   | SequenceClassifier   | 769   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----8.5 K     Trainable params
----108 M     Non-trainable params
----108 M     Total params
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.00       0.00       0.00        202
----! (label_id: 1)                                          0.00       0.00       0.00          4
----, (label_id: 2)                                          1.62       0.45       0.70        669
----- (label_id: 3)                                          3.48      27.27       6.17         66
----. (label_id: 4)                                         45.06      13.01      20.19        561
----: (label_id: 5)                                          1.52       6.67       2.47         15
----; (label_id: 6)                                          0.00       0.00       0.00         15
----? (label_id: 7)                                          8.70       7.32       7.95         82
----— (label_id: 8)                                          0.00       0.00       0.00         13
----… (label_id: 9)                                          0.00       0.00       0.00          1
-----------------------
----micro avg                                                6.20       6.20       6.20       1628
----macro avg                                                6.04       5.47       3.75       1628
----weighted avg                                            16.79       6.20       7.92       1628
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00        101
-----------------------
----micro avg                                              100.00     100.00     100.00        101
----macro avg                                              100.00     100.00     100.00        101
----weighted avg                                           100.00     100.00     100.00        101
----
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.34       0.73       0.46       4402
----! (label_id: 1)                                          0.42      13.95       0.82        129
----, (label_id: 2)                                          2.53       0.64       1.03      15243
----- (label_id: 3)                                          2.45      21.03       4.38       1322
----. (label_id: 4)                                         44.00      11.40      18.11      12542
----: (label_id: 5)                                          0.43       1.41       0.65        354
----; (label_id: 6)                                          0.00       0.00       0.00        163
----? (label_id: 7)                                          4.16       6.27       5.00       1117
----— (label_id: 8)                                          3.00       0.61       1.02        488
----… (label_id: 9)                                          0.97       6.17       1.68         81
-----------------------
----micro avg                                                5.41       5.41       5.41      35841
----macro avg                                                5.83       6.22       3.32      35841
----weighted avg                                            16.78       5.41       7.18      35841
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2201
-----------------------
----micro avg                                              100.00     100.00     100.00       2201
----macro avg                                              100.00     100.00     100.00       2201
----weighted avg                                           100.00     100.00     100.00       2201
----
----[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.20       0.43       0.27       4226
----! (label_id: 1)                                          0.44      14.17       0.86        127
----, (label_id: 2)                                          1.93       0.49       0.78      14611
----- (label_id: 3)                                          2.23      19.56       4.01       1237
----. (label_id: 4)                                         43.37      11.25      17.86      11977
----: (label_id: 5)                                          0.68       2.34       1.05        342
----; (label_id: 6)                                          0.00       0.00       0.00        129
----? (label_id: 7)                                          5.16       7.47       6.10       1058
----— (label_id: 8)                                          2.15       0.49       0.80        409
----… (label_id: 9)                                          0.69       4.23       1.19         71
-----------------------
----micro avg                                                5.23       5.23       5.23      34187
----macro avg                                                5.68       6.04       3.29      34187
----weighted avg                                            16.32       5.23       6.98      34187
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2113
-----------------------
----micro avg                                              100.00     100.00     100.00       2113
----macro avg                                              100.00     100.00     100.00       2113
----weighted avg                                           100.00     100.00     100.00       2113
----
----[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.28       0.61       0.39       4228
----! (label_id: 1)                                          0.30       8.28       0.58        145
----, (label_id: 2)                                          2.27       0.58       0.92      14495
----- (label_id: 3)                                          2.64      21.78       4.70       1327
----. (label_id: 4)                                         44.87      11.66      18.51      12193
----: (label_id: 5)                                          0.60       1.93       0.91        362
----; (label_id: 6)                                          0.00       0.00       0.00        164
----? (label_id: 7)                                          4.19       6.40       5.07       1078
----— (label_id: 8)                                          1.16       0.22       0.37        459
----… (label_id: 9)                                          0.85       4.17       1.41         96
-----------------------
----micro avg                                                5.54       5.54       5.54      34547
----macro avg                                                5.72       5.56       3.29      34547
----weighted avg                                            17.08       5.54       7.33      34547
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2114
-----------------------
----micro avg                                              100.00     100.00     100.00       2114
----macro avg                                              100.00     100.00     100.00       2114
----weighted avg                                           100.00     100.00     100.00       2114
----
----[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.29       0.63       0.40       4444
----! (label_id: 1)                                          0.38      10.67       0.74        150
----, (label_id: 2)                                          2.32       0.59       0.94      15290
----- (label_id: 3)                                          2.34      20.28       4.19       1292
----. (label_id: 4)                                         43.85      11.68      18.44      12599
----: (label_id: 5)                                          0.41       1.28       0.62        392
----; (label_id: 6)                                          0.00       0.00       0.00        164
----? (label_id: 7)                                          4.24       6.30       5.07       1111
----— (label_id: 8)                                          0.00       0.00       0.00        456
----… (label_id: 9)                                          0.38       2.41       0.65         83
-----------------------
----micro avg                                                5.40       5.40       5.40      35981
----macro avg                                                5.42       5.38       3.11      35981
----weighted avg                                            16.59       5.40       7.22      35981
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2222
-----------------------
----micro avg                                              100.00     100.00     100.00       2222
----macro avg                                              100.00     100.00     100.00       2222
----weighted avg                                           100.00     100.00     100.00       2222
----
----[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.35       0.73       0.48       3844
----! (label_id: 1)                                          0.54      14.62       1.04        130
----, (label_id: 2)                                          2.32       0.59       0.94      13056
----- (label_id: 3)                                          2.67      22.28       4.77       1194
----. (label_id: 4)                                         44.45      11.95      18.84      10791
----: (label_id: 5)                                          0.84       3.21       1.33        280
----; (label_id: 6)                                          0.00       0.00       0.00        140
----? (label_id: 7)                                          4.17       6.56       5.10        914
----— (label_id: 8)                                          0.00       0.00       0.00        401
----… (label_id: 9)                                          0.48       2.63       0.81         76
-----------------------
----micro avg                                                5.68       5.68       5.68      30826
----macro avg                                                5.58       6.26       3.33      30826
----weighted avg                                            16.82       5.68       7.41      30826
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       1922
-----------------------
----micro avg                                              100.00     100.00     100.00       1922
----macro avg                                              100.00     100.00     100.00       1922
----weighted avg                                           100.00     100.00     100.00       1922
----
----[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.28       0.60       0.39       3970
----! (label_id: 1)                                          0.35      10.66       0.68        122
----, (label_id: 2)                                          2.09       0.53       0.85      13469
----- (label_id: 3)                                          2.29      19.32       4.10       1201
----. (label_id: 4)                                         43.43      11.24      17.86      11227
----: (label_id: 5)                                          0.63       2.30       0.99        304
----; (label_id: 6)                                          0.00       0.00       0.00        141
----? (label_id: 7)                                          4.52       6.86       5.45       1006
----— (label_id: 8)                                          1.15       0.23       0.38        444
----… (label_id: 9)                                          0.45       2.67       0.78         75
-----------------------
----micro avg                                                5.26       5.26       5.26      31959
----macro avg                                                5.52       5.44       3.15      31959
----weighted avg                                            16.42       5.26       7.02      31959
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       1985
-----------------------
----micro avg                                              100.00     100.00     100.00       1985
----macro avg                                              100.00     100.00     100.00       1985
----weighted avg                                           100.00     100.00     100.00       1985
----
----[INFO] - Epoch 5, step 2394: val_loss was not in top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.23       0.48       0.31       4126
----! (label_id: 1)                                          0.29       9.40       0.56        117
----, (label_id: 2)                                          1.91       0.49       0.77      14019
----- (label_id: 3)                                          2.52      22.59       4.53       1164
----. (label_id: 4)                                         44.15      11.65      18.44      11789
----: (label_id: 5)                                          0.72       2.41       1.11        332
----; (label_id: 6)                                          0.56       0.61       0.58        165
----? (label_id: 7)                                          3.89       6.53       4.88        980
----— (label_id: 8)                                          2.30       0.47       0.77        430
----… (label_id: 9)                                          1.18       8.33       2.07         60
-----------------------
----micro avg                                                5.47       5.47       5.47      33182
----macro avg                                                5.77       6.30       3.40      33182
----weighted avg                                            16.77       5.47       7.25      33182
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2063
-----------------------
----micro avg                                              100.00     100.00     100.00       2063
----macro avg                                              100.00     100.00     100.00       2063
----weighted avg                                           100.00     100.00     100.00       2063
----
----[INFO] - Epoch 6, step 2793: val_loss was not in top 3
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/hparams.yaml
--deleted file mode 100644
--index 825d089..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/hparams.yaml
--+++ /dev/null
--@@ -1,107 +0,0 @@
---seed: 42
---trainer:
---  gpus: 1
---  num_nodes: 1
---  max_epochs: 2
---  max_steps: null
---  accumulate_grad_batches: 4
---  gradient_clip_val: 0
---  amp_level: O1
---  precision: 16
---  accelerator: ddp
---  checkpoint_callback: false
---  logger: false
---  log_every_n_steps: 1
---  val_check_interval: 1.0
---  resume_from_checkpoint: null
---exp_manager:
---  exp_dir: /home/nxingyu/project/
---  name: Punctuation_with_Domain_discriminator
---  create_tensorboard_logger: true
---  create_checkpoint_callback: true
---base_path: /home/nxingyu/data
---tmp_path: /home/nxingyu/data/tmp
---model:
---  nemo_path: null
---  transformer_path: google/electra-small-discriminator
---  unfrozen: 0
---  maximum_unfrozen: 2
---  unfreeze_step: 1
---  punct_label_ids:
---  - ''
---  - '!'
---  - ','
---  - '-'
---  - .
---  - ':'
---  - ;
---  - '?'
---  - —
---  - …
---  punct_class_weights: false
---  dataset:
---    data_dir: /home/nxingyu/data
---    labelled:
---    - /home/nxingyu/data/ted_talks_processed
---    unlabelled: null
---    max_seq_length: 128
---    pad_label: ''
---    ignore_extra_tokens: false
---    ignore_start_end: false
---    use_cache: false
---    num_workers: 0
---    pin_memory: true
---    drop_last: false
---    num_labels: 10
---    num_domains: 1
---    test_unlabelled: true
---    train_ds:
---      shuffle: true
---      num_samples: -1
---      batch_size: 8
---    validation_ds:
---      shuffle: true
---      num_samples: -1
---      batch_size: 8
---  tokenizer:
---    tokenizer_name: google/electra-small-discriminator
---    vocab_file: null
---    tokenizer_model: null
---    special_tokens: null
---  language_model:
---    pretrained_model_name: google/electra-small-discriminator
---    lm_checkpoint: null
---    config_file: null
---    config: null
---  punct_head:
---    punct_num_fc_layers: 1
---    fc_dropout: 0.1
---    activation: relu
---    log_softmax: false
---    use_transformer_init: true
---    loss: dice
---  domain_head:
---    domain_num_fc_layers: 1
---    fc_dropout: 0.1
---    activation: relu
---    log_softmax: false
---    use_transformer_init: true
---    loss: cel
---    gamma: 0.1
---  dice_loss:
---    epsilon: 0.01
---    alpha: 3
---    macro_average: true
---  focal_loss:
---    gamma: 5
---  optim:
---    name: adam
---    lr: 0.001
---    weight_decay: 0.0
---    sched:
---      name: WarmupAnnealing
---      warmup_steps: null
---      warmup_ratio: 0.1
---      last_epoch: -1
---      monitor: val_loss
---      reduce_on_plateau: false
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/lightning_logs.txt
--deleted file mode 100644
--index 5a813ec..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/lightning_logs.txt
--+++ /dev/null
--@@ -1,117 +0,0 @@
---Global seed set to 42
---GPU available: True, used: True
---TPU available: None, using: 0 TPU cores
---LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
---Using native 16bit precision.
---Global seed set to 42
---initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 257   
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---35.9 K    Trainable params
---13.4 M    Non-trainable params
---13.5 M    Total params
---Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/lr_find_temp_model.ckpt
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 257   
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---35.9 K    Trainable params
---13.4 M    Non-trainable params
---13.5 M    Total params
---Epoch 0, global step 100: val_loss reached 0.49661 (best 0.49661), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.50-epoch=0.ckpt" as top 3
---Epoch 1, global step 200: val_loss reached 0.50627 (best 0.49661), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.51-epoch=1.ckpt" as top 3
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 257   
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---825 K     Trainable params
---12.7 M    Non-trainable params
---13.5 M    Total params
---LR finder stopped early due to diverging loss.
---Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/lr_find_temp_model.ckpt
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 257   
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---825 K     Trainable params
---12.7 M    Non-trainable params
---13.5 M    Total params
---Epoch 0, global step 301: val_loss reached 0.30201 (best 0.30201), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.30-epoch=0.ckpt" as top 3
---Epoch 1, global step 401: val_loss reached 0.31043 (best 0.30201), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.31-epoch=1.ckpt" as top 3
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 257   
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---1.6 M     Trainable params
---11.9 M    Non-trainable params
---13.5 M    Total params
---LR finder stopped early due to diverging loss.
---Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/lr_find_temp_model.ckpt
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 257   
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---1.6 M     Trainable params
---11.9 M    Non-trainable params
---13.5 M    Total params
---Epoch 0, global step 502: val_loss reached 0.39765 (best 0.30201), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.40-epoch=0.ckpt" as top 3
---Epoch 1, step 602: val_loss was not in top 3
---GPU available: True, used: True
---TPU available: None, using: 0 TPU cores
---Using environment variable NODE_RANK for node rank (0).
---LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/nemo_error_log.txt
--deleted file mode 100644
--index 1eb1af1..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/nemo_error_log.txt
--+++ /dev/null
--@@ -1,43 +0,0 @@
---[NeMo W 2021-02-08 07:39:29 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-08 07:39:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:39:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:39:40 nemo_logging:349] /home/nxingyu/project/experiment/core/classification_report.py:116: UserWarning: This overload of nonzero is deprecated:
---    	nonzero(Tensor input, *, Tensor out)
---    Consider using one of the following signatures instead:
---    	nonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629411241/work/torch/csrc/utils/python_arg_parser.cpp:766.)
---      num_non_empty_classes = torch.nonzero(self.num_examples_per_class).size(0)
---    
---[NeMo W 2021-02-08 07:39:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:43:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:43:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:44:01 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:44:05 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:118: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
---[NeMo W 2021-02-08 07:48:43 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f3e417748e0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-08 07:49:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f3e4bb7b490> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-08 07:59:34 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 08:28:12 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 08:28:39 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f3e4bb7b5b0> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/nemo_log_globalrank-0_localrank-0.txt
--deleted file mode 100644
--index e7e35be..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/nemo_log_globalrank-0_localrank-0.txt
--+++ /dev/null
--@@ -1,45 +0,0 @@
---[NeMo I 2021-02-08 07:39:29 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29
---[NeMo I 2021-02-08 07:39:29 exp_manager:519] TensorboardLogger has been set up
---[NeMo W 2021-02-08 07:39:29 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-08 07:39:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:39:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:39:40 nemo_logging:349] /home/nxingyu/project/experiment/core/classification_report.py:116: UserWarning: This overload of nonzero is deprecated:
---    	nonzero(Tensor input, *, Tensor out)
---    Consider using one of the following signatures instead:
---    	nonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629411241/work/torch/csrc/utils/python_arg_parser.cpp:766.)
---      num_non_empty_classes = torch.nonzero(self.num_examples_per_class).size(0)
---    
---[NeMo W 2021-02-08 07:39:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:43:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:43:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:44:01 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:44:05 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:118: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
---[NeMo W 2021-02-08 07:48:43 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f3e417748e0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-08 07:49:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f3e4bb7b490> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-08 07:59:34 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 08:28:12 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 08:28:39 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f3e4bb7b5b0> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/cmd-args.log
--deleted file mode 100644
--index 11a5d8e..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/cmd-args.log
--+++ /dev/null
--@@ -1 +0,0 @@
---main.py
--\ No newline at end of file
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/events.out.tfevents.1612742534.Titan.5792.0 b/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/events.out.tfevents.1612742534.Titan.5792.0
--deleted file mode 100644
--index 51c6937..0000000
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/events.out.tfevents.1612742534.Titan.5792.0 and /dev/null differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/git-info.log
--deleted file mode 100644
--index 4f1e878..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/git-info.log
--+++ /dev/null
--@@ -1,415 +0,0 @@
---commit hash: 939a671c8c117db6975316767ced5d95449e2b27
---diff --git a/experiment/config.yaml b/experiment/config.yaml
---index fe58670..2306d25 100644
------ a/experiment/config.yaml
---+++ b/experiment/config.yaml
---@@ -1,47 +1,47 @@
--- seed: 42
--- trainer:
----    # gpus: 1 # the number of gpus, 0 for CPU
----    # num_nodes: 1
----    # max_epochs: 2
----    # max_steps: null # precedence over max_epochs
----    # accumulate_grad_batches: 4 # accumulates grads every k batches
----    # gradient_clip_val: 0
----    # amp_level: O1 # O1/O2 for mixed precision
----    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----    # accelerator: ddp
----    # checkpoint_callback: false  # Provided by exp_manager
----    # logger: false #false  # Provided by exp_manager
----    # log_every_n_steps: 1  # Interval of logging.
----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----    # resume_from_checkpoint: null
----
----    gpus: 0 # the number of gpus, 0 for CPU
---+    gpus: 1 # the number of gpus, 0 for CPU
---     num_nodes: 1
----    max_epochs: 8
---+    max_epochs: 2
---     max_steps: null # precedence over max_epochs
----    accumulate_grad_batches: 1 # accumulates grads every k batches
---+    accumulate_grad_batches: 4 # accumulates grads every k batches
---     gradient_clip_val: 0
----    amp_level: O0 # O1/O2 for mixed precision
----    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----    # accelerator: ddp
---+    amp_level: O1 # O1/O2 for mixed precision
---+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
---+    accelerator: ddp
---     checkpoint_callback: false  # Provided by exp_manager
---     logger: false #false  # Provided by exp_manager
---     log_every_n_steps: 1  # Interval of logging.
---     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----    reload_dataloaders_every_epoch: true
---     resume_from_checkpoint: null
--- 
---+    # gpus: 0 # the number of gpus, 0 for CPU
---+    # num_nodes: 1
---+    # max_epochs: 8
---+    # max_steps: null # precedence over max_epochs
---+    # accumulate_grad_batches: 1 # accumulates grads every k batches
---+    # gradient_clip_val: 0
---+    # amp_level: O0 # O1/O2 for mixed precision
---+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
---+    # # accelerator: ddp
---+    # checkpoint_callback: false  # Provided by exp_manager
---+    # logger: false #false  # Provided by exp_manager
---+    # log_every_n_steps: 1  # Interval of logging.
---+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
---+    # reload_dataloaders_every_epoch: true
---+    # resume_from_checkpoint: null
---+
--- exp_manager:
----    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
---+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
---     name: Punctuation_with_Domain_discriminator  # The name of your model
---     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
---     create_checkpoint_callback: true 
----base_path: /home/nxingyu2/data # /root/data # 
----tmp_path: /home/nxingyu2/data/tmp # /tmp # 
---+base_path: /home/nxingyu/data # /root/data # 
---+tmp_path: /home/nxingyu/data/tmp # /tmp # 
--- 
--- model:
---     nemo_path: null
----    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
---+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
---     unfrozen: 0
---     maximum_unfrozen: 2
---     unfreeze_step: 1
---@@ -58,10 +58,10 @@ model:
---         - "—"
---         - "…"
--- 
----    punct_class_weights: true
---+    punct_class_weights: false
---     
---     dataset:
----        data_dir: /home/nxingyu2/data # /root/data # 
---+        data_dir: /home/nxingyu/data # /root/data # 
---         labelled:
---             - ${base_path}/ted_talks_processed #
---         unlabelled:
---@@ -112,7 +112,7 @@ model:
---         activation: 'relu'
---         log_softmax: false
---         use_transformer_init: true
----        loss: 'dice'
---+        loss: 'crf'
--- 
---     domain_head:
---         domain_num_fc_layers: 1
---@@ -132,7 +132,7 @@ model:
---         gamma: 5
--- 
---     optim:
----        name: novograd
---+        name: adam
---         lr: 1e-3
---         weight_decay: 0.00
---         sched:
---diff --git a/experiment/info.log b/experiment/info.log
---index 2471fe9..e69de29 100644
------ a/experiment/info.log
---+++ b/experiment/info.log
---@@ -1,300 +0,0 @@
----[INFO] - GPU available: True, used: False
----[INFO] - TPU available: None, using: 0 TPU cores
----[INFO] - shuffling train set
----[INFO] - Optimizer config = Novograd (
----Parameter Group 0
----    amsgrad: False
----    betas: (0.95, 0.98)
----    eps: 1e-08
----    grad_averaging: False
----    lr: 0.001
----    weight_decay: 0.0
----)
----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
----will be used during training (effective maximum steps = 80) - 
----Parameters : 
----(warmup_steps: null
----warmup_ratio: 0.1
----last_epoch: -1
----max_steps: 80
----)
----[INFO] - 
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 108 M 
----1 | punct_classifier    | TokenClassifier      | 7.7 K 
----2 | domain_classifier   | SequenceClassifier   | 769   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----8.5 K     Trainable params
----108 M     Non-trainable params
----108 M     Total params
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.00       0.00       0.00        184
----! (label_id: 1)                                          0.00       0.00       0.00          4
----, (label_id: 2)                                          1.23       0.34       0.53        594
----- (label_id: 3)                                          3.06      25.42       5.46         59
----. (label_id: 4)                                         47.22      12.98      20.36        524
----: (label_id: 5)                                          0.00       0.00       0.00         18
----; (label_id: 6)                                          0.00       0.00       0.00         13
----? (label_id: 7)                                          8.45       6.32       7.23         95
----— (label_id: 8)                                          0.00       0.00       0.00         12
----… (label_id: 9)                                          0.00       0.00       0.00          0
-----------------------
----micro avg                                                6.05       6.05       6.05       1503
----macro avg                                                6.66       5.01       3.73       1503
----weighted avg                                            17.61       6.05       7.98       1503
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00         92
-----------------------
----micro avg                                              100.00     100.00     100.00         92
----macro avg                                              100.00     100.00     100.00         92
----weighted avg                                           100.00     100.00     100.00         92
----
----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
----[INFO] - Optimizer config = Novograd (
----Parameter Group 0
----    amsgrad: False
----    betas: (0.95, 0.98)
----    eps: 1e-08
----    grad_averaging: False
----    lr: 1.5848931924611143e-08
----    weight_decay: 0.0
----)
----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
----will be used during training (effective maximum steps = 3192) - 
----Parameters : 
----(warmup_steps: null
----warmup_ratio: 0.1
----last_epoch: -1
----max_steps: 3192
----)
----[INFO] - 
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 108 M 
----1 | punct_classifier    | TokenClassifier      | 7.7 K 
----2 | domain_classifier   | SequenceClassifier   | 769   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----8.5 K     Trainable params
----108 M     Non-trainable params
----108 M     Total params
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.00       0.00       0.00        202
----! (label_id: 1)                                          0.00       0.00       0.00          4
----, (label_id: 2)                                          1.62       0.45       0.70        669
----- (label_id: 3)                                          3.48      27.27       6.17         66
----. (label_id: 4)                                         45.06      13.01      20.19        561
----: (label_id: 5)                                          1.52       6.67       2.47         15
----; (label_id: 6)                                          0.00       0.00       0.00         15
----? (label_id: 7)                                          8.70       7.32       7.95         82
----— (label_id: 8)                                          0.00       0.00       0.00         13
----… (label_id: 9)                                          0.00       0.00       0.00          1
-----------------------
----micro avg                                                6.20       6.20       6.20       1628
----macro avg                                                6.04       5.47       3.75       1628
----weighted avg                                            16.79       6.20       7.92       1628
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00        101
-----------------------
----micro avg                                              100.00     100.00     100.00        101
----macro avg                                              100.00     100.00     100.00        101
----weighted avg                                           100.00     100.00     100.00        101
----
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.34       0.73       0.46       4402
----! (label_id: 1)                                          0.42      13.95       0.82        129
----, (label_id: 2)                                          2.53       0.64       1.03      15243
----- (label_id: 3)                                          2.45      21.03       4.38       1322
----. (label_id: 4)                                         44.00      11.40      18.11      12542
----: (label_id: 5)                                          0.43       1.41       0.65        354
----; (label_id: 6)                                          0.00       0.00       0.00        163
----? (label_id: 7)                                          4.16       6.27       5.00       1117
----— (label_id: 8)                                          3.00       0.61       1.02        488
----… (label_id: 9)                                          0.97       6.17       1.68         81
-----------------------
----micro avg                                                5.41       5.41       5.41      35841
----macro avg                                                5.83       6.22       3.32      35841
----weighted avg                                            16.78       5.41       7.18      35841
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2201
-----------------------
----micro avg                                              100.00     100.00     100.00       2201
----macro avg                                              100.00     100.00     100.00       2201
----weighted avg                                           100.00     100.00     100.00       2201
----
----[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.20       0.43       0.27       4226
----! (label_id: 1)                                          0.44      14.17       0.86        127
----, (label_id: 2)                                          1.93       0.49       0.78      14611
----- (label_id: 3)                                          2.23      19.56       4.01       1237
----. (label_id: 4)                                         43.37      11.25      17.86      11977
----: (label_id: 5)                                          0.68       2.34       1.05        342
----; (label_id: 6)                                          0.00       0.00       0.00        129
----? (label_id: 7)                                          5.16       7.47       6.10       1058
----— (label_id: 8)                                          2.15       0.49       0.80        409
----… (label_id: 9)                                          0.69       4.23       1.19         71
-----------------------
----micro avg                                                5.23       5.23       5.23      34187
----macro avg                                                5.68       6.04       3.29      34187
----weighted avg                                            16.32       5.23       6.98      34187
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2113
-----------------------
----micro avg                                              100.00     100.00     100.00       2113
----macro avg                                              100.00     100.00     100.00       2113
----weighted avg                                           100.00     100.00     100.00       2113
----
----[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.28       0.61       0.39       4228
----! (label_id: 1)                                          0.30       8.28       0.58        145
----, (label_id: 2)                                          2.27       0.58       0.92      14495
----- (label_id: 3)                                          2.64      21.78       4.70       1327
----. (label_id: 4)                                         44.87      11.66      18.51      12193
----: (label_id: 5)                                          0.60       1.93       0.91        362
----; (label_id: 6)                                          0.00       0.00       0.00        164
----? (label_id: 7)                                          4.19       6.40       5.07       1078
----— (label_id: 8)                                          1.16       0.22       0.37        459
----… (label_id: 9)                                          0.85       4.17       1.41         96
-----------------------
----micro avg                                                5.54       5.54       5.54      34547
----macro avg                                                5.72       5.56       3.29      34547
----weighted avg                                            17.08       5.54       7.33      34547
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2114
-----------------------
----micro avg                                              100.00     100.00     100.00       2114
----macro avg                                              100.00     100.00     100.00       2114
----weighted avg                                           100.00     100.00     100.00       2114
----
----[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.29       0.63       0.40       4444
----! (label_id: 1)                                          0.38      10.67       0.74        150
----, (label_id: 2)                                          2.32       0.59       0.94      15290
----- (label_id: 3)                                          2.34      20.28       4.19       1292
----. (label_id: 4)                                         43.85      11.68      18.44      12599
----: (label_id: 5)                                          0.41       1.28       0.62        392
----; (label_id: 6)                                          0.00       0.00       0.00        164
----? (label_id: 7)                                          4.24       6.30       5.07       1111
----— (label_id: 8)                                          0.00       0.00       0.00        456
----… (label_id: 9)                                          0.38       2.41       0.65         83
-----------------------
----micro avg                                                5.40       5.40       5.40      35981
----macro avg                                                5.42       5.38       3.11      35981
----weighted avg                                            16.59       5.40       7.22      35981
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2222
-----------------------
----micro avg                                              100.00     100.00     100.00       2222
----macro avg                                              100.00     100.00     100.00       2222
----weighted avg                                           100.00     100.00     100.00       2222
----
----[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.35       0.73       0.48       3844
----! (label_id: 1)                                          0.54      14.62       1.04        130
----, (label_id: 2)                                          2.32       0.59       0.94      13056
----- (label_id: 3)                                          2.67      22.28       4.77       1194
----. (label_id: 4)                                         44.45      11.95      18.84      10791
----: (label_id: 5)                                          0.84       3.21       1.33        280
----; (label_id: 6)                                          0.00       0.00       0.00        140
----? (label_id: 7)                                          4.17       6.56       5.10        914
----— (label_id: 8)                                          0.00       0.00       0.00        401
----… (label_id: 9)                                          0.48       2.63       0.81         76
-----------------------
----micro avg                                                5.68       5.68       5.68      30826
----macro avg                                                5.58       6.26       3.33      30826
----weighted avg                                            16.82       5.68       7.41      30826
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       1922
-----------------------
----micro avg                                              100.00     100.00     100.00       1922
----macro avg                                              100.00     100.00     100.00       1922
----weighted avg                                           100.00     100.00     100.00       1922
----
----[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.28       0.60       0.39       3970
----! (label_id: 1)                                          0.35      10.66       0.68        122
----, (label_id: 2)                                          2.09       0.53       0.85      13469
----- (label_id: 3)                                          2.29      19.32       4.10       1201
----. (label_id: 4)                                         43.43      11.24      17.86      11227
----: (label_id: 5)                                          0.63       2.30       0.99        304
----; (label_id: 6)                                          0.00       0.00       0.00        141
----? (label_id: 7)                                          4.52       6.86       5.45       1006
----— (label_id: 8)                                          1.15       0.23       0.38        444
----… (label_id: 9)                                          0.45       2.67       0.78         75
-----------------------
----micro avg                                                5.26       5.26       5.26      31959
----macro avg                                                5.52       5.44       3.15      31959
----weighted avg                                            16.42       5.26       7.02      31959
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       1985
-----------------------
----micro avg                                              100.00     100.00     100.00       1985
----macro avg                                              100.00     100.00     100.00       1985
----weighted avg                                           100.00     100.00     100.00       1985
----
----[INFO] - Epoch 5, step 2394: val_loss was not in top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.23       0.48       0.31       4126
----! (label_id: 1)                                          0.29       9.40       0.56        117
----, (label_id: 2)                                          1.91       0.49       0.77      14019
----- (label_id: 3)                                          2.52      22.59       4.53       1164
----. (label_id: 4)                                         44.15      11.65      18.44      11789
----: (label_id: 5)                                          0.72       2.41       1.11        332
----; (label_id: 6)                                          0.56       0.61       0.58        165
----? (label_id: 7)                                          3.89       6.53       4.88        980
----— (label_id: 8)                                          2.30       0.47       0.77        430
----… (label_id: 9)                                          1.18       8.33       2.07         60
-----------------------
----micro avg                                                5.47       5.47       5.47      33182
----macro avg                                                5.77       6.30       3.40      33182
----weighted avg                                            16.77       5.47       7.25      33182
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2063
-----------------------
----micro avg                                              100.00     100.00     100.00       2063
----macro avg                                              100.00     100.00     100.00       2063
----weighted avg                                           100.00     100.00     100.00       2063
----
----[INFO] - Epoch 6, step 2793: val_loss was not in top 3
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/hparams.yaml
--deleted file mode 100644
--index 081bb10..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/hparams.yaml
--+++ /dev/null
--@@ -1,107 +0,0 @@
---seed: 42
---trainer:
---  gpus: 1
---  num_nodes: 1
---  max_epochs: 2
---  max_steps: null
---  accumulate_grad_batches: 4
---  gradient_clip_val: 0
---  amp_level: O1
---  precision: 16
---  accelerator: ddp
---  checkpoint_callback: false
---  logger: false
---  log_every_n_steps: 1
---  val_check_interval: 1.0
---  resume_from_checkpoint: null
---exp_manager:
---  exp_dir: /home/nxingyu/project/
---  name: Punctuation_with_Domain_discriminator
---  create_tensorboard_logger: true
---  create_checkpoint_callback: true
---base_path: /home/nxingyu/data
---tmp_path: /home/nxingyu/data/tmp
---model:
---  nemo_path: null
---  transformer_path: google/electra-small-discriminator
---  unfrozen: 0
---  maximum_unfrozen: 2
---  unfreeze_step: 1
---  punct_label_ids:
---  - ''
---  - '!'
---  - ','
---  - '-'
---  - .
---  - ':'
---  - ;
---  - '?'
---  - —
---  - …
---  punct_class_weights: false
---  dataset:
---    data_dir: /home/nxingyu/data
---    labelled:
---    - /home/nxingyu/data/ted_talks_processed
---    unlabelled: null
---    max_seq_length: 128
---    pad_label: ''
---    ignore_extra_tokens: false
---    ignore_start_end: false
---    use_cache: false
---    num_workers: 0
---    pin_memory: true
---    drop_last: false
---    num_labels: 10
---    num_domains: 1
---    test_unlabelled: true
---    train_ds:
---      shuffle: true
---      num_samples: -1
---      batch_size: 8
---    validation_ds:
---      shuffle: true
---      num_samples: -1
---      batch_size: 8
---  tokenizer:
---    tokenizer_name: google/electra-small-discriminator
---    vocab_file: null
---    tokenizer_model: null
---    special_tokens: null
---  language_model:
---    pretrained_model_name: google/electra-small-discriminator
---    lm_checkpoint: null
---    config_file: null
---    config: null
---  punct_head:
---    punct_num_fc_layers: 1
---    fc_dropout: 0.1
---    activation: relu
---    log_softmax: false
---    use_transformer_init: true
---    loss: crf
---  domain_head:
---    domain_num_fc_layers: 1
---    fc_dropout: 0.1
---    activation: relu
---    log_softmax: false
---    use_transformer_init: true
---    loss: cel
---    gamma: 0.1
---  dice_loss:
---    epsilon: 0.01
---    alpha: 3
---    macro_average: true
---  focal_loss:
---    gamma: 5
---  optim:
---    name: adam
---    lr: 0.001
---    weight_decay: 0.0
---    sched:
---      name: WarmupAnnealing
---      warmup_steps: null
---      warmup_ratio: 0.1
---      last_epoch: -1
---      monitor: val_loss
---      reduce_on_plateau: false
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/lightning_logs.txt
--deleted file mode 100644
--index c01498c..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/lightning_logs.txt
--+++ /dev/null
--@@ -1,117 +0,0 @@
---Global seed set to 42
---GPU available: True, used: True
---TPU available: None, using: 0 TPU cores
---LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
---Using native 16bit precision.
---Global seed set to 42
---initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 257   
---3 | punctuation_loss    | LinearChainCRF       | 120   
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---36.0 K    Trainable params
---13.4 M    Non-trainable params
---13.5 M    Total params
---Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/lr_find_temp_model.ckpt
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 257   
---3 | punctuation_loss    | LinearChainCRF       | 120   
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---36.0 K    Trainable params
---13.4 M    Non-trainable params
---13.5 M    Total params
---Epoch 0, global step 100: val_loss reached 28.76837 (best 28.76837), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/checkpoints/Punctuation_with_Domain_discriminator---val_loss=28.77-epoch=0.ckpt" as top 3
---Epoch 1, global step 200: val_loss reached 18.16606 (best 18.16606), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/checkpoints/Punctuation_with_Domain_discriminator---val_loss=18.17-epoch=1.ckpt" as top 3
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 257   
---3 | punctuation_loss    | LinearChainCRF       | 120   
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---825 K     Trainable params
---12.7 M    Non-trainable params
---13.5 M    Total params
---LR finder stopped early due to diverging loss.
---Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/lr_find_temp_model.ckpt
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 257   
---3 | punctuation_loss    | LinearChainCRF       | 120   
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---825 K     Trainable params
---12.7 M    Non-trainable params
---13.5 M    Total params
---Epoch 0, global step 301: val_loss reached 14.52603 (best 14.52603), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/checkpoints/Punctuation_with_Domain_discriminator---val_loss=14.53-epoch=0.ckpt" as top 3
---Epoch 1, global step 401: val_loss reached 15.01920 (best 14.52603), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/checkpoints/Punctuation_with_Domain_discriminator---val_loss=15.02-epoch=1.ckpt" as top 3
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 257   
---3 | punctuation_loss    | LinearChainCRF       | 120   
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---1.6 M     Trainable params
---11.9 M    Non-trainable params
---13.5 M    Total params
---LR finder stopped early due to diverging loss.
---Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/lr_find_temp_model.ckpt
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 257   
---3 | punctuation_loss    | LinearChainCRF       | 120   
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---1.6 M     Trainable params
---11.9 M    Non-trainable params
---13.5 M    Total params
---Epoch 0, global step 502: val_loss reached 13.53691 (best 13.53691), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/checkpoints/Punctuation_with_Domain_discriminator---val_loss=13.54-epoch=0.ckpt" as top 3
---Epoch 1, step 602: val_loss was not in top 3
---GPU available: True, used: True
---TPU available: None, using: 0 TPU cores
---Using environment variable NODE_RANK for node rank (0).
---LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/nemo_error_log.txt
--deleted file mode 100644
--index 1be0620..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/nemo_error_log.txt
--+++ /dev/null
--@@ -1,46 +0,0 @@
---[NeMo W 2021-02-08 07:56:46 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-08 07:56:55 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:56:55 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:56:57 nemo_logging:349] /home/nxingyu/project/experiment/core/classification_report.py:116: UserWarning: This overload of nonzero is deprecated:
---    	nonzero(Tensor input, *, Tensor out)
---    Consider using one of the following signatures instead:
---    	nonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629411241/work/torch/csrc/utils/python_arg_parser.cpp:766.)
---      num_non_empty_classes = torch.nonzero(self.num_examples_per_class).size(0)
---    
---[NeMo W 2021-02-08 07:56:57 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:57:02 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:118: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
---[NeMo W 2021-02-08 08:02:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 08:02:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 08:02:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 08:02:21 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:118: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
---[NeMo W 2021-02-08 08:08:50 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f0210aec880> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-08 08:09:19 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f02190dca90> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-08 08:23:36 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 09:11:17 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 09:11:50 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f0219122550> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/nemo_log_globalrank-0_localrank-0.txt
--deleted file mode 100644
--index 2ca2442..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/nemo_log_globalrank-0_localrank-0.txt
--+++ /dev/null
--@@ -1,48 +0,0 @@
---[NeMo I 2021-02-08 07:56:46 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46
---[NeMo I 2021-02-08 07:56:46 exp_manager:519] TensorboardLogger has been set up
---[NeMo W 2021-02-08 07:56:46 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-08 07:56:55 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:56:55 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:56:57 nemo_logging:349] /home/nxingyu/project/experiment/core/classification_report.py:116: UserWarning: This overload of nonzero is deprecated:
---    	nonzero(Tensor input, *, Tensor out)
---    Consider using one of the following signatures instead:
---    	nonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629411241/work/torch/csrc/utils/python_arg_parser.cpp:766.)
---      num_non_empty_classes = torch.nonzero(self.num_examples_per_class).size(0)
---    
---[NeMo W 2021-02-08 07:56:57 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:57:02 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:118: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
---[NeMo W 2021-02-08 08:02:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 08:02:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 08:02:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 08:02:21 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:118: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
---[NeMo W 2021-02-08 08:08:50 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f0210aec880> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-08 08:09:19 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f02190dca90> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-08 08:23:36 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 09:11:17 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 09:11:50 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f0219122550> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/cmd-args.log
--deleted file mode 100644
--index 11a5d8e..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/cmd-args.log
--+++ /dev/null
--@@ -1 +0,0 @@
---main.py
--\ No newline at end of file
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/events.out.tfevents.1612771686.Titan.22251.0 b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/events.out.tfevents.1612771686.Titan.22251.0
--deleted file mode 100644
--index 6b14ff7..0000000
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/events.out.tfevents.1612771686.Titan.22251.0 and /dev/null differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/git-info.log
--deleted file mode 100644
--index e93204c..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/git-info.log
--+++ /dev/null
--@@ -1,181 +0,0 @@
---commit hash: d9cdb13829a1dfa2d74afb03fde5acec0f85d2cc
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/lightning_logs.txt
---index 2a26724..31870ad 100644
------ a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/lightning_logs.txt
---+++ b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/lightning_logs.txt
---@@ -20,3 +20,36 @@ initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--- 299 K     Trainable params
--- 13.2 M    Non-trainable params
--- 13.5 M    Total params
---+Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/lr_find_temp_model.ckpt
---+Global seed set to 42
---+
---+  | Name                | Type                 | Params
---+-------------------------------------------------------------
---+0 | transformer         | ElectraModel         | 13.5 M
---+1 | punct_classifier    | TokenClassifier      | 2.6 K 
---+2 | domain_classifier   | SequenceClassifier   | 1.0 K 
---+3 | punctuation_loss    | FocalDiceLoss        | 0     
---+4 | domain_loss         | CrossEntropyLoss     | 0     
---+5 | agg_loss            | AggregatorLoss       | 0     
---+6 | punct_class_report  | ClassificationReport | 0     
---+7 | domain_class_report | ClassificationReport | 0     
---+-------------------------------------------------------------
---+299 K     Trainable params
---+13.2 M    Non-trainable params
---+13.5 M    Total params
---+Global seed set to 42
---+
---+  | Name                | Type                 | Params
---+-------------------------------------------------------------
---+0 | transformer         | ElectraModel         | 13.5 M
---+1 | punct_classifier    | TokenClassifier      | 2.6 K 
---+2 | domain_classifier   | SequenceClassifier   | 1.0 K 
---+3 | punctuation_loss    | FocalDiceLoss        | 0     
---+4 | domain_loss         | CrossEntropyLoss     | 0     
---+5 | agg_loss            | AggregatorLoss       | 0     
---+6 | punct_class_report  | ClassificationReport | 0     
---+7 | domain_class_report | ClassificationReport | 0     
---+-------------------------------------------------------------
---+826 K     Trainable params
---+12.7 M    Non-trainable params
---+13.5 M    Total params
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_error_log.txt
---index 4ddbe1b..90e4c4c 100644
------ a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_error_log.txt
---+++ b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_error_log.txt
---@@ -2,3 +2,12 @@
--- [NeMo W 2021-02-08 15:34:34 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---       warnings.warn(*args, **kwargs)
---     
---+[NeMo W 2021-02-08 15:46:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---+      warnings.warn(*args, **kwargs)
---+    
---+[NeMo W 2021-02-08 15:51:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---+      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---+    
---+[NeMo W 2021-02-08 15:57:37 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
---+      warnings.warn(*args, **kwargs)
---+    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_log_globalrank-0_localrank-0.txt
---index dea36af..926854f 100644
------ a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_log_globalrank-0_localrank-0.txt
---+++ b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_log_globalrank-0_localrank-0.txt
---@@ -4,3 +4,12 @@
--- [NeMo W 2021-02-08 15:34:34 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---       warnings.warn(*args, **kwargs)
---     
---+[NeMo W 2021-02-08 15:46:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---+      warnings.warn(*args, **kwargs)
---+    
---+[NeMo W 2021-02-08 15:51:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---+      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---+    
---+[NeMo W 2021-02-08 15:57:37 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
---+      warnings.warn(*args, **kwargs)
---+    
---diff --git a/README.md b/README.md
---index beca3ef..ef7d22c 100644
------ a/README.md
---+++ b/README.md
---@@ -449,3 +449,7 @@ weighted avg                                            69.12      72.03      70
---  'punct_recall': 33.78831481933594,
---  'test_loss': 0.2638570964336395}
--- 
---+
---+
---+### domain adversarial dice 3, open l ted ul 
---+initial_lr 0.007943282347242822
---diff --git a/experiment/config.yaml b/experiment/config.yaml
---index 070bc4f..37d105e 100644
------ a/experiment/config.yaml
---+++ b/experiment/config.yaml
---@@ -85,7 +85,7 @@ model:
---         train_ds:
---             shuffle: true
---             num_samples: -1
----            batch_size: 8
---+            batch_size: 4
--- 
---         validation_ds:
---             # if evaluation data is not in the model.dataset.data_dir as the training data or multiple datasets are used for
---@@ -93,7 +93,7 @@ model:
---             # ds_item: null # expected format: [PATH_TO_DEV1,PATH_TO_DEV2] (Note no space between the paths and square brackets)
---             shuffle: true
---             num_samples: -1
----            batch_size: 8
---+            batch_size: 4
--- 
---     tokenizer:
---         tokenizer_name: ${model.language_model.pretrained_model_name} # or sentencepiece
---@@ -123,7 +123,7 @@ model:
---         log_softmax: false
---         use_transformer_init: true
---         loss: 'cel'
----        gamma: 0 #0.1 # coefficient of gradient reversal
---+        gamma: 0.1 #0.1 # coefficient of gradient reversal
---         pooling: 'mean_max' # 'mean' mean_max
---         idx_conditioned_on: 0
---     
---diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
---index bc844cd..4027bb2 100644
------ a/experiment/data/punctuation_dataset_multi.py
---+++ b/experiment/data/punctuation_dataset_multi.py
---@@ -63,7 +63,8 @@ class PunctuationDomainDataset(IterableDataset):
---         self.randomize=randomize
---         self.target_file=target_file
---         self.tmp_path=tmp_path
----        os.system(f'cp {self.csv_file} {self.target_file}')
---+        if not (os.path.exists(self.target_file)):
---+            os.system(f'cp {self.csv_file} {self.target_file}')
--- 
---     def __iter__(self):
---         self.dataset=iter(pd.read_csv(
---diff --git a/experiment/info.log b/experiment/info.log
---index 481b8ff..e69de29 100644
------ a/experiment/info.log
---+++ b/experiment/info.log
---@@ -1,43 +0,0 @@
----[INFO] - shuffling train set
----[INFO] - Optimizer config = AdamW (
----Parameter Group 0
----    amsgrad: False
----    betas: (0.9, 0.999)
----    eps: 1e-08
----    lr: 0.001
----    weight_decay: 0.0
----)
----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fd3155e83d0>" 
----will be used during training (effective maximum steps = 80) - 
----Parameters : 
----(warmup_steps: null
----warmup_ratio: 0.1
----last_epoch: -1
----max_steps: 80
----)
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.00       0.00       0.00        168
----! (label_id: 1)                                         14.29       7.69      10.00        104
----, (label_id: 2)                                         23.21      27.23      25.06        584
----- (label_id: 3)                                          4.02      46.67       7.39         45
----. (label_id: 4)                                         54.17       1.19       2.33       1091
----: (label_id: 5)                                          0.00       0.00       0.00          0
----; (label_id: 6)                                          0.00       0.00       0.00          0
----? (label_id: 7)                                          6.42      22.75      10.01        189
----— (label_id: 8)                                          0.00       0.00       0.00          0
----… (label_id: 9)                                          9.09       2.38       3.77         84
-----------------------
----micro avg                                               10.86      10.86      10.86       2265
----macro avg                                               15.88      15.42       8.37       2265
----weighted avg                                            33.68      10.86       9.17       2265
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                         50.00     100.00      66.67         84
----1 (label_id: 1)                                          0.00       0.00       0.00         84
-----------------------
----micro avg                                               50.00      50.00      50.00        168
----macro avg                                               25.00      50.00      33.33        168
----weighted avg                                            25.00      50.00      33.33        168
----
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/hparams.yaml
--deleted file mode 100644
--index ea7d4f7..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/hparams.yaml
--+++ /dev/null
--@@ -1,110 +0,0 @@
---seed: 42
---trainer:
---  gpus: 1
---  num_nodes: 1
---  max_epochs: 10
---  max_steps: null
---  accumulate_grad_batches: 4
---  gradient_clip_val: 0
---  amp_level: O1
---  precision: 16
---  accelerator: ddp
---  checkpoint_callback: false
---  logger: false
---  log_every_n_steps: 1
---  val_check_interval: 1.0
---  resume_from_checkpoint: null
---exp_manager:
---  exp_dir: /home/nxingyu/project/
---  name: Punctuation_with_Domain_discriminator
---  create_tensorboard_logger: true
---  create_checkpoint_callback: true
---base_path: /home/nxingyu/data
---tmp_path: /home/nxingyu/data/tmp
---model:
---  nemo_path: null
---  transformer_path: google/electra-small-discriminator
---  unfrozen: 0
---  maximum_unfrozen: 1
---  unfreeze_step: 1
---  punct_label_ids:
---  - ''
---  - '!'
---  - ','
---  - '-'
---  - .
---  - ':'
---  - ;
---  - '?'
---  - —
---  - …
---  punct_class_weights: false
---  dataset:
---    data_dir: /home/nxingyu/data
---    labelled:
---    - /home/nxingyu/data/open_subtitles_processed
---    unlabelled:
---    - /home/nxingyu/data/ted_talks_processed
---    max_seq_length: 128
---    pad_label: ''
---    ignore_extra_tokens: false
---    ignore_start_end: false
---    use_cache: false
---    num_workers: 4
---    pin_memory: true
---    drop_last: false
---    num_labels: 10
---    num_domains: 2
---    test_unlabelled: true
---    train_ds:
---      shuffle: true
---      num_samples: -1
---      batch_size: 4
---    validation_ds:
---      shuffle: true
---      num_samples: -1
---      batch_size: 4
---  tokenizer:
---    tokenizer_name: google/electra-small-discriminator
---    vocab_file: null
---    tokenizer_model: null
---    special_tokens: null
---  language_model:
---    pretrained_model_name: google/electra-small-discriminator
---    lm_checkpoint: null
---    config_file: null
---    config: null
---  punct_head:
---    punct_num_fc_layers: 1
---    fc_dropout: 0.1
---    activation: relu
---    log_softmax: false
---    use_transformer_init: true
---    loss: dice
---  domain_head:
---    domain_num_fc_layers: 1
---    fc_dropout: 0.1
---    activation: relu
---    log_softmax: false
---    use_transformer_init: true
---    loss: cel
---    gamma: 0.1
---    pooling: mean_max
---    idx_conditioned_on: 0
---  dice_loss:
---    epsilon: 0.01
---    alpha: 3
---    macro_average: true
---  focal_loss:
---    gamma: 1
---  optim:
---    name: adamw
---    lr: 0.001
---    weight_decay: 0.0
---    sched:
---      name: WarmupAnnealing
---      warmup_steps: null
---      warmup_ratio: 0.1
---      last_epoch: -1
---      monitor: val_loss
---      reduce_on_plateau: false
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/lightning_logs.txt
--deleted file mode 100644
--index 846b33e..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/lightning_logs.txt
--+++ /dev/null
--@@ -1,40 +0,0 @@
---Global seed set to 42
---GPU available: True, used: True
---TPU available: None, using: 0 TPU cores
---LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
---Using native 16bit precision.
---Global seed set to 42
---initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 1.0 K 
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---299 K     Trainable params
---13.2 M    Non-trainable params
---13.5 M    Total params
---Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/lr_find_temp_model.ckpt
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 1.0 K 
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---299 K     Trainable params
---13.2 M    Non-trainable params
---13.5 M    Total params
---Epoch 0, global step 5305: val_loss reached 0.69762 (best 0.69762), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.70-epoch=0.ckpt" as top 3
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_error_log.txt
--deleted file mode 100644
--index 55977aa..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_error_log.txt
--+++ /dev/null
--@@ -1,16 +0,0 @@
---[NeMo W 2021-02-08 16:00:26 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-08 16:00:55 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 16:08:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 16:13:48 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
---[NeMo W 2021-02-08 17:54:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fcf47c80f70> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-08 18:08:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fcf47c80550> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---      warnings.warn(warn_msg)
---    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_log_globalrank-0_localrank-0.txt
--deleted file mode 100644
--index 5270e5c..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_log_globalrank-0_localrank-0.txt
--+++ /dev/null
--@@ -1,18 +0,0 @@
---[NeMo I 2021-02-08 16:00:26 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26
---[NeMo I 2021-02-08 16:00:26 exp_manager:519] TensorboardLogger has been set up
---[NeMo W 2021-02-08 16:00:26 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-08 16:00:55 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 16:08:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 16:13:48 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
---[NeMo W 2021-02-08 17:54:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fcf47c80f70> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-08 18:08:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fcf47c80550> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---      warnings.warn(warn_msg)
---    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/cmd-args.log
--deleted file mode 100644
--index 11a5d8e..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/cmd-args.log
--+++ /dev/null
--@@ -1 +0,0 @@
---main.py
--\ No newline at end of file
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/events.out.tfevents.1612830887.Titan.6887.0 b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/events.out.tfevents.1612830887.Titan.6887.0
--deleted file mode 100644
--index cf34bb6..0000000
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/events.out.tfevents.1612830887.Titan.6887.0 and /dev/null differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/git-info.log
--deleted file mode 100644
--index b6da706..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/git-info.log
--+++ /dev/null
--@@ -1,273 +0,0 @@
---commit hash: 08007e7bd84203d450e193af808686ac2c929dce
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/events.out.tfevents.1612771686.Titan.22251.0 b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/events.out.tfevents.1612771686.Titan.22251.0
---index 2a40109..6b14ff7 100644
---Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/events.out.tfevents.1612771686.Titan.22251.0 and b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/events.out.tfevents.1612771686.Titan.22251.0 differ
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/lightning_logs.txt
---index 439dccb..846b33e 100644
------ a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/lightning_logs.txt
---+++ b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/lightning_logs.txt
---@@ -37,3 +37,4 @@ Global seed set to 42
--- 299 K     Trainable params
--- 13.2 M    Non-trainable params
--- 13.5 M    Total params
---+Epoch 0, global step 5305: val_loss reached 0.69762 (best 0.69762), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.70-epoch=0.ckpt" as top 3
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_error_log.txt
---index c85c2a3..55977aa 100644
------ a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_error_log.txt
---+++ b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_error_log.txt
---@@ -8,3 +8,9 @@
--- [NeMo W 2021-02-08 16:13:48 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---     
---+[NeMo W 2021-02-08 17:54:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fcf47c80f70> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---+      warnings.warn(warn_msg)
---+    
---+[NeMo W 2021-02-08 18:08:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fcf47c80550> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---+      warnings.warn(warn_msg)
---+    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_log_globalrank-0_localrank-0.txt
---index b01f19c..5270e5c 100644
------ a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_log_globalrank-0_localrank-0.txt
---+++ b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_log_globalrank-0_localrank-0.txt
---@@ -10,3 +10,9 @@
--- [NeMo W 2021-02-08 16:13:48 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---     
---+[NeMo W 2021-02-08 17:54:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fcf47c80f70> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---+      warnings.warn(warn_msg)
---+    
---+[NeMo W 2021-02-08 18:08:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fcf47c80550> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---+      warnings.warn(warn_msg)
---+    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/events.out.tfevents.1612772700.Titan.28938.0 b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/events.out.tfevents.1612772700.Titan.28938.0
---index ca85da6..04e8367 100644
---Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/events.out.tfevents.1612772700.Titan.28938.0 and b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/events.out.tfevents.1612772700.Titan.28938.0 differ
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/lightning_logs.txt
---index 50c4caa..0f48e2a 100644
------ a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/lightning_logs.txt
---+++ b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/lightning_logs.txt
---@@ -37,3 +37,29 @@ Global seed set to 42
--- 299 K     Trainable params
--- 13.2 M    Non-trainable params
--- 13.5 M    Total params
---+Epoch 0, global step 5305: val_loss reached 0.20905 (best 0.20905), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.21-epoch=0.ckpt" as top 3
---+Epoch 1, global step 10610: val_loss reached 0.17265 (best 0.17265), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.17-epoch=1.ckpt" as top 3
---+Epoch 2, global step 15915: val_loss reached 0.05470 (best 0.05470), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.05-epoch=2.ckpt" as top 3
---+Epoch 3, global step 21220: val_loss reached 0.02875 (best 0.02875), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.03-epoch=3.ckpt" as top 3
---+Epoch 4, global step 26525: val_loss reached -0.03932 (best -0.03932), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.04-epoch=4.ckpt" as top 3
---+Epoch 5, global step 31830: val_loss reached -0.04410 (best -0.04410), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.04-epoch=5.ckpt" as top 3
---+Epoch 6, global step 37135: val_loss reached -0.04524 (best -0.04524), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.05-epoch=6.ckpt" as top 3
---+Epoch 7, global step 42440: val_loss reached -0.04689 (best -0.04689), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.05-epoch=7.ckpt" as top 3
---+Epoch 8, global step 47745: val_loss reached -0.04978 (best -0.04978), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.05-epoch=8.ckpt" as top 3
---+Epoch 9, global step 53050: val_loss reached -0.04850 (best -0.04978), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.05-epoch=9.ckpt" as top 3
---+Global seed set to 42
---+
---+  | Name                | Type                 | Params
---+-------------------------------------------------------------
---+0 | transformer         | ElectraModel         | 13.5 M
---+1 | punct_classifier    | TokenClassifier      | 2.6 K 
---+2 | domain_classifier   | SequenceClassifier   | 513   
---+3 | punctuation_loss    | FocalDiceLoss        | 0     
---+4 | domain_loss         | CrossEntropyLoss     | 0     
---+5 | agg_loss            | AggregatorLoss       | 0     
---+6 | punct_class_report  | ClassificationReport | 0     
---+7 | domain_class_report | ClassificationReport | 0     
---+-------------------------------------------------------------
---+825 K     Trainable params
---+12.7 M    Non-trainable params
---+13.5 M    Total params
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_error_log.txt
---index 2503857..ce38185 100644
------ a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_error_log.txt
---+++ b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_error_log.txt
---@@ -8,3 +8,9 @@
--- [NeMo W 2021-02-08 16:25:22 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---     
---+[NeMo W 2021-02-08 17:13:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fbd320e2b50> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---+      warnings.warn(warn_msg)
---+    
---+[NeMo W 2021-02-08 17:20:28 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fbd3c4dd8e0> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---+      warnings.warn(warn_msg)
---+    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_log_globalrank-0_localrank-0.txt
---index 48278bd..26f3ea7 100644
------ a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_log_globalrank-0_localrank-0.txt
---+++ b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_log_globalrank-0_localrank-0.txt
---@@ -10,3 +10,9 @@
--- [NeMo W 2021-02-08 16:25:22 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---     
---+[NeMo W 2021-02-08 17:13:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fbd320e2b50> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---+      warnings.warn(warn_msg)
---+    
---+[NeMo W 2021-02-08 17:20:28 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fbd3c4dd8e0> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---+      warnings.warn(warn_msg)
---+    
---diff --git a/experiment/config.yaml b/experiment/config.yaml
---index e492246..e7f6783 100644
------ a/experiment/config.yaml
---+++ b/experiment/config.yaml
---@@ -63,11 +63,11 @@ model:
---     dataset:
---         data_dir: /home/nxingyu/data # /root/data # 
---         labelled:
----            # - ${base_path}/ted_talks_processed #
----            - ${base_path}/open_subtitles_processed #  
---+            - ${base_path}/ted_talks_processed #
---+            # - ${base_path}/open_subtitles_processed #  
---         unlabelled:
---             # - ${base_path}/ted_talks_processed #
----            # - ${base_path}/open_subtitles_processed #  
---+            - ${base_path}/open_subtitles_processed #  
---             # parameters for dataset preprocessing
---         max_seq_length: 128
---         pad_label: ''
---@@ -79,7 +79,7 @@ model:
---         pin_memory: true
---         drop_last: false
---         num_labels: 10
----        num_domains: 1
---+        num_domains: 2
---         test_unlabelled: true
--- 
---         train_ds:
---@@ -123,7 +123,7 @@ model:
---         log_softmax: false
---         use_transformer_init: true
---         loss: 'cel'
----        gamma: 0.1 #0.1 # coefficient of gradient reversal
---+        gamma: 0 #0.1 # coefficient of gradient reversal
---         pooling: 'mean_max' # 'mean' mean_max
---         idx_conditioned_on: 0
---     
---diff --git a/experiment/core/utils.py b/experiment/core/utils.py
---index 5b0efe3..058cc87 100644
------ a/experiment/core/utils.py
---+++ b/experiment/core/utils.py
---@@ -26,7 +26,7 @@ def position_to_mask(max_seq_length:int,indices:list):
---         o[np.array(indices)%(max_seq_length-2)+1]=1
---     except:
---         pp('position_to_mask',np.array(indices)%(max_seq_length-2)+1)
----        o[(np.array(indices)%(max_seq_length-2)+1).astype(int)]=1
---+        # o[(np.array(indices)%(max_seq_length-2)+1).astype(int)]=1
---     return o
--- 
--- def align_labels_to_mask(mask,labels):
---diff --git a/experiment/info.log b/experiment/info.log
---index 9e4b4d4..e69de29 100644
------ a/experiment/info.log
---+++ b/experiment/info.log
---@@ -1,83 +0,0 @@
----[INFO] - shuffling train set
----[INFO] - Optimizer config = AdamW (
----Parameter Group 0
----    amsgrad: False
----    betas: (0.9, 0.999)
----    eps: 1e-08
----    lr: 0.001
----    weight_decay: 0.0
----)
----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fbd32b00be0>" 
----will be used during training (effective maximum steps = 80) - 
----Parameters : 
----(warmup_steps: null
----warmup_ratio: 0.1
----last_epoch: -1
----max_steps: 80
----)
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.00       0.00       0.00        232
----! (label_id: 1)                                          5.41       2.63       3.54        152
----, (label_id: 2)                                         24.37      27.46      25.82        772
----- (label_id: 3)                                          4.66      53.57       8.57         56
----. (label_id: 4)                                         43.75       0.85       1.67       1642
----: (label_id: 5)                                          0.00       0.00       0.00          0
----; (label_id: 6)                                          0.00       0.00       0.00          0
----? (label_id: 7)                                          4.76      22.94       7.89        218
----— (label_id: 8)                                          0.00       0.00       0.00          0
----… (label_id: 9)                                          8.33       2.04       3.28         98
-----------------------
----micro avg                                                9.84       9.84       9.84       3170
----macro avg                                               13.04      15.64       7.25       3170
----weighted avg                                            29.52       9.84       8.12       3170
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00        116
-----------------------
----micro avg                                              100.00     100.00     100.00        116
----macro avg                                              100.00     100.00     100.00        116
----weighted avg                                           100.00     100.00     100.00        116
----
----[INFO] - Optimizer config = AdamW (
----Parameter Group 0
----    amsgrad: False
----    betas: (0.9, 0.999)
----    eps: 1e-08
----    lr: 0.007943282347242822
----    weight_decay: 0.0
----)
----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fbd3350f790>" 
----will be used during training (effective maximum steps = 53050) - 
----Parameters : 
----(warmup_steps: null
----warmup_ratio: 0.1
----last_epoch: -1
----max_steps: 53050
----)
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.00       0.00       0.00        232
----! (label_id: 1)                                          5.41       2.63       3.54        152
----, (label_id: 2)                                         24.37      27.46      25.82        772
----- (label_id: 3)                                          4.66      53.57       8.57         56
----. (label_id: 4)                                         43.75       0.85       1.67       1642
----: (label_id: 5)                                          0.00       0.00       0.00          0
----; (label_id: 6)                                          0.00       0.00       0.00          0
----? (label_id: 7)                                          4.76      22.94       7.89        218
----— (label_id: 8)                                          0.00       0.00       0.00          0
----… (label_id: 9)                                          8.33       2.04       3.28         98
-----------------------
----micro avg                                                9.84       9.84       9.84       3170
----macro avg                                               13.04      15.64       7.25       3170
----weighted avg                                            29.52       9.84       8.12       3170
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00        116
-----------------------
----micro avg                                              100.00     100.00     100.00        116
----macro avg                                              100.00     100.00     100.00        116
----weighted avg                                           100.00     100.00     100.00        116
----
---diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
---index 4ac54f4..c5db7b3 100644
------ a/experiment/models/punctuation_domain_model.py
---+++ b/experiment/models/punctuation_domain_model.py
---@@ -1,5 +1,6 @@
--- # %%
--- import copy
---+import math
--- import logging
--- import os
--- from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union
---@@ -153,14 +154,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---         Lightning calls this inside the training loop with the data from the training dataloader
---         passed in as `batch`.
---         """
----        p=(self.current_epoch*self.train_size+batch_idx)/(self.train_size*self.hparams.model.max_epochs)
----        self.grad_reverse.scale=2/(1+exp(-10*p))-1
---+        p=(self.current_epoch*self.train_size+batch_idx)/(self.train_size*self.hparams.trainer.max_epochs)
---+        self.grad_reverse.scale=2/(1+math.exp(-10*p))-1
---         loss, _, _ = self._make_step(batch)
---         lr = self._optimizer.param_groups[0]['lr']
--- 
--- 
---         self.log('lr', lr, prog_bar=True)
---         self.log('train_loss', loss)
---+        self.log('gamma', self.grad_reverse.scale)
--- 
---         return {'loss': loss, 'lr': lr}
--- 
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/hparams.yaml
--deleted file mode 100644
--index cbac11e..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/hparams.yaml
--+++ /dev/null
--@@ -1,110 +0,0 @@
---seed: 42
---trainer:
---  gpus: 1
---  num_nodes: 1
---  max_epochs: 10
---  max_steps: null
---  accumulate_grad_batches: 4
---  gradient_clip_val: 0
---  amp_level: O1
---  precision: 16
---  accelerator: ddp
---  checkpoint_callback: false
---  logger: false
---  log_every_n_steps: 1
---  val_check_interval: 1.0
---  resume_from_checkpoint: null
---exp_manager:
---  exp_dir: /home/nxingyu/project/
---  name: Punctuation_with_Domain_discriminator
---  create_tensorboard_logger: true
---  create_checkpoint_callback: true
---base_path: /home/nxingyu/data
---tmp_path: /home/nxingyu/data/tmp
---model:
---  nemo_path: null
---  transformer_path: google/electra-small-discriminator
---  unfrozen: 0
---  maximum_unfrozen: 1
---  unfreeze_step: 1
---  punct_label_ids:
---  - ''
---  - '!'
---  - ','
---  - '-'
---  - .
---  - ':'
---  - ;
---  - '?'
---  - —
---  - …
---  punct_class_weights: false
---  dataset:
---    data_dir: /home/nxingyu/data
---    labelled:
---    - /home/nxingyu/data/ted_talks_processed
---    unlabelled:
---    - /home/nxingyu/data/open_subtitles_processed
---    max_seq_length: 128
---    pad_label: ''
---    ignore_extra_tokens: false
---    ignore_start_end: false
---    use_cache: false
---    num_workers: 4
---    pin_memory: true
---    drop_last: false
---    num_labels: 10
---    num_domains: 2
---    test_unlabelled: true
---    train_ds:
---      shuffle: true
---      num_samples: -1
---      batch_size: 4
---    validation_ds:
---      shuffle: true
---      num_samples: -1
---      batch_size: 4
---  tokenizer:
---    tokenizer_name: google/electra-small-discriminator
---    vocab_file: null
---    tokenizer_model: null
---    special_tokens: null
---  language_model:
---    pretrained_model_name: google/electra-small-discriminator
---    lm_checkpoint: null
---    config_file: null
---    config: null
---  punct_head:
---    punct_num_fc_layers: 1
---    fc_dropout: 0.1
---    activation: relu
---    log_softmax: false
---    use_transformer_init: true
---    loss: dice
---  domain_head:
---    domain_num_fc_layers: 1
---    fc_dropout: 0.1
---    activation: relu
---    log_softmax: false
---    use_transformer_init: true
---    loss: cel
---    gamma: 0
---    pooling: mean_max
---    idx_conditioned_on: 0
---  dice_loss:
---    epsilon: 0.01
---    alpha: 3
---    macro_average: true
---  focal_loss:
---    gamma: 1
---  optim:
---    name: adamw
---    lr: 0.001
---    weight_decay: 0.0
---    sched:
---      name: WarmupAnnealing
---      warmup_steps: null
---      warmup_ratio: 0.1
---      last_epoch: -1
---      monitor: val_loss
---      reduce_on_plateau: false
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt
--deleted file mode 100644
--index 5be2535..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt
--+++ /dev/null
--@@ -1,40 +0,0 @@
---Global seed set to 42
---GPU available: True, used: True
---TPU available: None, using: 0 TPU cores
---LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
---Using native 16bit precision.
---Global seed set to 42
---initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 1.0 K 
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---299 K     Trainable params
---13.2 M    Non-trainable params
---13.5 M    Total params
---Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lr_find_temp_model.ckpt
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 1.0 K 
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---299 K     Trainable params
---13.2 M    Non-trainable params
---13.5 M    Total params
---Epoch 0, global step 5305: val_loss reached 10.18683 (best 10.18683), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/checkpoints/Punctuation_with_Domain_discriminator---val_loss=10.19-epoch=0.ckpt" as top 3
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt
--deleted file mode 100644
--index 2f2fa91..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt
--+++ /dev/null
--@@ -1,19 +0,0 @@
---[NeMo W 2021-02-09 08:26:56 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-09 08:27:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-09 08:34:47 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-09 08:40:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
---[NeMo W 2021-02-09 10:21:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fd0> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-09 10:36:30 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fa0> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-09 11:07:22 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
---      warnings.warn(*args, **kwargs)
---    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt
--deleted file mode 100644
--index 2546dc9..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt
--+++ /dev/null
--@@ -1,21 +0,0 @@
---[NeMo I 2021-02-09 08:26:56 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56
---[NeMo I 2021-02-09 08:26:56 exp_manager:519] TensorboardLogger has been set up
---[NeMo W 2021-02-09 08:26:56 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-09 08:27:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-09 08:34:47 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-09 08:40:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
---[NeMo W 2021-02-09 10:21:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fd0> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-09 10:36:30 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fa0> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-09 11:07:22 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
---      warnings.warn(*args, **kwargs)
---    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/cmd-args.log
--deleted file mode 100644
--index 11a5d8e..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/cmd-args.log
--+++ /dev/null
--@@ -1 +0,0 @@
---main.py
--\ No newline at end of file
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/events.out.tfevents.1612840713.Titan.20119.0 b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/events.out.tfevents.1612840713.Titan.20119.0
--deleted file mode 100644
--index a4eb1d2..0000000
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/events.out.tfevents.1612840713.Titan.20119.0 and /dev/null differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/git-info.log
--deleted file mode 100644
--index 5420ee2..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/git-info.log
--+++ /dev/null
--@@ -1,645 +0,0 @@
---commit hash: 089ad1caa03e468560d6d322ace7a5164a8178f3
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt
---index 2a26724..5be2535 100644
------ a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt
---+++ b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt
---@@ -20,3 +20,21 @@ initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--- 299 K     Trainable params
--- 13.2 M    Non-trainable params
--- 13.5 M    Total params
---+Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lr_find_temp_model.ckpt
---+Global seed set to 42
---+
---+  | Name                | Type                 | Params
---+-------------------------------------------------------------
---+0 | transformer         | ElectraModel         | 13.5 M
---+1 | punct_classifier    | TokenClassifier      | 2.6 K 
---+2 | domain_classifier   | SequenceClassifier   | 1.0 K 
---+3 | punctuation_loss    | FocalDiceLoss        | 0     
---+4 | domain_loss         | CrossEntropyLoss     | 0     
---+5 | agg_loss            | AggregatorLoss       | 0     
---+6 | punct_class_report  | ClassificationReport | 0     
---+7 | domain_class_report | ClassificationReport | 0     
---+-------------------------------------------------------------
---+299 K     Trainable params
---+13.2 M    Non-trainable params
---+13.5 M    Total params
---+Epoch 0, global step 5305: val_loss reached 10.18683 (best 10.18683), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/checkpoints/Punctuation_with_Domain_discriminator---val_loss=10.19-epoch=0.ckpt" as top 3
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt
---index 0f1c742..2f2fa91 100644
------ a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt
---+++ b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt
---@@ -2,3 +2,18 @@
--- [NeMo W 2021-02-09 08:27:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---       warnings.warn(*args, **kwargs)
---     
---+[NeMo W 2021-02-09 08:34:47 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---+      warnings.warn(*args, **kwargs)
---+    
---+[NeMo W 2021-02-09 08:40:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---+      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---+    
---+[NeMo W 2021-02-09 10:21:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fd0> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---+      warnings.warn(warn_msg)
---+    
---+[NeMo W 2021-02-09 10:36:30 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fa0> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---+      warnings.warn(warn_msg)
---+    
---+[NeMo W 2021-02-09 11:07:22 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
---+      warnings.warn(*args, **kwargs)
---+    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt
---index e609b5b..2546dc9 100644
------ a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt
---+++ b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt
---@@ -4,3 +4,18 @@
--- [NeMo W 2021-02-09 08:27:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---       warnings.warn(*args, **kwargs)
---     
---+[NeMo W 2021-02-09 08:34:47 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---+      warnings.warn(*args, **kwargs)
---+    
---+[NeMo W 2021-02-09 08:40:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---+      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---+    
---+[NeMo W 2021-02-09 10:21:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fd0> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---+      warnings.warn(warn_msg)
---+    
---+[NeMo W 2021-02-09 10:36:30 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fa0> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---+      warnings.warn(warn_msg)
---+    
---+[NeMo W 2021-02-09 11:07:22 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
---+      warnings.warn(*args, **kwargs)
---+    
---diff --git a/experiment/Nemo2Lightning.ipynb b/experiment/Nemo2Lightning.ipynb
---index d2ec988..0dbd499 100644
------ a/experiment/Nemo2Lightning.ipynb
---+++ b/experiment/Nemo2Lightning.ipynb
---@@ -2,7 +2,7 @@
---  "cells": [
---   {
---    "cell_type": "code",
----   "execution_count": 2,
---+   "execution_count": 1,
---    "metadata": {},
---    "outputs": [
---     {
---@@ -11,7 +11,7 @@
---      "text": [
---       "Using device: cuda\n",
---       "\n",
----      "Tesla T4\n",
---+      "GeForce GTX 1080 Ti\n",
---       "Memory Usage:\n",
---       "Allocated: 0.0 GB\n",
---       "Cached:    0.0 GB\n"
---@@ -33,16 +33,16 @@
---   },
---   {
---    "cell_type": "code",
----   "execution_count": 1,
---+   "execution_count": 2,
---    "metadata": {},
---    "outputs": [
---     {
---      "data": {
---       "text/plain": [
----       "{'seed': 42, 'trainer': {'gpus': 1, 'num_nodes': 1, 'max_epochs': 6, 'max_steps': None, 'accumulate_grad_batches': 8, 'gradient_clip_val': 0, 'amp_level': 'O1', 'precision': 16, 'accelerator': 'ddp', 'checkpoint_callback': False, 'logger': False, 'log_every_n_steps': 1, 'val_check_interval': 1.0, 'resume_from_checkpoint': None}, 'exp_manager': {'exp_dir': '/home/nxingyu2/project/', 'name': 'Punctuation_with_Domain_discriminator', 'create_tensorboard_logger': True, 'create_checkpoint_callback': True}, 'base_path': '/home/nxingyu2/data', 'tmp_path': '/home/nxingyu2/data/tmp', 'model': {'nemo_path': None, 'transformer_path': 'google/electra-base-discriminator', 'initial_unfrozen': 1, 'punct_label_ids': ['', '!', ',', '-', '.', ':', ';', '?', '—', '…'], 'dataset': {'data_dir': '/home/nxingyu2/data', 'labelled': ['${base_path}/open_subtitles_processed'], 'unlabelled': ['${base_path}/ted_talks_processed'], 'max_seq_length': 128, 'pad_label': '', 'ignore_extra_tokens': False, 'ignore_start_end': False, 'use_cache': False, 'num_workers': 0, 'pin_memory': True, 'drop_last': False, 'num_labels': 10, 'num_domains': 1, 'test_unlabelled': True, 'train_ds': {'shuffle': True, 'num_samples': -1, 'batch_size': 8}, 'validation_ds': {'shuffle': True, 'num_samples': -1, 'batch_size': 2}}, 'tokenizer': {'tokenizer_name': '${model.language_model.pretrained_model_name}', 'vocab_file': None, 'tokenizer_model': None, 'special_tokens': None}, 'language_model': {'pretrained_model_name': '${model.transformer_path}', 'lm_checkpoint': None, 'config_file': None, 'config': None}, 'punct_head': {'punct_num_fc_layers': 1, 'fc_dropout': 0.1, 'activation': 'relu', 'log_softmax': False, 'use_transformer_init': True, 'loss': 'crf'}, 'domain_head': {'domain_num_fc_layers': 1, 'fc_dropout': 0.1, 'activation': 'relu', 'log_softmax': False, 'use_transformer_init': True, 'loss': 'cel', 'gamma': 0.1}, 'dice_loss': {'epsilon': 0.01, 'alpha': 3, 'macro_average': True}, 'focal_loss': {'gamma': 5}, 'optim': {'name': 'adamw', 'lr': 0.001, 'weight_decay': 0.0, 'sched': {'name': 'WarmupAnnealing', 'warmup_steps': None, 'warmup_ratio': 0.1, 'last_epoch': -1, 'monitor': 'val_loss', 'reduce_on_plateau': False}}}}"
---+       "{'seed': 42, 'trainer': {'gpus': 1, 'num_nodes': 1, 'max_epochs': 10, 'max_steps': None, 'accumulate_grad_batches': 4, 'gradient_clip_val': 0, 'amp_level': 'O1', 'precision': 16, 'accelerator': 'ddp', 'checkpoint_callback': False, 'logger': False, 'log_every_n_steps': 1, 'val_check_interval': 1.0, 'resume_from_checkpoint': None}, 'exp_manager': {'exp_dir': '/home/nxingyu/project/', 'name': 'Punctuation_with_Domain_discriminator', 'create_tensorboard_logger': True, 'create_checkpoint_callback': True}, 'base_path': '/home/nxingyu/data', 'tmp_path': '/home/nxingyu/data/tmp', 'model': {'nemo_path': None, 'transformer_path': 'google/electra-small-discriminator', 'unfrozen': 0, 'maximum_unfrozen': 1, 'unfreeze_step': 1, 'punct_label_ids': ['', '!', ',', '-', '.', ':', ';', '?', '—', '…'], 'punct_class_weights': False, 'dataset': {'data_dir': '/home/nxingyu/data', 'labelled': ['${base_path}/ted_talks_processed'], 'unlabelled': ['${base_path}/open_subtitles_processed'], 'max_seq_length': 128, 'pad_label': '', 'ignore_extra_tokens': False, 'ignore_start_end': False, 'use_cache': False, 'num_workers': 4, 'pin_memory': True, 'drop_last': False, 'num_labels': 10, 'num_domains': 2, 'test_unlabelled': True, 'train_ds': {'shuffle': True, 'num_samples': -1, 'batch_size': 4}, 'validation_ds': {'shuffle': True, 'num_samples': -1, 'batch_size': 4}}, 'tokenizer': {'tokenizer_name': '${model.language_model.pretrained_model_name}', 'vocab_file': None, 'tokenizer_model': None, 'special_tokens': None}, 'language_model': {'pretrained_model_name': '${model.transformer_path}', 'lm_checkpoint': None, 'config_file': None, 'config': None}, 'punct_head': {'punct_num_fc_layers': 1, 'fc_dropout': 0.1, 'activation': 'relu', 'log_softmax': False, 'use_transformer_init': True, 'loss': 'dice'}, 'domain_head': {'domain_num_fc_layers': 1, 'fc_dropout': 0.1, 'activation': 'relu', 'log_softmax': False, 'use_transformer_init': True, 'loss': 'cel', 'gamma': 0, 'pooling': 'mean_max', 'idx_conditioned_on': 0}, 'dice_loss': {'epsilon': 0.01, 'alpha': 3, 'macro_average': True}, 'focal_loss': {'gamma': 1}, 'optim': {'name': 'adamw', 'lr': 0.001, 'weight_decay': 0.0, 'sched': {'name': 'WarmupAnnealing', 'warmup_steps': None, 'warmup_ratio': 0.1, 'last_epoch': -1, 'monitor': 'val_loss', 'reduce_on_plateau': False}}}}"
---       ]
---      },
----     "execution_count": 1,
---+     "execution_count": 2,
---      "metadata": {},
---      "output_type": "execute_result"
---     }
---@@ -74,79 +74,79 @@
---   },
---   {
---    "cell_type": "code",
----   "execution_count": 13,
---+   "execution_count": 3,
---    "metadata": {},
---    "outputs": [
---     {
---      "name": "stderr",
---      "output_type": "stream",
---      "text": [
----      "09:01:12.28 LOG:\n",
----      "09:01:12.30 .... 'cel none' = 'cel none'\n",
----      "09:01:12.31 .... output = tensor([1.5514, 1.5514, 0.5514, 0.5514], grad_fn=<NllLossBackward>)\n",
----      "09:01:12.31 LOG:\n",
----      "09:01:12.31 .... 'cel mean' = 'cel mean'\n",
----      "09:01:12.31 .... output = tensor(1.0514, grad_fn=<NllLossBackward>)\n",
----      "09:01:12.31 LOG:\n",
----      "09:01:12.31 .... 'cel sum' = 'cel sum'\n",
----      "09:01:12.31 .... output = tensor(4.2058, grad_fn=<NllLossBackward>)\n",
----      "09:01:12.31 LOG:\n",
----      "09:01:12.32 .... 'focal sum' = 'focal sum'\n",
----      "09:01:12.32 .... loss(inp, tar) = tensor(6.7352, grad_fn=<SumBackward0>)\n",
----      "09:01:12.32 LOG:\n",
----      "09:01:12.32 .... 'focal mean' = 'focal mean'\n",
----      "09:01:12.32 .... loss(inp, tar) = tensor(0.4210, grad_fn=<MeanBackward0>)\n",
----      "09:01:12.32 LOG:\n",
----      "09:01:12.33 .... 'focal none' = 'focal none'\n",
----      "09:01:12.33 .... loss(inp, tar) = tensor([0.9635, 0.9635, 0.0991, 0.0991], grad_fn=<MulBackward0>)\n",
----      "09:01:12.33 LOG:\n",
----      "09:01:12.33 .... 'focal none' = 'focal none'\n",
----      "09:01:12.33 .... loss(inp, tar) = tensor([0.4716, 0.4716, 0.0075, 0.0075], grad_fn=<MulBackward0>)\n",
----      "09:01:12.33 LOG:\n",
----      "09:01:12.34 .... 'crf,none' = 'crf,none'\n",
----      "09:01:12.34 .... output = tensor([4.2927], grad_fn=<NegBackward>)\n",
----      "09:01:12.34 LOG:\n",
----      "09:01:12.34 .... 'crf,mean' = 'crf,mean'\n",
----      "09:01:12.34 .... output = tensor(4.3138, grad_fn=<NegBackward>)\n",
----      "09:01:12.34 LOG:\n",
----      "09:01:12.35 .... 'crf,sum' = 'crf,sum'\n",
----      "09:01:12.35 .... output = tensor(4.2588, grad_fn=<NegBackward>)\n",
----      "09:01:12.35 LOG:\n",
----      "09:01:12.35 .... 'crf,token_mean' = 'crf,token_mean'\n",
----      "09:01:12.35 .... output = tensor(1.0429, grad_fn=<DivBackward0>)\n",
----      "09:01:12.35 LOG:\n",
----      "09:01:12.35 .... 'dice none,micro' = 'dice none,micro'\n",
----      "09:01:12.36 .... output = tensor(0.1876, grad_fn=<PowBackward0>)\n",
----      "09:01:12.36 LOG:\n",
----      "09:01:12.36 .... 'dice mean,micro' = 'dice mean,micro'\n",
----      "09:01:12.36 .... output = tensor(0.0625, grad_fn=<DivBackward0>)\n",
----      "09:01:12.36 LOG:\n",
----      "09:01:12.36 .... 'dice sum,micro' = 'dice sum,micro'\n",
----      "09:01:12.36 .... output = tensor(0.1876, grad_fn=<SumBackward0>)\n",
----      "09:01:12.37 LOG:\n",
----      "09:01:12.37 .... 'dice sum,micro' = 'dice sum,micro'\n",
----      "09:01:12.37 .... output = tensor(0.0812, grad_fn=<SumBackward0>)\n",
----      "09:01:12.37 LOG:\n",
----      "09:01:12.37 .... 'dice none,macro' = 'dice none,macro'\n",
----      "09:01:12.37 .... loss(inp, tar) = tensor([5.9547, 4.4872, 2.0551], grad_fn=<MulBackward0>)\n",
----      "09:01:12.37 LOG:\n",
----      "09:01:12.38 .... 'dice mean,macro' = 'dice mean,macro'\n",
----      "09:01:12.38 .... loss(inp, tar) = tensor(0.2112, grad_fn=<DivBackward0>)\n",
----      "09:01:12.38 LOG:\n",
----      "09:01:12.38 .... 'dice sum,macro' = 'dice sum,macro'\n",
----      "09:01:12.38 .... loss(inp, tar) = tensor(0.6335, grad_fn=<SumBackward0>)\n",
----      "09:01:12.38 LOG:\n",
----      "09:01:12.38 .... 'dice none,macro' = 'dice none,macro'\n",
----      "09:01:12.39 .... loss(inp, tar) = tensor([0.3340, 0.6546, 0.3057], grad_fn=<MulBackward0>)\n",
----      "09:01:12.39 LOG:\n",
----      "09:01:12.39 .... 'dice none,macro' = 'dice none,macro'\n",
----      "09:01:12.39 .... loss(inp, tar) = tensor([0.0373, 0.2805, 0.0286], grad_fn=<MulBackward0>)\n",
----      "09:01:12.39 LOG:\n",
----      "09:01:12.39 .... 'dice sum,macro' = 'dice sum,macro'\n",
----      "09:01:12.40 .... output = tensor([0.5989, 0.7696, 0.2411], grad_fn=<MulBackward0>)\n",
----      "09:01:12.40 LOG:\n",
----      "09:01:12.40 .... 'dice sum,macro' = 'dice sum,macro'\n",
----      "09:01:12.40 .... output = tensor([0.2148, 0.4559, 0.0140], grad_fn=<MulBackward0>)\n"
---+      "10:11:13.98 LOG:\n",
---+      "10:11:14.02 .... 'cel none' = 'cel none'\n",
---+      "10:11:14.02 .... output = tensor([1.5514, 1.5514, 0.5514, 0.5514], grad_fn=<NllLossBackward>)\n",
---+      "10:11:14.02 LOG:\n",
---+      "10:11:14.02 .... 'cel mean' = 'cel mean'\n",
---+      "10:11:14.02 .... output = tensor(1.0514, grad_fn=<NllLossBackward>)\n",
---+      "10:11:14.03 LOG:\n",
---+      "10:11:14.03 .... 'cel sum' = 'cel sum'\n",
---+      "10:11:14.03 .... output = tensor(4.2058, grad_fn=<NllLossBackward>)\n",
---+      "10:11:14.08 LOG:\n",
---+      "10:11:14.08 .... 'focal sum' = 'focal sum'\n",
---+      "10:11:14.08 .... loss(inp, tar) = tensor(6.7352, grad_fn=<SumBackward0>)\n",
---+      "10:11:14.08 LOG:\n",
---+      "10:11:14.08 .... 'focal mean' = 'focal mean'\n",
---+      "10:11:14.09 .... loss(inp, tar) = tensor(0.4210, grad_fn=<MeanBackward0>)\n",
---+      "10:11:14.09 LOG:\n",
---+      "10:11:14.09 .... 'focal none' = 'focal none'\n",
---+      "10:11:14.10 .... loss(inp, tar) = tensor([0.9635, 0.9635, 0.0991, 0.0991], grad_fn=<MulBackward0>)\n",
---+      "10:11:14.10 LOG:\n",
---+      "10:11:14.10 .... 'focal none' = 'focal none'\n",
---+      "10:11:14.10 .... loss(inp, tar) = tensor([0.4716, 0.4716, 0.0075, 0.0075], grad_fn=<MulBackward0>)\n",
---+      "10:11:14.11 LOG:\n",
---+      "10:11:14.12 .... 'crf,none' = 'crf,none'\n",
---+      "10:11:14.12 .... output = tensor([4.1689], grad_fn=<NegBackward>)\n",
---+      "10:11:14.12 LOG:\n",
---+      "10:11:14.12 .... 'crf,mean' = 'crf,mean'\n",
---+      "10:11:14.12 .... output = tensor(4.0544, grad_fn=<NegBackward>)\n",
---+      "10:11:14.12 LOG:\n",
---+      "10:11:14.13 .... 'crf,sum' = 'crf,sum'\n",
---+      "10:11:14.13 .... output = tensor(4.1272, grad_fn=<NegBackward>)\n",
---+      "10:11:14.13 LOG:\n",
---+      "10:11:14.13 .... 'crf,token_mean' = 'crf,token_mean'\n",
---+      "10:11:14.13 .... output = tensor(1.0815, grad_fn=<DivBackward0>)\n",
---+      "10:11:14.13 LOG:\n",
---+      "10:11:14.14 .... 'dice none,micro' = 'dice none,micro'\n",
---+      "10:11:14.14 .... output = tensor(0.4331, grad_fn=<PowBackward0>)\n",
---+      "10:11:14.14 LOG:\n",
---+      "10:11:14.14 .... 'dice mean,micro' = 'dice mean,micro'\n",
---+      "10:11:14.14 .... output = tensor(0.1444, grad_fn=<DivBackward0>)\n",
---+      "10:11:14.14 LOG:\n",
---+      "10:11:14.14 .... 'dice sum,micro' = 'dice sum,micro'\n",
---+      "10:11:14.15 .... output = tensor(0.4331, grad_fn=<SumBackward0>)\n",
---+      "10:11:14.15 LOG:\n",
---+      "10:11:14.15 .... 'dice sum,micro' = 'dice sum,micro'\n",
---+      "10:11:14.15 .... output = tensor(0.0812, grad_fn=<SumBackward0>)\n",
---+      "10:11:14.15 LOG:\n",
---+      "10:11:14.15 .... 'dice none,macro' = 'dice none,macro'\n",
---+      "10:11:14.16 .... loss(inp, tar) = tensor([5.9547, 4.4872, 2.0551], grad_fn=<MulBackward0>)\n",
---+      "10:11:14.16 LOG:\n",
---+      "10:11:14.16 .... 'dice mean,macro' = 'dice mean,macro'\n",
---+      "10:11:14.16 .... loss(inp, tar) = tensor(0.4314, grad_fn=<DivBackward0>)\n",
---+      "10:11:14.16 LOG:\n",
---+      "10:11:14.16 .... 'dice sum,macro' = 'dice sum,macro'\n",
---+      "10:11:14.16 .... loss(inp, tar) = tensor(1.2943, grad_fn=<SumBackward0>)\n",
---+      "10:11:14.17 LOG:\n",
---+      "10:11:14.17 .... 'dice none,macro' = 'dice none,macro'\n",
---+      "10:11:14.17 .... loss(inp, tar) = tensor([0.3340, 0.6546, 0.3057], grad_fn=<MulBackward0>)\n",
---+      "10:11:14.17 LOG:\n",
---+      "10:11:14.17 .... 'dice none,macro' = 'dice none,macro'\n",
---+      "10:11:14.17 .... loss(inp, tar) = tensor([0.0373, 0.2805, 0.0286], grad_fn=<MulBackward0>)\n",
---+      "10:11:14.18 LOG:\n",
---+      "10:11:14.18 .... 'dice sum,macro' = 'dice sum,macro'\n",
---+      "10:11:14.18 .... output = tensor([0.5989, 0.7696, 0.2411], grad_fn=<MulBackward0>)\n",
---+      "10:11:14.18 LOG:\n",
---+      "10:11:14.18 .... 'dice sum,macro' = 'dice sum,macro'\n",
---+      "10:11:14.18 .... output = tensor([0.2148, 0.4559, 0.0140], grad_fn=<MulBackward0>)\n"
---      ]
---     },
---     {
---@@ -155,7 +155,7 @@
---        "('dice sum,macro', tensor([0.2148, 0.4559, 0.0140], grad_fn=<MulBackward0>))"
---       ]
---      },
----     "execution_count": 13,
---+     "execution_count": 3,
---      "metadata": {},
---      "output_type": "execute_result"
---     }
---@@ -286,32 +286,25 @@
---   },
---   {
---    "cell_type": "code",
----   "execution_count": 2,
---+   "execution_count": 4,
---    "metadata": {},
---    "outputs": [
---     {
----     "name": "stderr",
----     "output_type": "stream",
----     "text": [
----      "10:05:46.40 LOG:\n",
----      "10:05:46.46 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
----      "10:05:46.66 LOG:\n",
----      "10:05:46.66 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
----      "10:06:04.19 LOG:\n",
----      "10:06:04.20 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
----      "10:06:04.34 LOG:\n",
----      "10:06:04.34 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n"
---+     "ename": "KeyboardInterrupt",
---+     "evalue": "",
---+     "output_type": "error",
---+     "traceback": [
---+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
---+      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
---+      "\u001b[0;32m<ipython-input-4-b22a0d7714b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mdata_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
---+      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_prepared_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
---+      "\u001b[0;32m~/project/experiment/data/punctuation_datamodule.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'fit'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             self.train_dataset = PunctuationDomainDatasets(split='train',\n\u001b[0m\u001b[1;32m     64\u001b[0m                     \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                     \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
---+      "\u001b[0;32m~/project/experiment/data/punctuation_dataset_multi.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, split, num_samples, max_seq_length, punct_label_ids, labelled, unlabelled, tokenizer, randomize, data_id, tmp_path)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munlabelled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             dataset=PunctuationDomainDataset(\n\u001b[0m\u001b[1;32m    187\u001b[0m                     \u001b[0mcsv_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'{path}.{split}.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                     \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
---+      "\u001b[0;32m~/project/experiment/data/punctuation_dataset_multi.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, csv_file, tokenizer, num_samples, max_seq_length, degree, punct_label_ids, domain, labelled, randomize, target_file, tmp_path, start, end)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv_file\u001b[0m \u001b[0;34m=\u001b[0m   \u001b[0mcsv_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m   \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m=\u001b[0m  \u001b[0mdomain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunct_label_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpunct_label_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
---+      "\u001b[0;32m~/project/experiment/data/punctuation_dataset_multi.py\u001b[0m in \u001b[0;36mset_num_samples\u001b[0;34m(self, csv_file, num_samples)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-l'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTDOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_samples\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
---+      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1009\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stdin_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m                 \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
---+      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
---      ]
----    },
----    {
----     "data": {
----      "text/plain": [
----       "10609"
----      ]
----     },
----     "execution_count": 2,
----     "metadata": {},
----     "output_type": "execute_result"
---     }
---    ],
---    "source": [
---@@ -343,20 +336,9 @@
---   },
---   {
---    "cell_type": "code",
----   "execution_count": 4,
---+   "execution_count": null,
---    "metadata": {},
----   "outputs": [
----    {
----     "data": {
----      "text/plain": [
----       "10609"
----      ]
----     },
----     "execution_count": 4,
----     "metadata": {},
----     "output_type": "execute_result"
----    }
----   ],
---+   "outputs": [],
---    "source": [
---     "# it=dm.train_dataset\n",
---     "# ni=next(it)\n",
---diff --git a/experiment/core/losses/linear_chain_crf.py b/experiment/core/losses/linear_chain_crf.py
---index ed813a9..8dc59cc 100644
------ a/experiment/core/losses/linear_chain_crf.py
---+++ b/experiment/core/losses/linear_chain_crf.py
---@@ -92,6 +92,17 @@ class LinearChainCRF(torch.nn.Module):
---             mask = logits.new_ones(logits.shape[:2], dtype=torch.bool)
---         return self._viterbi_decode(logits,mask)
--- 
---+    @jit.export
---+    def predict(self, logits: Tensor, mask: Optional[Tensor] = None) -> LongTensor:
---+        self._validate(logits, mask=mask)
---+
---+        if mask is None:
---+            mask = logits.new_ones(logits.shape[:2], dtype=torch.bool)
---+        out=[]
---+        for p,m in iter(zip(logits,mask)):
---+            out.append(pad_to_len(logits.shape[1],self._viterbi_decode(p.unsqueeze(0),m.unsqueeze(0))))
---+        return torch.tensor(out)
---+        
---     def _viterbi_decode(self, logits: Tensor, mask: Tensor) -> LongTensor:
---         """
---         decode labels using viterbi algorithm
---diff --git a/experiment/core/utils.py b/experiment/core/utils.py
---index 058cc87..4be7503 100644
------ a/experiment/core/utils.py
---+++ b/experiment/core/utils.py
---@@ -3,6 +3,7 @@ import torch
--- from torch import nn
--- import regex as re
--- import snoop
---+from copy import deepcopy
--- 
--- __all__ = ['chunk_examples_with_degree', 'chunk_to_len_batch', 'view_aligned']
--- 
---@@ -26,14 +27,15 @@ def position_to_mask(max_seq_length:int,indices:list):
---         o[np.array(indices)%(max_seq_length-2)+1]=1
---     except:
---         pp('position_to_mask',np.array(indices)%(max_seq_length-2)+1)
----        # o[(np.array(indices)%(max_seq_length-2)+1).astype(int)]=1
---+        o[(np.array(indices)%(max_seq_length-2)+1).astype(int)]=1
---     return o
--- 
--- def align_labels_to_mask(mask,labels):
---     '''[0,1,0],[2] -> [0,2,0]'''
---     assert(sum(mask)==len(labels))
----    mask[mask>0]=torch.tensor(labels)
----    return mask.tolist()
---+    m1=mask.copy()
---+    m1[mask>0]=torch.tensor(labels)
---+    return m1.tolist()
--- 
--- def view_aligned(texts,tags,tokenizer,labels_to_ids):
---         return [re.sub(' ##','',' '.join(
---@@ -101,7 +103,9 @@ def chunk_to_len(max_seq_length,tokenizer,tokens,labels=None):
---     split_token_end_idxs=np.array_split(token_end_idxs,breakpoints)
---     split_subwords=np.array_split(subwords,np.arange(max_seq_length-2,len(subwords),max_seq_length-2))
---     ids=[pad_to_len(max_seq_length,tokenizer.convert_tokens_to_ids(['[CLS]']+list(_)+['[SEP]'])) for _ in split_subwords]
----    masks=[position_to_mask(max_seq_length,_) for _ in split_token_end_idxs]
---+    masks=[]
---+    for _ in split_token_end_idxs:
---+        masks.append(position_to_mask(max_seq_length,_).copy())
---     padded_labels=None
---     if labels!=None:
---         split_labels=np.array_split(labels,breakpoints)
---@@ -121,7 +125,7 @@ def chunk_to_len_batch(max_seq_length,tokenizer,tokens,labels=None,labelled=True
---     output = {'input_ids': torch.as_tensor(batch_ids, dtype=torch.long),
---               'attention_mask': torch.as_tensor(batch_ids, dtype=torch.bool),
---               'subtoken_mask': torch.as_tensor(batch_masks,dtype=torch.bool)}
----    output['subtoken_mask']|=(output['input_ids']==101)|(output['input_ids']==102)
---+    output['subtoken_mask']|=((output['input_ids']==101)|(output['input_ids']==102))
---     output['subtoken_mask']&=labelled
---     output['labels']=torch.as_tensor(batch_labels,dtype=torch.long) if labelled==True else torch.zeros_like(output['input_ids'],dtype=torch.long)
---     return output
---diff --git a/experiment/data/punctuation_dataset.py b/experiment/data/punctuation_dataset.py
---index bfd015c..c3d9fb6 100644
------ a/experiment/data/punctuation_dataset.py
---+++ b/experiment/data/punctuation_dataset.py
---@@ -10,6 +10,7 @@ import torch
--- import subprocess
--- from time import time
--- from itertools import cycle
---+import math
--- 
--- class PunctuationDomainDataset(IterableDataset):
--- 
---@@ -242,18 +243,23 @@ class PunctuationInferenceDataset(Dataset):
---             "labels": NeuralType(('B', 'T'), ChannelType()),
---         }
--- 
----    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], degree:int = 0, ):
---+    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], num_samples:int=256, degree:int = 0):
---         """ Initializes BertPunctuationInferDataset. """
---+        self.degree=degree
---+        self.punct_label_ids=punct_label_ids
---         chunked=chunk_examples_with_degree(self.degree, self.punct_label_ids)(queries)
---         features = chunk_to_len_batch(max_seq_length=max_seq_length, tokenizer=tokenizer,tokens=chunked['texts'],labelled=False)
---         self.all_input_ids = features['input_ids']
---         self.all_attention_mask = features['attention_mask']
---         self.all_subtoken_mask = features['subtoken_mask']
---+        self.num_samples=num_samples
--- 
---     def __len__(self):
----        return len(self.all_input_ids)
---+        return math.ceil(len(self.all_input_ids)/self.num_samples)
--- 
---     def __getitem__(self, idx):
----        return {'input_ids':self.all_input_ids[idx],
----                'attention_mask':self.all_attention_mask[idx],
----                'subtoken_mask':self.all_subtoken_mask[idx]}
---+        lower=idx*self.num_samples
---+        upper=min(len(self.all_input_ids),(idx+1)*self.num_samples+1)
---+        return {'input_ids':self.all_input_ids[lower:upper],
---+                'attention_mask':self.all_attention_mask[lower:upper],
---+                'subtoken_mask':self.all_subtoken_mask[lower:upper]}
---diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
---index 4027bb2..b3fe282 100644
------ a/experiment/data/punctuation_dataset_multi.py
---+++ b/experiment/data/punctuation_dataset_multi.py
---@@ -261,18 +261,23 @@ class PunctuationInferenceDataset(Dataset):
---             "labels": NeuralType(('B', 'T'), ChannelType()),
---         }
--- 
----    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], degree:int = 0, ):
---+    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], num_samples:int=256, degree:int = 0):
---         """ Initializes BertPunctuationInferDataset. """
---+        self.degree=degree
---+        self.punct_label_ids=punct_label_ids
---         chunked=chunk_examples_with_degree(self.degree, self.punct_label_ids)(queries)
---         features = chunk_to_len_batch(max_seq_length=max_seq_length, tokenizer=tokenizer,tokens=chunked['texts'],labelled=False)
---         self.all_input_ids = features['input_ids']
---         self.all_attention_mask = features['attention_mask']
---         self.all_subtoken_mask = features['subtoken_mask']
---+        self.num_samples=num_samples
--- 
---     def __len__(self):
----        return len(self.all_input_ids)
---+        return math.ceil(len(self.all_input_ids)/self.num_samples)
--- 
---     def __getitem__(self, idx):
----        return {'input_ids':self.all_input_ids[idx],
----                'attention_mask':self.all_attention_mask[idx],
----                'subtoken_mask':self.all_subtoken_mask[idx]}
---+        lower=idx*self.num_samples
---+        upper=min(len(self.all_input_ids),(idx+1)*self.num_samples+1)
---+        return {'input_ids':self.all_input_ids[lower:upper],
---+                'attention_mask':self.all_attention_mask[lower:upper],
---+                'subtoken_mask':self.all_subtoken_mask[lower:upper]}
---diff --git a/experiment/info.log b/experiment/info.log
---index d9d501b..e69de29 100644
------ a/experiment/info.log
---+++ b/experiment/info.log
---@@ -1,17 +0,0 @@
----[INFO] - shuffling train set
----[INFO] - Optimizer config = AdamW (
----Parameter Group 0
----    amsgrad: False
----    betas: (0.9, 0.999)
----    eps: 1e-08
----    lr: 0.001
----    weight_decay: 0.0
----)
----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f412fde0d90>" 
----will be used during training (effective maximum steps = 80) - 
----Parameters : 
----(warmup_steps: null
----warmup_ratio: 0.1
----last_epoch: -1
----max_steps: 80
----)
---diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
---index c5db7b3..aa05eac 100644
------ a/experiment/models/punctuation_domain_model.py
---+++ b/experiment/models/punctuation_domain_model.py
---@@ -15,9 +15,9 @@ from core.losses import (AggregatorLoss, CrossEntropyLoss, FocalDiceLoss, FocalL
--- from pytorch_lightning.utilities import rank_zero_only
--- from core.optim import get_optimizer, parse_optimizer_args, prepare_lr_scheduler
--- from omegaconf import DictConfig, OmegaConf, open_dict
----from transformers import AutoModel
---+from transformers import AutoModel, AutoTokenizer
--- import torch.utils.data.dataloader as dataloader
----from data import PunctuationDataModule
---+from data import PunctuationDataModule, PunctuationInferenceDataset
--- from os import path
--- import tempfile
--- from core.common import Serialization, FileIO
---@@ -51,6 +51,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---         self._trainer = trainer
--- 
---         self.transformer = AutoModel.from_pretrained(self.hparams.model.transformer_path)
---+        self.tokenizer=AutoTokenizer.from_pretrained(self._cfg.model.transformer_path)
---         self.ids_to_labels = {_[0]: _[1]
---                               for _ in enumerate(self.hparams.model.punct_label_ids)}
---         self.labels_to_ids = {_[1]: _[0]
---@@ -707,4 +708,23 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---             if 'PL_TRAINER_GPUS' in os.environ:
---                 os.environ.pop('PL_TRAINER_GPUS')
--- 
----        super().teardown(stage)
---\ No newline at end of file
---+        super().teardown(stage)
---+
---+    def add_punctuation(self, queries):
---+        infer_ds=PunctuationInferenceDataset(
---+            tokenizer= self._cfg.model.transformer_path,
---+            queries=queries, 
---+            max_seq_length=self.hparams.model.dataset.max_seq_length,
---+            punct_label_ids=self._cfg.model.punct_label_ids)
---+        attention_mask = batch['attention_mask']
---+        subtoken_mask = batch['subtoken_mask']
---+        punct_labels = batch['labels']
---+        domain_labels = batch['domain']
---+        input_ids = batch['input_ids']
---+
---+        labelled_mask=(subtoken_mask[:,0]>0)
---+        test_loss, punct_logits, domain_logits = self._make_step(batch)
---+        # attention_mask = attention_mask > 0.5
---+        punct_preds = self.punctuation_loss.predict(punct_logits[labelled_mask], subtoken_mask[labelled_mask]) \
---+            if self.hparams.model.punct_head.loss == 'crf' else torch.argmax(punct_logits[labelled_mask], axis=-1)[subtoken_mask[labelled_mask]]
---+        return view_aligned(input_ids,punct_preds, self.tokenizer,self.ids_to_labels)
---\ No newline at end of file
---diff --git a/experiment/utils/__init__.py b/experiment/utils/__init__.py
---deleted file mode 100644
---index 9a292b8..0000000
------ a/experiment/utils/__init__.py
---+++ /dev/null
---@@ -1,2 +0,0 @@
----from utils.logging import Logger as _Logger
----logging = _Logger()
---diff --git a/experiment/utils/logging.py b/experiment/utils/logging.py
---deleted file mode 100644
---index 15511fd..0000000
------ a/experiment/utils/logging.py
---+++ /dev/null
---@@ -1,69 +0,0 @@
----import os.path
----import logging
----import traceback
----
----from logging import DEBUG, WARNING, ERROR, INFO
----__all__ = ['Logger']
----
----class Logger(object):
----
----    show_source_location = True
----    # Formats the message as needed and calls the correct logging method
----    # to actually handle it
----    def _raw_log(self, logfn, message, exc_info):
----        cname = ''
----        loc = ''
----        fn = ''
----        tb = traceback.extract_stack()
----        if len(tb) > 2:
----            if self.show_source_location:
----                loc = '(%s:%d):' % (os.path.basename(tb[-3][0]), tb[-3][1])
----            fn = tb[-3][2]
----            if fn != '<module>':
----                if self.__class__.__name__ != Logger.__name__:
----                    fn = self.__class__.__name__ + '.' + fn
----                fn += '()'
----
----        logfn(loc + cname + fn + ': ' + message, exc_info=exc_info)
----
----    def info(self, message, exc_info=False):
----        """
----        Log a info-level message. If exc_info is True, if an exception
----        was caught, show the exception information (message and stack trace).
----        """
----        self._raw_log(logging.info, message, exc_info)
----
----    def debug(self, message, exc_info=False):
----        """
----        Log a debug-level message. If exc_info is True, if an exception
----        was caught, show the exception information (message and stack trace).
----        """
----        self._raw_log(logging.debug, message, exc_info)
----
----    def warning(self, message, exc_info=False):
----        """
----        Log a warning-level message. If exc_info is True, if an exception
----        was caught, show the exception information (message and stack trace).
----        """
----        self._raw_log(logging.warning, message, exc_info)
----
----    def error(self, message, exc_info=False):
----        """
----        Log an error-level message. If exc_info is True, if an exception
----        was caught, show the exception information (message and stack trace).
----        """
----        self._raw_log(logging.error, message, exc_info)
----
----    @staticmethod
----    def basicConfig(level=DEBUG):
----        """
----        Apply a basic logging configuration which outputs the log to the
----        console (stderr). Optionally, the minimum log level can be set, one
----        of DEBUG, WARNING, ERROR (or any of the levels from the logging
----        module). If not set, DEBUG log level is used as minimum.
----        """
----        logging.basicConfig(level=level,
----                format='%(asctime)s %(levelname)s %(message)s',
----                datefmt='%Y-%m-%d %H:%M:%S')
----
----        logger = Logger()
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/hparams.yaml
--deleted file mode 100644
--index cbac11e..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/hparams.yaml
--+++ /dev/null
--@@ -1,110 +0,0 @@
---seed: 42
---trainer:
---  gpus: 1
---  num_nodes: 1
---  max_epochs: 10
---  max_steps: null
---  accumulate_grad_batches: 4
---  gradient_clip_val: 0
---  amp_level: O1
---  precision: 16
---  accelerator: ddp
---  checkpoint_callback: false
---  logger: false
---  log_every_n_steps: 1
---  val_check_interval: 1.0
---  resume_from_checkpoint: null
---exp_manager:
---  exp_dir: /home/nxingyu/project/
---  name: Punctuation_with_Domain_discriminator
---  create_tensorboard_logger: true
---  create_checkpoint_callback: true
---base_path: /home/nxingyu/data
---tmp_path: /home/nxingyu/data/tmp
---model:
---  nemo_path: null
---  transformer_path: google/electra-small-discriminator
---  unfrozen: 0
---  maximum_unfrozen: 1
---  unfreeze_step: 1
---  punct_label_ids:
---  - ''
---  - '!'
---  - ','
---  - '-'
---  - .
---  - ':'
---  - ;
---  - '?'
---  - —
---  - …
---  punct_class_weights: false
---  dataset:
---    data_dir: /home/nxingyu/data
---    labelled:
---    - /home/nxingyu/data/ted_talks_processed
---    unlabelled:
---    - /home/nxingyu/data/open_subtitles_processed
---    max_seq_length: 128
---    pad_label: ''
---    ignore_extra_tokens: false
---    ignore_start_end: false
---    use_cache: false
---    num_workers: 4
---    pin_memory: true
---    drop_last: false
---    num_labels: 10
---    num_domains: 2
---    test_unlabelled: true
---    train_ds:
---      shuffle: true
---      num_samples: -1
---      batch_size: 4
---    validation_ds:
---      shuffle: true
---      num_samples: -1
---      batch_size: 4
---  tokenizer:
---    tokenizer_name: google/electra-small-discriminator
---    vocab_file: null
---    tokenizer_model: null
---    special_tokens: null
---  language_model:
---    pretrained_model_name: google/electra-small-discriminator
---    lm_checkpoint: null
---    config_file: null
---    config: null
---  punct_head:
---    punct_num_fc_layers: 1
---    fc_dropout: 0.1
---    activation: relu
---    log_softmax: false
---    use_transformer_init: true
---    loss: dice
---  domain_head:
---    domain_num_fc_layers: 1
---    fc_dropout: 0.1
---    activation: relu
---    log_softmax: false
---    use_transformer_init: true
---    loss: cel
---    gamma: 0
---    pooling: mean_max
---    idx_conditioned_on: 0
---  dice_loss:
---    epsilon: 0.01
---    alpha: 3
---    macro_average: true
---  focal_loss:
---    gamma: 1
---  optim:
---    name: adamw
---    lr: 0.001
---    weight_decay: 0.0
---    sched:
---      name: WarmupAnnealing
---      warmup_steps: null
---      warmup_ratio: 0.1
---      last_epoch: -1
---      monitor: val_loss
---      reduce_on_plateau: false
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/lightning_logs.txt
--deleted file mode 100644
--index c7d0c2d..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/lightning_logs.txt
--+++ /dev/null
--@@ -1,39 +0,0 @@
---Global seed set to 42
---GPU available: True, used: True
---TPU available: None, using: 0 TPU cores
---LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
---Using native 16bit precision.
---Global seed set to 42
---initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 1.0 K 
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---299 K     Trainable params
---13.2 M    Non-trainable params
---13.5 M    Total params
---Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/lr_find_temp_model.ckpt
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 1.0 K 
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---299 K     Trainable params
---13.2 M    Non-trainable params
---13.5 M    Total params
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_error_log.txt
--deleted file mode 100644
--index 0c8b389..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_error_log.txt
--+++ /dev/null
--@@ -1,10 +0,0 @@
---[NeMo W 2021-02-09 11:10:37 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-09 11:11:08 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-09 11:18:33 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-09 11:24:13 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_log_globalrank-0_localrank-0.txt
--deleted file mode 100644
--index 84d61e5..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_log_globalrank-0_localrank-0.txt
--+++ /dev/null
--@@ -1,12 +0,0 @@
---[NeMo I 2021-02-09 11:10:37 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37
---[NeMo I 2021-02-09 11:10:37 exp_manager:519] TensorboardLogger has been set up
---[NeMo W 2021-02-09 11:10:37 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-09 11:11:08 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-09 11:18:33 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-09 11:24:13 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
--diff --git a/experiment/Untitled.ipynb b/experiment/Untitled.ipynb
--index 1a5526d..f5bb089 100644
----- a/experiment/Untitled.ipynb
--+++ b/experiment/Untitled.ipynb
--@@ -3,33 +3,33 @@
--   {
--    "cell_type": "code",
--    "execution_count": 1,
---   "id": "dense-meaning",
--+   "id": "modern-amplifier",
--    "metadata": {},
--    "outputs": [
--     {
--      "name": "stderr",
--      "output_type": "stream",
--      "text": [
---      "12:16:24.02 LOG:\n"
--+      "14:59:48.20 LOG:\n"
--      ]
--     },
--     {
--      "name": "stdout",
--      "output_type": "stream",
--      "text": [
---      "shuffling <data.punctuation_dataset_multi.PunctuationDomainDataset object at 0x7f86e5bc2220>\n"
--+      "shuffling <data.punctuation_dataset_multi.PunctuationDomainDataset object at 0x7f44ea05e5b0>\n"
--      ]
--     },
--     {
--      "name": "stderr",
--      "output_type": "stream",
--      "text": [
---      "12:16:24.11 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
---      "12:16:24.17 LOG:\n",
---      "12:16:24.48 .... f\"1st {n} encoder layers of transformer frozen\" = '1st 11 encoder layers of transformer frozen'\n",
--+      "14:59:48.28 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
--+      "14:59:48.34 LOG:\n",
--+      "14:59:48.63 .... f\"1st {n} encoder layers of transformer frozen\" = '1st 11 encoder layers of transformer frozen'\n",
--       "GPU available: True, used: False\n",
--       "TPU available: None, using: 0 TPU cores\n",
---      "[NeMo W 2021-02-09 12:16:24 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
--+      "[NeMo W 2021-02-09 14:59:48 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
--       "      warnings.warn(*args, **kwargs)\n",
--       "    \n"
--      ]
--@@ -67,168 +67,27 @@
--     "ids_to_labels = {_[0]:_[1] for _ in enumerate(cfg.model.punct_label_ids)}\n",
--     "\n",
--     "model = PunctuationDomainModel.load_from_checkpoint(\n",
---    "    checkpoint_path=\"/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/checkpoints/Punctuation_with_Domain_discriminator-last.ckpt\")\n",
--+    "    checkpoint_path=\"/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_14-05-14/checkpoints/Punctuation_with_Domain_discriminator-last.ckpt\")\n",
--     "\n",
--     "trainer = pl.Trainer(**cfg.trainer)"
--    ]
--   },
--   {
--    "cell_type": "code",
---   "execution_count": 2,
---   "id": "potential-adrian",
--+   "execution_count": 8,
--+   "id": "hairy-proxy",
--    "metadata": {},
--    "outputs": [
--     {
---     "name": "stderr",
---     "output_type": "stream",
---     "text": [
---      "12:16:24.62 LOG:\n",
---      "12:16:24.69 .... chunk_to_len_batch(max_seq_length, tokenizer,chunked['texts'],chunked['tags'],True) = {'attention_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
---      "12:16:24.69                                                                                                      True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
---      "12:16:24.69                                                                                                      True, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False],\n",
---      "12:16:24.69                                                                                                    [ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False],\n",
---      "12:16:24.69                                                                                                    [ True,  True,  True,  True,  True, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False]]),\n",
---      "12:16:24.69                                                                                             'input_ids': tensor([[  101,  2057,  4149,  2176, 11344,  2028,  7279,  1998,  1037, 14757,\n",
---      "12:16:24.69                                                                                                      2013,  1996,  1050, 17258,  2401,  6718,  3573,  1999,  4203, 10254,\n",
---      "12:16:24.69                                                                                                       102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0],\n",
---      "12:16:24.69                                                                                                    [  101,  2054,  2064,  1045,  2079,  2005,  2017,  2651,   102,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0],\n",
---      "12:16:24.69                                                                                                    [  101,  2129,  2024,  2017,   102,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0]]),\n",
---      "12:16:24.69                                                                                             'labels': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0],\n",
---      "12:16:24.69                                                                                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0],\n",
---      "12:16:24.69                                                                                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0]]),\n",
---      "12:16:24.69                                                                                             'subtoken_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
---      "12:16:24.69                                                                                                      True,  True, False, False,  True,  True,  True,  True,  True,  True,\n",
---      "12:16:24.69                                                                                                      True, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False],\n",
---      "12:16:24.69                                                                                                    [ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False],\n",
---      "12:16:24.69                                                                                                    [ True,  True,  True,  True,  True, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False]])}\n"
---     ]
---    },
---    {
--      "data": {
--       "text/plain": [
---       "['[CLS] we, bought, four- shirts: one, pen, and, a, mug, from, the, nvidia- gear, store, in, santa- clara, [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
---       " '[CLS] what? can— i? do? for? you? today? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
---       " '[CLS] how? are? you? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']"
--+       "[' we bought four shirts, one pen, and a mug from the nvidia gear store in santa clara                                                                                                            ',\n",
--+       " ' what can i do for you today?                                                                                                                        ',\n",
--+       " ' , how are you? ,                                                                                                                           ',\n",
--+       " ' good morning. everyone? how have your weekends been? its a really great day?                                                                                                                  ']"
--       ]
--      },
---     "execution_count": 2,
--+     "execution_count": 8,
--      "metadata": {},
--      "output_type": "execute_result"
--     }
--@@ -238,6 +97,7 @@
--     "    'we bought four shirts one pen and a mug from the nvidia gear store in santa clara',\n",
--     "    'what can i do for you today',\n",
--     "    'how are you',\n",
--+    "    'good morning everyone how have your weekends been its a really great day'\n",
--     "]\n",
--     "inference_results = model.add_punctuation(queries)\n",
--     "inference_results"
--@@ -246,7 +106,7 @@
--   {
--    "cell_type": "code",
--    "execution_count": null,
---   "id": "amateur-production",
--+   "id": "employed-station",
--    "metadata": {},
--    "outputs": [],
--    "source": []
--diff --git a/experiment/config.yaml b/experiment/config.yaml
--index efa7a5d..0aeaa8b 100644
----- a/experiment/config.yaml
--+++ b/experiment/config.yaml
--@@ -41,7 +41,7 @@ tmp_path: /home/nxingyu/data/tmp # /tmp #
-- 
-- model:
--     nemo_path: null
---    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
--+    transformer_path: distilbert-base-uncased #google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
--     unfrozen: 0
--     maximum_unfrozen: 1
--     unfreeze_step: 1
--@@ -67,7 +67,7 @@ model:
--             # - ${base_path}/open_subtitles_processed #  
--         unlabelled:
--             # - ${base_path}/ted_talks_processed #
---            - ${base_path}/open_subtitles_processed #  
--+            # - ${base_path}/open_subtitles_processed #  
--             # parameters for dataset preprocessing
--         max_seq_length: 128
--         pad_label: ''
--@@ -75,11 +75,11 @@ model:
--         ignore_start_end: false
--         use_cache: false
--         # shared among dataloaders
---        num_workers:  4
--+        num_workers:  2
--         pin_memory: true
---        drop_last: false
--+        drop_last: true
--         num_labels: 10
---        num_domains: 2
--+        num_domains: 1
--         test_unlabelled: true
-- 
--         train_ds:
--@@ -114,7 +114,7 @@ model:
--         activation: 'relu'
--         log_softmax: false
--         use_transformer_init: true
---        loss: 'dice'
--+        loss: 'crf'
-- 
--     domain_head:
--         domain_num_fc_layers: 1
--@@ -137,7 +137,7 @@ model:
-- 
--     optim:
--         name: adamw
---        lr: 1e-3
--+        lr: 0.009261935523740748 #1e-3
--         weight_decay: 0.00
--         sched:
--             name: WarmupAnnealing #CyclicLR
--diff --git a/experiment/core/utils.py b/experiment/core/utils.py
--index 4be7503..ce7436b 100644
----- a/experiment/core/utils.py
--+++ b/experiment/core/utils.py
--@@ -38,12 +38,12 @@ def align_labels_to_mask(mask,labels):
--     return m1.tolist()
-- 
-- def view_aligned(texts,tags,tokenizer,labels_to_ids):
---        return [re.sub(' ##','',' '.join(
--+        return [re.sub(r'( ?\[((PAD)|(CLS)|(SEP))\] ?)',' ',re.sub(' ##','',' '.join(
--             [_[0]+_[1] for _ in list(
--                 zip(tokenizer.convert_ids_to_tokens(_[0]),
--                     [labels_to_ids[id] for id in _[1].tolist()])
--             )]
---        )) for _ in zip(texts,tags)]
--+        ))) for _ in zip(texts,tags)]
-- 
-- def text2masks(n, labels_to_ids):
--     def text2masks(text):
--diff --git a/experiment/data/punctuation_datamodule.py b/experiment/data/punctuation_datamodule.py
--index 6978318..20a4093 100644
----- a/experiment/data/punctuation_datamodule.py
--+++ b/experiment/data/punctuation_datamodule.py
--@@ -54,9 +54,10 @@ class PunctuationDataModule(LightningDataModule):
--         self.test_unlabelled=test_unlabelled
--     
--     def reset(self):
---        self.train_dataset.__iter__()
---        self.val_dataset.__iter__()
---        self.test_dataset.__iter__()
--+        # self.setup('fit')
--+        self.train_dataset=iter(self.train_dataset)
--+        self.val_dataset=iter(self.val_dataset)
--+        self.test_dataset=iter(self.test_dataset)
-- 
--     def setup(self, stage=None):
--         if stage=='fit' or stage is None:
--diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
--index 03d661c..88a268f 100644
----- a/experiment/data/punctuation_dataset_multi.py
--+++ b/experiment/data/punctuation_dataset_multi.py
--@@ -167,7 +167,7 @@ class PunctuationDomainDatasets(IterableDataset):
--         self.max_length=max(self.ds_lengths)
--         self.len=int(self.max_length/num_samples)
--         self.per_worker=int(self.max_length/self.num_workers)
---
--+        self.class_weights=None
-- 
--         for i,path in enumerate(labelled):
--             target=os.path.join(tmp_path,os.path.split(path)[1])
--diff --git a/experiment/info.log b/experiment/info.log
--index 69e9a76..e69de29 100644
----- a/experiment/info.log
--+++ b/experiment/info.log
--@@ -1,85 +0,0 @@
---[INFO] - shuffling train set
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.001
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f717ffe5b80>" 
---will be used during training (effective maximum steps = 80) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---last_epoch: -1
---max_steps: 80
---)
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          89.19      22.74      36.24       3012
---! (label_id: 1)                                          0.00       0.00       0.00          1
---, (label_id: 2)                                          7.31      36.21      12.16        243
---- (label_id: 3)                                          2.27      21.43       4.11         28
---. (label_id: 4)                                          1.68       1.65       1.66        182
---: (label_id: 5)                                          0.00       0.00       0.00          5
---; (label_id: 6)                                          0.00       0.00       0.00          3
---? (label_id: 7)                                          0.24      22.22       0.48          9
---— (label_id: 8)                                          0.00       0.00       0.00         10
---… (label_id: 9)                                          0.00       0.00       0.00          1
----------------------
---micro avg                                               22.44      22.44      22.44       3494
---macro avg                                               10.07      10.43       5.47       3494
---weighted avg                                            77.50      22.44      32.21       3494
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                         50.00     100.00      66.67         34
---1 (label_id: 1)                                          0.00       0.00       0.00         34
----------------------
---micro avg                                               50.00      50.00      50.00         68
---macro avg                                               25.00      50.00      33.33         68
---weighted avg                                            25.00      50.00      33.33         68
---
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.007943282347242822
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f717f560fa0>" 
---will be used during training (effective maximum steps = 53050) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---last_epoch: -1
---max_steps: 53050
---)
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          89.19      22.74      36.24       3012
---! (label_id: 1)                                          0.00       0.00       0.00          1
---, (label_id: 2)                                          7.31      36.21      12.16        243
---- (label_id: 3)                                          2.27      21.43       4.11         28
---. (label_id: 4)                                          1.68       1.65       1.66        182
---: (label_id: 5)                                          0.00       0.00       0.00          5
---; (label_id: 6)                                          0.00       0.00       0.00          3
---? (label_id: 7)                                          0.24      22.22       0.48          9
---— (label_id: 8)                                          0.00       0.00       0.00         10
---… (label_id: 9)                                          0.00       0.00       0.00          1
----------------------
---micro avg                                               22.44      22.44      22.44       3494
---macro avg                                               10.07      10.43       5.47       3494
---weighted avg                                            77.50      22.44      32.21       3494
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                         50.00     100.00      66.67         34
---1 (label_id: 1)                                          0.00       0.00       0.00         34
----------------------
---micro avg                                               50.00      50.00      50.00         68
---macro avg                                               25.00      50.00      33.33         68
---weighted avg                                            25.00      50.00      33.33         68
---
--diff --git a/experiment/main.py b/experiment/main.py
--index 6f0a8ea..6b15e25 100644
----- a/experiment/main.py
--+++ b/experiment/main.py
--@@ -39,7 +39,7 @@ def main(cfg: DictConfig)->None:
--     
--     while(model.hparams.model.unfrozen<=cfg.model.maximum_unfrozen and model.hparams.model.unfrozen>=0):
--         trainer.current_epoch=0
---        lr_finder = trainer.tuner.lr_find(model,min_lr=1e-10, max_lr=1e-02, num_training=80, early_stop_threshold=None)
--+        lr_finder = trainer.tuner.lr_find(model,min_lr=1e-8, max_lr=0.5, num_training=80) #, early_stop_threshold=None
--         # Results can be found in
--         pp(lr_finder.results)
--         new_lr = lr_finder.suggestion()
--diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
--index dab395e..999ae0d 100644
----- a/experiment/models/punctuation_domain_model.py
--+++ b/experiment/models/punctuation_domain_model.py
--@@ -130,9 +130,13 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
--         )[0]
--         punct_logits = self.punct_classifier(hidden_states=hidden_states)
--         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
--+        assert not torch.isnan(input_ids).any(), (input_ids,'inputid')
--+        assert not torch.isnan(attention_mask).any(), ('amask',attention_mask)
--+        assert not torch.isnan(hidden_states).any(), (hidden_states,attention_mask.sum(1),'hiddenstate')
--         domain_logits = self.domain_classifier(
--             hidden_states=reverse_grad_hidden_states,
--             attention_mask=attention_mask)
--+        # print(attention_mask.sum(axis=1),domain_logits)
--         return punct_logits, domain_logits
-- 
--     def _make_step(self, batch):
--@@ -157,6 +161,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
--         passed in as `batch`.
--         """
--         p=(self.current_epoch*self.train_size+batch_idx)/(self.train_size*self.hparams.trainer.max_epochs)
--+        if (batch_idx%1000==0):
--+            print('gamma:',p)
--         self.grad_reverse.scale=2/(1+math.exp(-10*p))-1
--         loss, _, _ = self._make_step(batch)
--         lr = self._optimizer.param_groups[0]['lr']
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/hparams.yaml
-deleted file mode 100644
-index 458f253..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/hparams.yaml
-+++ /dev/null
-@@ -1,109 +0,0 @@
--seed: 42
--trainer:
--  gpus: 1
--  num_nodes: 1
--  max_epochs: 10
--  max_steps: null
--  accumulate_grad_batches: 4
--  gradient_clip_val: 0
--  amp_level: O1
--  precision: 16
--  accelerator: ddp
--  checkpoint_callback: false
--  logger: false
--  log_every_n_steps: 1
--  val_check_interval: 1.0
--  resume_from_checkpoint: null
--exp_manager:
--  exp_dir: /home/nxingyu/project/
--  name: Punctuation_with_Domain_discriminator
--  create_tensorboard_logger: true
--  create_checkpoint_callback: true
--base_path: /home/nxingyu/data
--tmp_path: /home/nxingyu/data/tmp
--model:
--  nemo_path: null
--  transformer_path: distilbert-base-uncased
--  unfrozen: 0
--  maximum_unfrozen: 1
--  unfreeze_step: 1
--  punct_label_ids:
--  - ''
--  - '!'
--  - ','
--  - '-'
--  - .
--  - ':'
--  - ;
--  - '?'
--  - —
--  - …
--  punct_class_weights: false
--  dataset:
--    data_dir: /home/nxingyu/data
--    labelled:
--    - /home/nxingyu/data/ted_talks_processed
--    unlabelled: null
--    max_seq_length: 128
--    pad_label: ''
--    ignore_extra_tokens: false
--    ignore_start_end: false
--    use_cache: false
--    num_workers: 2
--    pin_memory: true
--    drop_last: true
--    num_labels: 10
--    num_domains: 1
--    test_unlabelled: true
--    train_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--    validation_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--  tokenizer:
--    tokenizer_name: distilbert-base-uncased
--    vocab_file: null
--    tokenizer_model: null
--    special_tokens: null
--  language_model:
--    pretrained_model_name: distilbert-base-uncased
--    lm_checkpoint: null
--    config_file: null
--    config: null
--  punct_head:
--    punct_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: crf
--  domain_head:
--    domain_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: cel
--    gamma: 0.1
--    pooling: mean_max
--    idx_conditioned_on: 0
--  dice_loss:
--    epsilon: 0.01
--    alpha: 3
--    macro_average: true
--  focal_loss:
--    gamma: 1
--  optim:
--    name: adamw
--    lr: 0.009261935523740748
--    weight_decay: 0.0
--    sched:
--      name: WarmupAnnealing
--      warmup_steps: null
--      warmup_ratio: 0.1
--      last_epoch: -1
--      monitor: val_loss
--      reduce_on_plateau: false
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
-deleted file mode 100644
-index 0f4c210..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
-+++ /dev/null
-@@ -1,91 +0,0 @@
--Global seed set to 42
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
--Using native 16bit precision.
--Global seed set to 42
--initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | DistilBertModel      | 66.4 M
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--3 | punctuation_loss    | LinearChainCRF       | 120   
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--10.9 K    Trainable params
--66.4 M    Non-trainable params
--66.4 M    Total params
--Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lr_find_temp_model.ckpt
--Global seed set to 42
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | DistilBertModel      | 66.4 M
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--3 | punctuation_loss    | LinearChainCRF       | 120   
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--10.9 K    Trainable params
--66.4 M    Non-trainable params
--66.4 M    Total params
--Epoch 0, global step 200: val_loss reached 108.49911 (best 108.49911), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=108.50-epoch=0.ckpt" as top 3
--Epoch 1, global step 400: val_loss reached 155.59180 (best 108.49911), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=155.59-epoch=1.ckpt" as top 3
--Epoch 2, global step 600: val_loss reached 46.96625 (best 46.96625), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=46.97-epoch=2.ckpt" as top 3
--Epoch 3, global step 800: val_loss reached 63.33499 (best 46.96625), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=63.33-epoch=3.ckpt" as top 3
--Epoch 4, global step 1000: val_loss reached 33.09097 (best 33.09097), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=33.09-epoch=4.ckpt" as top 3
--Epoch 5, global step 1200: val_loss reached 30.02221 (best 30.02221), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=30.02-epoch=5.ckpt" as top 3
--Epoch 6, global step 1400: val_loss reached 31.40887 (best 30.02221), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=31.41-epoch=6.ckpt" as top 3
--Epoch 7, step 1600: val_loss was not in top 3
--Epoch 8, global step 1800: val_loss reached 28.61345 (best 28.61345), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=28.61-epoch=8.ckpt" as top 3
--Epoch 9, global step 2000: val_loss reached 26.27475 (best 26.27475), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=26.27-epoch=9.ckpt" as top 3
--Global seed set to 42
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | DistilBertModel      | 66.4 M
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--3 | punctuation_loss    | LinearChainCRF       | 120   
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--7.1 M     Trainable params
--59.3 M    Non-trainable params
--66.4 M    Total params
--LR finder stopped early due to diverging loss.
--Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lr_find_temp_model.ckpt
--Failed to compute suggesting for `lr`. There might not be enough points.
--Traceback (most recent call last):
--  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
--    min_grad = np.gradient(loss).argmin()
--  File "<__array_function__ internals>", line 5, in gradient
--  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
--    raise ValueError(
--ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
--Global seed set to 42
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | DistilBertModel      | 66.4 M
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--3 | punctuation_loss    | LinearChainCRF       | 120   
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--7.1 M     Trainable params
--59.3 M    Non-trainable params
--66.4 M    Total params
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
-deleted file mode 100644
-index cab9655..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
-+++ /dev/null
-@@ -1,42 +0,0 @@
--[NeMo W 2021-02-09 15:09:54 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 15:10:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:10:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:10:09 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:10:11 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-09 15:11:51 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:11:51 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:11:52 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:11:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-09 15:16:02 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce8610> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 15:16:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce40a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 15:57:31 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--      warnings.warn(*args, **kwargs)
--    
--Failed to compute suggesting for `lr`. There might not be enough points.
--Traceback (most recent call last):
--  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
--    min_grad = np.gradient(loss).argmin()
--  File "<__array_function__ internals>", line 5, in gradient
--  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
--    raise ValueError(
--ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
-deleted file mode 100644
-index d19ea43..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
-+++ /dev/null
-@@ -1,36 +0,0 @@
--[NeMo I 2021-02-09 15:09:54 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54
--[NeMo I 2021-02-09 15:09:54 exp_manager:519] TensorboardLogger has been set up
--[NeMo W 2021-02-09 15:09:54 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 15:10:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:10:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:10:09 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:10:11 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-09 15:11:51 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:11:51 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:11:52 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:11:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-09 15:16:02 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce8610> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 15:16:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce40a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 15:57:31 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--      warnings.warn(*args, **kwargs)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/cmd-args.log
-deleted file mode 100644
-index 11a5d8e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/cmd-args.log
-+++ /dev/null
-@@ -1 +0,0 @@
--main.py
-\ No newline at end of file
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/git-info.log
-deleted file mode 100644
-index 4d85d01..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/git-info.log
-+++ /dev/null
-@@ -1,334 +0,0 @@
--commit hash: 836632f0fdebe90f93105efab295fe4cd83af4ca
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0 b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0
--index c3ff071..0e8637d 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0 differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt
--index 7f6eddf..a1b895b 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt
--@@ -39,3 +39,53 @@ Global seed set to 42
-- 66.4 M    Total params
-- Epoch 0, global step 200: val_loss reached 0.26631 (best 0.26631), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=0.ckpt" as top 3
-- Epoch 1, global step 400: val_loss reached 0.26336 (best 0.26336), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=1.ckpt" as top 3
--+Epoch 2, global step 600: val_loss reached 0.26109 (best 0.26109), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=2.ckpt" as top 3
--+Epoch 3, global step 800: val_loss reached 0.25805 (best 0.25805), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=3.ckpt" as top 3
--+Epoch 4, global step 1000: val_loss reached 0.25547 (best 0.25547), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=4.ckpt" as top 3
--+Epoch 5, global step 1200: val_loss reached 0.25339 (best 0.25339), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.25-epoch=5.ckpt" as top 3
--+Epoch 6, global step 1400: val_loss reached 0.24810 (best 0.24810), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.25-epoch=6.ckpt" as top 3
--+Epoch 7, global step 1600: val_loss reached 0.24127 (best 0.24127), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=7.ckpt" as top 3
--+Epoch 8, global step 1800: val_loss reached 0.23864 (best 0.23864), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=8.ckpt" as top 3
--+Epoch 9, global step 2000: val_loss reached 0.23646 (best 0.23646), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=9.ckpt" as top 3
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | DistilBertModel      | 66.4 M
--+1 | punct_classifier    | TokenClassifier      | 7.7 K 
--+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--+3 | punctuation_loss    | FocalDiceLoss        | 0     
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+7.1 M     Trainable params
--+59.3 M    Non-trainable params
--+66.4 M    Total params
--+LR finder stopped early due to diverging loss.
--+Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lr_find_temp_model.ckpt
--+Failed to compute suggesting for `lr`. There might not be enough points.
--+Traceback (most recent call last):
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
--+    min_grad = np.gradient(loss).argmin()
--+  File "<__array_function__ internals>", line 5, in gradient
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
--+    raise ValueError(
--+ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | DistilBertModel      | 66.4 M
--+1 | punct_classifier    | TokenClassifier      | 7.7 K 
--+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--+3 | punctuation_loss    | FocalDiceLoss        | 0     
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+7.1 M     Trainable params
--+59.3 M    Non-trainable params
--+66.4 M    Total params
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt
--index 2d9ccfb..a9e0895 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt
--@@ -14,3 +14,14 @@
-- [NeMo W 2021-02-09 15:11:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fa1136ae0a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 15:25:45 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--+      warnings.warn(*args, **kwargs)
--+    
--+Failed to compute suggesting for `lr`. There might not be enough points.
--+Traceback (most recent call last):
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
--+    min_grad = np.gradient(loss).argmin()
--+  File "<__array_function__ internals>", line 5, in gradient
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
--+    raise ValueError(
--+ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt
--index 80a7030..dfff301 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt
--@@ -16,3 +16,6 @@
-- [NeMo W 2021-02-09 15:11:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fa1136ae0a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 15:25:45 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--+      warnings.warn(*args, **kwargs)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0
--index bb4d846..b06f296 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
--index 2869be1..b9491d6 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
--@@ -37,3 +37,10 @@ Global seed set to 42
-- 10.9 K    Trainable params
-- 66.4 M    Non-trainable params
-- 66.4 M    Total params
--+Epoch 0, global step 200: val_loss reached 108.49911 (best 108.49911), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=108.50-epoch=0.ckpt" as top 3
--+Epoch 1, global step 400: val_loss reached 155.59180 (best 108.49911), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=155.59-epoch=1.ckpt" as top 3
--+Epoch 2, global step 600: val_loss reached 46.96625 (best 46.96625), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=46.97-epoch=2.ckpt" as top 3
--+Epoch 3, global step 800: val_loss reached 63.33499 (best 46.96625), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=63.33-epoch=3.ckpt" as top 3
--+Epoch 4, global step 1000: val_loss reached 33.09097 (best 33.09097), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=33.09-epoch=4.ckpt" as top 3
--+Epoch 5, global step 1200: val_loss reached 30.02221 (best 30.02221), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=30.02-epoch=5.ckpt" as top 3
--+Epoch 6, global step 1400: val_loss reached 31.40887 (best 30.02221), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=31.41-epoch=6.ckpt" as top 3
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
--index 568694f..6bd1322 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
--@@ -23,3 +23,9 @@
-- [NeMo W 2021-02-09 15:11:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--     
--+[NeMo W 2021-02-09 15:16:02 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce8610> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--+[NeMo W 2021-02-09 15:16:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce40a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
--index 7533c2c..e5eaea5 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
--@@ -25,3 +25,9 @@
-- [NeMo W 2021-02-09 15:11:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--     
--+[NeMo W 2021-02-09 15:16:02 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce8610> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--+[NeMo W 2021-02-09 15:16:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce40a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--diff --git a/experiment/config.yaml b/experiment/config.yaml
--index 0aeaa8b..670f79a 100644
----- a/experiment/config.yaml
--+++ b/experiment/config.yaml
--@@ -2,10 +2,10 @@ seed: 42
-- trainer:
--     gpus: 1 # the number of gpus, 0 for CPU
--     num_nodes: 1
---    max_epochs: 10
--+    max_epochs: 2
--     max_steps: null # precedence over max_epochs
--     accumulate_grad_batches: 4 # accumulates grads every k batches
---    gradient_clip_val: 0
--+    gradient_clip_val: 1
--     amp_level: O1 # O1/O2 for mixed precision
--     precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--     accelerator: ddp
--@@ -41,7 +41,7 @@ tmp_path: /home/nxingyu/data/tmp # /tmp #
-- 
-- model:
--     nemo_path: null
---    transformer_path: distilbert-base-uncased #google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
--+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
--     unfrozen: 0
--     maximum_unfrozen: 1
--     unfreeze_step: 1
--@@ -114,7 +114,7 @@ model:
--         activation: 'relu'
--         log_softmax: false
--         use_transformer_init: true
---        loss: 'crf'
--+        loss: 'dice'
-- 
--     domain_head:
--         domain_num_fc_layers: 1
--@@ -129,7 +129,7 @@ model:
--     
--     dice_loss:
--         epsilon: 0.01
---        alpha: 3
--+        alpha: 1
--         macro_average: true
-- 
--     focal_loss: 
--diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
--index 88a268f..31923de 100644
----- a/experiment/data/punctuation_dataset_multi.py
--+++ b/experiment/data/punctuation_dataset_multi.py
--@@ -165,8 +165,8 @@ class PunctuationDomainDatasets(IterableDataset):
--         for path in labelled+unlabelled:
--             self.ds_lengths.append(int(subprocess.Popen(['wc', '-l', f'{path}.{split}.csv'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0]))
--         self.max_length=max(self.ds_lengths)
---        self.len=int(self.max_length/num_samples)
--         self.per_worker=int(self.max_length/self.num_workers)
--+        self.len=int(self.per_worker/num_samples)
--         self.class_weights=None
-- 
--         for i,path in enumerate(labelled):
--diff --git a/experiment/info.log b/experiment/info.log
--index ff2d606..e69de29 100644
----- a/experiment/info.log
--+++ b/experiment/info.log
--@@ -1,133 +0,0 @@
---[INFO] - shuffling train set
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.009261935523740748
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f596c78a3d0>" 
---will be used during training (effective maximum steps = 80) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---last_epoch: -1
---max_steps: 80
---)
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          80.85       4.94       9.30       1540
---! (label_id: 1)                                          0.00       0.00       0.00          2
---, (label_id: 2)                                          7.69       1.41       2.38        142
---- (label_id: 3)                                          1.67      20.00       3.08         20
---. (label_id: 4)                                          6.82      12.50       8.82         96
---: (label_id: 5)                                          0.00       0.00       0.00          0
---; (label_id: 6)                                          0.00       0.00       0.00          0
---? (label_id: 7)                                          0.00       0.00       0.00          4
---— (label_id: 8)                                          0.00       0.00       0.00          0
---… (label_id: 9)                                          0.00       0.00       0.00          0
----------------------
---micro avg                                                5.21       5.21       5.21       1804
---macro avg                                               16.17       6.47       3.93       1804
---weighted avg                                            70.01       5.21       8.63       1804
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00         20
----------------------
---micro avg                                              100.00     100.00     100.00         20
---macro avg                                              100.00     100.00     100.00         20
---weighted avg                                           100.00     100.00     100.00         20
---
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.0024506370946974477
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f596c570f40>" 
---will be used during training (effective maximum steps = 2000) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---last_epoch: -1
---max_steps: 2000
---)
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          80.85       4.94       9.30       1540
---! (label_id: 1)                                          0.00       0.00       0.00          2
---, (label_id: 2)                                          7.69       1.41       2.38        142
---- (label_id: 3)                                          1.67      20.00       3.08         20
---. (label_id: 4)                                          6.82      12.50       8.82         96
---: (label_id: 5)                                          0.00       0.00       0.00          0
---; (label_id: 6)                                          0.00       0.00       0.00          0
---? (label_id: 7)                                          0.00       0.00       0.00          4
---— (label_id: 8)                                          0.00       0.00       0.00          0
---… (label_id: 9)                                          0.00       0.00       0.00          0
----------------------
---micro avg                                                5.21       5.21       5.21       1804
---macro avg                                               16.17       6.47       3.93       1804
---weighted avg                                            70.01       5.21       8.63       1804
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00         20
----------------------
---micro avg                                              100.00     100.00     100.00         20
---macro avg                                              100.00     100.00     100.00         20
---weighted avg                                           100.00     100.00     100.00         20
---
---NFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          90.44      95.02      92.67     179900
---! (label_id: 1)                                          0.00       0.00       0.00         52
---, (label_id: 2)                                         21.92       9.58      13.34      13074
---- (label_id: 3)                                         44.94       6.69      11.64       1062
---. (label_id: 4)                                         28.64      30.78      29.67      11077
---: (label_id: 5)                                          0.00       0.00       0.00        330
---; (label_id: 6)                                          0.00       0.00       0.00        108
---? (label_id: 7)                                          0.00       0.00       0.00        899
---— (label_id: 8)                                          0.00       0.00       0.00        276
---… (label_id: 9)                                          0.00       0.00       0.00         28
----------------------
---micro avg                                               84.95      84.95      84.95     206806
---macro avg                                               18.59      14.21      14.73     206806
---weighted avg                                            81.82      84.95      83.11     206806
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1959
----------------------
---micro avg                                              100.00     100.00     100.00       1959
---macro avg                                              100.00     100.00     100.00       1959
---weighted avg                                           100.00     100.00     100.00       1959
---
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          90.62      94.69      92.61     179900
---! (label_id: 1)                                          0.00       0.00       0.00         52
---, (label_id: 2)                                         21.80      10.36      14.04      13074
---- (label_id: 3)                                         44.94       6.69      11.64       1062
---. (label_id: 4)                                         28.46      32.05      30.15      11077
---: (label_id: 5)                                          0.00       0.00       0.00        330
---; (label_id: 6)                                          0.00       0.00       0.00        108
---? (label_id: 7)                                          0.00       0.00       0.00        899
---— (label_id: 8)                                          0.00       0.00       0.00        276
---… (label_id: 9)                                          0.00       0.00       0.00         28
----------------------
---micro avg                                               84.77      84.77      84.77     206806
---macro avg                                               18.58      14.38      14.84     206806
---weighted avg                                            81.97      84.77      83.12     206806
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1959
----------------------
---micro avg                                              100.00     100.00     100.00       1959
---macro avg                                              100.00     100.00     100.00       1959
---weighted avg                                           100.00     100.00     100.00       1959
---
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/lightning_logs.txt
-deleted file mode 100644
-index 1562e1d..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/lightning_logs.txt
-+++ /dev/null
-@@ -1,22 +0,0 @@
--Global seed set to 42
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
--Using native 16bit precision.
--Global seed set to 42
--initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 513   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--299 K     Trainable params
--13.2 M    Non-trainable params
--13.5 M    Total params
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/nemo_error_log.txt
-deleted file mode 100644
-index 60f6fbd..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/nemo_error_log.txt
-+++ /dev/null
-@@ -1,10 +0,0 @@
--[NeMo W 2021-02-09 15:46:00 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 15:46:12 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:46:12 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:46:13 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/nemo_log_globalrank-0_localrank-0.txt
-deleted file mode 100644
-index 6aba001..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/nemo_log_globalrank-0_localrank-0.txt
-+++ /dev/null
-@@ -1,12 +0,0 @@
--[NeMo I 2021-02-09 15:46:00 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00
--[NeMo I 2021-02-09 15:46:00 exp_manager:519] TensorboardLogger has been set up
--[NeMo W 2021-02-09 15:46:00 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 15:46:12 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:46:12 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:46:13 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/cmd-args.log
-deleted file mode 100644
-index 11a5d8e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/cmd-args.log
-+++ /dev/null
-@@ -1 +0,0 @@
--main.py
-\ No newline at end of file
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/events.out.tfevents.1612856909.Titan.20492.0 b/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/events.out.tfevents.1612856909.Titan.20492.0
-deleted file mode 100644
-index 603495e..0000000
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/events.out.tfevents.1612856909.Titan.20492.0 and /dev/null differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/git-info.log
-deleted file mode 100644
-index cafa992..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/git-info.log
-+++ /dev/null
-@@ -1,355 +0,0 @@
--commit hash: 836632f0fdebe90f93105efab295fe4cd83af4ca
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0 b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0
--index c3ff071..0e8637d 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0 differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt
--index 7f6eddf..a1b895b 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt
--@@ -39,3 +39,53 @@ Global seed set to 42
-- 66.4 M    Total params
-- Epoch 0, global step 200: val_loss reached 0.26631 (best 0.26631), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=0.ckpt" as top 3
-- Epoch 1, global step 400: val_loss reached 0.26336 (best 0.26336), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=1.ckpt" as top 3
--+Epoch 2, global step 600: val_loss reached 0.26109 (best 0.26109), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=2.ckpt" as top 3
--+Epoch 3, global step 800: val_loss reached 0.25805 (best 0.25805), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=3.ckpt" as top 3
--+Epoch 4, global step 1000: val_loss reached 0.25547 (best 0.25547), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=4.ckpt" as top 3
--+Epoch 5, global step 1200: val_loss reached 0.25339 (best 0.25339), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.25-epoch=5.ckpt" as top 3
--+Epoch 6, global step 1400: val_loss reached 0.24810 (best 0.24810), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.25-epoch=6.ckpt" as top 3
--+Epoch 7, global step 1600: val_loss reached 0.24127 (best 0.24127), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=7.ckpt" as top 3
--+Epoch 8, global step 1800: val_loss reached 0.23864 (best 0.23864), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=8.ckpt" as top 3
--+Epoch 9, global step 2000: val_loss reached 0.23646 (best 0.23646), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=9.ckpt" as top 3
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | DistilBertModel      | 66.4 M
--+1 | punct_classifier    | TokenClassifier      | 7.7 K 
--+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--+3 | punctuation_loss    | FocalDiceLoss        | 0     
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+7.1 M     Trainable params
--+59.3 M    Non-trainable params
--+66.4 M    Total params
--+LR finder stopped early due to diverging loss.
--+Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lr_find_temp_model.ckpt
--+Failed to compute suggesting for `lr`. There might not be enough points.
--+Traceback (most recent call last):
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
--+    min_grad = np.gradient(loss).argmin()
--+  File "<__array_function__ internals>", line 5, in gradient
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
--+    raise ValueError(
--+ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | DistilBertModel      | 66.4 M
--+1 | punct_classifier    | TokenClassifier      | 7.7 K 
--+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--+3 | punctuation_loss    | FocalDiceLoss        | 0     
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+7.1 M     Trainable params
--+59.3 M    Non-trainable params
--+66.4 M    Total params
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt
--index 2d9ccfb..a9e0895 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt
--@@ -14,3 +14,14 @@
-- [NeMo W 2021-02-09 15:11:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fa1136ae0a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 15:25:45 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--+      warnings.warn(*args, **kwargs)
--+    
--+Failed to compute suggesting for `lr`. There might not be enough points.
--+Traceback (most recent call last):
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
--+    min_grad = np.gradient(loss).argmin()
--+  File "<__array_function__ internals>", line 5, in gradient
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
--+    raise ValueError(
--+ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt
--index 80a7030..dfff301 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt
--@@ -16,3 +16,6 @@
-- [NeMo W 2021-02-09 15:11:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fa1136ae0a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 15:25:45 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--+      warnings.warn(*args, **kwargs)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0
--index bb4d846..19d4690 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
--index 2869be1..b9491d6 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
--@@ -37,3 +37,10 @@ Global seed set to 42
-- 10.9 K    Trainable params
-- 66.4 M    Non-trainable params
-- 66.4 M    Total params
--+Epoch 0, global step 200: val_loss reached 108.49911 (best 108.49911), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=108.50-epoch=0.ckpt" as top 3
--+Epoch 1, global step 400: val_loss reached 155.59180 (best 108.49911), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=155.59-epoch=1.ckpt" as top 3
--+Epoch 2, global step 600: val_loss reached 46.96625 (best 46.96625), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=46.97-epoch=2.ckpt" as top 3
--+Epoch 3, global step 800: val_loss reached 63.33499 (best 46.96625), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=63.33-epoch=3.ckpt" as top 3
--+Epoch 4, global step 1000: val_loss reached 33.09097 (best 33.09097), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=33.09-epoch=4.ckpt" as top 3
--+Epoch 5, global step 1200: val_loss reached 30.02221 (best 30.02221), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=30.02-epoch=5.ckpt" as top 3
--+Epoch 6, global step 1400: val_loss reached 31.40887 (best 30.02221), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=31.41-epoch=6.ckpt" as top 3
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
--index 568694f..6bd1322 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
--@@ -23,3 +23,9 @@
-- [NeMo W 2021-02-09 15:11:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--     
--+[NeMo W 2021-02-09 15:16:02 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce8610> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--+[NeMo W 2021-02-09 15:16:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce40a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
--index 7533c2c..e5eaea5 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
--@@ -25,3 +25,9 @@
-- [NeMo W 2021-02-09 15:11:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--     
--+[NeMo W 2021-02-09 15:16:02 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce8610> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--+[NeMo W 2021-02-09 15:16:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce40a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--diff --git a/experiment/config.yaml b/experiment/config.yaml
--index 0aeaa8b..670f79a 100644
----- a/experiment/config.yaml
--+++ b/experiment/config.yaml
--@@ -2,10 +2,10 @@ seed: 42
-- trainer:
--     gpus: 1 # the number of gpus, 0 for CPU
--     num_nodes: 1
---    max_epochs: 10
--+    max_epochs: 2
--     max_steps: null # precedence over max_epochs
--     accumulate_grad_batches: 4 # accumulates grads every k batches
---    gradient_clip_val: 0
--+    gradient_clip_val: 1
--     amp_level: O1 # O1/O2 for mixed precision
--     precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--     accelerator: ddp
--@@ -41,7 +41,7 @@ tmp_path: /home/nxingyu/data/tmp # /tmp #
-- 
-- model:
--     nemo_path: null
---    transformer_path: distilbert-base-uncased #google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
--+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
--     unfrozen: 0
--     maximum_unfrozen: 1
--     unfreeze_step: 1
--@@ -114,7 +114,7 @@ model:
--         activation: 'relu'
--         log_softmax: false
--         use_transformer_init: true
---        loss: 'crf'
--+        loss: 'dice'
-- 
--     domain_head:
--         domain_num_fc_layers: 1
--@@ -129,7 +129,7 @@ model:
--     
--     dice_loss:
--         epsilon: 0.01
---        alpha: 3
--+        alpha: 1
--         macro_average: true
-- 
--     focal_loss: 
--diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
--index 88a268f..31923de 100644
----- a/experiment/data/punctuation_dataset_multi.py
--+++ b/experiment/data/punctuation_dataset_multi.py
--@@ -165,8 +165,8 @@ class PunctuationDomainDatasets(IterableDataset):
--         for path in labelled+unlabelled:
--             self.ds_lengths.append(int(subprocess.Popen(['wc', '-l', f'{path}.{split}.csv'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0]))
--         self.max_length=max(self.ds_lengths)
---        self.len=int(self.max_length/num_samples)
--         self.per_worker=int(self.max_length/self.num_workers)
--+        self.len=int(self.per_worker/num_samples)
--         self.class_weights=None
-- 
--         for i,path in enumerate(labelled):
--diff --git a/experiment/info.log b/experiment/info.log
--index ff2d606..e69de29 100644
----- a/experiment/info.log
--+++ b/experiment/info.log
--@@ -1,133 +0,0 @@
---[INFO] - shuffling train set
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.009261935523740748
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f596c78a3d0>" 
---will be used during training (effective maximum steps = 80) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---last_epoch: -1
---max_steps: 80
---)
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          80.85       4.94       9.30       1540
---! (label_id: 1)                                          0.00       0.00       0.00          2
---, (label_id: 2)                                          7.69       1.41       2.38        142
---- (label_id: 3)                                          1.67      20.00       3.08         20
---. (label_id: 4)                                          6.82      12.50       8.82         96
---: (label_id: 5)                                          0.00       0.00       0.00          0
---; (label_id: 6)                                          0.00       0.00       0.00          0
---? (label_id: 7)                                          0.00       0.00       0.00          4
---— (label_id: 8)                                          0.00       0.00       0.00          0
---… (label_id: 9)                                          0.00       0.00       0.00          0
----------------------
---micro avg                                                5.21       5.21       5.21       1804
---macro avg                                               16.17       6.47       3.93       1804
---weighted avg                                            70.01       5.21       8.63       1804
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00         20
----------------------
---micro avg                                              100.00     100.00     100.00         20
---macro avg                                              100.00     100.00     100.00         20
---weighted avg                                           100.00     100.00     100.00         20
---
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.0024506370946974477
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f596c570f40>" 
---will be used during training (effective maximum steps = 2000) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---last_epoch: -1
---max_steps: 2000
---)
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          80.85       4.94       9.30       1540
---! (label_id: 1)                                          0.00       0.00       0.00          2
---, (label_id: 2)                                          7.69       1.41       2.38        142
---- (label_id: 3)                                          1.67      20.00       3.08         20
---. (label_id: 4)                                          6.82      12.50       8.82         96
---: (label_id: 5)                                          0.00       0.00       0.00          0
---; (label_id: 6)                                          0.00       0.00       0.00          0
---? (label_id: 7)                                          0.00       0.00       0.00          4
---— (label_id: 8)                                          0.00       0.00       0.00          0
---… (label_id: 9)                                          0.00       0.00       0.00          0
----------------------
---micro avg                                                5.21       5.21       5.21       1804
---macro avg                                               16.17       6.47       3.93       1804
---weighted avg                                            70.01       5.21       8.63       1804
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00         20
----------------------
---micro avg                                              100.00     100.00     100.00         20
---macro avg                                              100.00     100.00     100.00         20
---weighted avg                                           100.00     100.00     100.00         20
---
---NFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          90.44      95.02      92.67     179900
---! (label_id: 1)                                          0.00       0.00       0.00         52
---, (label_id: 2)                                         21.92       9.58      13.34      13074
---- (label_id: 3)                                         44.94       6.69      11.64       1062
---. (label_id: 4)                                         28.64      30.78      29.67      11077
---: (label_id: 5)                                          0.00       0.00       0.00        330
---; (label_id: 6)                                          0.00       0.00       0.00        108
---? (label_id: 7)                                          0.00       0.00       0.00        899
---— (label_id: 8)                                          0.00       0.00       0.00        276
---… (label_id: 9)                                          0.00       0.00       0.00         28
----------------------
---micro avg                                               84.95      84.95      84.95     206806
---macro avg                                               18.59      14.21      14.73     206806
---weighted avg                                            81.82      84.95      83.11     206806
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1959
----------------------
---micro avg                                              100.00     100.00     100.00       1959
---macro avg                                              100.00     100.00     100.00       1959
---weighted avg                                           100.00     100.00     100.00       1959
---
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          90.62      94.69      92.61     179900
---! (label_id: 1)                                          0.00       0.00       0.00         52
---, (label_id: 2)                                         21.80      10.36      14.04      13074
---- (label_id: 3)                                         44.94       6.69      11.64       1062
---. (label_id: 4)                                         28.46      32.05      30.15      11077
---: (label_id: 5)                                          0.00       0.00       0.00        330
---; (label_id: 6)                                          0.00       0.00       0.00        108
---? (label_id: 7)                                          0.00       0.00       0.00        899
---— (label_id: 8)                                          0.00       0.00       0.00        276
---… (label_id: 9)                                          0.00       0.00       0.00         28
----------------------
---micro avg                                               84.77      84.77      84.77     206806
---macro avg                                               18.58      14.38      14.84     206806
---weighted avg                                            81.97      84.77      83.12     206806
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1959
----------------------
---micro avg                                              100.00     100.00     100.00       1959
---macro avg                                              100.00     100.00     100.00       1959
---weighted avg                                           100.00     100.00     100.00       1959
---
--diff --git a/experiment/main.py b/experiment/main.py
--index 6b15e25..a779a56 100644
----- a/experiment/main.py
--+++ b/experiment/main.py
--@@ -39,11 +39,11 @@ def main(cfg: DictConfig)->None:
--     
--     while(model.hparams.model.unfrozen<=cfg.model.maximum_unfrozen and model.hparams.model.unfrozen>=0):
--         trainer.current_epoch=0
---        lr_finder = trainer.tuner.lr_find(model,min_lr=1e-8, max_lr=0.5, num_training=80) #, early_stop_threshold=None
---        # Results can be found in
---        pp(lr_finder.results)
---        new_lr = lr_finder.suggestion()
---        model.hparams.model.optim.lr = new_lr
--+        # lr_finder = trainer.tuner.lr_find(model,min_lr=1e-8, max_lr=0.5, num_training=80) #, early_stop_threshold=None
--+        # # Results can be found in
--+        # pp(lr_finder.results)
--+        # new_lr = lr_finder.suggestion()
--+        # model.hparams.model.optim.lr = new_lr
--         model.dm.reset()
--         trainer.current_epoch=0
--         trainer.fit(model)
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/hparams.yaml
-deleted file mode 100644
-index a595688..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/hparams.yaml
-+++ /dev/null
-@@ -1,109 +0,0 @@
--seed: 42
--trainer:
--  gpus: 1
--  num_nodes: 1
--  max_epochs: 2
--  max_steps: null
--  accumulate_grad_batches: 4
--  gradient_clip_val: 1
--  amp_level: O1
--  precision: 16
--  accelerator: ddp
--  checkpoint_callback: false
--  logger: false
--  log_every_n_steps: 1
--  val_check_interval: 1.0
--  resume_from_checkpoint: null
--exp_manager:
--  exp_dir: /home/nxingyu/project/
--  name: Punctuation_with_Domain_discriminator
--  create_tensorboard_logger: true
--  create_checkpoint_callback: true
--base_path: /home/nxingyu/data
--tmp_path: /home/nxingyu/data/tmp
--model:
--  nemo_path: null
--  transformer_path: google/electra-small-discriminator
--  unfrozen: 0
--  maximum_unfrozen: 1
--  unfreeze_step: 1
--  punct_label_ids:
--  - ''
--  - '!'
--  - ','
--  - '-'
--  - .
--  - ':'
--  - ;
--  - '?'
--  - —
--  - …
--  punct_class_weights: false
--  dataset:
--    data_dir: /home/nxingyu/data
--    labelled:
--    - /home/nxingyu/data/ted_talks_processed
--    unlabelled: null
--    max_seq_length: 128
--    pad_label: ''
--    ignore_extra_tokens: false
--    ignore_start_end: false
--    use_cache: false
--    num_workers: 2
--    pin_memory: true
--    drop_last: true
--    num_labels: 10
--    num_domains: 1
--    test_unlabelled: true
--    train_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--    validation_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--  tokenizer:
--    tokenizer_name: google/electra-small-discriminator
--    vocab_file: null
--    tokenizer_model: null
--    special_tokens: null
--  language_model:
--    pretrained_model_name: google/electra-small-discriminator
--    lm_checkpoint: null
--    config_file: null
--    config: null
--  punct_head:
--    punct_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: dice
--  domain_head:
--    domain_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: cel
--    gamma: 0.1
--    pooling: mean_max
--    idx_conditioned_on: 0
--  dice_loss:
--    epsilon: 0.01
--    alpha: 1
--    macro_average: true
--  focal_loss:
--    gamma: 1
--  optim:
--    name: adamw
--    lr: 0.009261935523740748
--    weight_decay: 0.0
--    sched:
--      name: WarmupAnnealing
--      warmup_steps: null
--      warmup_ratio: 0.1
--      last_epoch: -1
--      monitor: val_loss
--      reduce_on_plateau: false
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/lightning_logs.txt
-deleted file mode 100644
-index 84e5133..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/lightning_logs.txt
-+++ /dev/null
-@@ -1,47 +0,0 @@
--Global seed set to 42
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
--Using native 16bit precision.
--Global seed set to 42
--initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 513   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--299 K     Trainable params
--13.2 M    Non-trainable params
--13.5 M    Total params
--Epoch 0, global step 199: val_loss reached 0.28475 (best 0.28475), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.28-epoch=0.ckpt" as top 3
--Epoch 1, global step 399: val_loss reached 0.27252 (best 0.27252), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=1.ckpt" as top 3
--Saving latest checkpoint...
--Global seed set to 42
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 513   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--825 K     Trainable params
--12.7 M    Non-trainable params
--13.5 M    Total params
--Epoch 0, global step 599: val_loss reached 0.27252 (best 0.27252), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=0.ckpt" as top 3
--Epoch 1, global step 799: val_loss reached 0.27252 (best 0.27252), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=1-v0.ckpt" as top 3
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--Using environment variable NODE_RANK for node rank (0).
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/nemo_error_log.txt
-deleted file mode 100644
-index 8fd05fc..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/nemo_error_log.txt
-+++ /dev/null
-@@ -1,25 +0,0 @@
--[NeMo W 2021-02-09 15:48:17 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 15:48:30 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:48:30 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:48:31 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:51:08 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4fa07d5970> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 15:51:22 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4fa9ce8850> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 15:54:23 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-09 15:59:50 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 16:00:08 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4fa9ce85e0> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/nemo_log_globalrank-0_localrank-0.txt
-deleted file mode 100644
-index d2d8b9e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/nemo_log_globalrank-0_localrank-0.txt
-+++ /dev/null
-@@ -1,27 +0,0 @@
--[NeMo I 2021-02-09 15:48:17 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17
--[NeMo I 2021-02-09 15:48:17 exp_manager:519] TensorboardLogger has been set up
--[NeMo W 2021-02-09 15:48:17 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 15:48:30 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:48:30 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:48:31 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:51:08 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4fa07d5970> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 15:51:22 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4fa9ce8850> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 15:54:23 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-09 15:59:50 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 16:00:08 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4fa9ce85e0> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/cmd-args.log
-deleted file mode 100644
-index 11a5d8e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/cmd-args.log
-+++ /dev/null
-@@ -1 +0,0 @@
--main.py
-\ No newline at end of file
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/events.out.tfevents.1612858892.Titan.27659.0 b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/events.out.tfevents.1612858892.Titan.27659.0
-deleted file mode 100644
-index e8dc27e..0000000
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/events.out.tfevents.1612858892.Titan.27659.0 and /dev/null differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/git-info.log
-deleted file mode 100644
-index 3570852..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/git-info.log
-+++ /dev/null
-@@ -1,449 +0,0 @@
--commit hash: 836632f0fdebe90f93105efab295fe4cd83af4ca
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0 b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0
--index c3ff071..0e8637d 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0 differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt
--index 7f6eddf..a1b895b 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt
--@@ -39,3 +39,53 @@ Global seed set to 42
-- 66.4 M    Total params
-- Epoch 0, global step 200: val_loss reached 0.26631 (best 0.26631), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=0.ckpt" as top 3
-- Epoch 1, global step 400: val_loss reached 0.26336 (best 0.26336), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=1.ckpt" as top 3
--+Epoch 2, global step 600: val_loss reached 0.26109 (best 0.26109), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=2.ckpt" as top 3
--+Epoch 3, global step 800: val_loss reached 0.25805 (best 0.25805), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=3.ckpt" as top 3
--+Epoch 4, global step 1000: val_loss reached 0.25547 (best 0.25547), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=4.ckpt" as top 3
--+Epoch 5, global step 1200: val_loss reached 0.25339 (best 0.25339), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.25-epoch=5.ckpt" as top 3
--+Epoch 6, global step 1400: val_loss reached 0.24810 (best 0.24810), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.25-epoch=6.ckpt" as top 3
--+Epoch 7, global step 1600: val_loss reached 0.24127 (best 0.24127), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=7.ckpt" as top 3
--+Epoch 8, global step 1800: val_loss reached 0.23864 (best 0.23864), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=8.ckpt" as top 3
--+Epoch 9, global step 2000: val_loss reached 0.23646 (best 0.23646), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=9.ckpt" as top 3
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | DistilBertModel      | 66.4 M
--+1 | punct_classifier    | TokenClassifier      | 7.7 K 
--+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--+3 | punctuation_loss    | FocalDiceLoss        | 0     
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+7.1 M     Trainable params
--+59.3 M    Non-trainable params
--+66.4 M    Total params
--+LR finder stopped early due to diverging loss.
--+Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lr_find_temp_model.ckpt
--+Failed to compute suggesting for `lr`. There might not be enough points.
--+Traceback (most recent call last):
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
--+    min_grad = np.gradient(loss).argmin()
--+  File "<__array_function__ internals>", line 5, in gradient
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
--+    raise ValueError(
--+ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | DistilBertModel      | 66.4 M
--+1 | punct_classifier    | TokenClassifier      | 7.7 K 
--+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--+3 | punctuation_loss    | FocalDiceLoss        | 0     
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+7.1 M     Trainable params
--+59.3 M    Non-trainable params
--+66.4 M    Total params
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt
--index 2d9ccfb..a9e0895 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt
--@@ -14,3 +14,14 @@
-- [NeMo W 2021-02-09 15:11:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fa1136ae0a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 15:25:45 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--+      warnings.warn(*args, **kwargs)
--+    
--+Failed to compute suggesting for `lr`. There might not be enough points.
--+Traceback (most recent call last):
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
--+    min_grad = np.gradient(loss).argmin()
--+  File "<__array_function__ internals>", line 5, in gradient
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
--+    raise ValueError(
--+ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt
--index 80a7030..dfff301 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt
--@@ -16,3 +16,6 @@
-- [NeMo W 2021-02-09 15:11:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fa1136ae0a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 15:25:45 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--+      warnings.warn(*args, **kwargs)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0
--index bb4d846..53377de 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
--index 2869be1..0f4c210 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
--@@ -37,3 +37,55 @@ Global seed set to 42
-- 10.9 K    Trainable params
-- 66.4 M    Non-trainable params
-- 66.4 M    Total params
--+Epoch 0, global step 200: val_loss reached 108.49911 (best 108.49911), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=108.50-epoch=0.ckpt" as top 3
--+Epoch 1, global step 400: val_loss reached 155.59180 (best 108.49911), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=155.59-epoch=1.ckpt" as top 3
--+Epoch 2, global step 600: val_loss reached 46.96625 (best 46.96625), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=46.97-epoch=2.ckpt" as top 3
--+Epoch 3, global step 800: val_loss reached 63.33499 (best 46.96625), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=63.33-epoch=3.ckpt" as top 3
--+Epoch 4, global step 1000: val_loss reached 33.09097 (best 33.09097), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=33.09-epoch=4.ckpt" as top 3
--+Epoch 5, global step 1200: val_loss reached 30.02221 (best 30.02221), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=30.02-epoch=5.ckpt" as top 3
--+Epoch 6, global step 1400: val_loss reached 31.40887 (best 30.02221), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=31.41-epoch=6.ckpt" as top 3
--+Epoch 7, step 1600: val_loss was not in top 3
--+Epoch 8, global step 1800: val_loss reached 28.61345 (best 28.61345), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=28.61-epoch=8.ckpt" as top 3
--+Epoch 9, global step 2000: val_loss reached 26.27475 (best 26.27475), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=26.27-epoch=9.ckpt" as top 3
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | DistilBertModel      | 66.4 M
--+1 | punct_classifier    | TokenClassifier      | 7.7 K 
--+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--+3 | punctuation_loss    | LinearChainCRF       | 120   
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+7.1 M     Trainable params
--+59.3 M    Non-trainable params
--+66.4 M    Total params
--+LR finder stopped early due to diverging loss.
--+Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lr_find_temp_model.ckpt
--+Failed to compute suggesting for `lr`. There might not be enough points.
--+Traceback (most recent call last):
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
--+    min_grad = np.gradient(loss).argmin()
--+  File "<__array_function__ internals>", line 5, in gradient
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
--+    raise ValueError(
--+ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | DistilBertModel      | 66.4 M
--+1 | punct_classifier    | TokenClassifier      | 7.7 K 
--+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--+3 | punctuation_loss    | LinearChainCRF       | 120   
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+7.1 M     Trainable params
--+59.3 M    Non-trainable params
--+66.4 M    Total params
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
--index 568694f..cab9655 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
--@@ -23,3 +23,20 @@
-- [NeMo W 2021-02-09 15:11:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--     
--+[NeMo W 2021-02-09 15:16:02 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce8610> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--+[NeMo W 2021-02-09 15:16:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce40a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--+[NeMo W 2021-02-09 15:57:31 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--+      warnings.warn(*args, **kwargs)
--+    
--+Failed to compute suggesting for `lr`. There might not be enough points.
--+Traceback (most recent call last):
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
--+    min_grad = np.gradient(loss).argmin()
--+  File "<__array_function__ internals>", line 5, in gradient
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
--+    raise ValueError(
--+ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
--index 7533c2c..d19ea43 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
--@@ -25,3 +25,12 @@
-- [NeMo W 2021-02-09 15:11:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--     
--+[NeMo W 2021-02-09 15:16:02 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce8610> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--+[NeMo W 2021-02-09 15:16:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce40a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--+[NeMo W 2021-02-09 15:57:31 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--+      warnings.warn(*args, **kwargs)
--+    
--diff --git a/experiment/config.yaml b/experiment/config.yaml
--index 0aeaa8b..7a28782 100644
----- a/experiment/config.yaml
--+++ b/experiment/config.yaml
--@@ -41,7 +41,7 @@ tmp_path: /home/nxingyu/data/tmp # /tmp #
-- 
-- model:
--     nemo_path: null
---    transformer_path: distilbert-base-uncased #google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
--+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
--     unfrozen: 0
--     maximum_unfrozen: 1
--     unfreeze_step: 1
--@@ -75,7 +75,7 @@ model:
--         ignore_start_end: false
--         use_cache: false
--         # shared among dataloaders
---        num_workers:  2
--+        num_workers:  4
--         pin_memory: true
--         drop_last: true
--         num_labels: 10
--@@ -114,7 +114,7 @@ model:
--         activation: 'relu'
--         log_softmax: false
--         use_transformer_init: true
---        loss: 'crf'
--+        loss: 'dice'
-- 
--     domain_head:
--         domain_num_fc_layers: 1
--diff --git a/experiment/data/punctuation_datamodule.py b/experiment/data/punctuation_datamodule.py
--index 20a4093..8711456 100644
----- a/experiment/data/punctuation_datamodule.py
--+++ b/experiment/data/punctuation_datamodule.py
--@@ -108,7 +108,8 @@ class PunctuationDataModule(LightningDataModule):
-- 
--         logging.info(f"shuffling train set")
--         # self.train_dataset.shuffle(randomize=False)
---        self.train_dataset.shuffle(randomize=True, seed=self.seed)
--+        if (self.train_shuffle):
--+            self.train_dataset.shuffle(randomize=True, seed=self.seed)
-- 
--     def train_dataloader(self):
--         return DataLoader(self.train_dataset,batch_size=None,num_workers=self.num_workers,pin_memory=self.pin_memory,drop_last=self.drop_last)
--diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
--index 88a268f..31923de 100644
----- a/experiment/data/punctuation_dataset_multi.py
--+++ b/experiment/data/punctuation_dataset_multi.py
--@@ -165,8 +165,8 @@ class PunctuationDomainDatasets(IterableDataset):
--         for path in labelled+unlabelled:
--             self.ds_lengths.append(int(subprocess.Popen(['wc', '-l', f'{path}.{split}.csv'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0]))
--         self.max_length=max(self.ds_lengths)
---        self.len=int(self.max_length/num_samples)
--         self.per_worker=int(self.max_length/self.num_workers)
--+        self.len=int(self.per_worker/num_samples)
--         self.class_weights=None
-- 
--         for i,path in enumerate(labelled):
--diff --git a/experiment/info.log b/experiment/info.log
--index ff2d606..e69de29 100644
----- a/experiment/info.log
--+++ b/experiment/info.log
--@@ -1,133 +0,0 @@
---[INFO] - shuffling train set
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.009261935523740748
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f596c78a3d0>" 
---will be used during training (effective maximum steps = 80) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---last_epoch: -1
---max_steps: 80
---)
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          80.85       4.94       9.30       1540
---! (label_id: 1)                                          0.00       0.00       0.00          2
---, (label_id: 2)                                          7.69       1.41       2.38        142
---- (label_id: 3)                                          1.67      20.00       3.08         20
---. (label_id: 4)                                          6.82      12.50       8.82         96
---: (label_id: 5)                                          0.00       0.00       0.00          0
---; (label_id: 6)                                          0.00       0.00       0.00          0
---? (label_id: 7)                                          0.00       0.00       0.00          4
---— (label_id: 8)                                          0.00       0.00       0.00          0
---… (label_id: 9)                                          0.00       0.00       0.00          0
----------------------
---micro avg                                                5.21       5.21       5.21       1804
---macro avg                                               16.17       6.47       3.93       1804
---weighted avg                                            70.01       5.21       8.63       1804
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00         20
----------------------
---micro avg                                              100.00     100.00     100.00         20
---macro avg                                              100.00     100.00     100.00         20
---weighted avg                                           100.00     100.00     100.00         20
---
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.0024506370946974477
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f596c570f40>" 
---will be used during training (effective maximum steps = 2000) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---last_epoch: -1
---max_steps: 2000
---)
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          80.85       4.94       9.30       1540
---! (label_id: 1)                                          0.00       0.00       0.00          2
---, (label_id: 2)                                          7.69       1.41       2.38        142
---- (label_id: 3)                                          1.67      20.00       3.08         20
---. (label_id: 4)                                          6.82      12.50       8.82         96
---: (label_id: 5)                                          0.00       0.00       0.00          0
---; (label_id: 6)                                          0.00       0.00       0.00          0
---? (label_id: 7)                                          0.00       0.00       0.00          4
---— (label_id: 8)                                          0.00       0.00       0.00          0
---… (label_id: 9)                                          0.00       0.00       0.00          0
----------------------
---micro avg                                                5.21       5.21       5.21       1804
---macro avg                                               16.17       6.47       3.93       1804
---weighted avg                                            70.01       5.21       8.63       1804
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00         20
----------------------
---micro avg                                              100.00     100.00     100.00         20
---macro avg                                              100.00     100.00     100.00         20
---weighted avg                                           100.00     100.00     100.00         20
---
---NFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          90.44      95.02      92.67     179900
---! (label_id: 1)                                          0.00       0.00       0.00         52
---, (label_id: 2)                                         21.92       9.58      13.34      13074
---- (label_id: 3)                                         44.94       6.69      11.64       1062
---. (label_id: 4)                                         28.64      30.78      29.67      11077
---: (label_id: 5)                                          0.00       0.00       0.00        330
---; (label_id: 6)                                          0.00       0.00       0.00        108
---? (label_id: 7)                                          0.00       0.00       0.00        899
---— (label_id: 8)                                          0.00       0.00       0.00        276
---… (label_id: 9)                                          0.00       0.00       0.00         28
----------------------
---micro avg                                               84.95      84.95      84.95     206806
---macro avg                                               18.59      14.21      14.73     206806
---weighted avg                                            81.82      84.95      83.11     206806
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1959
----------------------
---micro avg                                              100.00     100.00     100.00       1959
---macro avg                                              100.00     100.00     100.00       1959
---weighted avg                                           100.00     100.00     100.00       1959
---
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          90.62      94.69      92.61     179900
---! (label_id: 1)                                          0.00       0.00       0.00         52
---, (label_id: 2)                                         21.80      10.36      14.04      13074
---- (label_id: 3)                                         44.94       6.69      11.64       1062
---. (label_id: 4)                                         28.46      32.05      30.15      11077
---: (label_id: 5)                                          0.00       0.00       0.00        330
---; (label_id: 6)                                          0.00       0.00       0.00        108
---? (label_id: 7)                                          0.00       0.00       0.00        899
---— (label_id: 8)                                          0.00       0.00       0.00        276
---… (label_id: 9)                                          0.00       0.00       0.00         28
----------------------
---micro avg                                               84.77      84.77      84.77     206806
---macro avg                                               18.58      14.38      14.84     206806
---weighted avg                                            81.97      84.77      83.12     206806
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1959
----------------------
---micro avg                                              100.00     100.00     100.00       1959
---macro avg                                              100.00     100.00     100.00       1959
---weighted avg                                           100.00     100.00     100.00       1959
---
--diff --git a/experiment/main.py b/experiment/main.py
--index 6b15e25..1f44193 100644
----- a/experiment/main.py
--+++ b/experiment/main.py
--@@ -26,7 +26,7 @@ def main(cfg: DictConfig)->None:
--     data_id = str(int(time()))
--     def savecounter():
--         # pp(os.system(f'rm -r {cfg.model.dataset.data_dir}/*.{data_id}.csv'))
---        pp(os.system(f'rm -r {cfg.tmp_path}/*.{data_id}.csv'))
--+        pp(os.system(f'rm -r {cfg.tmp_path}/*.{data_id}*'))
--     atexit.register(savecounter)
-- 
--     cfg.model.maximum_unfrozen=max(cfg.model.maximum_unfrozen,cfg.model.unfrozen)
--@@ -37,14 +37,34 @@ def main(cfg: DictConfig)->None:
--     exp_manager(trainer, cfg.exp_manager)
--     model = PunctuationDomainModel(cfg=cfg, trainer=trainer, data_id = data_id)
--     
--+    # lr_finder_dm=PunctuationDataModule(
--+    #         tokenizer= cfg.model.transformer_path,
--+    #         labelled= list(cfg.model.dataset.labelled),
--+    #         unlabelled= list(cfg.model.dataset.unlabelled),
--+    #         punct_label_ids= {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)},
--+    #         train_batch_size= cfg.model.dataset.train_ds.batch_size,
--+    #         max_seq_length= cfg.model.dataset.max_seq_length,
--+    #         val_batch_size= cfg.model.dataset.validation_ds.batch_size,
--+    #         num_workers= 1,
--+    #         pin_memory= False,
--+    #         train_shuffle= True,
--+    #         val_shuffle= False,
--+    #         seed=cfg.seed,
--+    #         data_id=data_id+'lr',
--+    #         tmp_path=cfg.tmp_path,
--+    #         test_unlabelled=False,
--+    #     )
--+    lrs=[1e-4,1e-6]
--     while(model.hparams.model.unfrozen<=cfg.model.maximum_unfrozen and model.hparams.model.unfrozen>=0):
---        trainer.current_epoch=0
---        lr_finder = trainer.tuner.lr_find(model,min_lr=1e-8, max_lr=0.5, num_training=80) #, early_stop_threshold=None
---        # Results can be found in
---        pp(lr_finder.results)
---        new_lr = lr_finder.suggestion()
---        model.hparams.model.optim.lr = new_lr
---        model.dm.reset()
--+        # trainer.current_epoch=0
--+        # lr_finder = trainer.tuner.lr_find(model,datamodule=lr_finder_dm,min_lr=1e-8, max_lr=0.5, num_training=80) #, early_stop_threshold=None
--+        # # Results can be found in
--+        # pp(lr_finder.results)
--+        # new_lr = lr_finder.suggestion()
--+        # model.hparams.model.optim.lr = new_lr
--+        # lr_finder_dm.reset()
--+        # model.dm.reset()
--+        model.hparams.model.optim.lr = lrs[model.hparams.model.unfrozen]
--         trainer.current_epoch=0
--         trainer.fit(model)
--         try:
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/hparams.yaml
-deleted file mode 100644
-index 4922992..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/hparams.yaml
-+++ /dev/null
-@@ -1,109 +0,0 @@
--seed: 42
--trainer:
--  gpus: 1
--  num_nodes: 1
--  max_epochs: 10
--  max_steps: null
--  accumulate_grad_batches: 4
--  gradient_clip_val: 0
--  amp_level: O1
--  precision: 16
--  accelerator: ddp
--  checkpoint_callback: false
--  logger: false
--  log_every_n_steps: 1
--  val_check_interval: 1.0
--  resume_from_checkpoint: null
--exp_manager:
--  exp_dir: /home/nxingyu/project/
--  name: Punctuation_with_Domain_discriminator
--  create_tensorboard_logger: true
--  create_checkpoint_callback: true
--base_path: /home/nxingyu/data
--tmp_path: /home/nxingyu/data/tmp
--model:
--  nemo_path: null
--  transformer_path: google/electra-small-discriminator
--  unfrozen: 0
--  maximum_unfrozen: 1
--  unfreeze_step: 1
--  punct_label_ids:
--  - ''
--  - '!'
--  - ','
--  - '-'
--  - .
--  - ':'
--  - ;
--  - '?'
--  - —
--  - …
--  punct_class_weights: false
--  dataset:
--    data_dir: /home/nxingyu/data
--    labelled:
--    - /home/nxingyu/data/ted_talks_processed
--    unlabelled: null
--    max_seq_length: 128
--    pad_label: ''
--    ignore_extra_tokens: false
--    ignore_start_end: false
--    use_cache: false
--    num_workers: 4
--    pin_memory: true
--    drop_last: true
--    num_labels: 10
--    num_domains: 1
--    test_unlabelled: true
--    train_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--    validation_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--  tokenizer:
--    tokenizer_name: google/electra-small-discriminator
--    vocab_file: null
--    tokenizer_model: null
--    special_tokens: null
--  language_model:
--    pretrained_model_name: google/electra-small-discriminator
--    lm_checkpoint: null
--    config_file: null
--    config: null
--  punct_head:
--    punct_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: dice
--  domain_head:
--    domain_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: cel
--    gamma: 0.1
--    pooling: mean_max
--    idx_conditioned_on: 0
--  dice_loss:
--    epsilon: 0.01
--    alpha: 3
--    macro_average: true
--  focal_loss:
--    gamma: 1
--  optim:
--    name: adamw
--    lr: 0.009261935523740748
--    weight_decay: 0.0
--    sched:
--      name: WarmupAnnealing
--      warmup_steps: null
--      warmup_ratio: 0.1
--      last_epoch: -1
--      monitor: val_loss
--      reduce_on_plateau: false
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/lightning_logs.txt
-deleted file mode 100644
-index 40964a1..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/lightning_logs.txt
-+++ /dev/null
-@@ -1,63 +0,0 @@
--Global seed set to 42
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
--Using native 16bit precision.
--Global seed set to 42
--initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 513   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--299 K     Trainable params
--13.2 M    Non-trainable params
--13.5 M    Total params
--Epoch 0, global step 199: val_loss reached 0.66148 (best 0.66148), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.66-epoch=0.ckpt" as top 3
--Epoch 1, global step 399: val_loss reached 0.23663 (best 0.23663), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=1.ckpt" as top 3
--Epoch 2, global step 599: val_loss reached 0.17288 (best 0.17288), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.17-epoch=2.ckpt" as top 3
--Epoch 3, global step 799: val_loss reached 0.15670 (best 0.15670), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.16-epoch=3.ckpt" as top 3
--Epoch 4, global step 999: val_loss reached 0.14782 (best 0.14782), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.15-epoch=4.ckpt" as top 3
--Epoch 5, global step 1199: val_loss reached 0.14246 (best 0.14246), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=5.ckpt" as top 3
--Epoch 6, global step 1399: val_loss reached 0.13934 (best 0.13934), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=6.ckpt" as top 3
--Epoch 7, global step 1599: val_loss reached 0.13714 (best 0.13714), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=7.ckpt" as top 3
--Epoch 8, global step 1799: val_loss reached 0.13633 (best 0.13633), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=8.ckpt" as top 3
--Epoch 9, global step 1999: val_loss reached 0.13605 (best 0.13605), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=9.ckpt" as top 3
--Saving latest checkpoint...
--Global seed set to 42
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 513   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--825 K     Trainable params
--12.7 M    Non-trainable params
--13.5 M    Total params
--Epoch 0, global step 2199: val_loss reached 0.13605 (best 0.13605), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=0.ckpt" as top 3
--Epoch 1, global step 2399: val_loss reached 0.13605 (best 0.13605), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=1.ckpt" as top 3
--Epoch 2, step 2599: val_loss was not in top 3
--Epoch 3, step 2799: val_loss was not in top 3
--Epoch 4, step 2999: val_loss was not in top 3
--Epoch 5, step 3199: val_loss was not in top 3
--Epoch 6, step 3399: val_loss was not in top 3
--Epoch 7, step 3599: val_loss was not in top 3
--Epoch 8, step 3799: val_loss was not in top 3
--Epoch 9, step 3999: val_loss was not in top 3
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--Using environment variable NODE_RANK for node rank (0).
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_error_log.txt
-deleted file mode 100644
-index 2dc7701..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_error_log.txt
-+++ /dev/null
-@@ -1,16 +0,0 @@
--[NeMo W 2021-02-09 16:21:19 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 16:21:32 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 16:22:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f1a20badbb0> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 16:23:04 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f1a2a0bb850> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 16:36:51 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-09 16:52:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f1a2a0bb5e0> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_log_globalrank-0_localrank-0.txt
-deleted file mode 100644
-index 282de52..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_log_globalrank-0_localrank-0.txt
-+++ /dev/null
-@@ -1,18 +0,0 @@
--[NeMo I 2021-02-09 16:21:19 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19
--[NeMo I 2021-02-09 16:21:19 exp_manager:519] TensorboardLogger has been set up
--[NeMo W 2021-02-09 16:21:19 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 16:21:32 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 16:22:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f1a20badbb0> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 16:23:04 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f1a2a0bb850> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 16:36:51 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-09 16:52:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f1a2a0bb5e0> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/cmd-args.log
-deleted file mode 100644
-index 11a5d8e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/cmd-args.log
-+++ /dev/null
-@@ -1 +0,0 @@
--main.py
-\ No newline at end of file
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/events.out.tfevents.1612860293.Titan.7050.0 b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/events.out.tfevents.1612860293.Titan.7050.0
-deleted file mode 100644
-index 600f922..0000000
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/events.out.tfevents.1612860293.Titan.7050.0 and /dev/null differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/git-info.log
-deleted file mode 100644
-index 76e19ab..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/git-info.log
-+++ /dev/null
-@@ -1,275 +0,0 @@
--commit hash: 22df8b7032fa2ae6a488d957253de2d56042b4d6
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/events.out.tfevents.1612858879.Titan.27601.0 b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/events.out.tfevents.1612858879.Titan.27601.0
--index 4337671..16a35c9 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/events.out.tfevents.1612858879.Titan.27601.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/events.out.tfevents.1612858879.Titan.27601.0 differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/lightning_logs.txt
--index a3fc0fb..201da45 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/lightning_logs.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/lightning_logs.txt
--@@ -21,3 +21,27 @@ initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-- 13.2 M    Non-trainable params
-- 13.5 M    Total params
-- Epoch 0, global step 199: val_loss reached 54.61362 (best 54.61362), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/checkpoints/Punctuation_with_Domain_discriminator---val_loss=54.61-epoch=0.ckpt" as top 3
--+Epoch 1, global step 399: val_loss reached 42.83130 (best 42.83130), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/checkpoints/Punctuation_with_Domain_discriminator---val_loss=42.83-epoch=1.ckpt" as top 3
--+Epoch 2, global step 599: val_loss reached 32.94712 (best 32.94712), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/checkpoints/Punctuation_with_Domain_discriminator---val_loss=32.95-epoch=2.ckpt" as top 3
--+Saving latest checkpoint...
--+Epoch 3, global step 732: val_loss reached 32.94712 (best 32.94712), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/checkpoints/Punctuation_with_Domain_discriminator---val_loss=32.95-epoch=3.ckpt" as top 3
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | ElectraModel         | 13.5 M
--+1 | punct_classifier    | TokenClassifier      | 2.6 K 
--+2 | domain_classifier   | SequenceClassifier   | 513   
--+3 | punctuation_loss    | LinearChainCRF       | 120   
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+825 K     Trainable params
--+12.7 M    Non-trainable params
--+13.5 M    Total params
--+GPU available: True, used: True
--+TPU available: None, using: 0 TPU cores
--+Using environment variable NODE_RANK for node rank (0).
--+LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_error_log.txt
--index f051996..43ff45a 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_error_log.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_error_log.txt
--@@ -11,3 +11,9 @@
-- [NeMo W 2021-02-09 16:25:45 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fe13a7e4820> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 16:37:26 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
--+      warnings.warn(*args, **kwargs)
--+    
--+[NeMo W 2021-02-09 16:37:50 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fe13a7e45b0> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_log_globalrank-0_localrank-0.txt
--index a183522..da0fc8b 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_log_globalrank-0_localrank-0.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_log_globalrank-0_localrank-0.txt
--@@ -13,3 +13,9 @@
-- [NeMo W 2021-02-09 16:25:45 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fe13a7e4820> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 16:37:26 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
--+      warnings.warn(*args, **kwargs)
--+    
--+[NeMo W 2021-02-09 16:37:50 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fe13a7e45b0> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/events.out.tfevents.1612858892.Titan.27659.0 b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/events.out.tfevents.1612858892.Titan.27659.0
--index 5b09bdd..303bdf7 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/events.out.tfevents.1612858892.Titan.27659.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/events.out.tfevents.1612858892.Titan.27659.0 differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/lightning_logs.txt
--index 4f17a12..a2aef56 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/lightning_logs.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/lightning_logs.txt
--@@ -23,3 +23,32 @@ initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-- Epoch 0, global step 199: val_loss reached 0.66148 (best 0.66148), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.66-epoch=0.ckpt" as top 3
-- Epoch 1, global step 399: val_loss reached 0.23663 (best 0.23663), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=1.ckpt" as top 3
-- Epoch 2, global step 599: val_loss reached 0.17288 (best 0.17288), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.17-epoch=2.ckpt" as top 3
--+Epoch 3, global step 799: val_loss reached 0.15670 (best 0.15670), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.16-epoch=3.ckpt" as top 3
--+Epoch 4, global step 999: val_loss reached 0.14782 (best 0.14782), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.15-epoch=4.ckpt" as top 3
--+Epoch 5, global step 1199: val_loss reached 0.14246 (best 0.14246), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=5.ckpt" as top 3
--+Epoch 6, global step 1399: val_loss reached 0.13934 (best 0.13934), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=6.ckpt" as top 3
--+Epoch 7, global step 1599: val_loss reached 0.13714 (best 0.13714), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=7.ckpt" as top 3
--+Epoch 8, global step 1799: val_loss reached 0.13633 (best 0.13633), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=8.ckpt" as top 3
--+Epoch 9, global step 1999: val_loss reached 0.13605 (best 0.13605), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=9.ckpt" as top 3
--+Saving latest checkpoint...
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | ElectraModel         | 13.5 M
--+1 | punct_classifier    | TokenClassifier      | 2.6 K 
--+2 | domain_classifier   | SequenceClassifier   | 513   
--+3 | punctuation_loss    | FocalDiceLoss        | 0     
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+825 K     Trainable params
--+12.7 M    Non-trainable params
--+13.5 M    Total params
--+Epoch 0, global step 2199: val_loss reached 0.13605 (best 0.13605), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=0.ckpt" as top 3
--+Epoch 1, global step 2399: val_loss reached 0.13605 (best 0.13605), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=1.ckpt" as top 3
--+Epoch 2, step 2599: val_loss was not in top 3
--+Epoch 3, step 2799: val_loss was not in top 3
--+Epoch 4, step 2999: val_loss was not in top 3
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_error_log.txt
--index 39ac72a..89932b9 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_error_log.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_error_log.txt
--@@ -8,3 +8,6 @@
-- [NeMo W 2021-02-09 16:23:04 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f1a2a0bb850> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 16:36:51 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--+      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_log_globalrank-0_localrank-0.txt
--index baa087a..cf62f3e 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_log_globalrank-0_localrank-0.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_log_globalrank-0_localrank-0.txt
--@@ -10,3 +10,6 @@
-- [NeMo W 2021-02-09 16:23:04 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f1a2a0bb850> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 16:36:51 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--+      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--+    
--diff --git a/experiment/config.yaml b/experiment/config.yaml
--index 7a28782..fa6c702 100644
----- a/experiment/config.yaml
--+++ b/experiment/config.yaml
--@@ -137,13 +137,20 @@ model:
-- 
--     optim:
--         name: adamw
---        lr: 0.009261935523740748 #1e-3
--+        lr: 1e-3 #1e-3
--         weight_decay: 0.00
--         sched:
---            name: WarmupAnnealing #CyclicLR
--+            # name: CyclicLR
--+            # base_lr: 1e-5
--+            # max_lr: 1e-1
--+            # mode: 'triangular2'
--+            # last_epoch: -1
--+
--+            name: CosineAnnealing #WarmupAnnealing #CyclicLR
--             # Scheduler params
--             warmup_steps: null
--             warmup_ratio: 0.1
--+            min_lr: 1e-10
--             # hold_steps: 6
--             last_epoch: -1
-- 
--diff --git a/experiment/info.log b/experiment/info.log
--index 38d3b4b..e69de29 100644
----- a/experiment/info.log
--+++ b/experiment/info.log
--@@ -1,117 +0,0 @@
---[INFO] - shuffling train set
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.0001
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f1a21b62cd0>" 
---will be used during training (effective maximum steps = 2000) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---last_epoch: -1
---max_steps: 2000
---)
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          86.16      25.06      38.83       1540
---! (label_id: 1)                                          0.00       0.00       0.00          2
---, (label_id: 2)                                          8.60      38.03      14.03        142
---- (label_id: 3)                                          4.62      30.00       8.00         20
---. (label_id: 4)                                          0.00       0.00       0.00         96
---: (label_id: 5)                                          0.00       0.00       0.00          0
---; (label_id: 6)                                          0.00       0.00       0.00          0
---? (label_id: 7)                                          0.00       0.00       0.00          4
---— (label_id: 8)                                          0.00       0.00       0.00          0
---… (label_id: 9)                                          0.00       0.00       0.00          0
----------------------
---micro avg                                               24.72      24.72      24.72       1804
---macro avg                                               16.56      15.52      10.14       1804
---weighted avg                                            74.28      24.72      34.34       1804
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00         20
----------------------
---micro avg                                              100.00     100.00     100.00         20
---macro avg                                              100.00     100.00     100.00         20
---weighted avg                                           100.00     100.00     100.00         20
---
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          86.99     100.00      93.04     179900
---! (label_id: 1)                                          0.00       0.00       0.00         52
---, (label_id: 2)                                          0.00       0.00       0.00      13074
---- (label_id: 3)                                          0.00       0.00       0.00       1062
---. (label_id: 4)                                          0.00       0.00       0.00      11077
---: (label_id: 5)                                          0.00       0.00       0.00        330
---; (label_id: 6)                                          0.00       0.00       0.00        108
---? (label_id: 7)                                          0.00       0.00       0.00        899
---— (label_id: 8)                                          0.00       0.00       0.00        276
---… (label_id: 9)                                          0.00       0.00       0.00         28
----------------------
---micro avg                                               86.99      86.99      86.99     206806
---macro avg                                                8.70      10.00       9.30     206806
---weighted avg                                            75.67      86.99      80.94     206806
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1959
----------------------
---micro avg                                              100.00     100.00     100.00       1959
---macro avg                                              100.00     100.00     100.00       1959
---weighted avg                                           100.00     100.00     100.00       1959
---
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          94.70      97.81      96.23     179900
---! (label_id: 1)                                          0.00       0.00       0.00         52
---, (label_id: 2)                                         48.60      28.63      36.03      13074
---- (label_id: 3)                                         63.84      53.86      58.43       1062
---. (label_id: 4)                                         53.31      59.38      56.18      11077
---: (label_id: 5)                                          0.00       0.00       0.00        330
---; (label_id: 6)                                          0.00       0.00       0.00        108
---? (label_id: 7)                                         20.00       1.22       2.31        899
---— (label_id: 8)                                          0.00       0.00       0.00        276
---… (label_id: 9)                                          0.00       0.00       0.00         28
----------------------
---micro avg                                               90.36      90.36      90.36     206806
---macro avg                                               28.04      24.09      24.92     206806
---weighted avg                                            88.72      90.36      89.31     206806
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1959
----------------------
---micro avg                                              100.00     100.00     100.00       1959
---macro avg                                              100.00     100.00     100.00       1959
---weighted avg                                           100.00     100.00     100.00       1959
---
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          96.35      96.40      96.38     179900
---! (label_id: 1)                                          0.00       0.00       0.00         52
---, (label_id: 2)                                         43.32      40.33      41.77      13074
---- (label_id: 3)                                         64.46      56.87      60.43       1062
---. (label_id: 4)                                         53.28      62.91      57.69      11077
---: (label_id: 5)                                          0.00       0.00       0.00        330
---; (label_id: 6)                                          0.00       0.00       0.00        108
---? (label_id: 7)                                         19.81       9.23      12.59        899
---— (label_id: 8)                                          5.63       4.35       4.91        276
---… (label_id: 9)                                          0.00       0.00       0.00         28
----------------------
---micro avg                                               90.11      90.11      90.11     206806
---macro avg                                               28.29      27.01      27.38     206806
---weighted avg                                            89.83      90.11      89.94     206806
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1959
----------------------
---micro avg                                              100.00     100.00     100.00       1959
---macro avg                                              100.00     100.00     100.00       1959
---weighted avg                                           100.00     100.00     100.00       1959
---
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/hparams.yaml
-deleted file mode 100644
-index b405ca5..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/hparams.yaml
-+++ /dev/null
-@@ -1,110 +0,0 @@
--seed: 42
--trainer:
--  gpus: 1
--  num_nodes: 1
--  max_epochs: 10
--  max_steps: null
--  accumulate_grad_batches: 4
--  gradient_clip_val: 0
--  amp_level: O1
--  precision: 16
--  accelerator: ddp
--  checkpoint_callback: false
--  logger: false
--  log_every_n_steps: 1
--  val_check_interval: 1.0
--  resume_from_checkpoint: null
--exp_manager:
--  exp_dir: /home/nxingyu/project/
--  name: Punctuation_with_Domain_discriminator
--  create_tensorboard_logger: true
--  create_checkpoint_callback: true
--base_path: /home/nxingyu/data
--tmp_path: /home/nxingyu/data/tmp
--model:
--  nemo_path: null
--  transformer_path: google/electra-small-discriminator
--  unfrozen: 0
--  maximum_unfrozen: 1
--  unfreeze_step: 1
--  punct_label_ids:
--  - ''
--  - '!'
--  - ','
--  - '-'
--  - .
--  - ':'
--  - ;
--  - '?'
--  - —
--  - …
--  punct_class_weights: false
--  dataset:
--    data_dir: /home/nxingyu/data
--    labelled:
--    - /home/nxingyu/data/ted_talks_processed
--    unlabelled: null
--    max_seq_length: 128
--    pad_label: ''
--    ignore_extra_tokens: false
--    ignore_start_end: false
--    use_cache: false
--    num_workers: 4
--    pin_memory: true
--    drop_last: true
--    num_labels: 10
--    num_domains: 1
--    test_unlabelled: true
--    train_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--    validation_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--  tokenizer:
--    tokenizer_name: google/electra-small-discriminator
--    vocab_file: null
--    tokenizer_model: null
--    special_tokens: null
--  language_model:
--    pretrained_model_name: google/electra-small-discriminator
--    lm_checkpoint: null
--    config_file: null
--    config: null
--  punct_head:
--    punct_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: dice
--  domain_head:
--    domain_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: cel
--    gamma: 0.1
--    pooling: mean_max
--    idx_conditioned_on: 0
--  dice_loss:
--    epsilon: 0.01
--    alpha: 3
--    macro_average: true
--  focal_loss:
--    gamma: 1
--  optim:
--    name: adamw
--    lr: 0.001
--    weight_decay: 0.0
--    sched:
--      name: CosineAnnealing
--      warmup_steps: null
--      warmup_ratio: 0.1
--      min_lr: 1.0e-10
--      last_epoch: -1
--      monitor: val_loss
--      reduce_on_plateau: false
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/lightning_logs.txt
-deleted file mode 100644
-index 2fe9063..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/lightning_logs.txt
-+++ /dev/null
-@@ -1,63 +0,0 @@
--Global seed set to 42
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
--Using native 16bit precision.
--Global seed set to 42
--initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 513   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--299 K     Trainable params
--13.2 M    Non-trainable params
--13.5 M    Total params
--Epoch 0, global step 199: val_loss reached 0.66148 (best 0.66148), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.66-epoch=0.ckpt" as top 3
--Epoch 1, global step 399: val_loss reached 0.22688 (best 0.22688), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.23-epoch=1.ckpt" as top 3
--Epoch 2, global step 599: val_loss reached 0.16884 (best 0.16884), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.17-epoch=2.ckpt" as top 3
--Epoch 3, global step 799: val_loss reached 0.15254 (best 0.15254), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.15-epoch=3.ckpt" as top 3
--Epoch 4, global step 999: val_loss reached 0.14450 (best 0.14450), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=4.ckpt" as top 3
--Epoch 5, global step 1199: val_loss reached 0.13992 (best 0.13992), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=5.ckpt" as top 3
--Epoch 6, global step 1399: val_loss reached 0.13759 (best 0.13759), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=6.ckpt" as top 3
--Epoch 7, global step 1599: val_loss reached 0.13629 (best 0.13629), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=7.ckpt" as top 3
--Epoch 8, global step 1799: val_loss reached 0.13606 (best 0.13606), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=8.ckpt" as top 3
--Epoch 9, global step 1999: val_loss reached 0.13598 (best 0.13598), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=9.ckpt" as top 3
--Saving latest checkpoint...
--Global seed set to 42
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 513   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--825 K     Trainable params
--12.7 M    Non-trainable params
--13.5 M    Total params
--Epoch 0, global step 2199: val_loss reached 0.13598 (best 0.13598), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=0.ckpt" as top 3
--Epoch 1, global step 2399: val_loss reached 0.13598 (best 0.13598), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=1.ckpt" as top 3
--Epoch 2, global step 2599: val_loss reached 0.13598 (best 0.13598), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=2.ckpt" as top 3
--Epoch 3, global step 2799: val_loss reached 0.13598 (best 0.13598), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=3.ckpt" as top 3
--Epoch 4, global step 2999: val_loss reached 0.13598 (best 0.13598), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=4.ckpt" as top 3
--Epoch 5, step 3199: val_loss was not in top 3
--Epoch 6, step 3399: val_loss was not in top 3
--Epoch 7, step 3599: val_loss was not in top 3
--Epoch 8, step 3799: val_loss was not in top 3
--Epoch 9, step 3999: val_loss was not in top 3
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--Using environment variable NODE_RANK for node rank (0).
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/nemo_error_log.txt
-deleted file mode 100644
-index 955d1d3..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/nemo_error_log.txt
-+++ /dev/null
-@@ -1,16 +0,0 @@
--[NeMo W 2021-02-09 16:44:40 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 16:44:53 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 16:46:18 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fce2633fe20> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 16:46:27 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fce307368e0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 17:00:43 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-09 17:16:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fce30736670> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/nemo_log_globalrank-0_localrank-0.txt
-deleted file mode 100644
-index 006a75e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/nemo_log_globalrank-0_localrank-0.txt
-+++ /dev/null
-@@ -1,18 +0,0 @@
--[NeMo I 2021-02-09 16:44:40 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40
--[NeMo I 2021-02-09 16:44:40 exp_manager:519] TensorboardLogger has been set up
--[NeMo W 2021-02-09 16:44:40 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 16:44:53 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 16:46:18 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fce2633fe20> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 16:46:27 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fce307368e0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 17:00:43 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-09 17:16:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fce30736670> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/cmd-args.log
-deleted file mode 100644
-index 11a5d8e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/cmd-args.log
-+++ /dev/null
-@@ -1 +0,0 @@
--main.py
-\ No newline at end of file
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/events.out.tfevents.1612860903.Titan.12736.0 b/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/events.out.tfevents.1612860903.Titan.12736.0
-deleted file mode 100644
-index f809c63..0000000
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/events.out.tfevents.1612860903.Titan.12736.0 and /dev/null differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/git-info.log
-deleted file mode 100644
-index ed0e8ce..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/git-info.log
-+++ /dev/null
-@@ -1,357 +0,0 @@
--commit hash: 22df8b7032fa2ae6a488d957253de2d56042b4d6
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/events.out.tfevents.1612858879.Titan.27601.0 b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/events.out.tfevents.1612858879.Titan.27601.0
--index 4337671..16a35c9 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/events.out.tfevents.1612858879.Titan.27601.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/events.out.tfevents.1612858879.Titan.27601.0 differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/lightning_logs.txt
--index a3fc0fb..201da45 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/lightning_logs.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/lightning_logs.txt
--@@ -21,3 +21,27 @@ initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-- 13.2 M    Non-trainable params
-- 13.5 M    Total params
-- Epoch 0, global step 199: val_loss reached 54.61362 (best 54.61362), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/checkpoints/Punctuation_with_Domain_discriminator---val_loss=54.61-epoch=0.ckpt" as top 3
--+Epoch 1, global step 399: val_loss reached 42.83130 (best 42.83130), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/checkpoints/Punctuation_with_Domain_discriminator---val_loss=42.83-epoch=1.ckpt" as top 3
--+Epoch 2, global step 599: val_loss reached 32.94712 (best 32.94712), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/checkpoints/Punctuation_with_Domain_discriminator---val_loss=32.95-epoch=2.ckpt" as top 3
--+Saving latest checkpoint...
--+Epoch 3, global step 732: val_loss reached 32.94712 (best 32.94712), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/checkpoints/Punctuation_with_Domain_discriminator---val_loss=32.95-epoch=3.ckpt" as top 3
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | ElectraModel         | 13.5 M
--+1 | punct_classifier    | TokenClassifier      | 2.6 K 
--+2 | domain_classifier   | SequenceClassifier   | 513   
--+3 | punctuation_loss    | LinearChainCRF       | 120   
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+825 K     Trainable params
--+12.7 M    Non-trainable params
--+13.5 M    Total params
--+GPU available: True, used: True
--+TPU available: None, using: 0 TPU cores
--+Using environment variable NODE_RANK for node rank (0).
--+LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_error_log.txt
--index f051996..43ff45a 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_error_log.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_error_log.txt
--@@ -11,3 +11,9 @@
-- [NeMo W 2021-02-09 16:25:45 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fe13a7e4820> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 16:37:26 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
--+      warnings.warn(*args, **kwargs)
--+    
--+[NeMo W 2021-02-09 16:37:50 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fe13a7e45b0> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_log_globalrank-0_localrank-0.txt
--index a183522..da0fc8b 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_log_globalrank-0_localrank-0.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_log_globalrank-0_localrank-0.txt
--@@ -13,3 +13,9 @@
-- [NeMo W 2021-02-09 16:25:45 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fe13a7e4820> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 16:37:26 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
--+      warnings.warn(*args, **kwargs)
--+    
--+[NeMo W 2021-02-09 16:37:50 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fe13a7e45b0> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/events.out.tfevents.1612858892.Titan.27659.0 b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/events.out.tfevents.1612858892.Titan.27659.0
--index 5b09bdd..e8dc27e 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/events.out.tfevents.1612858892.Titan.27659.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/events.out.tfevents.1612858892.Titan.27659.0 differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/lightning_logs.txt
--index 4f17a12..40964a1 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/lightning_logs.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/lightning_logs.txt
--@@ -23,3 +23,41 @@ initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-- Epoch 0, global step 199: val_loss reached 0.66148 (best 0.66148), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.66-epoch=0.ckpt" as top 3
-- Epoch 1, global step 399: val_loss reached 0.23663 (best 0.23663), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=1.ckpt" as top 3
-- Epoch 2, global step 599: val_loss reached 0.17288 (best 0.17288), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.17-epoch=2.ckpt" as top 3
--+Epoch 3, global step 799: val_loss reached 0.15670 (best 0.15670), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.16-epoch=3.ckpt" as top 3
--+Epoch 4, global step 999: val_loss reached 0.14782 (best 0.14782), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.15-epoch=4.ckpt" as top 3
--+Epoch 5, global step 1199: val_loss reached 0.14246 (best 0.14246), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=5.ckpt" as top 3
--+Epoch 6, global step 1399: val_loss reached 0.13934 (best 0.13934), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=6.ckpt" as top 3
--+Epoch 7, global step 1599: val_loss reached 0.13714 (best 0.13714), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=7.ckpt" as top 3
--+Epoch 8, global step 1799: val_loss reached 0.13633 (best 0.13633), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=8.ckpt" as top 3
--+Epoch 9, global step 1999: val_loss reached 0.13605 (best 0.13605), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=9.ckpt" as top 3
--+Saving latest checkpoint...
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | ElectraModel         | 13.5 M
--+1 | punct_classifier    | TokenClassifier      | 2.6 K 
--+2 | domain_classifier   | SequenceClassifier   | 513   
--+3 | punctuation_loss    | FocalDiceLoss        | 0     
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+825 K     Trainable params
--+12.7 M    Non-trainable params
--+13.5 M    Total params
--+Epoch 0, global step 2199: val_loss reached 0.13605 (best 0.13605), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=0.ckpt" as top 3
--+Epoch 1, global step 2399: val_loss reached 0.13605 (best 0.13605), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=1.ckpt" as top 3
--+Epoch 2, step 2599: val_loss was not in top 3
--+Epoch 3, step 2799: val_loss was not in top 3
--+Epoch 4, step 2999: val_loss was not in top 3
--+Epoch 5, step 3199: val_loss was not in top 3
--+Epoch 6, step 3399: val_loss was not in top 3
--+Epoch 7, step 3599: val_loss was not in top 3
--+Epoch 8, step 3799: val_loss was not in top 3
--+Epoch 9, step 3999: val_loss was not in top 3
--+GPU available: True, used: True
--+TPU available: None, using: 0 TPU cores
--+Using environment variable NODE_RANK for node rank (0).
--+LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_error_log.txt
--index 39ac72a..2dc7701 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_error_log.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_error_log.txt
--@@ -8,3 +8,9 @@
-- [NeMo W 2021-02-09 16:23:04 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f1a2a0bb850> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 16:36:51 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--+      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--+    
--+[NeMo W 2021-02-09 16:52:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f1a2a0bb5e0> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_log_globalrank-0_localrank-0.txt
--index baa087a..282de52 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_log_globalrank-0_localrank-0.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_log_globalrank-0_localrank-0.txt
--@@ -10,3 +10,9 @@
-- [NeMo W 2021-02-09 16:23:04 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f1a2a0bb850> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 16:36:51 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--+      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--+    
--+[NeMo W 2021-02-09 16:52:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f1a2a0bb5e0> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--diff --git a/experiment/config.yaml b/experiment/config.yaml
--index 7a28782..1911de8 100644
----- a/experiment/config.yaml
--+++ b/experiment/config.yaml
--@@ -2,7 +2,7 @@ seed: 42
-- trainer:
--     gpus: 1 # the number of gpus, 0 for CPU
--     num_nodes: 1
---    max_epochs: 10
--+    max_epochs: 15
--     max_steps: null # precedence over max_epochs
--     accumulate_grad_batches: 4 # accumulates grads every k batches
--     gradient_clip_val: 0
--@@ -63,10 +63,10 @@ model:
--     dataset:
--         data_dir: /home/nxingyu/data # /root/data # 
--         labelled:
---            - ${base_path}/ted_talks_processed #
---            # - ${base_path}/open_subtitles_processed #  
---        unlabelled:
--             # - ${base_path}/ted_talks_processed #
--+            - ${base_path}/open_subtitles_processed #  
--+        unlabelled:
--+            - ${base_path}/ted_talks_processed #
--             # - ${base_path}/open_subtitles_processed #  
--             # parameters for dataset preprocessing
--         max_seq_length: 128
--@@ -79,7 +79,7 @@ model:
--         pin_memory: true
--         drop_last: true
--         num_labels: 10
---        num_domains: 1
--+        num_domains: 2
--         test_unlabelled: true
-- 
--         train_ds:
--@@ -123,7 +123,7 @@ model:
--         log_softmax: false
--         use_transformer_init: true
--         loss: 'cel'
---        gamma: 0.1 #0.1 # coefficient of gradient reversal
--+        gamma: 0.2 #0.1 # coefficient of gradient reversal
--         pooling: 'mean_max' # 'mean' mean_max
--         idx_conditioned_on: 0
--     
--@@ -137,13 +137,20 @@ model:
-- 
--     optim:
--         name: adamw
---        lr: 0.009261935523740748 #1e-3
--+        lr: 1e-2 #1e-3
--         weight_decay: 0.00
--         sched:
---            name: WarmupAnnealing #CyclicLR
--+            # name: CyclicLR
--+            # base_lr: 1e-5
--+            # max_lr: 1e-1
--+            # mode: 'triangular2'
--+            # last_epoch: -1
--+
--+            name: CosineAnnealing #WarmupAnnealing #CyclicLR
--             # Scheduler params
--             warmup_steps: null
--             warmup_ratio: 0.1
--+            min_lr: 1e-10
--             # hold_steps: 6
--             last_epoch: -1
-- 
--diff --git a/experiment/info.log b/experiment/info.log
--index 38d3b4b..e69de29 100644
----- a/experiment/info.log
--+++ b/experiment/info.log
--@@ -1,117 +0,0 @@
---[INFO] - shuffling train set
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.0001
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f1a21b62cd0>" 
---will be used during training (effective maximum steps = 2000) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---last_epoch: -1
---max_steps: 2000
---)
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          86.16      25.06      38.83       1540
---! (label_id: 1)                                          0.00       0.00       0.00          2
---, (label_id: 2)                                          8.60      38.03      14.03        142
---- (label_id: 3)                                          4.62      30.00       8.00         20
---. (label_id: 4)                                          0.00       0.00       0.00         96
---: (label_id: 5)                                          0.00       0.00       0.00          0
---; (label_id: 6)                                          0.00       0.00       0.00          0
---? (label_id: 7)                                          0.00       0.00       0.00          4
---— (label_id: 8)                                          0.00       0.00       0.00          0
---… (label_id: 9)                                          0.00       0.00       0.00          0
----------------------
---micro avg                                               24.72      24.72      24.72       1804
---macro avg                                               16.56      15.52      10.14       1804
---weighted avg                                            74.28      24.72      34.34       1804
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00         20
----------------------
---micro avg                                              100.00     100.00     100.00         20
---macro avg                                              100.00     100.00     100.00         20
---weighted avg                                           100.00     100.00     100.00         20
---
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          86.99     100.00      93.04     179900
---! (label_id: 1)                                          0.00       0.00       0.00         52
---, (label_id: 2)                                          0.00       0.00       0.00      13074
---- (label_id: 3)                                          0.00       0.00       0.00       1062
---. (label_id: 4)                                          0.00       0.00       0.00      11077
---: (label_id: 5)                                          0.00       0.00       0.00        330
---; (label_id: 6)                                          0.00       0.00       0.00        108
---? (label_id: 7)                                          0.00       0.00       0.00        899
---— (label_id: 8)                                          0.00       0.00       0.00        276
---… (label_id: 9)                                          0.00       0.00       0.00         28
----------------------
---micro avg                                               86.99      86.99      86.99     206806
---macro avg                                                8.70      10.00       9.30     206806
---weighted avg                                            75.67      86.99      80.94     206806
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1959
----------------------
---micro avg                                              100.00     100.00     100.00       1959
---macro avg                                              100.00     100.00     100.00       1959
---weighted avg                                           100.00     100.00     100.00       1959
---
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          94.70      97.81      96.23     179900
---! (label_id: 1)                                          0.00       0.00       0.00         52
---, (label_id: 2)                                         48.60      28.63      36.03      13074
---- (label_id: 3)                                         63.84      53.86      58.43       1062
---. (label_id: 4)                                         53.31      59.38      56.18      11077
---: (label_id: 5)                                          0.00       0.00       0.00        330
---; (label_id: 6)                                          0.00       0.00       0.00        108
---? (label_id: 7)                                         20.00       1.22       2.31        899
---— (label_id: 8)                                          0.00       0.00       0.00        276
---… (label_id: 9)                                          0.00       0.00       0.00         28
----------------------
---micro avg                                               90.36      90.36      90.36     206806
---macro avg                                               28.04      24.09      24.92     206806
---weighted avg                                            88.72      90.36      89.31     206806
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1959
----------------------
---micro avg                                              100.00     100.00     100.00       1959
---macro avg                                              100.00     100.00     100.00       1959
---weighted avg                                           100.00     100.00     100.00       1959
---
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          96.35      96.40      96.38     179900
---! (label_id: 1)                                          0.00       0.00       0.00         52
---, (label_id: 2)                                         43.32      40.33      41.77      13074
---- (label_id: 3)                                         64.46      56.87      60.43       1062
---. (label_id: 4)                                         53.28      62.91      57.69      11077
---: (label_id: 5)                                          0.00       0.00       0.00        330
---; (label_id: 6)                                          0.00       0.00       0.00        108
---? (label_id: 7)                                         19.81       9.23      12.59        899
---— (label_id: 8)                                          5.63       4.35       4.91        276
---… (label_id: 9)                                          0.00       0.00       0.00         28
----------------------
---micro avg                                               90.11      90.11      90.11     206806
---macro avg                                               28.29      27.01      27.38     206806
---weighted avg                                            89.83      90.11      89.94     206806
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1959
----------------------
---micro avg                                              100.00     100.00     100.00       1959
---macro avg                                              100.00     100.00     100.00       1959
---weighted avg                                           100.00     100.00     100.00       1959
---
--diff --git a/experiment/main.py b/experiment/main.py
--index 1f44193..4ae03e6 100644
----- a/experiment/main.py
--+++ b/experiment/main.py
--@@ -54,7 +54,7 @@ def main(cfg: DictConfig)->None:
--     #         tmp_path=cfg.tmp_path,
--     #         test_unlabelled=False,
--     #     )
---    lrs=[1e-4,1e-6]
--+    lrs=[1e-2,1e-5]
--     while(model.hparams.model.unfrozen<=cfg.model.maximum_unfrozen and model.hparams.model.unfrozen>=0):
--         # trainer.current_epoch=0
--         # lr_finder = trainer.tuner.lr_find(model,datamodule=lr_finder_dm,min_lr=1e-8, max_lr=0.5, num_training=80) #, early_stop_threshold=None
--diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
--index 999ae0d..5b5d668 100644
----- a/experiment/models/punctuation_domain_model.py
--+++ b/experiment/models/punctuation_domain_model.py
--@@ -163,7 +163,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
--         p=(self.current_epoch*self.train_size+batch_idx)/(self.train_size*self.hparams.trainer.max_epochs)
--         if (batch_idx%1000==0):
--             print('gamma:',p)
---        self.grad_reverse.scale=2/(1+math.exp(-10*p))-1
--+        self.grad_reverse.scale=(2/(1+math.exp(-10*p))-1)*self.hparams.model.domain_head.gamma
--         loss, _, _ = self._make_step(batch)
--         lr = self._optimizer.param_groups[0]['lr']
-- 
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/hparams.yaml
-deleted file mode 100644
-index ccb8491..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/hparams.yaml
-+++ /dev/null
-@@ -1,111 +0,0 @@
--seed: 42
--trainer:
--  gpus: 1
--  num_nodes: 1
--  max_epochs: 15
--  max_steps: null
--  accumulate_grad_batches: 4
--  gradient_clip_val: 0
--  amp_level: O1
--  precision: 16
--  accelerator: ddp
--  checkpoint_callback: false
--  logger: false
--  log_every_n_steps: 1
--  val_check_interval: 1.0
--  resume_from_checkpoint: null
--exp_manager:
--  exp_dir: /home/nxingyu/project/
--  name: Punctuation_with_Domain_discriminator
--  create_tensorboard_logger: true
--  create_checkpoint_callback: true
--base_path: /home/nxingyu/data
--tmp_path: /home/nxingyu/data/tmp
--model:
--  nemo_path: null
--  transformer_path: google/electra-small-discriminator
--  unfrozen: 0
--  maximum_unfrozen: 1
--  unfreeze_step: 1
--  punct_label_ids:
--  - ''
--  - '!'
--  - ','
--  - '-'
--  - .
--  - ':'
--  - ;
--  - '?'
--  - —
--  - …
--  punct_class_weights: false
--  dataset:
--    data_dir: /home/nxingyu/data
--    labelled:
--    - /home/nxingyu/data/open_subtitles_processed
--    unlabelled:
--    - /home/nxingyu/data/ted_talks_processed
--    max_seq_length: 128
--    pad_label: ''
--    ignore_extra_tokens: false
--    ignore_start_end: false
--    use_cache: false
--    num_workers: 4
--    pin_memory: true
--    drop_last: true
--    num_labels: 10
--    num_domains: 2
--    test_unlabelled: true
--    train_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--    validation_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--  tokenizer:
--    tokenizer_name: google/electra-small-discriminator
--    vocab_file: null
--    tokenizer_model: null
--    special_tokens: null
--  language_model:
--    pretrained_model_name: google/electra-small-discriminator
--    lm_checkpoint: null
--    config_file: null
--    config: null
--  punct_head:
--    punct_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: dice
--  domain_head:
--    domain_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: cel
--    gamma: 0.2
--    pooling: mean_max
--    idx_conditioned_on: 0
--  dice_loss:
--    epsilon: 0.01
--    alpha: 3
--    macro_average: true
--  focal_loss:
--    gamma: 1
--  optim:
--    name: adamw
--    lr: 0.01
--    weight_decay: 0.0
--    sched:
--      name: CosineAnnealing
--      warmup_steps: null
--      warmup_ratio: 0.1
--      min_lr: 1.0e-10
--      last_epoch: -1
--      monitor: val_loss
--      reduce_on_plateau: false
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/lightning_logs.txt
-deleted file mode 100644
-index 073ea87..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/lightning_logs.txt
-+++ /dev/null
-@@ -1,23 +0,0 @@
--Global seed set to 42
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
--Using native 16bit precision.
--Global seed set to 42
--initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 1.0 K 
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--299 K     Trainable params
--13.2 M    Non-trainable params
--13.5 M    Total params
--Saving latest checkpoint...
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/nemo_error_log.txt
-deleted file mode 100644
-index 6cc1f9b..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/nemo_error_log.txt
-+++ /dev/null
-@@ -1,7 +0,0 @@
--[NeMo W 2021-02-09 16:54:29 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 16:55:03 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 17:23:50 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
--      warnings.warn(*args, **kwargs)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/nemo_log_globalrank-0_localrank-0.txt
-deleted file mode 100644
-index b5e9717..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/nemo_log_globalrank-0_localrank-0.txt
-+++ /dev/null
-@@ -1,9 +0,0 @@
--[NeMo I 2021-02-09 16:54:29 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29
--[NeMo I 2021-02-09 16:54:29 exp_manager:519] TensorboardLogger has been set up
--[NeMo W 2021-02-09 16:54:29 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 16:55:03 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 17:23:50 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
--      warnings.warn(*args, **kwargs)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/cmd-args.log
-deleted file mode 100644
-index 11a5d8e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/cmd-args.log
-+++ /dev/null
-@@ -1 +0,0 @@
--main.py
-\ No newline at end of file
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/events.out.tfevents.1612862415.Titan.18732.0 b/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/events.out.tfevents.1612862415.Titan.18732.0
-deleted file mode 100644
-index 95812bc..0000000
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/events.out.tfevents.1612862415.Titan.18732.0 and /dev/null differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/git-info.log
-deleted file mode 100644
-index b7eda36..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/git-info.log
-+++ /dev/null
-@@ -1,163 +0,0 @@
--commit hash: 953fa623bede4ee117149aa46c2acf589001ede6
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/events.out.tfevents.1612860293.Titan.7050.0 b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/events.out.tfevents.1612860293.Titan.7050.0
--index c1dbdac..600f922 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/events.out.tfevents.1612860293.Titan.7050.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/events.out.tfevents.1612860293.Titan.7050.0 differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/lightning_logs.txt
--index 085adc8..2fe9063 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/lightning_logs.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/lightning_logs.txt
--@@ -47,3 +47,17 @@ Global seed set to 42
-- 825 K     Trainable params
-- 12.7 M    Non-trainable params
-- 13.5 M    Total params
--+Epoch 0, global step 2199: val_loss reached 0.13598 (best 0.13598), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=0.ckpt" as top 3
--+Epoch 1, global step 2399: val_loss reached 0.13598 (best 0.13598), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=1.ckpt" as top 3
--+Epoch 2, global step 2599: val_loss reached 0.13598 (best 0.13598), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=2.ckpt" as top 3
--+Epoch 3, global step 2799: val_loss reached 0.13598 (best 0.13598), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=3.ckpt" as top 3
--+Epoch 4, global step 2999: val_loss reached 0.13598 (best 0.13598), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=4.ckpt" as top 3
--+Epoch 5, step 3199: val_loss was not in top 3
--+Epoch 6, step 3399: val_loss was not in top 3
--+Epoch 7, step 3599: val_loss was not in top 3
--+Epoch 8, step 3799: val_loss was not in top 3
--+Epoch 9, step 3999: val_loss was not in top 3
--+GPU available: True, used: True
--+TPU available: None, using: 0 TPU cores
--+Using environment variable NODE_RANK for node rank (0).
--+LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/nemo_error_log.txt
--index 21bc8ca..955d1d3 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/nemo_error_log.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/nemo_error_log.txt
--@@ -11,3 +11,6 @@
-- [NeMo W 2021-02-09 17:00:43 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--     
--+[NeMo W 2021-02-09 17:16:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fce30736670> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/nemo_log_globalrank-0_localrank-0.txt
--index 8f639a0..006a75e 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/nemo_log_globalrank-0_localrank-0.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/nemo_log_globalrank-0_localrank-0.txt
--@@ -13,3 +13,6 @@
-- [NeMo W 2021-02-09 17:00:43 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--     
--+[NeMo W 2021-02-09 17:16:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fce30736670> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/events.out.tfevents.1612860903.Titan.12736.0 b/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/events.out.tfevents.1612860903.Titan.12736.0
--index 71d7b9a..b40aed4 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/events.out.tfevents.1612860903.Titan.12736.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/events.out.tfevents.1612860903.Titan.12736.0 differ
--diff --git a/README.md b/README.md
--index ab4fdd1..e08912e 100644
----- a/README.md
--+++ b/README.md
--@@ -459,5 +459,73 @@ testing gamma 0.1 vs just open subtitles:
-- 
-- https://www.aclweb.org/anthology/2020.acl-main.370.pdf uses the formula of 2/(1+e^(-10p))-1 where p varies from 0 to 1. To repeat cycle every unfrozen layer.
-- 
--+### 2021-02-09_16-21-19 warmup ted 
-- 
--+: (label_id: 5)                                         20.69      19.57      20.11        368
--+; (label_id: 6)                                          0.00       0.00       0.00        200
--+? (label_id: 7)                                         22.42      29.15      25.35       1372
--+ (label_id: 8)                                          6.83       9.44       7.93        932
--+… (label_id: 9)                                          0.00       0.00       0.00        124
--+-------------------
--+micro avg                                               89.82      89.82      89.82     300124
--+macro avg                                               31.58      32.56      31.95     300124
--+weighted avg                                            90.46      89.82      90.11     300124
--+
--+[INFO] - Domain report:
--+label                                                precision    recall       f1           support
--+0 (label_id: 0)                                        100.00     100.00     100.00       2744
--+-------------------
--+micro avg                                              100.00     100.00     100.00       2744
--+macro avg                                              100.00     100.00     100.00       2744
--+weighted avg                                           100.00     100.00     100.00       2744
--+
--+Testing: 100%|| 100/100 [00:10<00:00,  9.74it/s]
--+--------------------------------------------------------------------------------
--+DATALOADER:0 TEST RESULTS
--+{'domain_f1': 100.0,
--+ 'domain_precision': 100.0,
--+ 'domain_recall': 100.0,
--+ 'punct_f1': 31.946725845336914,
--+ 'punct_precision': 31.575754165649414,
--+ 'punct_recall': 32.5594596862793,
--+ 'test_loss': 0.23392203450202942}
--+
--+### 2021-02-09_16-44-40 cosine ted around the same:
--+
--+ (label_id: 0)                                          97.17      95.67      96.41     259964
--+! (label_id: 1)                                          0.00       0.00       0.00        152
--+, (label_id: 2)                                         43.51      47.93      45.61      19336
--+- (label_id: 3)                                         69.47      61.49      65.23       1776
--+. (label_id: 4)                                         55.49      62.29      58.69      15900
--+: (label_id: 5)                                         20.45      19.57      20.00        368
--+; (label_id: 6)                                          0.00       0.00       0.00        200
--+? (label_id: 7)                                         22.67      29.74      25.73       1372
--+ (label_id: 8)                                          6.85       9.44       7.94        932
--+… (label_id: 9)                                          0.00       0.00       0.00        124
--+-------------------
--+micro avg                                               89.81      89.81      89.81     300124
--+macro avg                                               31.56      32.61      31.96     300124
--+weighted avg                                            90.47      89.81      90.11     300124
--+
--+[INFO] - Domain report:
--+label                                                precision    recall       f1           support
--+0 (label_id: 0)                                        100.00     100.00     100.00       2744
--+-------------------
--+micro avg                                              100.00     100.00     100.00       2744
--+macro avg                                              100.00     100.00     100.00       2744
--+weighted avg                                           100.00     100.00     100.00       2744
--+
--+Testing: 100%|| 100/100 [00:10<00:00,  9.29it/s]
--+--------------------------------------------------------------------------------
--+DATALOADER:0 TEST RESULTS
--+{'domain_f1': 100.0,
--+ 'domain_precision': 100.0,
--+ 'domain_recall': 100.0,
--+ 'punct_f1': 31.962158203125,
--+ 'punct_precision': 31.560827255249023,
--+ 'punct_recall': 32.61237335205078,
--+ 'test_loss': 0.23370929062366486}
--+
--+ #####################################################################
--+### 2021-02-09_16-54-29 domain adversarial
-- 
--diff --git a/experiment/config.yaml b/experiment/config.yaml
--index 1911de8..8226922 100644
----- a/experiment/config.yaml
--+++ b/experiment/config.yaml
--@@ -66,7 +66,7 @@ model:
--             # - ${base_path}/ted_talks_processed #
--             - ${base_path}/open_subtitles_processed #  
--         unlabelled:
---            - ${base_path}/ted_talks_processed #
--+            # - ${base_path}/ted_talks_processed #
--             # - ${base_path}/open_subtitles_processed #  
--             # parameters for dataset preprocessing
--         max_seq_length: 128
--@@ -79,7 +79,7 @@ model:
--         pin_memory: true
--         drop_last: true
--         num_labels: 10
---        num_domains: 2
--+        num_domains: 1
--         test_unlabelled: true
-- 
--         train_ds:
--@@ -123,7 +123,7 @@ model:
--         log_softmax: false
--         use_transformer_init: true
--         loss: 'cel'
---        gamma: 0.2 #0.1 # coefficient of gradient reversal
--+        gamma: 0 #0.1 # coefficient of gradient reversal
--         pooling: 'mean_max' # 'mean' mean_max
--         idx_conditioned_on: 0
--     
--diff --git a/experiment/info.log b/experiment/info.log
--index bc52a1b..e69de29 100644
--Binary files a/experiment/info.log and b/experiment/info.log differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/hparams.yaml
-deleted file mode 100644
-index 679f833..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/hparams.yaml
-+++ /dev/null
-@@ -1,110 +0,0 @@
--seed: 42
--trainer:
--  gpus: 1
--  num_nodes: 1
--  max_epochs: 15
--  max_steps: null
--  accumulate_grad_batches: 4
--  gradient_clip_val: 0
--  amp_level: O1
--  precision: 16
--  accelerator: ddp
--  checkpoint_callback: false
--  logger: false
--  log_every_n_steps: 1
--  val_check_interval: 1.0
--  resume_from_checkpoint: null
--exp_manager:
--  exp_dir: /home/nxingyu/project/
--  name: Punctuation_with_Domain_discriminator
--  create_tensorboard_logger: true
--  create_checkpoint_callback: true
--base_path: /home/nxingyu/data
--tmp_path: /home/nxingyu/data/tmp
--model:
--  nemo_path: null
--  transformer_path: google/electra-small-discriminator
--  unfrozen: 0
--  maximum_unfrozen: 1
--  unfreeze_step: 1
--  punct_label_ids:
--  - ''
--  - '!'
--  - ','
--  - '-'
--  - .
--  - ':'
--  - ;
--  - '?'
--  - —
--  - …
--  punct_class_weights: false
--  dataset:
--    data_dir: /home/nxingyu/data
--    labelled:
--    - /home/nxingyu/data/open_subtitles_processed
--    unlabelled: null
--    max_seq_length: 128
--    pad_label: ''
--    ignore_extra_tokens: false
--    ignore_start_end: false
--    use_cache: false
--    num_workers: 4
--    pin_memory: true
--    drop_last: true
--    num_labels: 10
--    num_domains: 1
--    test_unlabelled: true
--    train_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--    validation_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--  tokenizer:
--    tokenizer_name: google/electra-small-discriminator
--    vocab_file: null
--    tokenizer_model: null
--    special_tokens: null
--  language_model:
--    pretrained_model_name: google/electra-small-discriminator
--    lm_checkpoint: null
--    config_file: null
--    config: null
--  punct_head:
--    punct_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: dice
--  domain_head:
--    domain_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: cel
--    gamma: 0
--    pooling: mean_max
--    idx_conditioned_on: 0
--  dice_loss:
--    epsilon: 0.01
--    alpha: 3
--    macro_average: true
--  focal_loss:
--    gamma: 1
--  optim:
--    name: adamw
--    lr: 0.01
--    weight_decay: 0.0
--    sched:
--      name: CosineAnnealing
--      warmup_steps: null
--      warmup_ratio: 0.1
--      min_lr: 1.0e-10
--      last_epoch: -1
--      monitor: val_loss
--      reduce_on_plateau: false
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/lightning_logs.txt
-deleted file mode 100644
-index e496097..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/lightning_logs.txt
-+++ /dev/null
-@@ -1,26 +0,0 @@
--Global seed set to 42
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
--Using native 16bit precision.
--Global seed set to 42
--initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 513   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--299 K     Trainable params
--13.2 M    Non-trainable params
--13.5 M    Total params
--Epoch 0, global step 5304: val_loss reached 0.00440 (best 0.00440), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.00-epoch=0.ckpt" as top 3
--Epoch 1, global step 10609: val_loss reached -0.00234 (best -0.00234), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.00-epoch=1.ckpt" as top 3
--Saving latest checkpoint...
--Epoch 2, global step 12262: val_loss reached -0.00234 (best -0.00234), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.00-epoch=2.ckpt" as top 3
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/nemo_error_log.txt
-deleted file mode 100644
-index 1193d63..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/nemo_error_log.txt
-+++ /dev/null
-@@ -1,10 +0,0 @@
--[NeMo W 2021-02-09 17:19:42 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 17:20:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 18:07:34 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f68046f2df0> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 18:14:10 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f680eaeb8b0> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/nemo_log_globalrank-0_localrank-0.txt
-deleted file mode 100644
-index bd490f0..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/nemo_log_globalrank-0_localrank-0.txt
-+++ /dev/null
-@@ -1,12 +0,0 @@
--[NeMo I 2021-02-09 17:19:42 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42
--[NeMo I 2021-02-09 17:19:42 exp_manager:519] TensorboardLogger has been set up
--[NeMo W 2021-02-09 17:19:42 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 17:20:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 18:07:34 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f68046f2df0> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 18:14:10 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f680eaeb8b0> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/events.out.tfevents.1613379232.Titan.2419.0 b/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/events.out.tfevents.1613379232.Titan.2419.0
-index 55e3ccc..dbd3b7a 100644
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/events.out.tfevents.1613379232.Titan.2419.0 and b/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/events.out.tfevents.1613379232.Titan.2419.0 differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/lightning_logs.txt
-index 46e1e6e..74b73ca 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/lightning_logs.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/lightning_logs.txt
-@@ -21,3 +21,28 @@ initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
- 7.1 M     Trainable params
- 108 M     Non-trainable params
- 115 M     Total params
-+Epoch 0, global step 656: val_loss reached 146.20628 (best 146.20628), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=146.21-epoch=0.ckpt" as top 3
-+Epoch 1, global step 1313: val_loss reached 25.70970 (best 25.70970), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=25.71-epoch=1.ckpt" as top 3
-+Epoch 2, global step 1970: val_loss reached 7.76796 (best 7.76796), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=7.77-epoch=2.ckpt" as top 3
-+Epoch 3, global step 2627: val_loss reached 1.31258 (best 1.31258), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=1.31-epoch=3.ckpt" as top 3
-+Epoch 4, global step 3284: val_loss reached 2.27040 (best 1.31258), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=2.27-epoch=4.ckpt" as top 3
-+Epoch 5, global step 3941: val_loss reached 7.67466 (best 1.31258), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=7.67-epoch=5.ckpt" as top 3
-+Saving latest checkpoint...
-+Epoch 6, step 4031: val_loss was not in top 3
-+Global seed set to 42
-+
-+  | Name                | Type                 | Params
-+-------------------------------------------------------------
-+0 | transformer         | ElectraModel         | 108 M 
-+1 | punct_classifier    | TokenClassifier      | 7.7 K 
-+2 | domain_classifier   | SequenceClassifier   | 3.1 K 
-+3 | punctuation_loss    | FocalDiceLoss        | 0     
-+4 | bilstm              | LSTM                 | 7.1 M 
-+5 | domain_loss         | CrossEntropyLoss     | 0     
-+6 | agg_loss            | AggregatorLoss       | 0     
-+7 | punct_class_report  | ClassificationReport | 0     
-+8 | domain_class_report | ClassificationReport | 0     
-+-------------------------------------------------------------
-+14.2 M    Trainable params
-+101 M     Non-trainable params
-+115 M     Total params
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/nemo_error_log.txt
-index 8512f6f..40a936f 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/nemo_error_log.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/nemo_error_log.txt
-@@ -2,3 +2,12 @@
- [NeMo W 2021-02-15 16:53:52 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-       warnings.warn(*args, **kwargs)
-     
-+[NeMo W 2021-02-15 19:19:00 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5700031460> was reported to be 2627 (when accessing len(dataloader)), but 2628 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-+[NeMo W 2021-02-15 19:29:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5700085d30> was reported to be 328 (when accessing len(dataloader)), but 329 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-+[NeMo W 2021-02-16 08:30:30 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-+      warnings.warn(*args, **kwargs)
-+    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/nemo_log_globalrank-0_localrank-0.txt
-index ffe9ee9..bf9ce42 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/nemo_log_globalrank-0_localrank-0.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/nemo_log_globalrank-0_localrank-0.txt
-@@ -4,3 +4,12 @@
- [NeMo W 2021-02-15 16:53:52 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-       warnings.warn(*args, **kwargs)
-     
-+[NeMo W 2021-02-15 19:19:00 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5700031460> was reported to be 2627 (when accessing len(dataloader)), but 2628 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-+[NeMo W 2021-02-15 19:29:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5700085d30> was reported to be 328 (when accessing len(dataloader)), but 329 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-+[NeMo W 2021-02-16 08:30:30 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-+      warnings.warn(*args, **kwargs)
-+    
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index 9762568..e26a40e 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -2,7 +2,7 @@ seed: 42
- trainer:
-     gpus: 1 # the number of gpus, 0 for CPU
-     num_nodes: 1
--    max_epochs: 8
-+    max_epochs: 15
-     max_steps: null # precedence over max_epochs
-     accumulate_grad_batches: 4 # accumulates grads every k batches
-     gradient_clip_val: 0
-@@ -24,7 +24,7 @@ trainer:
-     # amp_level: O0 # O1/O2 for mixed precision
-     # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-     # # accelerator: ddp
--    # checkpoint_callback: false  # Provided by exp_manager
-+    # checkpoint_callback: false # Provided by exp_manager
-     # logger: false #false  # Provided by exp_manager
-     # log_every_n_steps: 1  # Interval of logging.
-     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-@@ -41,11 +41,10 @@ tmp_path: /home/nxingyu/data/tmp # /tmp #
- 
- model:
-     nemo_path: null
--    transformer_path: google/electra-base-discriminator # roberta-base #google/electra-base-discriminator # distilbert-base-uncased # filename to save the model and associated artifacts to .nemo file
-+    transformer_path: google/electra-base-discriminator #google/electra-base-discriminator # roberta-base #google/electra-base-discriminator # distilbert-base-uncased # filename to save the model and associated artifacts to .nemo file
-     unfrozen: 0
--    maximum_unfrozen: 2
-+    maximum_unfrozen: 1
-     unfreeze_step: 1
--    # unfreeze_every: 3
-     punct_label_ids:
-         - ""
-         - "!"
-@@ -58,16 +57,16 @@ model:
-         - "—"
-         - "…"
- 
--    punct_class_weights: false
-+    punct_class_weights: false #false
-     
-     dataset:
-         data_dir: /home/nxingyu/data # /root/data # 
-         labelled:
--            # - ${base_path}/ted_talks_processed #
--            - ${base_path}/open_subtitles_processed #  
--        unlabelled:
-             - ${base_path}/ted_talks_processed #
-             # - ${base_path}/open_subtitles_processed #  
-+        unlabelled:
-+            # - ${base_path}/ted_talks_processed #
-+            # - ${base_path}/open_subtitles_processed #  
-             # parameters for dataset preprocessing
-         max_seq_length: 128
-         pad_label: ''
-@@ -75,18 +74,18 @@ model:
-         ignore_start_end: false
-         use_cache: false
-         # shared among dataloaders
--        num_workers:  12
--        pin_memory: true
-+        num_workers:  4
-+        pin_memory: false
-         drop_last: true
-         num_labels: 10
--        num_domains: 2
-+        num_domains: 1
-         test_unlabelled: true
-         attach_label_to_end: none # false if attach to start none if dont mask
- 
-         train_ds:
-             shuffle: true
-             num_samples: -1
--            batch_size: 32
-+            batch_size: 16
-             manual_len: 0 #default 0 84074
- 
-         validation_ds:
-@@ -111,13 +110,13 @@ model:
-         # unfrozen_layers: 1
-     
-     punct_head:
--        punct_num_fc_layers: 1
-+        punct_num_fc_layers: 2
-         fc_dropout: 0.1
--        activation: 'relu'
-+        activation: 'gelu'
-         log_softmax: false
-         use_transformer_init: true
-         loss: 'dice'
--        bilstm: true
-+        bilstm: false
- 
-     domain_head:
-         domain_num_fc_layers: 1
-@@ -126,7 +125,7 @@ model:
-         log_softmax: false
-         use_transformer_init: true
-         loss: 'cel'
--        gamma: 0.01 #0.1 # coefficient of gradient reversal
-+        gamma: 0.1 #0.1 # coefficient of gradient reversal
-         pooling: 'mean_max' # 'mean' mean_max
-         idx_conditioned_on: 0
-     
-@@ -136,11 +135,11 @@ model:
-         macro_average: true
- 
-     focal_loss: 
--        gamma: 1
-+        gamma: 2
- 
-     frozen_lr:
-         - 1e-2
--        - 5e-3
-+        - 1e-3
- 
-     optim:
-         name: adamw
-diff --git a/experiment/core/layers/multi_layer_perceptron.py b/experiment/core/layers/multi_layer_perceptron.py
-index 63318cc..ba4489f 100644
---- a/experiment/core/layers/multi_layer_perceptron.py
-+++ b/experiment/core/layers/multi_layer_perceptron.py
-@@ -14,6 +14,7 @@
- # limitations under the License.
- 
- import torch
-+from torch import nn
- 
- class MultiLayerPerceptron(torch.nn.Module):
-     """
-@@ -37,10 +38,16 @@ class MultiLayerPerceptron(torch.nn.Module):
-     ):
-         super().__init__()
-         self.layers = 0
-+        activations = {
-+            'relu': nn.ReLU(),
-+            'gelu': nn.GELU(),
-+            'sigmoid': nn.Sigmoid(),
-+            'tanh': nn.Tanh()
-+        }
-         for _ in range(num_layers - 1):
-             layer = torch.nn.Linear(hidden_size, hidden_size)
-             setattr(self, f'layer{self.layers}', layer)
--            setattr(self, f'layer{self.layers + 1}', getattr(torch, activation))
-+            setattr(self, f'layer{self.layers + 1}', activations[activation]) #getattr(torch, activation)
-             self.layers += 2
-         layer = torch.nn.Linear(hidden_size, num_classes)
-         setattr(self, f'layer{self.layers}', layer)
-diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
-index d5e42aa..82b8e1f 100644
---- a/experiment/data/punctuation_dataset_multi.py
-+++ b/experiment/data/punctuation_dataset_multi.py
-@@ -66,12 +66,12 @@ class PunctuationDomainDataset(IterableDataset):
-         self.tmp_path=tmp_path
-         self.attach_label_to_end=attach_label_to_end
-         if not (os.path.exists(self.target_file)):
--            os.system(f'cp {self.csv_file} {self.target_file}')
-+            os.system(f"sed '1d' {self.csv_file} > {self.target_file}")
- 
-     def __iter__(self):
-         self.dataset=iter(pd.read_csv(
-                 self.csv_file,
--                skiprows=(0 % self.len)*self.num_samples+1,
-+                skiprows=(0 % self.len)*self.num_samples,
-                 header=None,
-                 dtype=str,
-                 chunksize=self.num_samples,
-@@ -83,7 +83,7 @@ class PunctuationDomainDataset(IterableDataset):
-         batch = next(self.dataset)[1]
- 
-         l=batch.str.split().map(len).values
--        n=16
-+        n=8
-         a=np.maximum((l-self.max_seq_length*n).clip(min=0),(l*np.random.random(l.__len__())).astype(int))
-         b=np.minimum(l,a+self.max_seq_length*n)
-         batch=pd.DataFrame({'t':batch,'a':a,'b':b}).apply(lambda row: ' '.join(row.t.split()[row.a:row.b]),axis=1)
-diff --git a/experiment/info.log b/experiment/info.log
-index 188b5f1..e69de29 100644
-Binary files a/experiment/info.log and b/experiment/info.log differ
-diff --git a/experiment/main.py b/experiment/main.py
-index 324f489..7f97cb4 100644
---- a/experiment/main.py
-+++ b/experiment/main.py
-@@ -23,6 +23,7 @@ snoop.install()
- 
- @hydra.main(config_name="config")
- def main(cfg: DictConfig)->None:
-+    os.environ["TOKENIZERS_PARALLELISM"] = "false"
-     torch.set_printoptions(sci_mode=False)
-     data_id = str(int(time()))
-     def savecounter():
-@@ -34,7 +35,7 @@ def main(cfg: DictConfig)->None:
- 
-     pp(cfg)
-     pl.seed_everything(cfg.seed)
--    trainer = pl.Trainer(**cfg.trainer)
-+    trainer = pl.Trainer(**cfg.trainer,track_grad_norm=2)
-     exp_manager(trainer, cfg.exp_manager)
-     model = PunctuationDomainModel(cfg=cfg, trainer=trainer, data_id = data_id)
-     
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index 0e46e67..edb0d55 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -245,9 +245,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         }
- 
-     def validation_epoch_end(self, outputs):
--        # print('next epoch:',self.current_epoch+1, (self.current_epoch+1)%self.hparams.model.unfreeze_every)
--        # if ((self.current_epoch+1)%self.hparams.model.unfreeze_every==0):
--        #     self.unfreeze(self.hparams.model.unfreeze_step)
-+        
-         self.dm.train_dataset.shuffle()
-         if outputs is not None and len(outputs) == 0:
-             return {}
-@@ -690,6 +688,10 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         # for last in last_iter:
-         #     continue
-         # set_requires_grad_for_module(last, True)
-+        for name, param in self.transformer.named_parameters():                
-+            if param.requires_grad:
-+                print(name)
-+
- 
-     def freeze(self) -> None:
-         try:
-@@ -701,6 +703,9 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
- 
-         self.frozen = len(encoder.layer)-self.hparams.model.unfrozen
-         self.freeze_transformer_to(self.frozen)
-+        for name, param in encoder.named_parameters(): 
-+            if param.requires_grad: 
-+                print(name, param.data)
- 
-     def unfreeze(self, i: int = 1):
-         self.frozen -= i
-@@ -710,6 +715,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         #     self.frozen+=self.hparams.model.unfrozen-self.hparams.model.maximum_unfrozen
-         #     self.hparams.model.unfrozen=self.hparams.model.maximum_unfrozen
-         self.freeze_transformer_to(max(0, self.frozen))
-+        
- 
-     def teardown(self, stage: str):
-         """
diff --git a/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/hparams.yaml
deleted file mode 100644
index 4521d80..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/hparams.yaml
+++ /dev/null
@@ -1,116 +0,0 @@
-seed: 42
-trainer:
-  gpus: 1
-  num_nodes: 1
-  max_epochs: 15
-  max_steps: null
-  accumulate_grad_batches: 4
-  gradient_clip_val: 0
-  amp_level: O1
-  precision: 16
-  accelerator: ddp
-  checkpoint_callback: false
-  logger: false
-  log_every_n_steps: 1
-  val_check_interval: 1.0
-  resume_from_checkpoint: null
-exp_manager:
-  exp_dir: /home/nxingyu/project/
-  name: Punctuation_with_Domain_discriminator
-  create_tensorboard_logger: true
-  create_checkpoint_callback: true
-base_path: /home/nxingyu/data
-tmp_path: /home/nxingyu/data/tmp
-model:
-  nemo_path: null
-  transformer_path: google/electra-base-discriminator
-  unfrozen: 0
-  maximum_unfrozen: 1
-  unfreeze_step: 1
-  punct_label_ids:
-  - ''
-  - '!'
-  - ','
-  - '-'
-  - .
-  - ':'
-  - ;
-  - '?'
-  - —
-  - …
-  punct_class_weights: false
-  dataset:
-    data_dir: /home/nxingyu/data
-    labelled:
-    - /home/nxingyu/data/ted_talks_processed
-    unlabelled: null
-    max_seq_length: 128
-    pad_label: ''
-    ignore_extra_tokens: false
-    ignore_start_end: false
-    use_cache: false
-    num_workers: 4
-    pin_memory: false
-    drop_last: true
-    num_labels: 10
-    num_domains: 1
-    test_unlabelled: true
-    attach_label_to_end: none
-    train_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 16
-      manual_len: 0
-    validation_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 32
-  tokenizer:
-    tokenizer_name: google/electra-base-discriminator
-    vocab_file: null
-    tokenizer_model: null
-    special_tokens: null
-  language_model:
-    pretrained_model_name: google/electra-base-discriminator
-    lm_checkpoint: null
-    config_file: null
-    config: null
-  punct_head:
-    punct_num_fc_layers: 2
-    fc_dropout: 0.1
-    activation: gelu
-    log_softmax: false
-    use_transformer_init: true
-    loss: dice
-    bilstm: false
-  domain_head:
-    domain_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: cel
-    gamma: 0.1
-    pooling: mean_max
-    idx_conditioned_on: 0
-  dice_loss:
-    epsilon: 0.01
-    alpha: 3
-    macro_average: true
-  focal_loss:
-    gamma: 2
-  frozen_lr:
-  - 0.01
-  - 0.001
-  optim:
-    name: adamw
-    lr: 0.01
-    weight_decay: 0.0
-    sched:
-      name: WarmupAnnealing
-      warmup_steps: null
-      warmup_ratio: 0.1
-      min_lr: 1.0e-08
-      last_epoch: -1
-      monitor: val_loss
-      reduce_on_plateau: false
diff --git a/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/lightning_logs.txt
deleted file mode 100644
index 9b9d718..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/lightning_logs.txt
+++ /dev/null
@@ -1,59 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-Using native 16bit precision.
-Global seed set to 42
-initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 108 M 
-1 | punct_classifier    | TokenClassifier      | 598 K 
-2 | domain_classifier   | SequenceClassifier   | 1.5 K 
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-599 K     Trainable params
-108 M     Non-trainable params
-109 M     Total params
-Epoch 0, global step 49: val_loss reached 0.42020 (best 0.42020), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.42-epoch=0.ckpt" as top 3
-Epoch 1, global step 99: val_loss reached 0.46060 (best 0.42020), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.46-epoch=1.ckpt" as top 3
-Epoch 2, global step 149: val_loss reached 0.43533 (best 0.42020), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.44-epoch=2.ckpt" as top 3
-Epoch 3, global step 199: val_loss reached 0.42568 (best 0.42020), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.43-epoch=3.ckpt" as top 3
-Epoch 4, global step 249: val_loss reached 0.42917 (best 0.42020), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.43-epoch=4.ckpt" as top 3
-Epoch 5, global step 299: val_loss reached 0.40775 (best 0.40775), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.41-epoch=5.ckpt" as top 3
-Epoch 6, global step 349: val_loss reached 0.40911 (best 0.40775), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.41-epoch=6.ckpt" as top 3
-Epoch 7, global step 399: val_loss reached 0.41860 (best 0.40775), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.42-epoch=7.ckpt" as top 3
-Epoch 8, global step 449: val_loss reached 0.41203 (best 0.40775), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.41-epoch=8.ckpt" as top 3
-Epoch 9, step 499: val_loss was not in top 3
-Epoch 10, step 549: val_loss was not in top 3
-Epoch 11, step 599: val_loss was not in top 3
-Epoch 12, step 649: val_loss was not in top 3
-Epoch 13, step 699: val_loss was not in top 3
-Epoch 14, global step 749: val_loss reached 0.40851 (best 0.40775), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.41-epoch=14.ckpt" as top 3
-Saving latest checkpoint...
-Global seed set to 42
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 108 M 
-1 | punct_classifier    | TokenClassifier      | 598 K 
-2 | domain_classifier   | SequenceClassifier   | 1.5 K 
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-7.7 M     Trainable params
-101 M     Non-trainable params
-109 M     Total params
-Epoch 0, global step 799: val_loss reached 0.40851 (best 0.40775), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.41-epoch=0.ckpt" as top 3
-Epoch 1, step 849: val_loss was not in top 3
-Epoch 2, step 899: val_loss was not in top 3
-Epoch 3, step 949: val_loss was not in top 3
-Epoch 4, step 999: val_loss was not in top 3
diff --git a/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/nemo_error_log.txt
deleted file mode 100644
index 2d7cddb..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/nemo_error_log.txt
+++ /dev/null
@@ -1,13 +0,0 @@
-[NeMo W 2021-02-16 13:10:15 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-16 13:10:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-16 13:11:57 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f647c08ff70> was reported to be 199 (when accessing len(dataloader)), but 200 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-16 13:12:08 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f64c3f53f40> was reported to be 12 (when accessing len(dataloader)), but 13 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-16 13:35:19 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 9871091..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,15 +0,0 @@
-[NeMo I 2021-02-16 13:10:15 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-16_13-10-15
-[NeMo I 2021-02-16 13:10:15 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-16 13:10:15 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-16 13:10:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-16 13:11:57 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f647c08ff70> was reported to be 199 (when accessing len(dataloader)), but 200 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-16 13:12:08 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f64c3f53f40> was reported to be 12 (when accessing len(dataloader)), but 13 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-16 13:35:19 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/events.out.tfevents.1613452288.Titan.28992.0 b/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/events.out.tfevents.1613452288.Titan.28992.0
deleted file mode 100644
index f90c29e..0000000
Binary files a/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/events.out.tfevents.1613452288.Titan.28992.0 and /dev/null differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/git-info.log
deleted file mode 100644
index 70d1192..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/git-info.log
+++ /dev/null
@@ -1,7557 +0,0 @@
-commit hash: c523cbaa0b6d3efca5ac9fbc9e89b8678b7e7788
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/cmd-args.log
-deleted file mode 100644
-index 11a5d8e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/cmd-args.log
-+++ /dev/null
-@@ -1 +0,0 @@
--main.py
-\ No newline at end of file
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0
-deleted file mode 100644
-index 53377de..0000000
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 and /dev/null differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/git-info.log
-deleted file mode 100644
-index 3a42620..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/git-info.log
-+++ /dev/null
-@@ -1,3780 +0,0 @@
--commit hash: 4cccf1ef224a31f16ab4158a6dbd1472d2705371
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/cmd-args.log
--deleted file mode 100644
--index 11a5d8e..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/cmd-args.log
--+++ /dev/null
--@@ -1 +0,0 @@
---main.py
--\ No newline at end of file
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/events.out.tfevents.1612741439.Titan.4686.0 b/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/events.out.tfevents.1612741439.Titan.4686.0
--deleted file mode 100644
--index dc6761d..0000000
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/events.out.tfevents.1612741439.Titan.4686.0 and /dev/null differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/git-info.log
--deleted file mode 100644
--index 83e328a..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/git-info.log
--+++ /dev/null
--@@ -1,406 +0,0 @@
---commit hash: 939a671c8c117db6975316767ced5d95449e2b27
---diff --git a/experiment/config.yaml b/experiment/config.yaml
---index fe58670..1442409 100644
------ a/experiment/config.yaml
---+++ b/experiment/config.yaml
---@@ -1,47 +1,47 @@
--- seed: 42
--- trainer:
----    # gpus: 1 # the number of gpus, 0 for CPU
----    # num_nodes: 1
----    # max_epochs: 2
----    # max_steps: null # precedence over max_epochs
----    # accumulate_grad_batches: 4 # accumulates grads every k batches
----    # gradient_clip_val: 0
----    # amp_level: O1 # O1/O2 for mixed precision
----    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----    # accelerator: ddp
----    # checkpoint_callback: false  # Provided by exp_manager
----    # logger: false #false  # Provided by exp_manager
----    # log_every_n_steps: 1  # Interval of logging.
----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----    # resume_from_checkpoint: null
----
----    gpus: 0 # the number of gpus, 0 for CPU
---+    gpus: 1 # the number of gpus, 0 for CPU
---     num_nodes: 1
----    max_epochs: 8
---+    max_epochs: 2
---     max_steps: null # precedence over max_epochs
----    accumulate_grad_batches: 1 # accumulates grads every k batches
---+    accumulate_grad_batches: 4 # accumulates grads every k batches
---     gradient_clip_val: 0
----    amp_level: O0 # O1/O2 for mixed precision
----    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----    # accelerator: ddp
---+    amp_level: O1 # O1/O2 for mixed precision
---+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
---+    accelerator: ddp
---     checkpoint_callback: false  # Provided by exp_manager
---     logger: false #false  # Provided by exp_manager
---     log_every_n_steps: 1  # Interval of logging.
---     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----    reload_dataloaders_every_epoch: true
---     resume_from_checkpoint: null
--- 
---+    # gpus: 0 # the number of gpus, 0 for CPU
---+    # num_nodes: 1
---+    # max_epochs: 8
---+    # max_steps: null # precedence over max_epochs
---+    # accumulate_grad_batches: 1 # accumulates grads every k batches
---+    # gradient_clip_val: 0
---+    # amp_level: O0 # O1/O2 for mixed precision
---+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
---+    # # accelerator: ddp
---+    # checkpoint_callback: false  # Provided by exp_manager
---+    # logger: false #false  # Provided by exp_manager
---+    # log_every_n_steps: 1  # Interval of logging.
---+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
---+    # reload_dataloaders_every_epoch: true
---+    # resume_from_checkpoint: null
---+
--- exp_manager:
----    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
---+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
---     name: Punctuation_with_Domain_discriminator  # The name of your model
---     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
---     create_checkpoint_callback: true 
----base_path: /home/nxingyu2/data # /root/data # 
----tmp_path: /home/nxingyu2/data/tmp # /tmp # 
---+base_path: /home/nxingyu/data # /root/data # 
---+tmp_path: /home/nxingyu/data/tmp # /tmp # 
--- 
--- model:
---     nemo_path: null
----    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
---+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
---     unfrozen: 0
---     maximum_unfrozen: 2
---     unfreeze_step: 1
---@@ -58,10 +58,10 @@ model:
---         - "—"
---         - "…"
--- 
----    punct_class_weights: true
---+    punct_class_weights: false
---     
---     dataset:
----        data_dir: /home/nxingyu2/data # /root/data # 
---+        data_dir: /home/nxingyu/data # /root/data # 
---         labelled:
---             - ${base_path}/ted_talks_processed #
---         unlabelled:
---@@ -132,7 +132,7 @@ model:
---         gamma: 5
--- 
---     optim:
----        name: novograd
---+        name: adam
---         lr: 1e-3
---         weight_decay: 0.00
---         sched:
---diff --git a/experiment/info.log b/experiment/info.log
---index 2471fe9..e69de29 100644
------ a/experiment/info.log
---+++ b/experiment/info.log
---@@ -1,300 +0,0 @@
----[INFO] - GPU available: True, used: False
----[INFO] - TPU available: None, using: 0 TPU cores
----[INFO] - shuffling train set
----[INFO] - Optimizer config = Novograd (
----Parameter Group 0
----    amsgrad: False
----    betas: (0.95, 0.98)
----    eps: 1e-08
----    grad_averaging: False
----    lr: 0.001
----    weight_decay: 0.0
----)
----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
----will be used during training (effective maximum steps = 80) - 
----Parameters : 
----(warmup_steps: null
----warmup_ratio: 0.1
----last_epoch: -1
----max_steps: 80
----)
----[INFO] - 
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 108 M 
----1 | punct_classifier    | TokenClassifier      | 7.7 K 
----2 | domain_classifier   | SequenceClassifier   | 769   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----8.5 K     Trainable params
----108 M     Non-trainable params
----108 M     Total params
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.00       0.00       0.00        184
----! (label_id: 1)                                          0.00       0.00       0.00          4
----, (label_id: 2)                                          1.23       0.34       0.53        594
----- (label_id: 3)                                          3.06      25.42       5.46         59
----. (label_id: 4)                                         47.22      12.98      20.36        524
----: (label_id: 5)                                          0.00       0.00       0.00         18
----; (label_id: 6)                                          0.00       0.00       0.00         13
----? (label_id: 7)                                          8.45       6.32       7.23         95
----— (label_id: 8)                                          0.00       0.00       0.00         12
----… (label_id: 9)                                          0.00       0.00       0.00          0
-----------------------
----micro avg                                                6.05       6.05       6.05       1503
----macro avg                                                6.66       5.01       3.73       1503
----weighted avg                                            17.61       6.05       7.98       1503
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00         92
-----------------------
----micro avg                                              100.00     100.00     100.00         92
----macro avg                                              100.00     100.00     100.00         92
----weighted avg                                           100.00     100.00     100.00         92
----
----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
----[INFO] - Optimizer config = Novograd (
----Parameter Group 0
----    amsgrad: False
----    betas: (0.95, 0.98)
----    eps: 1e-08
----    grad_averaging: False
----    lr: 1.5848931924611143e-08
----    weight_decay: 0.0
----)
----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
----will be used during training (effective maximum steps = 3192) - 
----Parameters : 
----(warmup_steps: null
----warmup_ratio: 0.1
----last_epoch: -1
----max_steps: 3192
----)
----[INFO] - 
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 108 M 
----1 | punct_classifier    | TokenClassifier      | 7.7 K 
----2 | domain_classifier   | SequenceClassifier   | 769   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----8.5 K     Trainable params
----108 M     Non-trainable params
----108 M     Total params
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.00       0.00       0.00        202
----! (label_id: 1)                                          0.00       0.00       0.00          4
----, (label_id: 2)                                          1.62       0.45       0.70        669
----- (label_id: 3)                                          3.48      27.27       6.17         66
----. (label_id: 4)                                         45.06      13.01      20.19        561
----: (label_id: 5)                                          1.52       6.67       2.47         15
----; (label_id: 6)                                          0.00       0.00       0.00         15
----? (label_id: 7)                                          8.70       7.32       7.95         82
----— (label_id: 8)                                          0.00       0.00       0.00         13
----… (label_id: 9)                                          0.00       0.00       0.00          1
-----------------------
----micro avg                                                6.20       6.20       6.20       1628
----macro avg                                                6.04       5.47       3.75       1628
----weighted avg                                            16.79       6.20       7.92       1628
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00        101
-----------------------
----micro avg                                              100.00     100.00     100.00        101
----macro avg                                              100.00     100.00     100.00        101
----weighted avg                                           100.00     100.00     100.00        101
----
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.34       0.73       0.46       4402
----! (label_id: 1)                                          0.42      13.95       0.82        129
----, (label_id: 2)                                          2.53       0.64       1.03      15243
----- (label_id: 3)                                          2.45      21.03       4.38       1322
----. (label_id: 4)                                         44.00      11.40      18.11      12542
----: (label_id: 5)                                          0.43       1.41       0.65        354
----; (label_id: 6)                                          0.00       0.00       0.00        163
----? (label_id: 7)                                          4.16       6.27       5.00       1117
----— (label_id: 8)                                          3.00       0.61       1.02        488
----… (label_id: 9)                                          0.97       6.17       1.68         81
-----------------------
----micro avg                                                5.41       5.41       5.41      35841
----macro avg                                                5.83       6.22       3.32      35841
----weighted avg                                            16.78       5.41       7.18      35841
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2201
-----------------------
----micro avg                                              100.00     100.00     100.00       2201
----macro avg                                              100.00     100.00     100.00       2201
----weighted avg                                           100.00     100.00     100.00       2201
----
----[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.20       0.43       0.27       4226
----! (label_id: 1)                                          0.44      14.17       0.86        127
----, (label_id: 2)                                          1.93       0.49       0.78      14611
----- (label_id: 3)                                          2.23      19.56       4.01       1237
----. (label_id: 4)                                         43.37      11.25      17.86      11977
----: (label_id: 5)                                          0.68       2.34       1.05        342
----; (label_id: 6)                                          0.00       0.00       0.00        129
----? (label_id: 7)                                          5.16       7.47       6.10       1058
----— (label_id: 8)                                          2.15       0.49       0.80        409
----… (label_id: 9)                                          0.69       4.23       1.19         71
-----------------------
----micro avg                                                5.23       5.23       5.23      34187
----macro avg                                                5.68       6.04       3.29      34187
----weighted avg                                            16.32       5.23       6.98      34187
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2113
-----------------------
----micro avg                                              100.00     100.00     100.00       2113
----macro avg                                              100.00     100.00     100.00       2113
----weighted avg                                           100.00     100.00     100.00       2113
----
----[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.28       0.61       0.39       4228
----! (label_id: 1)                                          0.30       8.28       0.58        145
----, (label_id: 2)                                          2.27       0.58       0.92      14495
----- (label_id: 3)                                          2.64      21.78       4.70       1327
----. (label_id: 4)                                         44.87      11.66      18.51      12193
----: (label_id: 5)                                          0.60       1.93       0.91        362
----; (label_id: 6)                                          0.00       0.00       0.00        164
----? (label_id: 7)                                          4.19       6.40       5.07       1078
----— (label_id: 8)                                          1.16       0.22       0.37        459
----… (label_id: 9)                                          0.85       4.17       1.41         96
-----------------------
----micro avg                                                5.54       5.54       5.54      34547
----macro avg                                                5.72       5.56       3.29      34547
----weighted avg                                            17.08       5.54       7.33      34547
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2114
-----------------------
----micro avg                                              100.00     100.00     100.00       2114
----macro avg                                              100.00     100.00     100.00       2114
----weighted avg                                           100.00     100.00     100.00       2114
----
----[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.29       0.63       0.40       4444
----! (label_id: 1)                                          0.38      10.67       0.74        150
----, (label_id: 2)                                          2.32       0.59       0.94      15290
----- (label_id: 3)                                          2.34      20.28       4.19       1292
----. (label_id: 4)                                         43.85      11.68      18.44      12599
----: (label_id: 5)                                          0.41       1.28       0.62        392
----; (label_id: 6)                                          0.00       0.00       0.00        164
----? (label_id: 7)                                          4.24       6.30       5.07       1111
----— (label_id: 8)                                          0.00       0.00       0.00        456
----… (label_id: 9)                                          0.38       2.41       0.65         83
-----------------------
----micro avg                                                5.40       5.40       5.40      35981
----macro avg                                                5.42       5.38       3.11      35981
----weighted avg                                            16.59       5.40       7.22      35981
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2222
-----------------------
----micro avg                                              100.00     100.00     100.00       2222
----macro avg                                              100.00     100.00     100.00       2222
----weighted avg                                           100.00     100.00     100.00       2222
----
----[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.35       0.73       0.48       3844
----! (label_id: 1)                                          0.54      14.62       1.04        130
----, (label_id: 2)                                          2.32       0.59       0.94      13056
----- (label_id: 3)                                          2.67      22.28       4.77       1194
----. (label_id: 4)                                         44.45      11.95      18.84      10791
----: (label_id: 5)                                          0.84       3.21       1.33        280
----; (label_id: 6)                                          0.00       0.00       0.00        140
----? (label_id: 7)                                          4.17       6.56       5.10        914
----— (label_id: 8)                                          0.00       0.00       0.00        401
----… (label_id: 9)                                          0.48       2.63       0.81         76
-----------------------
----micro avg                                                5.68       5.68       5.68      30826
----macro avg                                                5.58       6.26       3.33      30826
----weighted avg                                            16.82       5.68       7.41      30826
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       1922
-----------------------
----micro avg                                              100.00     100.00     100.00       1922
----macro avg                                              100.00     100.00     100.00       1922
----weighted avg                                           100.00     100.00     100.00       1922
----
----[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.28       0.60       0.39       3970
----! (label_id: 1)                                          0.35      10.66       0.68        122
----, (label_id: 2)                                          2.09       0.53       0.85      13469
----- (label_id: 3)                                          2.29      19.32       4.10       1201
----. (label_id: 4)                                         43.43      11.24      17.86      11227
----: (label_id: 5)                                          0.63       2.30       0.99        304
----; (label_id: 6)                                          0.00       0.00       0.00        141
----? (label_id: 7)                                          4.52       6.86       5.45       1006
----— (label_id: 8)                                          1.15       0.23       0.38        444
----… (label_id: 9)                                          0.45       2.67       0.78         75
-----------------------
----micro avg                                                5.26       5.26       5.26      31959
----macro avg                                                5.52       5.44       3.15      31959
----weighted avg                                            16.42       5.26       7.02      31959
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       1985
-----------------------
----micro avg                                              100.00     100.00     100.00       1985
----macro avg                                              100.00     100.00     100.00       1985
----weighted avg                                           100.00     100.00     100.00       1985
----
----[INFO] - Epoch 5, step 2394: val_loss was not in top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.23       0.48       0.31       4126
----! (label_id: 1)                                          0.29       9.40       0.56        117
----, (label_id: 2)                                          1.91       0.49       0.77      14019
----- (label_id: 3)                                          2.52      22.59       4.53       1164
----. (label_id: 4)                                         44.15      11.65      18.44      11789
----: (label_id: 5)                                          0.72       2.41       1.11        332
----; (label_id: 6)                                          0.56       0.61       0.58        165
----? (label_id: 7)                                          3.89       6.53       4.88        980
----— (label_id: 8)                                          2.30       0.47       0.77        430
----… (label_id: 9)                                          1.18       8.33       2.07         60
-----------------------
----micro avg                                                5.47       5.47       5.47      33182
----macro avg                                                5.77       6.30       3.40      33182
----weighted avg                                            16.77       5.47       7.25      33182
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2063
-----------------------
----micro avg                                              100.00     100.00     100.00       2063
----macro avg                                              100.00     100.00     100.00       2063
----weighted avg                                           100.00     100.00     100.00       2063
----
----[INFO] - Epoch 6, step 2793: val_loss was not in top 3
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/hparams.yaml
--deleted file mode 100644
--index 825d089..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/hparams.yaml
--+++ /dev/null
--@@ -1,107 +0,0 @@
---seed: 42
---trainer:
---  gpus: 1
---  num_nodes: 1
---  max_epochs: 2
---  max_steps: null
---  accumulate_grad_batches: 4
---  gradient_clip_val: 0
---  amp_level: O1
---  precision: 16
---  accelerator: ddp
---  checkpoint_callback: false
---  logger: false
---  log_every_n_steps: 1
---  val_check_interval: 1.0
---  resume_from_checkpoint: null
---exp_manager:
---  exp_dir: /home/nxingyu/project/
---  name: Punctuation_with_Domain_discriminator
---  create_tensorboard_logger: true
---  create_checkpoint_callback: true
---base_path: /home/nxingyu/data
---tmp_path: /home/nxingyu/data/tmp
---model:
---  nemo_path: null
---  transformer_path: google/electra-small-discriminator
---  unfrozen: 0
---  maximum_unfrozen: 2
---  unfreeze_step: 1
---  punct_label_ids:
---  - ''
---  - '!'
---  - ','
---  - '-'
---  - .
---  - ':'
---  - ;
---  - '?'
---  - —
---  - …
---  punct_class_weights: false
---  dataset:
---    data_dir: /home/nxingyu/data
---    labelled:
---    - /home/nxingyu/data/ted_talks_processed
---    unlabelled: null
---    max_seq_length: 128
---    pad_label: ''
---    ignore_extra_tokens: false
---    ignore_start_end: false
---    use_cache: false
---    num_workers: 0
---    pin_memory: true
---    drop_last: false
---    num_labels: 10
---    num_domains: 1
---    test_unlabelled: true
---    train_ds:
---      shuffle: true
---      num_samples: -1
---      batch_size: 8
---    validation_ds:
---      shuffle: true
---      num_samples: -1
---      batch_size: 8
---  tokenizer:
---    tokenizer_name: google/electra-small-discriminator
---    vocab_file: null
---    tokenizer_model: null
---    special_tokens: null
---  language_model:
---    pretrained_model_name: google/electra-small-discriminator
---    lm_checkpoint: null
---    config_file: null
---    config: null
---  punct_head:
---    punct_num_fc_layers: 1
---    fc_dropout: 0.1
---    activation: relu
---    log_softmax: false
---    use_transformer_init: true
---    loss: dice
---  domain_head:
---    domain_num_fc_layers: 1
---    fc_dropout: 0.1
---    activation: relu
---    log_softmax: false
---    use_transformer_init: true
---    loss: cel
---    gamma: 0.1
---  dice_loss:
---    epsilon: 0.01
---    alpha: 3
---    macro_average: true
---  focal_loss:
---    gamma: 5
---  optim:
---    name: adam
---    lr: 0.001
---    weight_decay: 0.0
---    sched:
---      name: WarmupAnnealing
---      warmup_steps: null
---      warmup_ratio: 0.1
---      last_epoch: -1
---      monitor: val_loss
---      reduce_on_plateau: false
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/lightning_logs.txt
--deleted file mode 100644
--index 5a813ec..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/lightning_logs.txt
--+++ /dev/null
--@@ -1,117 +0,0 @@
---Global seed set to 42
---GPU available: True, used: True
---TPU available: None, using: 0 TPU cores
---LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
---Using native 16bit precision.
---Global seed set to 42
---initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 257   
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---35.9 K    Trainable params
---13.4 M    Non-trainable params
---13.5 M    Total params
---Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/lr_find_temp_model.ckpt
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 257   
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---35.9 K    Trainable params
---13.4 M    Non-trainable params
---13.5 M    Total params
---Epoch 0, global step 100: val_loss reached 0.49661 (best 0.49661), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.50-epoch=0.ckpt" as top 3
---Epoch 1, global step 200: val_loss reached 0.50627 (best 0.49661), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.51-epoch=1.ckpt" as top 3
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 257   
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---825 K     Trainable params
---12.7 M    Non-trainable params
---13.5 M    Total params
---LR finder stopped early due to diverging loss.
---Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/lr_find_temp_model.ckpt
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 257   
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---825 K     Trainable params
---12.7 M    Non-trainable params
---13.5 M    Total params
---Epoch 0, global step 301: val_loss reached 0.30201 (best 0.30201), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.30-epoch=0.ckpt" as top 3
---Epoch 1, global step 401: val_loss reached 0.31043 (best 0.30201), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.31-epoch=1.ckpt" as top 3
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 257   
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---1.6 M     Trainable params
---11.9 M    Non-trainable params
---13.5 M    Total params
---LR finder stopped early due to diverging loss.
---Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/lr_find_temp_model.ckpt
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 257   
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---1.6 M     Trainable params
---11.9 M    Non-trainable params
---13.5 M    Total params
---Epoch 0, global step 502: val_loss reached 0.39765 (best 0.30201), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.40-epoch=0.ckpt" as top 3
---Epoch 1, step 602: val_loss was not in top 3
---GPU available: True, used: True
---TPU available: None, using: 0 TPU cores
---Using environment variable NODE_RANK for node rank (0).
---LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/nemo_error_log.txt
--deleted file mode 100644
--index 1eb1af1..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/nemo_error_log.txt
--+++ /dev/null
--@@ -1,43 +0,0 @@
---[NeMo W 2021-02-08 07:39:29 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-08 07:39:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:39:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:39:40 nemo_logging:349] /home/nxingyu/project/experiment/core/classification_report.py:116: UserWarning: This overload of nonzero is deprecated:
---    	nonzero(Tensor input, *, Tensor out)
---    Consider using one of the following signatures instead:
---    	nonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629411241/work/torch/csrc/utils/python_arg_parser.cpp:766.)
---      num_non_empty_classes = torch.nonzero(self.num_examples_per_class).size(0)
---    
---[NeMo W 2021-02-08 07:39:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:43:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:43:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:44:01 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:44:05 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:118: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
---[NeMo W 2021-02-08 07:48:43 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f3e417748e0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-08 07:49:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f3e4bb7b490> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-08 07:59:34 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 08:28:12 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 08:28:39 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f3e4bb7b5b0> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/nemo_log_globalrank-0_localrank-0.txt
--deleted file mode 100644
--index e7e35be..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/nemo_log_globalrank-0_localrank-0.txt
--+++ /dev/null
--@@ -1,45 +0,0 @@
---[NeMo I 2021-02-08 07:39:29 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29
---[NeMo I 2021-02-08 07:39:29 exp_manager:519] TensorboardLogger has been set up
---[NeMo W 2021-02-08 07:39:29 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-08 07:39:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:39:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:39:40 nemo_logging:349] /home/nxingyu/project/experiment/core/classification_report.py:116: UserWarning: This overload of nonzero is deprecated:
---    	nonzero(Tensor input, *, Tensor out)
---    Consider using one of the following signatures instead:
---    	nonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629411241/work/torch/csrc/utils/python_arg_parser.cpp:766.)
---      num_non_empty_classes = torch.nonzero(self.num_examples_per_class).size(0)
---    
---[NeMo W 2021-02-08 07:39:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:43:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:43:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:44:01 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:44:05 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:118: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
---[NeMo W 2021-02-08 07:48:43 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f3e417748e0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-08 07:49:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f3e4bb7b490> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-08 07:59:34 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 08:28:12 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 08:28:39 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f3e4bb7b5b0> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/cmd-args.log
--deleted file mode 100644
--index 11a5d8e..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/cmd-args.log
--+++ /dev/null
--@@ -1 +0,0 @@
---main.py
--\ No newline at end of file
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/events.out.tfevents.1612742534.Titan.5792.0 b/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/events.out.tfevents.1612742534.Titan.5792.0
--deleted file mode 100644
--index 51c6937..0000000
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/events.out.tfevents.1612742534.Titan.5792.0 and /dev/null differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/git-info.log
--deleted file mode 100644
--index 4f1e878..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/git-info.log
--+++ /dev/null
--@@ -1,415 +0,0 @@
---commit hash: 939a671c8c117db6975316767ced5d95449e2b27
---diff --git a/experiment/config.yaml b/experiment/config.yaml
---index fe58670..2306d25 100644
------ a/experiment/config.yaml
---+++ b/experiment/config.yaml
---@@ -1,47 +1,47 @@
--- seed: 42
--- trainer:
----    # gpus: 1 # the number of gpus, 0 for CPU
----    # num_nodes: 1
----    # max_epochs: 2
----    # max_steps: null # precedence over max_epochs
----    # accumulate_grad_batches: 4 # accumulates grads every k batches
----    # gradient_clip_val: 0
----    # amp_level: O1 # O1/O2 for mixed precision
----    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----    # accelerator: ddp
----    # checkpoint_callback: false  # Provided by exp_manager
----    # logger: false #false  # Provided by exp_manager
----    # log_every_n_steps: 1  # Interval of logging.
----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----    # resume_from_checkpoint: null
----
----    gpus: 0 # the number of gpus, 0 for CPU
---+    gpus: 1 # the number of gpus, 0 for CPU
---     num_nodes: 1
----    max_epochs: 8
---+    max_epochs: 2
---     max_steps: null # precedence over max_epochs
----    accumulate_grad_batches: 1 # accumulates grads every k batches
---+    accumulate_grad_batches: 4 # accumulates grads every k batches
---     gradient_clip_val: 0
----    amp_level: O0 # O1/O2 for mixed precision
----    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----    # accelerator: ddp
---+    amp_level: O1 # O1/O2 for mixed precision
---+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
---+    accelerator: ddp
---     checkpoint_callback: false  # Provided by exp_manager
---     logger: false #false  # Provided by exp_manager
---     log_every_n_steps: 1  # Interval of logging.
---     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----    reload_dataloaders_every_epoch: true
---     resume_from_checkpoint: null
--- 
---+    # gpus: 0 # the number of gpus, 0 for CPU
---+    # num_nodes: 1
---+    # max_epochs: 8
---+    # max_steps: null # precedence over max_epochs
---+    # accumulate_grad_batches: 1 # accumulates grads every k batches
---+    # gradient_clip_val: 0
---+    # amp_level: O0 # O1/O2 for mixed precision
---+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
---+    # # accelerator: ddp
---+    # checkpoint_callback: false  # Provided by exp_manager
---+    # logger: false #false  # Provided by exp_manager
---+    # log_every_n_steps: 1  # Interval of logging.
---+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
---+    # reload_dataloaders_every_epoch: true
---+    # resume_from_checkpoint: null
---+
--- exp_manager:
----    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
---+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
---     name: Punctuation_with_Domain_discriminator  # The name of your model
---     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
---     create_checkpoint_callback: true 
----base_path: /home/nxingyu2/data # /root/data # 
----tmp_path: /home/nxingyu2/data/tmp # /tmp # 
---+base_path: /home/nxingyu/data # /root/data # 
---+tmp_path: /home/nxingyu/data/tmp # /tmp # 
--- 
--- model:
---     nemo_path: null
----    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
---+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
---     unfrozen: 0
---     maximum_unfrozen: 2
---     unfreeze_step: 1
---@@ -58,10 +58,10 @@ model:
---         - "—"
---         - "…"
--- 
----    punct_class_weights: true
---+    punct_class_weights: false
---     
---     dataset:
----        data_dir: /home/nxingyu2/data # /root/data # 
---+        data_dir: /home/nxingyu/data # /root/data # 
---         labelled:
---             - ${base_path}/ted_talks_processed #
---         unlabelled:
---@@ -112,7 +112,7 @@ model:
---         activation: 'relu'
---         log_softmax: false
---         use_transformer_init: true
----        loss: 'dice'
---+        loss: 'crf'
--- 
---     domain_head:
---         domain_num_fc_layers: 1
---@@ -132,7 +132,7 @@ model:
---         gamma: 5
--- 
---     optim:
----        name: novograd
---+        name: adam
---         lr: 1e-3
---         weight_decay: 0.00
---         sched:
---diff --git a/experiment/info.log b/experiment/info.log
---index 2471fe9..e69de29 100644
------ a/experiment/info.log
---+++ b/experiment/info.log
---@@ -1,300 +0,0 @@
----[INFO] - GPU available: True, used: False
----[INFO] - TPU available: None, using: 0 TPU cores
----[INFO] - shuffling train set
----[INFO] - Optimizer config = Novograd (
----Parameter Group 0
----    amsgrad: False
----    betas: (0.95, 0.98)
----    eps: 1e-08
----    grad_averaging: False
----    lr: 0.001
----    weight_decay: 0.0
----)
----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
----will be used during training (effective maximum steps = 80) - 
----Parameters : 
----(warmup_steps: null
----warmup_ratio: 0.1
----last_epoch: -1
----max_steps: 80
----)
----[INFO] - 
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 108 M 
----1 | punct_classifier    | TokenClassifier      | 7.7 K 
----2 | domain_classifier   | SequenceClassifier   | 769   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----8.5 K     Trainable params
----108 M     Non-trainable params
----108 M     Total params
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.00       0.00       0.00        184
----! (label_id: 1)                                          0.00       0.00       0.00          4
----, (label_id: 2)                                          1.23       0.34       0.53        594
----- (label_id: 3)                                          3.06      25.42       5.46         59
----. (label_id: 4)                                         47.22      12.98      20.36        524
----: (label_id: 5)                                          0.00       0.00       0.00         18
----; (label_id: 6)                                          0.00       0.00       0.00         13
----? (label_id: 7)                                          8.45       6.32       7.23         95
----— (label_id: 8)                                          0.00       0.00       0.00         12
----… (label_id: 9)                                          0.00       0.00       0.00          0
-----------------------
----micro avg                                                6.05       6.05       6.05       1503
----macro avg                                                6.66       5.01       3.73       1503
----weighted avg                                            17.61       6.05       7.98       1503
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00         92
-----------------------
----micro avg                                              100.00     100.00     100.00         92
----macro avg                                              100.00     100.00     100.00         92
----weighted avg                                           100.00     100.00     100.00         92
----
----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
----[INFO] - Optimizer config = Novograd (
----Parameter Group 0
----    amsgrad: False
----    betas: (0.95, 0.98)
----    eps: 1e-08
----    grad_averaging: False
----    lr: 1.5848931924611143e-08
----    weight_decay: 0.0
----)
----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
----will be used during training (effective maximum steps = 3192) - 
----Parameters : 
----(warmup_steps: null
----warmup_ratio: 0.1
----last_epoch: -1
----max_steps: 3192
----)
----[INFO] - 
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 108 M 
----1 | punct_classifier    | TokenClassifier      | 7.7 K 
----2 | domain_classifier   | SequenceClassifier   | 769   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----8.5 K     Trainable params
----108 M     Non-trainable params
----108 M     Total params
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.00       0.00       0.00        202
----! (label_id: 1)                                          0.00       0.00       0.00          4
----, (label_id: 2)                                          1.62       0.45       0.70        669
----- (label_id: 3)                                          3.48      27.27       6.17         66
----. (label_id: 4)                                         45.06      13.01      20.19        561
----: (label_id: 5)                                          1.52       6.67       2.47         15
----; (label_id: 6)                                          0.00       0.00       0.00         15
----? (label_id: 7)                                          8.70       7.32       7.95         82
----— (label_id: 8)                                          0.00       0.00       0.00         13
----… (label_id: 9)                                          0.00       0.00       0.00          1
-----------------------
----micro avg                                                6.20       6.20       6.20       1628
----macro avg                                                6.04       5.47       3.75       1628
----weighted avg                                            16.79       6.20       7.92       1628
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00        101
-----------------------
----micro avg                                              100.00     100.00     100.00        101
----macro avg                                              100.00     100.00     100.00        101
----weighted avg                                           100.00     100.00     100.00        101
----
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.34       0.73       0.46       4402
----! (label_id: 1)                                          0.42      13.95       0.82        129
----, (label_id: 2)                                          2.53       0.64       1.03      15243
----- (label_id: 3)                                          2.45      21.03       4.38       1322
----. (label_id: 4)                                         44.00      11.40      18.11      12542
----: (label_id: 5)                                          0.43       1.41       0.65        354
----; (label_id: 6)                                          0.00       0.00       0.00        163
----? (label_id: 7)                                          4.16       6.27       5.00       1117
----— (label_id: 8)                                          3.00       0.61       1.02        488
----… (label_id: 9)                                          0.97       6.17       1.68         81
-----------------------
----micro avg                                                5.41       5.41       5.41      35841
----macro avg                                                5.83       6.22       3.32      35841
----weighted avg                                            16.78       5.41       7.18      35841
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2201
-----------------------
----micro avg                                              100.00     100.00     100.00       2201
----macro avg                                              100.00     100.00     100.00       2201
----weighted avg                                           100.00     100.00     100.00       2201
----
----[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.20       0.43       0.27       4226
----! (label_id: 1)                                          0.44      14.17       0.86        127
----, (label_id: 2)                                          1.93       0.49       0.78      14611
----- (label_id: 3)                                          2.23      19.56       4.01       1237
----. (label_id: 4)                                         43.37      11.25      17.86      11977
----: (label_id: 5)                                          0.68       2.34       1.05        342
----; (label_id: 6)                                          0.00       0.00       0.00        129
----? (label_id: 7)                                          5.16       7.47       6.10       1058
----— (label_id: 8)                                          2.15       0.49       0.80        409
----… (label_id: 9)                                          0.69       4.23       1.19         71
-----------------------
----micro avg                                                5.23       5.23       5.23      34187
----macro avg                                                5.68       6.04       3.29      34187
----weighted avg                                            16.32       5.23       6.98      34187
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2113
-----------------------
----micro avg                                              100.00     100.00     100.00       2113
----macro avg                                              100.00     100.00     100.00       2113
----weighted avg                                           100.00     100.00     100.00       2113
----
----[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.28       0.61       0.39       4228
----! (label_id: 1)                                          0.30       8.28       0.58        145
----, (label_id: 2)                                          2.27       0.58       0.92      14495
----- (label_id: 3)                                          2.64      21.78       4.70       1327
----. (label_id: 4)                                         44.87      11.66      18.51      12193
----: (label_id: 5)                                          0.60       1.93       0.91        362
----; (label_id: 6)                                          0.00       0.00       0.00        164
----? (label_id: 7)                                          4.19       6.40       5.07       1078
----— (label_id: 8)                                          1.16       0.22       0.37        459
----… (label_id: 9)                                          0.85       4.17       1.41         96
-----------------------
----micro avg                                                5.54       5.54       5.54      34547
----macro avg                                                5.72       5.56       3.29      34547
----weighted avg                                            17.08       5.54       7.33      34547
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2114
-----------------------
----micro avg                                              100.00     100.00     100.00       2114
----macro avg                                              100.00     100.00     100.00       2114
----weighted avg                                           100.00     100.00     100.00       2114
----
----[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.29       0.63       0.40       4444
----! (label_id: 1)                                          0.38      10.67       0.74        150
----, (label_id: 2)                                          2.32       0.59       0.94      15290
----- (label_id: 3)                                          2.34      20.28       4.19       1292
----. (label_id: 4)                                         43.85      11.68      18.44      12599
----: (label_id: 5)                                          0.41       1.28       0.62        392
----; (label_id: 6)                                          0.00       0.00       0.00        164
----? (label_id: 7)                                          4.24       6.30       5.07       1111
----— (label_id: 8)                                          0.00       0.00       0.00        456
----… (label_id: 9)                                          0.38       2.41       0.65         83
-----------------------
----micro avg                                                5.40       5.40       5.40      35981
----macro avg                                                5.42       5.38       3.11      35981
----weighted avg                                            16.59       5.40       7.22      35981
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2222
-----------------------
----micro avg                                              100.00     100.00     100.00       2222
----macro avg                                              100.00     100.00     100.00       2222
----weighted avg                                           100.00     100.00     100.00       2222
----
----[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.35       0.73       0.48       3844
----! (label_id: 1)                                          0.54      14.62       1.04        130
----, (label_id: 2)                                          2.32       0.59       0.94      13056
----- (label_id: 3)                                          2.67      22.28       4.77       1194
----. (label_id: 4)                                         44.45      11.95      18.84      10791
----: (label_id: 5)                                          0.84       3.21       1.33        280
----; (label_id: 6)                                          0.00       0.00       0.00        140
----? (label_id: 7)                                          4.17       6.56       5.10        914
----— (label_id: 8)                                          0.00       0.00       0.00        401
----… (label_id: 9)                                          0.48       2.63       0.81         76
-----------------------
----micro avg                                                5.68       5.68       5.68      30826
----macro avg                                                5.58       6.26       3.33      30826
----weighted avg                                            16.82       5.68       7.41      30826
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       1922
-----------------------
----micro avg                                              100.00     100.00     100.00       1922
----macro avg                                              100.00     100.00     100.00       1922
----weighted avg                                           100.00     100.00     100.00       1922
----
----[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.28       0.60       0.39       3970
----! (label_id: 1)                                          0.35      10.66       0.68        122
----, (label_id: 2)                                          2.09       0.53       0.85      13469
----- (label_id: 3)                                          2.29      19.32       4.10       1201
----. (label_id: 4)                                         43.43      11.24      17.86      11227
----: (label_id: 5)                                          0.63       2.30       0.99        304
----; (label_id: 6)                                          0.00       0.00       0.00        141
----? (label_id: 7)                                          4.52       6.86       5.45       1006
----— (label_id: 8)                                          1.15       0.23       0.38        444
----… (label_id: 9)                                          0.45       2.67       0.78         75
-----------------------
----micro avg                                                5.26       5.26       5.26      31959
----macro avg                                                5.52       5.44       3.15      31959
----weighted avg                                            16.42       5.26       7.02      31959
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       1985
-----------------------
----micro avg                                              100.00     100.00     100.00       1985
----macro avg                                              100.00     100.00     100.00       1985
----weighted avg                                           100.00     100.00     100.00       1985
----
----[INFO] - Epoch 5, step 2394: val_loss was not in top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.23       0.48       0.31       4126
----! (label_id: 1)                                          0.29       9.40       0.56        117
----, (label_id: 2)                                          1.91       0.49       0.77      14019
----- (label_id: 3)                                          2.52      22.59       4.53       1164
----. (label_id: 4)                                         44.15      11.65      18.44      11789
----: (label_id: 5)                                          0.72       2.41       1.11        332
----; (label_id: 6)                                          0.56       0.61       0.58        165
----? (label_id: 7)                                          3.89       6.53       4.88        980
----— (label_id: 8)                                          2.30       0.47       0.77        430
----… (label_id: 9)                                          1.18       8.33       2.07         60
-----------------------
----micro avg                                                5.47       5.47       5.47      33182
----macro avg                                                5.77       6.30       3.40      33182
----weighted avg                                            16.77       5.47       7.25      33182
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2063
-----------------------
----micro avg                                              100.00     100.00     100.00       2063
----macro avg                                              100.00     100.00     100.00       2063
----weighted avg                                           100.00     100.00     100.00       2063
----
----[INFO] - Epoch 6, step 2793: val_loss was not in top 3
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/hparams.yaml
--deleted file mode 100644
--index 081bb10..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/hparams.yaml
--+++ /dev/null
--@@ -1,107 +0,0 @@
---seed: 42
---trainer:
---  gpus: 1
---  num_nodes: 1
---  max_epochs: 2
---  max_steps: null
---  accumulate_grad_batches: 4
---  gradient_clip_val: 0
---  amp_level: O1
---  precision: 16
---  accelerator: ddp
---  checkpoint_callback: false
---  logger: false
---  log_every_n_steps: 1
---  val_check_interval: 1.0
---  resume_from_checkpoint: null
---exp_manager:
---  exp_dir: /home/nxingyu/project/
---  name: Punctuation_with_Domain_discriminator
---  create_tensorboard_logger: true
---  create_checkpoint_callback: true
---base_path: /home/nxingyu/data
---tmp_path: /home/nxingyu/data/tmp
---model:
---  nemo_path: null
---  transformer_path: google/electra-small-discriminator
---  unfrozen: 0
---  maximum_unfrozen: 2
---  unfreeze_step: 1
---  punct_label_ids:
---  - ''
---  - '!'
---  - ','
---  - '-'
---  - .
---  - ':'
---  - ;
---  - '?'
---  - —
---  - …
---  punct_class_weights: false
---  dataset:
---    data_dir: /home/nxingyu/data
---    labelled:
---    - /home/nxingyu/data/ted_talks_processed
---    unlabelled: null
---    max_seq_length: 128
---    pad_label: ''
---    ignore_extra_tokens: false
---    ignore_start_end: false
---    use_cache: false
---    num_workers: 0
---    pin_memory: true
---    drop_last: false
---    num_labels: 10
---    num_domains: 1
---    test_unlabelled: true
---    train_ds:
---      shuffle: true
---      num_samples: -1
---      batch_size: 8
---    validation_ds:
---      shuffle: true
---      num_samples: -1
---      batch_size: 8
---  tokenizer:
---    tokenizer_name: google/electra-small-discriminator
---    vocab_file: null
---    tokenizer_model: null
---    special_tokens: null
---  language_model:
---    pretrained_model_name: google/electra-small-discriminator
---    lm_checkpoint: null
---    config_file: null
---    config: null
---  punct_head:
---    punct_num_fc_layers: 1
---    fc_dropout: 0.1
---    activation: relu
---    log_softmax: false
---    use_transformer_init: true
---    loss: crf
---  domain_head:
---    domain_num_fc_layers: 1
---    fc_dropout: 0.1
---    activation: relu
---    log_softmax: false
---    use_transformer_init: true
---    loss: cel
---    gamma: 0.1
---  dice_loss:
---    epsilon: 0.01
---    alpha: 3
---    macro_average: true
---  focal_loss:
---    gamma: 5
---  optim:
---    name: adam
---    lr: 0.001
---    weight_decay: 0.0
---    sched:
---      name: WarmupAnnealing
---      warmup_steps: null
---      warmup_ratio: 0.1
---      last_epoch: -1
---      monitor: val_loss
---      reduce_on_plateau: false
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/lightning_logs.txt
--deleted file mode 100644
--index c01498c..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/lightning_logs.txt
--+++ /dev/null
--@@ -1,117 +0,0 @@
---Global seed set to 42
---GPU available: True, used: True
---TPU available: None, using: 0 TPU cores
---LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
---Using native 16bit precision.
---Global seed set to 42
---initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 257   
---3 | punctuation_loss    | LinearChainCRF       | 120   
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---36.0 K    Trainable params
---13.4 M    Non-trainable params
---13.5 M    Total params
---Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/lr_find_temp_model.ckpt
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 257   
---3 | punctuation_loss    | LinearChainCRF       | 120   
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---36.0 K    Trainable params
---13.4 M    Non-trainable params
---13.5 M    Total params
---Epoch 0, global step 100: val_loss reached 28.76837 (best 28.76837), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/checkpoints/Punctuation_with_Domain_discriminator---val_loss=28.77-epoch=0.ckpt" as top 3
---Epoch 1, global step 200: val_loss reached 18.16606 (best 18.16606), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/checkpoints/Punctuation_with_Domain_discriminator---val_loss=18.17-epoch=1.ckpt" as top 3
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 257   
---3 | punctuation_loss    | LinearChainCRF       | 120   
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---825 K     Trainable params
---12.7 M    Non-trainable params
---13.5 M    Total params
---LR finder stopped early due to diverging loss.
---Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/lr_find_temp_model.ckpt
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 257   
---3 | punctuation_loss    | LinearChainCRF       | 120   
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---825 K     Trainable params
---12.7 M    Non-trainable params
---13.5 M    Total params
---Epoch 0, global step 301: val_loss reached 14.52603 (best 14.52603), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/checkpoints/Punctuation_with_Domain_discriminator---val_loss=14.53-epoch=0.ckpt" as top 3
---Epoch 1, global step 401: val_loss reached 15.01920 (best 14.52603), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/checkpoints/Punctuation_with_Domain_discriminator---val_loss=15.02-epoch=1.ckpt" as top 3
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 257   
---3 | punctuation_loss    | LinearChainCRF       | 120   
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---1.6 M     Trainable params
---11.9 M    Non-trainable params
---13.5 M    Total params
---LR finder stopped early due to diverging loss.
---Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/lr_find_temp_model.ckpt
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 257   
---3 | punctuation_loss    | LinearChainCRF       | 120   
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---1.6 M     Trainable params
---11.9 M    Non-trainable params
---13.5 M    Total params
---Epoch 0, global step 502: val_loss reached 13.53691 (best 13.53691), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/checkpoints/Punctuation_with_Domain_discriminator---val_loss=13.54-epoch=0.ckpt" as top 3
---Epoch 1, step 602: val_loss was not in top 3
---GPU available: True, used: True
---TPU available: None, using: 0 TPU cores
---Using environment variable NODE_RANK for node rank (0).
---LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/nemo_error_log.txt
--deleted file mode 100644
--index 1be0620..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/nemo_error_log.txt
--+++ /dev/null
--@@ -1,46 +0,0 @@
---[NeMo W 2021-02-08 07:56:46 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-08 07:56:55 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:56:55 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:56:57 nemo_logging:349] /home/nxingyu/project/experiment/core/classification_report.py:116: UserWarning: This overload of nonzero is deprecated:
---    	nonzero(Tensor input, *, Tensor out)
---    Consider using one of the following signatures instead:
---    	nonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629411241/work/torch/csrc/utils/python_arg_parser.cpp:766.)
---      num_non_empty_classes = torch.nonzero(self.num_examples_per_class).size(0)
---    
---[NeMo W 2021-02-08 07:56:57 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:57:02 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:118: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
---[NeMo W 2021-02-08 08:02:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 08:02:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 08:02:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 08:02:21 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:118: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
---[NeMo W 2021-02-08 08:08:50 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f0210aec880> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-08 08:09:19 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f02190dca90> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-08 08:23:36 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 09:11:17 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 09:11:50 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f0219122550> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/nemo_log_globalrank-0_localrank-0.txt
--deleted file mode 100644
--index 2ca2442..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/nemo_log_globalrank-0_localrank-0.txt
--+++ /dev/null
--@@ -1,48 +0,0 @@
---[NeMo I 2021-02-08 07:56:46 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46
---[NeMo I 2021-02-08 07:56:46 exp_manager:519] TensorboardLogger has been set up
---[NeMo W 2021-02-08 07:56:46 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-08 07:56:55 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:56:55 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:56:57 nemo_logging:349] /home/nxingyu/project/experiment/core/classification_report.py:116: UserWarning: This overload of nonzero is deprecated:
---    	nonzero(Tensor input, *, Tensor out)
---    Consider using one of the following signatures instead:
---    	nonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629411241/work/torch/csrc/utils/python_arg_parser.cpp:766.)
---      num_non_empty_classes = torch.nonzero(self.num_examples_per_class).size(0)
---    
---[NeMo W 2021-02-08 07:56:57 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 07:57:02 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:118: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
---[NeMo W 2021-02-08 08:02:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 08:02:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 08:02:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 08:02:21 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:118: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
---[NeMo W 2021-02-08 08:08:50 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f0210aec880> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-08 08:09:19 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f02190dca90> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-08 08:23:36 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 09:11:17 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 09:11:50 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f0219122550> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/cmd-args.log
--deleted file mode 100644
--index 11a5d8e..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/cmd-args.log
--+++ /dev/null
--@@ -1 +0,0 @@
---main.py
--\ No newline at end of file
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/events.out.tfevents.1612771686.Titan.22251.0 b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/events.out.tfevents.1612771686.Titan.22251.0
--deleted file mode 100644
--index 6b14ff7..0000000
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/events.out.tfevents.1612771686.Titan.22251.0 and /dev/null differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/git-info.log
--deleted file mode 100644
--index e93204c..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/git-info.log
--+++ /dev/null
--@@ -1,181 +0,0 @@
---commit hash: d9cdb13829a1dfa2d74afb03fde5acec0f85d2cc
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/lightning_logs.txt
---index 2a26724..31870ad 100644
------ a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/lightning_logs.txt
---+++ b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/lightning_logs.txt
---@@ -20,3 +20,36 @@ initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--- 299 K     Trainable params
--- 13.2 M    Non-trainable params
--- 13.5 M    Total params
---+Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/lr_find_temp_model.ckpt
---+Global seed set to 42
---+
---+  | Name                | Type                 | Params
---+-------------------------------------------------------------
---+0 | transformer         | ElectraModel         | 13.5 M
---+1 | punct_classifier    | TokenClassifier      | 2.6 K 
---+2 | domain_classifier   | SequenceClassifier   | 1.0 K 
---+3 | punctuation_loss    | FocalDiceLoss        | 0     
---+4 | domain_loss         | CrossEntropyLoss     | 0     
---+5 | agg_loss            | AggregatorLoss       | 0     
---+6 | punct_class_report  | ClassificationReport | 0     
---+7 | domain_class_report | ClassificationReport | 0     
---+-------------------------------------------------------------
---+299 K     Trainable params
---+13.2 M    Non-trainable params
---+13.5 M    Total params
---+Global seed set to 42
---+
---+  | Name                | Type                 | Params
---+-------------------------------------------------------------
---+0 | transformer         | ElectraModel         | 13.5 M
---+1 | punct_classifier    | TokenClassifier      | 2.6 K 
---+2 | domain_classifier   | SequenceClassifier   | 1.0 K 
---+3 | punctuation_loss    | FocalDiceLoss        | 0     
---+4 | domain_loss         | CrossEntropyLoss     | 0     
---+5 | agg_loss            | AggregatorLoss       | 0     
---+6 | punct_class_report  | ClassificationReport | 0     
---+7 | domain_class_report | ClassificationReport | 0     
---+-------------------------------------------------------------
---+826 K     Trainable params
---+12.7 M    Non-trainable params
---+13.5 M    Total params
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_error_log.txt
---index 4ddbe1b..90e4c4c 100644
------ a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_error_log.txt
---+++ b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_error_log.txt
---@@ -2,3 +2,12 @@
--- [NeMo W 2021-02-08 15:34:34 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---       warnings.warn(*args, **kwargs)
---     
---+[NeMo W 2021-02-08 15:46:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---+      warnings.warn(*args, **kwargs)
---+    
---+[NeMo W 2021-02-08 15:51:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---+      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---+    
---+[NeMo W 2021-02-08 15:57:37 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
---+      warnings.warn(*args, **kwargs)
---+    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_log_globalrank-0_localrank-0.txt
---index dea36af..926854f 100644
------ a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_log_globalrank-0_localrank-0.txt
---+++ b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_log_globalrank-0_localrank-0.txt
---@@ -4,3 +4,12 @@
--- [NeMo W 2021-02-08 15:34:34 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---       warnings.warn(*args, **kwargs)
---     
---+[NeMo W 2021-02-08 15:46:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---+      warnings.warn(*args, **kwargs)
---+    
---+[NeMo W 2021-02-08 15:51:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---+      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---+    
---+[NeMo W 2021-02-08 15:57:37 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
---+      warnings.warn(*args, **kwargs)
---+    
---diff --git a/README.md b/README.md
---index beca3ef..ef7d22c 100644
------ a/README.md
---+++ b/README.md
---@@ -449,3 +449,7 @@ weighted avg                                            69.12      72.03      70
---  'punct_recall': 33.78831481933594,
---  'test_loss': 0.2638570964336395}
--- 
---+
---+
---+### domain adversarial dice 3, open l ted ul 
---+initial_lr 0.007943282347242822
---diff --git a/experiment/config.yaml b/experiment/config.yaml
---index 070bc4f..37d105e 100644
------ a/experiment/config.yaml
---+++ b/experiment/config.yaml
---@@ -85,7 +85,7 @@ model:
---         train_ds:
---             shuffle: true
---             num_samples: -1
----            batch_size: 8
---+            batch_size: 4
--- 
---         validation_ds:
---             # if evaluation data is not in the model.dataset.data_dir as the training data or multiple datasets are used for
---@@ -93,7 +93,7 @@ model:
---             # ds_item: null # expected format: [PATH_TO_DEV1,PATH_TO_DEV2] (Note no space between the paths and square brackets)
---             shuffle: true
---             num_samples: -1
----            batch_size: 8
---+            batch_size: 4
--- 
---     tokenizer:
---         tokenizer_name: ${model.language_model.pretrained_model_name} # or sentencepiece
---@@ -123,7 +123,7 @@ model:
---         log_softmax: false
---         use_transformer_init: true
---         loss: 'cel'
----        gamma: 0 #0.1 # coefficient of gradient reversal
---+        gamma: 0.1 #0.1 # coefficient of gradient reversal
---         pooling: 'mean_max' # 'mean' mean_max
---         idx_conditioned_on: 0
---     
---diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
---index bc844cd..4027bb2 100644
------ a/experiment/data/punctuation_dataset_multi.py
---+++ b/experiment/data/punctuation_dataset_multi.py
---@@ -63,7 +63,8 @@ class PunctuationDomainDataset(IterableDataset):
---         self.randomize=randomize
---         self.target_file=target_file
---         self.tmp_path=tmp_path
----        os.system(f'cp {self.csv_file} {self.target_file}')
---+        if not (os.path.exists(self.target_file)):
---+            os.system(f'cp {self.csv_file} {self.target_file}')
--- 
---     def __iter__(self):
---         self.dataset=iter(pd.read_csv(
---diff --git a/experiment/info.log b/experiment/info.log
---index 481b8ff..e69de29 100644
------ a/experiment/info.log
---+++ b/experiment/info.log
---@@ -1,43 +0,0 @@
----[INFO] - shuffling train set
----[INFO] - Optimizer config = AdamW (
----Parameter Group 0
----    amsgrad: False
----    betas: (0.9, 0.999)
----    eps: 1e-08
----    lr: 0.001
----    weight_decay: 0.0
----)
----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fd3155e83d0>" 
----will be used during training (effective maximum steps = 80) - 
----Parameters : 
----(warmup_steps: null
----warmup_ratio: 0.1
----last_epoch: -1
----max_steps: 80
----)
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.00       0.00       0.00        168
----! (label_id: 1)                                         14.29       7.69      10.00        104
----, (label_id: 2)                                         23.21      27.23      25.06        584
----- (label_id: 3)                                          4.02      46.67       7.39         45
----. (label_id: 4)                                         54.17       1.19       2.33       1091
----: (label_id: 5)                                          0.00       0.00       0.00          0
----; (label_id: 6)                                          0.00       0.00       0.00          0
----? (label_id: 7)                                          6.42      22.75      10.01        189
----— (label_id: 8)                                          0.00       0.00       0.00          0
----… (label_id: 9)                                          9.09       2.38       3.77         84
-----------------------
----micro avg                                               10.86      10.86      10.86       2265
----macro avg                                               15.88      15.42       8.37       2265
----weighted avg                                            33.68      10.86       9.17       2265
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                         50.00     100.00      66.67         84
----1 (label_id: 1)                                          0.00       0.00       0.00         84
-----------------------
----micro avg                                               50.00      50.00      50.00        168
----macro avg                                               25.00      50.00      33.33        168
----weighted avg                                            25.00      50.00      33.33        168
----
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/hparams.yaml
--deleted file mode 100644
--index ea7d4f7..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/hparams.yaml
--+++ /dev/null
--@@ -1,110 +0,0 @@
---seed: 42
---trainer:
---  gpus: 1
---  num_nodes: 1
---  max_epochs: 10
---  max_steps: null
---  accumulate_grad_batches: 4
---  gradient_clip_val: 0
---  amp_level: O1
---  precision: 16
---  accelerator: ddp
---  checkpoint_callback: false
---  logger: false
---  log_every_n_steps: 1
---  val_check_interval: 1.0
---  resume_from_checkpoint: null
---exp_manager:
---  exp_dir: /home/nxingyu/project/
---  name: Punctuation_with_Domain_discriminator
---  create_tensorboard_logger: true
---  create_checkpoint_callback: true
---base_path: /home/nxingyu/data
---tmp_path: /home/nxingyu/data/tmp
---model:
---  nemo_path: null
---  transformer_path: google/electra-small-discriminator
---  unfrozen: 0
---  maximum_unfrozen: 1
---  unfreeze_step: 1
---  punct_label_ids:
---  - ''
---  - '!'
---  - ','
---  - '-'
---  - .
---  - ':'
---  - ;
---  - '?'
---  - —
---  - …
---  punct_class_weights: false
---  dataset:
---    data_dir: /home/nxingyu/data
---    labelled:
---    - /home/nxingyu/data/open_subtitles_processed
---    unlabelled:
---    - /home/nxingyu/data/ted_talks_processed
---    max_seq_length: 128
---    pad_label: ''
---    ignore_extra_tokens: false
---    ignore_start_end: false
---    use_cache: false
---    num_workers: 4
---    pin_memory: true
---    drop_last: false
---    num_labels: 10
---    num_domains: 2
---    test_unlabelled: true
---    train_ds:
---      shuffle: true
---      num_samples: -1
---      batch_size: 4
---    validation_ds:
---      shuffle: true
---      num_samples: -1
---      batch_size: 4
---  tokenizer:
---    tokenizer_name: google/electra-small-discriminator
---    vocab_file: null
---    tokenizer_model: null
---    special_tokens: null
---  language_model:
---    pretrained_model_name: google/electra-small-discriminator
---    lm_checkpoint: null
---    config_file: null
---    config: null
---  punct_head:
---    punct_num_fc_layers: 1
---    fc_dropout: 0.1
---    activation: relu
---    log_softmax: false
---    use_transformer_init: true
---    loss: dice
---  domain_head:
---    domain_num_fc_layers: 1
---    fc_dropout: 0.1
---    activation: relu
---    log_softmax: false
---    use_transformer_init: true
---    loss: cel
---    gamma: 0.1
---    pooling: mean_max
---    idx_conditioned_on: 0
---  dice_loss:
---    epsilon: 0.01
---    alpha: 3
---    macro_average: true
---  focal_loss:
---    gamma: 1
---  optim:
---    name: adamw
---    lr: 0.001
---    weight_decay: 0.0
---    sched:
---      name: WarmupAnnealing
---      warmup_steps: null
---      warmup_ratio: 0.1
---      last_epoch: -1
---      monitor: val_loss
---      reduce_on_plateau: false
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/lightning_logs.txt
--deleted file mode 100644
--index 846b33e..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/lightning_logs.txt
--+++ /dev/null
--@@ -1,40 +0,0 @@
---Global seed set to 42
---GPU available: True, used: True
---TPU available: None, using: 0 TPU cores
---LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
---Using native 16bit precision.
---Global seed set to 42
---initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 1.0 K 
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---299 K     Trainable params
---13.2 M    Non-trainable params
---13.5 M    Total params
---Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/lr_find_temp_model.ckpt
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 1.0 K 
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---299 K     Trainable params
---13.2 M    Non-trainable params
---13.5 M    Total params
---Epoch 0, global step 5305: val_loss reached 0.69762 (best 0.69762), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.70-epoch=0.ckpt" as top 3
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_error_log.txt
--deleted file mode 100644
--index 55977aa..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_error_log.txt
--+++ /dev/null
--@@ -1,16 +0,0 @@
---[NeMo W 2021-02-08 16:00:26 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-08 16:00:55 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 16:08:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 16:13:48 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
---[NeMo W 2021-02-08 17:54:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fcf47c80f70> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-08 18:08:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fcf47c80550> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---      warnings.warn(warn_msg)
---    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_log_globalrank-0_localrank-0.txt
--deleted file mode 100644
--index 5270e5c..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_log_globalrank-0_localrank-0.txt
--+++ /dev/null
--@@ -1,18 +0,0 @@
---[NeMo I 2021-02-08 16:00:26 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26
---[NeMo I 2021-02-08 16:00:26 exp_manager:519] TensorboardLogger has been set up
---[NeMo W 2021-02-08 16:00:26 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-08 16:00:55 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 16:08:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 16:13:48 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
---[NeMo W 2021-02-08 17:54:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fcf47c80f70> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-08 18:08:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fcf47c80550> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---      warnings.warn(warn_msg)
---    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/cmd-args.log
--deleted file mode 100644
--index 11a5d8e..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/cmd-args.log
--+++ /dev/null
--@@ -1 +0,0 @@
---main.py
--\ No newline at end of file
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/events.out.tfevents.1612830887.Titan.6887.0 b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/events.out.tfevents.1612830887.Titan.6887.0
--deleted file mode 100644
--index cf34bb6..0000000
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/events.out.tfevents.1612830887.Titan.6887.0 and /dev/null differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/git-info.log
--deleted file mode 100644
--index b6da706..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/git-info.log
--+++ /dev/null
--@@ -1,273 +0,0 @@
---commit hash: 08007e7bd84203d450e193af808686ac2c929dce
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/events.out.tfevents.1612771686.Titan.22251.0 b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/events.out.tfevents.1612771686.Titan.22251.0
---index 2a40109..6b14ff7 100644
---Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/events.out.tfevents.1612771686.Titan.22251.0 and b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/events.out.tfevents.1612771686.Titan.22251.0 differ
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/lightning_logs.txt
---index 439dccb..846b33e 100644
------ a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/lightning_logs.txt
---+++ b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/lightning_logs.txt
---@@ -37,3 +37,4 @@ Global seed set to 42
--- 299 K     Trainable params
--- 13.2 M    Non-trainable params
--- 13.5 M    Total params
---+Epoch 0, global step 5305: val_loss reached 0.69762 (best 0.69762), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.70-epoch=0.ckpt" as top 3
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_error_log.txt
---index c85c2a3..55977aa 100644
------ a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_error_log.txt
---+++ b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_error_log.txt
---@@ -8,3 +8,9 @@
--- [NeMo W 2021-02-08 16:13:48 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---     
---+[NeMo W 2021-02-08 17:54:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fcf47c80f70> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---+      warnings.warn(warn_msg)
---+    
---+[NeMo W 2021-02-08 18:08:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fcf47c80550> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---+      warnings.warn(warn_msg)
---+    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_log_globalrank-0_localrank-0.txt
---index b01f19c..5270e5c 100644
------ a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_log_globalrank-0_localrank-0.txt
---+++ b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_log_globalrank-0_localrank-0.txt
---@@ -10,3 +10,9 @@
--- [NeMo W 2021-02-08 16:13:48 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---     
---+[NeMo W 2021-02-08 17:54:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fcf47c80f70> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---+      warnings.warn(warn_msg)
---+    
---+[NeMo W 2021-02-08 18:08:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fcf47c80550> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---+      warnings.warn(warn_msg)
---+    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/events.out.tfevents.1612772700.Titan.28938.0 b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/events.out.tfevents.1612772700.Titan.28938.0
---index ca85da6..04e8367 100644
---Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/events.out.tfevents.1612772700.Titan.28938.0 and b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/events.out.tfevents.1612772700.Titan.28938.0 differ
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/lightning_logs.txt
---index 50c4caa..0f48e2a 100644
------ a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/lightning_logs.txt
---+++ b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/lightning_logs.txt
---@@ -37,3 +37,29 @@ Global seed set to 42
--- 299 K     Trainable params
--- 13.2 M    Non-trainable params
--- 13.5 M    Total params
---+Epoch 0, global step 5305: val_loss reached 0.20905 (best 0.20905), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.21-epoch=0.ckpt" as top 3
---+Epoch 1, global step 10610: val_loss reached 0.17265 (best 0.17265), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.17-epoch=1.ckpt" as top 3
---+Epoch 2, global step 15915: val_loss reached 0.05470 (best 0.05470), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.05-epoch=2.ckpt" as top 3
---+Epoch 3, global step 21220: val_loss reached 0.02875 (best 0.02875), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.03-epoch=3.ckpt" as top 3
---+Epoch 4, global step 26525: val_loss reached -0.03932 (best -0.03932), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.04-epoch=4.ckpt" as top 3
---+Epoch 5, global step 31830: val_loss reached -0.04410 (best -0.04410), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.04-epoch=5.ckpt" as top 3
---+Epoch 6, global step 37135: val_loss reached -0.04524 (best -0.04524), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.05-epoch=6.ckpt" as top 3
---+Epoch 7, global step 42440: val_loss reached -0.04689 (best -0.04689), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.05-epoch=7.ckpt" as top 3
---+Epoch 8, global step 47745: val_loss reached -0.04978 (best -0.04978), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.05-epoch=8.ckpt" as top 3
---+Epoch 9, global step 53050: val_loss reached -0.04850 (best -0.04978), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.05-epoch=9.ckpt" as top 3
---+Global seed set to 42
---+
---+  | Name                | Type                 | Params
---+-------------------------------------------------------------
---+0 | transformer         | ElectraModel         | 13.5 M
---+1 | punct_classifier    | TokenClassifier      | 2.6 K 
---+2 | domain_classifier   | SequenceClassifier   | 513   
---+3 | punctuation_loss    | FocalDiceLoss        | 0     
---+4 | domain_loss         | CrossEntropyLoss     | 0     
---+5 | agg_loss            | AggregatorLoss       | 0     
---+6 | punct_class_report  | ClassificationReport | 0     
---+7 | domain_class_report | ClassificationReport | 0     
---+-------------------------------------------------------------
---+825 K     Trainable params
---+12.7 M    Non-trainable params
---+13.5 M    Total params
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_error_log.txt
---index 2503857..ce38185 100644
------ a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_error_log.txt
---+++ b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_error_log.txt
---@@ -8,3 +8,9 @@
--- [NeMo W 2021-02-08 16:25:22 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---     
---+[NeMo W 2021-02-08 17:13:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fbd320e2b50> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---+      warnings.warn(warn_msg)
---+    
---+[NeMo W 2021-02-08 17:20:28 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fbd3c4dd8e0> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---+      warnings.warn(warn_msg)
---+    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_log_globalrank-0_localrank-0.txt
---index 48278bd..26f3ea7 100644
------ a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_log_globalrank-0_localrank-0.txt
---+++ b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_log_globalrank-0_localrank-0.txt
---@@ -10,3 +10,9 @@
--- [NeMo W 2021-02-08 16:25:22 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---     
---+[NeMo W 2021-02-08 17:13:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fbd320e2b50> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---+      warnings.warn(warn_msg)
---+    
---+[NeMo W 2021-02-08 17:20:28 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fbd3c4dd8e0> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---+      warnings.warn(warn_msg)
---+    
---diff --git a/experiment/config.yaml b/experiment/config.yaml
---index e492246..e7f6783 100644
------ a/experiment/config.yaml
---+++ b/experiment/config.yaml
---@@ -63,11 +63,11 @@ model:
---     dataset:
---         data_dir: /home/nxingyu/data # /root/data # 
---         labelled:
----            # - ${base_path}/ted_talks_processed #
----            - ${base_path}/open_subtitles_processed #  
---+            - ${base_path}/ted_talks_processed #
---+            # - ${base_path}/open_subtitles_processed #  
---         unlabelled:
---             # - ${base_path}/ted_talks_processed #
----            # - ${base_path}/open_subtitles_processed #  
---+            - ${base_path}/open_subtitles_processed #  
---             # parameters for dataset preprocessing
---         max_seq_length: 128
---         pad_label: ''
---@@ -79,7 +79,7 @@ model:
---         pin_memory: true
---         drop_last: false
---         num_labels: 10
----        num_domains: 1
---+        num_domains: 2
---         test_unlabelled: true
--- 
---         train_ds:
---@@ -123,7 +123,7 @@ model:
---         log_softmax: false
---         use_transformer_init: true
---         loss: 'cel'
----        gamma: 0.1 #0.1 # coefficient of gradient reversal
---+        gamma: 0 #0.1 # coefficient of gradient reversal
---         pooling: 'mean_max' # 'mean' mean_max
---         idx_conditioned_on: 0
---     
---diff --git a/experiment/core/utils.py b/experiment/core/utils.py
---index 5b0efe3..058cc87 100644
------ a/experiment/core/utils.py
---+++ b/experiment/core/utils.py
---@@ -26,7 +26,7 @@ def position_to_mask(max_seq_length:int,indices:list):
---         o[np.array(indices)%(max_seq_length-2)+1]=1
---     except:
---         pp('position_to_mask',np.array(indices)%(max_seq_length-2)+1)
----        o[(np.array(indices)%(max_seq_length-2)+1).astype(int)]=1
---+        # o[(np.array(indices)%(max_seq_length-2)+1).astype(int)]=1
---     return o
--- 
--- def align_labels_to_mask(mask,labels):
---diff --git a/experiment/info.log b/experiment/info.log
---index 9e4b4d4..e69de29 100644
------ a/experiment/info.log
---+++ b/experiment/info.log
---@@ -1,83 +0,0 @@
----[INFO] - shuffling train set
----[INFO] - Optimizer config = AdamW (
----Parameter Group 0
----    amsgrad: False
----    betas: (0.9, 0.999)
----    eps: 1e-08
----    lr: 0.001
----    weight_decay: 0.0
----)
----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fbd32b00be0>" 
----will be used during training (effective maximum steps = 80) - 
----Parameters : 
----(warmup_steps: null
----warmup_ratio: 0.1
----last_epoch: -1
----max_steps: 80
----)
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.00       0.00       0.00        232
----! (label_id: 1)                                          5.41       2.63       3.54        152
----, (label_id: 2)                                         24.37      27.46      25.82        772
----- (label_id: 3)                                          4.66      53.57       8.57         56
----. (label_id: 4)                                         43.75       0.85       1.67       1642
----: (label_id: 5)                                          0.00       0.00       0.00          0
----; (label_id: 6)                                          0.00       0.00       0.00          0
----? (label_id: 7)                                          4.76      22.94       7.89        218
----— (label_id: 8)                                          0.00       0.00       0.00          0
----… (label_id: 9)                                          8.33       2.04       3.28         98
-----------------------
----micro avg                                                9.84       9.84       9.84       3170
----macro avg                                               13.04      15.64       7.25       3170
----weighted avg                                            29.52       9.84       8.12       3170
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00        116
-----------------------
----micro avg                                              100.00     100.00     100.00        116
----macro avg                                              100.00     100.00     100.00        116
----weighted avg                                           100.00     100.00     100.00        116
----
----[INFO] - Optimizer config = AdamW (
----Parameter Group 0
----    amsgrad: False
----    betas: (0.9, 0.999)
----    eps: 1e-08
----    lr: 0.007943282347242822
----    weight_decay: 0.0
----)
----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fbd3350f790>" 
----will be used during training (effective maximum steps = 53050) - 
----Parameters : 
----(warmup_steps: null
----warmup_ratio: 0.1
----last_epoch: -1
----max_steps: 53050
----)
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.00       0.00       0.00        232
----! (label_id: 1)                                          5.41       2.63       3.54        152
----, (label_id: 2)                                         24.37      27.46      25.82        772
----- (label_id: 3)                                          4.66      53.57       8.57         56
----. (label_id: 4)                                         43.75       0.85       1.67       1642
----: (label_id: 5)                                          0.00       0.00       0.00          0
----; (label_id: 6)                                          0.00       0.00       0.00          0
----? (label_id: 7)                                          4.76      22.94       7.89        218
----— (label_id: 8)                                          0.00       0.00       0.00          0
----… (label_id: 9)                                          8.33       2.04       3.28         98
-----------------------
----micro avg                                                9.84       9.84       9.84       3170
----macro avg                                               13.04      15.64       7.25       3170
----weighted avg                                            29.52       9.84       8.12       3170
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00        116
-----------------------
----micro avg                                              100.00     100.00     100.00        116
----macro avg                                              100.00     100.00     100.00        116
----weighted avg                                           100.00     100.00     100.00        116
----
---diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
---index 4ac54f4..c5db7b3 100644
------ a/experiment/models/punctuation_domain_model.py
---+++ b/experiment/models/punctuation_domain_model.py
---@@ -1,5 +1,6 @@
--- # %%
--- import copy
---+import math
--- import logging
--- import os
--- from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union
---@@ -153,14 +154,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---         Lightning calls this inside the training loop with the data from the training dataloader
---         passed in as `batch`.
---         """
----        p=(self.current_epoch*self.train_size+batch_idx)/(self.train_size*self.hparams.model.max_epochs)
----        self.grad_reverse.scale=2/(1+exp(-10*p))-1
---+        p=(self.current_epoch*self.train_size+batch_idx)/(self.train_size*self.hparams.trainer.max_epochs)
---+        self.grad_reverse.scale=2/(1+math.exp(-10*p))-1
---         loss, _, _ = self._make_step(batch)
---         lr = self._optimizer.param_groups[0]['lr']
--- 
--- 
---         self.log('lr', lr, prog_bar=True)
---         self.log('train_loss', loss)
---+        self.log('gamma', self.grad_reverse.scale)
--- 
---         return {'loss': loss, 'lr': lr}
--- 
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/hparams.yaml
--deleted file mode 100644
--index cbac11e..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/hparams.yaml
--+++ /dev/null
--@@ -1,110 +0,0 @@
---seed: 42
---trainer:
---  gpus: 1
---  num_nodes: 1
---  max_epochs: 10
---  max_steps: null
---  accumulate_grad_batches: 4
---  gradient_clip_val: 0
---  amp_level: O1
---  precision: 16
---  accelerator: ddp
---  checkpoint_callback: false
---  logger: false
---  log_every_n_steps: 1
---  val_check_interval: 1.0
---  resume_from_checkpoint: null
---exp_manager:
---  exp_dir: /home/nxingyu/project/
---  name: Punctuation_with_Domain_discriminator
---  create_tensorboard_logger: true
---  create_checkpoint_callback: true
---base_path: /home/nxingyu/data
---tmp_path: /home/nxingyu/data/tmp
---model:
---  nemo_path: null
---  transformer_path: google/electra-small-discriminator
---  unfrozen: 0
---  maximum_unfrozen: 1
---  unfreeze_step: 1
---  punct_label_ids:
---  - ''
---  - '!'
---  - ','
---  - '-'
---  - .
---  - ':'
---  - ;
---  - '?'
---  - —
---  - …
---  punct_class_weights: false
---  dataset:
---    data_dir: /home/nxingyu/data
---    labelled:
---    - /home/nxingyu/data/ted_talks_processed
---    unlabelled:
---    - /home/nxingyu/data/open_subtitles_processed
---    max_seq_length: 128
---    pad_label: ''
---    ignore_extra_tokens: false
---    ignore_start_end: false
---    use_cache: false
---    num_workers: 4
---    pin_memory: true
---    drop_last: false
---    num_labels: 10
---    num_domains: 2
---    test_unlabelled: true
---    train_ds:
---      shuffle: true
---      num_samples: -1
---      batch_size: 4
---    validation_ds:
---      shuffle: true
---      num_samples: -1
---      batch_size: 4
---  tokenizer:
---    tokenizer_name: google/electra-small-discriminator
---    vocab_file: null
---    tokenizer_model: null
---    special_tokens: null
---  language_model:
---    pretrained_model_name: google/electra-small-discriminator
---    lm_checkpoint: null
---    config_file: null
---    config: null
---  punct_head:
---    punct_num_fc_layers: 1
---    fc_dropout: 0.1
---    activation: relu
---    log_softmax: false
---    use_transformer_init: true
---    loss: dice
---  domain_head:
---    domain_num_fc_layers: 1
---    fc_dropout: 0.1
---    activation: relu
---    log_softmax: false
---    use_transformer_init: true
---    loss: cel
---    gamma: 0
---    pooling: mean_max
---    idx_conditioned_on: 0
---  dice_loss:
---    epsilon: 0.01
---    alpha: 3
---    macro_average: true
---  focal_loss:
---    gamma: 1
---  optim:
---    name: adamw
---    lr: 0.001
---    weight_decay: 0.0
---    sched:
---      name: WarmupAnnealing
---      warmup_steps: null
---      warmup_ratio: 0.1
---      last_epoch: -1
---      monitor: val_loss
---      reduce_on_plateau: false
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt
--deleted file mode 100644
--index 5be2535..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt
--+++ /dev/null
--@@ -1,40 +0,0 @@
---Global seed set to 42
---GPU available: True, used: True
---TPU available: None, using: 0 TPU cores
---LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
---Using native 16bit precision.
---Global seed set to 42
---initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 1.0 K 
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---299 K     Trainable params
---13.2 M    Non-trainable params
---13.5 M    Total params
---Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lr_find_temp_model.ckpt
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 1.0 K 
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---299 K     Trainable params
---13.2 M    Non-trainable params
---13.5 M    Total params
---Epoch 0, global step 5305: val_loss reached 10.18683 (best 10.18683), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/checkpoints/Punctuation_with_Domain_discriminator---val_loss=10.19-epoch=0.ckpt" as top 3
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt
--deleted file mode 100644
--index 2f2fa91..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt
--+++ /dev/null
--@@ -1,19 +0,0 @@
---[NeMo W 2021-02-09 08:26:56 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-09 08:27:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-09 08:34:47 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-09 08:40:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
---[NeMo W 2021-02-09 10:21:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fd0> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-09 10:36:30 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fa0> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-09 11:07:22 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
---      warnings.warn(*args, **kwargs)
---    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt
--deleted file mode 100644
--index 2546dc9..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt
--+++ /dev/null
--@@ -1,21 +0,0 @@
---[NeMo I 2021-02-09 08:26:56 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56
---[NeMo I 2021-02-09 08:26:56 exp_manager:519] TensorboardLogger has been set up
---[NeMo W 2021-02-09 08:26:56 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-09 08:27:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-09 08:34:47 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-09 08:40:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
---[NeMo W 2021-02-09 10:21:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fd0> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-09 10:36:30 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fa0> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-09 11:07:22 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
---      warnings.warn(*args, **kwargs)
---    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/cmd-args.log
--deleted file mode 100644
--index 11a5d8e..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/cmd-args.log
--+++ /dev/null
--@@ -1 +0,0 @@
---main.py
--\ No newline at end of file
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/events.out.tfevents.1612840713.Titan.20119.0 b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/events.out.tfevents.1612840713.Titan.20119.0
--deleted file mode 100644
--index a4eb1d2..0000000
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/events.out.tfevents.1612840713.Titan.20119.0 and /dev/null differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/git-info.log
--deleted file mode 100644
--index 5420ee2..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/git-info.log
--+++ /dev/null
--@@ -1,645 +0,0 @@
---commit hash: 089ad1caa03e468560d6d322ace7a5164a8178f3
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt
---index 2a26724..5be2535 100644
------ a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt
---+++ b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt
---@@ -20,3 +20,21 @@ initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--- 299 K     Trainable params
--- 13.2 M    Non-trainable params
--- 13.5 M    Total params
---+Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lr_find_temp_model.ckpt
---+Global seed set to 42
---+
---+  | Name                | Type                 | Params
---+-------------------------------------------------------------
---+0 | transformer         | ElectraModel         | 13.5 M
---+1 | punct_classifier    | TokenClassifier      | 2.6 K 
---+2 | domain_classifier   | SequenceClassifier   | 1.0 K 
---+3 | punctuation_loss    | FocalDiceLoss        | 0     
---+4 | domain_loss         | CrossEntropyLoss     | 0     
---+5 | agg_loss            | AggregatorLoss       | 0     
---+6 | punct_class_report  | ClassificationReport | 0     
---+7 | domain_class_report | ClassificationReport | 0     
---+-------------------------------------------------------------
---+299 K     Trainable params
---+13.2 M    Non-trainable params
---+13.5 M    Total params
---+Epoch 0, global step 5305: val_loss reached 10.18683 (best 10.18683), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/checkpoints/Punctuation_with_Domain_discriminator---val_loss=10.19-epoch=0.ckpt" as top 3
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt
---index 0f1c742..2f2fa91 100644
------ a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt
---+++ b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt
---@@ -2,3 +2,18 @@
--- [NeMo W 2021-02-09 08:27:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---       warnings.warn(*args, **kwargs)
---     
---+[NeMo W 2021-02-09 08:34:47 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---+      warnings.warn(*args, **kwargs)
---+    
---+[NeMo W 2021-02-09 08:40:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---+      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---+    
---+[NeMo W 2021-02-09 10:21:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fd0> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---+      warnings.warn(warn_msg)
---+    
---+[NeMo W 2021-02-09 10:36:30 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fa0> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---+      warnings.warn(warn_msg)
---+    
---+[NeMo W 2021-02-09 11:07:22 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
---+      warnings.warn(*args, **kwargs)
---+    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt
---index e609b5b..2546dc9 100644
------ a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt
---+++ b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt
---@@ -4,3 +4,18 @@
--- [NeMo W 2021-02-09 08:27:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---       warnings.warn(*args, **kwargs)
---     
---+[NeMo W 2021-02-09 08:34:47 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---+      warnings.warn(*args, **kwargs)
---+    
---+[NeMo W 2021-02-09 08:40:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---+      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---+    
---+[NeMo W 2021-02-09 10:21:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fd0> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---+      warnings.warn(warn_msg)
---+    
---+[NeMo W 2021-02-09 10:36:30 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fa0> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
---+      warnings.warn(warn_msg)
---+    
---+[NeMo W 2021-02-09 11:07:22 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
---+      warnings.warn(*args, **kwargs)
---+    
---diff --git a/experiment/Nemo2Lightning.ipynb b/experiment/Nemo2Lightning.ipynb
---index d2ec988..0dbd499 100644
------ a/experiment/Nemo2Lightning.ipynb
---+++ b/experiment/Nemo2Lightning.ipynb
---@@ -2,7 +2,7 @@
---  "cells": [
---   {
---    "cell_type": "code",
----   "execution_count": 2,
---+   "execution_count": 1,
---    "metadata": {},
---    "outputs": [
---     {
---@@ -11,7 +11,7 @@
---      "text": [
---       "Using device: cuda\n",
---       "\n",
----      "Tesla T4\n",
---+      "GeForce GTX 1080 Ti\n",
---       "Memory Usage:\n",
---       "Allocated: 0.0 GB\n",
---       "Cached:    0.0 GB\n"
---@@ -33,16 +33,16 @@
---   },
---   {
---    "cell_type": "code",
----   "execution_count": 1,
---+   "execution_count": 2,
---    "metadata": {},
---    "outputs": [
---     {
---      "data": {
---       "text/plain": [
----       "{'seed': 42, 'trainer': {'gpus': 1, 'num_nodes': 1, 'max_epochs': 6, 'max_steps': None, 'accumulate_grad_batches': 8, 'gradient_clip_val': 0, 'amp_level': 'O1', 'precision': 16, 'accelerator': 'ddp', 'checkpoint_callback': False, 'logger': False, 'log_every_n_steps': 1, 'val_check_interval': 1.0, 'resume_from_checkpoint': None}, 'exp_manager': {'exp_dir': '/home/nxingyu2/project/', 'name': 'Punctuation_with_Domain_discriminator', 'create_tensorboard_logger': True, 'create_checkpoint_callback': True}, 'base_path': '/home/nxingyu2/data', 'tmp_path': '/home/nxingyu2/data/tmp', 'model': {'nemo_path': None, 'transformer_path': 'google/electra-base-discriminator', 'initial_unfrozen': 1, 'punct_label_ids': ['', '!', ',', '-', '.', ':', ';', '?', '—', '…'], 'dataset': {'data_dir': '/home/nxingyu2/data', 'labelled': ['${base_path}/open_subtitles_processed'], 'unlabelled': ['${base_path}/ted_talks_processed'], 'max_seq_length': 128, 'pad_label': '', 'ignore_extra_tokens': False, 'ignore_start_end': False, 'use_cache': False, 'num_workers': 0, 'pin_memory': True, 'drop_last': False, 'num_labels': 10, 'num_domains': 1, 'test_unlabelled': True, 'train_ds': {'shuffle': True, 'num_samples': -1, 'batch_size': 8}, 'validation_ds': {'shuffle': True, 'num_samples': -1, 'batch_size': 2}}, 'tokenizer': {'tokenizer_name': '${model.language_model.pretrained_model_name}', 'vocab_file': None, 'tokenizer_model': None, 'special_tokens': None}, 'language_model': {'pretrained_model_name': '${model.transformer_path}', 'lm_checkpoint': None, 'config_file': None, 'config': None}, 'punct_head': {'punct_num_fc_layers': 1, 'fc_dropout': 0.1, 'activation': 'relu', 'log_softmax': False, 'use_transformer_init': True, 'loss': 'crf'}, 'domain_head': {'domain_num_fc_layers': 1, 'fc_dropout': 0.1, 'activation': 'relu', 'log_softmax': False, 'use_transformer_init': True, 'loss': 'cel', 'gamma': 0.1}, 'dice_loss': {'epsilon': 0.01, 'alpha': 3, 'macro_average': True}, 'focal_loss': {'gamma': 5}, 'optim': {'name': 'adamw', 'lr': 0.001, 'weight_decay': 0.0, 'sched': {'name': 'WarmupAnnealing', 'warmup_steps': None, 'warmup_ratio': 0.1, 'last_epoch': -1, 'monitor': 'val_loss', 'reduce_on_plateau': False}}}}"
---+       "{'seed': 42, 'trainer': {'gpus': 1, 'num_nodes': 1, 'max_epochs': 10, 'max_steps': None, 'accumulate_grad_batches': 4, 'gradient_clip_val': 0, 'amp_level': 'O1', 'precision': 16, 'accelerator': 'ddp', 'checkpoint_callback': False, 'logger': False, 'log_every_n_steps': 1, 'val_check_interval': 1.0, 'resume_from_checkpoint': None}, 'exp_manager': {'exp_dir': '/home/nxingyu/project/', 'name': 'Punctuation_with_Domain_discriminator', 'create_tensorboard_logger': True, 'create_checkpoint_callback': True}, 'base_path': '/home/nxingyu/data', 'tmp_path': '/home/nxingyu/data/tmp', 'model': {'nemo_path': None, 'transformer_path': 'google/electra-small-discriminator', 'unfrozen': 0, 'maximum_unfrozen': 1, 'unfreeze_step': 1, 'punct_label_ids': ['', '!', ',', '-', '.', ':', ';', '?', '—', '…'], 'punct_class_weights': False, 'dataset': {'data_dir': '/home/nxingyu/data', 'labelled': ['${base_path}/ted_talks_processed'], 'unlabelled': ['${base_path}/open_subtitles_processed'], 'max_seq_length': 128, 'pad_label': '', 'ignore_extra_tokens': False, 'ignore_start_end': False, 'use_cache': False, 'num_workers': 4, 'pin_memory': True, 'drop_last': False, 'num_labels': 10, 'num_domains': 2, 'test_unlabelled': True, 'train_ds': {'shuffle': True, 'num_samples': -1, 'batch_size': 4}, 'validation_ds': {'shuffle': True, 'num_samples': -1, 'batch_size': 4}}, 'tokenizer': {'tokenizer_name': '${model.language_model.pretrained_model_name}', 'vocab_file': None, 'tokenizer_model': None, 'special_tokens': None}, 'language_model': {'pretrained_model_name': '${model.transformer_path}', 'lm_checkpoint': None, 'config_file': None, 'config': None}, 'punct_head': {'punct_num_fc_layers': 1, 'fc_dropout': 0.1, 'activation': 'relu', 'log_softmax': False, 'use_transformer_init': True, 'loss': 'dice'}, 'domain_head': {'domain_num_fc_layers': 1, 'fc_dropout': 0.1, 'activation': 'relu', 'log_softmax': False, 'use_transformer_init': True, 'loss': 'cel', 'gamma': 0, 'pooling': 'mean_max', 'idx_conditioned_on': 0}, 'dice_loss': {'epsilon': 0.01, 'alpha': 3, 'macro_average': True}, 'focal_loss': {'gamma': 1}, 'optim': {'name': 'adamw', 'lr': 0.001, 'weight_decay': 0.0, 'sched': {'name': 'WarmupAnnealing', 'warmup_steps': None, 'warmup_ratio': 0.1, 'last_epoch': -1, 'monitor': 'val_loss', 'reduce_on_plateau': False}}}}"
---       ]
---      },
----     "execution_count": 1,
---+     "execution_count": 2,
---      "metadata": {},
---      "output_type": "execute_result"
---     }
---@@ -74,79 +74,79 @@
---   },
---   {
---    "cell_type": "code",
----   "execution_count": 13,
---+   "execution_count": 3,
---    "metadata": {},
---    "outputs": [
---     {
---      "name": "stderr",
---      "output_type": "stream",
---      "text": [
----      "09:01:12.28 LOG:\n",
----      "09:01:12.30 .... 'cel none' = 'cel none'\n",
----      "09:01:12.31 .... output = tensor([1.5514, 1.5514, 0.5514, 0.5514], grad_fn=<NllLossBackward>)\n",
----      "09:01:12.31 LOG:\n",
----      "09:01:12.31 .... 'cel mean' = 'cel mean'\n",
----      "09:01:12.31 .... output = tensor(1.0514, grad_fn=<NllLossBackward>)\n",
----      "09:01:12.31 LOG:\n",
----      "09:01:12.31 .... 'cel sum' = 'cel sum'\n",
----      "09:01:12.31 .... output = tensor(4.2058, grad_fn=<NllLossBackward>)\n",
----      "09:01:12.31 LOG:\n",
----      "09:01:12.32 .... 'focal sum' = 'focal sum'\n",
----      "09:01:12.32 .... loss(inp, tar) = tensor(6.7352, grad_fn=<SumBackward0>)\n",
----      "09:01:12.32 LOG:\n",
----      "09:01:12.32 .... 'focal mean' = 'focal mean'\n",
----      "09:01:12.32 .... loss(inp, tar) = tensor(0.4210, grad_fn=<MeanBackward0>)\n",
----      "09:01:12.32 LOG:\n",
----      "09:01:12.33 .... 'focal none' = 'focal none'\n",
----      "09:01:12.33 .... loss(inp, tar) = tensor([0.9635, 0.9635, 0.0991, 0.0991], grad_fn=<MulBackward0>)\n",
----      "09:01:12.33 LOG:\n",
----      "09:01:12.33 .... 'focal none' = 'focal none'\n",
----      "09:01:12.33 .... loss(inp, tar) = tensor([0.4716, 0.4716, 0.0075, 0.0075], grad_fn=<MulBackward0>)\n",
----      "09:01:12.33 LOG:\n",
----      "09:01:12.34 .... 'crf,none' = 'crf,none'\n",
----      "09:01:12.34 .... output = tensor([4.2927], grad_fn=<NegBackward>)\n",
----      "09:01:12.34 LOG:\n",
----      "09:01:12.34 .... 'crf,mean' = 'crf,mean'\n",
----      "09:01:12.34 .... output = tensor(4.3138, grad_fn=<NegBackward>)\n",
----      "09:01:12.34 LOG:\n",
----      "09:01:12.35 .... 'crf,sum' = 'crf,sum'\n",
----      "09:01:12.35 .... output = tensor(4.2588, grad_fn=<NegBackward>)\n",
----      "09:01:12.35 LOG:\n",
----      "09:01:12.35 .... 'crf,token_mean' = 'crf,token_mean'\n",
----      "09:01:12.35 .... output = tensor(1.0429, grad_fn=<DivBackward0>)\n",
----      "09:01:12.35 LOG:\n",
----      "09:01:12.35 .... 'dice none,micro' = 'dice none,micro'\n",
----      "09:01:12.36 .... output = tensor(0.1876, grad_fn=<PowBackward0>)\n",
----      "09:01:12.36 LOG:\n",
----      "09:01:12.36 .... 'dice mean,micro' = 'dice mean,micro'\n",
----      "09:01:12.36 .... output = tensor(0.0625, grad_fn=<DivBackward0>)\n",
----      "09:01:12.36 LOG:\n",
----      "09:01:12.36 .... 'dice sum,micro' = 'dice sum,micro'\n",
----      "09:01:12.36 .... output = tensor(0.1876, grad_fn=<SumBackward0>)\n",
----      "09:01:12.37 LOG:\n",
----      "09:01:12.37 .... 'dice sum,micro' = 'dice sum,micro'\n",
----      "09:01:12.37 .... output = tensor(0.0812, grad_fn=<SumBackward0>)\n",
----      "09:01:12.37 LOG:\n",
----      "09:01:12.37 .... 'dice none,macro' = 'dice none,macro'\n",
----      "09:01:12.37 .... loss(inp, tar) = tensor([5.9547, 4.4872, 2.0551], grad_fn=<MulBackward0>)\n",
----      "09:01:12.37 LOG:\n",
----      "09:01:12.38 .... 'dice mean,macro' = 'dice mean,macro'\n",
----      "09:01:12.38 .... loss(inp, tar) = tensor(0.2112, grad_fn=<DivBackward0>)\n",
----      "09:01:12.38 LOG:\n",
----      "09:01:12.38 .... 'dice sum,macro' = 'dice sum,macro'\n",
----      "09:01:12.38 .... loss(inp, tar) = tensor(0.6335, grad_fn=<SumBackward0>)\n",
----      "09:01:12.38 LOG:\n",
----      "09:01:12.38 .... 'dice none,macro' = 'dice none,macro'\n",
----      "09:01:12.39 .... loss(inp, tar) = tensor([0.3340, 0.6546, 0.3057], grad_fn=<MulBackward0>)\n",
----      "09:01:12.39 LOG:\n",
----      "09:01:12.39 .... 'dice none,macro' = 'dice none,macro'\n",
----      "09:01:12.39 .... loss(inp, tar) = tensor([0.0373, 0.2805, 0.0286], grad_fn=<MulBackward0>)\n",
----      "09:01:12.39 LOG:\n",
----      "09:01:12.39 .... 'dice sum,macro' = 'dice sum,macro'\n",
----      "09:01:12.40 .... output = tensor([0.5989, 0.7696, 0.2411], grad_fn=<MulBackward0>)\n",
----      "09:01:12.40 LOG:\n",
----      "09:01:12.40 .... 'dice sum,macro' = 'dice sum,macro'\n",
----      "09:01:12.40 .... output = tensor([0.2148, 0.4559, 0.0140], grad_fn=<MulBackward0>)\n"
---+      "10:11:13.98 LOG:\n",
---+      "10:11:14.02 .... 'cel none' = 'cel none'\n",
---+      "10:11:14.02 .... output = tensor([1.5514, 1.5514, 0.5514, 0.5514], grad_fn=<NllLossBackward>)\n",
---+      "10:11:14.02 LOG:\n",
---+      "10:11:14.02 .... 'cel mean' = 'cel mean'\n",
---+      "10:11:14.02 .... output = tensor(1.0514, grad_fn=<NllLossBackward>)\n",
---+      "10:11:14.03 LOG:\n",
---+      "10:11:14.03 .... 'cel sum' = 'cel sum'\n",
---+      "10:11:14.03 .... output = tensor(4.2058, grad_fn=<NllLossBackward>)\n",
---+      "10:11:14.08 LOG:\n",
---+      "10:11:14.08 .... 'focal sum' = 'focal sum'\n",
---+      "10:11:14.08 .... loss(inp, tar) = tensor(6.7352, grad_fn=<SumBackward0>)\n",
---+      "10:11:14.08 LOG:\n",
---+      "10:11:14.08 .... 'focal mean' = 'focal mean'\n",
---+      "10:11:14.09 .... loss(inp, tar) = tensor(0.4210, grad_fn=<MeanBackward0>)\n",
---+      "10:11:14.09 LOG:\n",
---+      "10:11:14.09 .... 'focal none' = 'focal none'\n",
---+      "10:11:14.10 .... loss(inp, tar) = tensor([0.9635, 0.9635, 0.0991, 0.0991], grad_fn=<MulBackward0>)\n",
---+      "10:11:14.10 LOG:\n",
---+      "10:11:14.10 .... 'focal none' = 'focal none'\n",
---+      "10:11:14.10 .... loss(inp, tar) = tensor([0.4716, 0.4716, 0.0075, 0.0075], grad_fn=<MulBackward0>)\n",
---+      "10:11:14.11 LOG:\n",
---+      "10:11:14.12 .... 'crf,none' = 'crf,none'\n",
---+      "10:11:14.12 .... output = tensor([4.1689], grad_fn=<NegBackward>)\n",
---+      "10:11:14.12 LOG:\n",
---+      "10:11:14.12 .... 'crf,mean' = 'crf,mean'\n",
---+      "10:11:14.12 .... output = tensor(4.0544, grad_fn=<NegBackward>)\n",
---+      "10:11:14.12 LOG:\n",
---+      "10:11:14.13 .... 'crf,sum' = 'crf,sum'\n",
---+      "10:11:14.13 .... output = tensor(4.1272, grad_fn=<NegBackward>)\n",
---+      "10:11:14.13 LOG:\n",
---+      "10:11:14.13 .... 'crf,token_mean' = 'crf,token_mean'\n",
---+      "10:11:14.13 .... output = tensor(1.0815, grad_fn=<DivBackward0>)\n",
---+      "10:11:14.13 LOG:\n",
---+      "10:11:14.14 .... 'dice none,micro' = 'dice none,micro'\n",
---+      "10:11:14.14 .... output = tensor(0.4331, grad_fn=<PowBackward0>)\n",
---+      "10:11:14.14 LOG:\n",
---+      "10:11:14.14 .... 'dice mean,micro' = 'dice mean,micro'\n",
---+      "10:11:14.14 .... output = tensor(0.1444, grad_fn=<DivBackward0>)\n",
---+      "10:11:14.14 LOG:\n",
---+      "10:11:14.14 .... 'dice sum,micro' = 'dice sum,micro'\n",
---+      "10:11:14.15 .... output = tensor(0.4331, grad_fn=<SumBackward0>)\n",
---+      "10:11:14.15 LOG:\n",
---+      "10:11:14.15 .... 'dice sum,micro' = 'dice sum,micro'\n",
---+      "10:11:14.15 .... output = tensor(0.0812, grad_fn=<SumBackward0>)\n",
---+      "10:11:14.15 LOG:\n",
---+      "10:11:14.15 .... 'dice none,macro' = 'dice none,macro'\n",
---+      "10:11:14.16 .... loss(inp, tar) = tensor([5.9547, 4.4872, 2.0551], grad_fn=<MulBackward0>)\n",
---+      "10:11:14.16 LOG:\n",
---+      "10:11:14.16 .... 'dice mean,macro' = 'dice mean,macro'\n",
---+      "10:11:14.16 .... loss(inp, tar) = tensor(0.4314, grad_fn=<DivBackward0>)\n",
---+      "10:11:14.16 LOG:\n",
---+      "10:11:14.16 .... 'dice sum,macro' = 'dice sum,macro'\n",
---+      "10:11:14.16 .... loss(inp, tar) = tensor(1.2943, grad_fn=<SumBackward0>)\n",
---+      "10:11:14.17 LOG:\n",
---+      "10:11:14.17 .... 'dice none,macro' = 'dice none,macro'\n",
---+      "10:11:14.17 .... loss(inp, tar) = tensor([0.3340, 0.6546, 0.3057], grad_fn=<MulBackward0>)\n",
---+      "10:11:14.17 LOG:\n",
---+      "10:11:14.17 .... 'dice none,macro' = 'dice none,macro'\n",
---+      "10:11:14.17 .... loss(inp, tar) = tensor([0.0373, 0.2805, 0.0286], grad_fn=<MulBackward0>)\n",
---+      "10:11:14.18 LOG:\n",
---+      "10:11:14.18 .... 'dice sum,macro' = 'dice sum,macro'\n",
---+      "10:11:14.18 .... output = tensor([0.5989, 0.7696, 0.2411], grad_fn=<MulBackward0>)\n",
---+      "10:11:14.18 LOG:\n",
---+      "10:11:14.18 .... 'dice sum,macro' = 'dice sum,macro'\n",
---+      "10:11:14.18 .... output = tensor([0.2148, 0.4559, 0.0140], grad_fn=<MulBackward0>)\n"
---      ]
---     },
---     {
---@@ -155,7 +155,7 @@
---        "('dice sum,macro', tensor([0.2148, 0.4559, 0.0140], grad_fn=<MulBackward0>))"
---       ]
---      },
----     "execution_count": 13,
---+     "execution_count": 3,
---      "metadata": {},
---      "output_type": "execute_result"
---     }
---@@ -286,32 +286,25 @@
---   },
---   {
---    "cell_type": "code",
----   "execution_count": 2,
---+   "execution_count": 4,
---    "metadata": {},
---    "outputs": [
---     {
----     "name": "stderr",
----     "output_type": "stream",
----     "text": [
----      "10:05:46.40 LOG:\n",
----      "10:05:46.46 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
----      "10:05:46.66 LOG:\n",
----      "10:05:46.66 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
----      "10:06:04.19 LOG:\n",
----      "10:06:04.20 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
----      "10:06:04.34 LOG:\n",
----      "10:06:04.34 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n"
---+     "ename": "KeyboardInterrupt",
---+     "evalue": "",
---+     "output_type": "error",
---+     "traceback": [
---+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
---+      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
---+      "\u001b[0;32m<ipython-input-4-b22a0d7714b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mdata_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
---+      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_prepared_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
---+      "\u001b[0;32m~/project/experiment/data/punctuation_datamodule.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'fit'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             self.train_dataset = PunctuationDomainDatasets(split='train',\n\u001b[0m\u001b[1;32m     64\u001b[0m                     \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                     \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
---+      "\u001b[0;32m~/project/experiment/data/punctuation_dataset_multi.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, split, num_samples, max_seq_length, punct_label_ids, labelled, unlabelled, tokenizer, randomize, data_id, tmp_path)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munlabelled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             dataset=PunctuationDomainDataset(\n\u001b[0m\u001b[1;32m    187\u001b[0m                     \u001b[0mcsv_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'{path}.{split}.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                     \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
---+      "\u001b[0;32m~/project/experiment/data/punctuation_dataset_multi.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, csv_file, tokenizer, num_samples, max_seq_length, degree, punct_label_ids, domain, labelled, randomize, target_file, tmp_path, start, end)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv_file\u001b[0m \u001b[0;34m=\u001b[0m   \u001b[0mcsv_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m   \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m=\u001b[0m  \u001b[0mdomain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunct_label_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpunct_label_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
---+      "\u001b[0;32m~/project/experiment/data/punctuation_dataset_multi.py\u001b[0m in \u001b[0;36mset_num_samples\u001b[0;34m(self, csv_file, num_samples)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-l'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTDOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_samples\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
---+      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1009\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stdin_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m                 \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
---+      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
---      ]
----    },
----    {
----     "data": {
----      "text/plain": [
----       "10609"
----      ]
----     },
----     "execution_count": 2,
----     "metadata": {},
----     "output_type": "execute_result"
---     }
---    ],
---    "source": [
---@@ -343,20 +336,9 @@
---   },
---   {
---    "cell_type": "code",
----   "execution_count": 4,
---+   "execution_count": null,
---    "metadata": {},
----   "outputs": [
----    {
----     "data": {
----      "text/plain": [
----       "10609"
----      ]
----     },
----     "execution_count": 4,
----     "metadata": {},
----     "output_type": "execute_result"
----    }
----   ],
---+   "outputs": [],
---    "source": [
---     "# it=dm.train_dataset\n",
---     "# ni=next(it)\n",
---diff --git a/experiment/core/losses/linear_chain_crf.py b/experiment/core/losses/linear_chain_crf.py
---index ed813a9..8dc59cc 100644
------ a/experiment/core/losses/linear_chain_crf.py
---+++ b/experiment/core/losses/linear_chain_crf.py
---@@ -92,6 +92,17 @@ class LinearChainCRF(torch.nn.Module):
---             mask = logits.new_ones(logits.shape[:2], dtype=torch.bool)
---         return self._viterbi_decode(logits,mask)
--- 
---+    @jit.export
---+    def predict(self, logits: Tensor, mask: Optional[Tensor] = None) -> LongTensor:
---+        self._validate(logits, mask=mask)
---+
---+        if mask is None:
---+            mask = logits.new_ones(logits.shape[:2], dtype=torch.bool)
---+        out=[]
---+        for p,m in iter(zip(logits,mask)):
---+            out.append(pad_to_len(logits.shape[1],self._viterbi_decode(p.unsqueeze(0),m.unsqueeze(0))))
---+        return torch.tensor(out)
---+        
---     def _viterbi_decode(self, logits: Tensor, mask: Tensor) -> LongTensor:
---         """
---         decode labels using viterbi algorithm
---diff --git a/experiment/core/utils.py b/experiment/core/utils.py
---index 058cc87..4be7503 100644
------ a/experiment/core/utils.py
---+++ b/experiment/core/utils.py
---@@ -3,6 +3,7 @@ import torch
--- from torch import nn
--- import regex as re
--- import snoop
---+from copy import deepcopy
--- 
--- __all__ = ['chunk_examples_with_degree', 'chunk_to_len_batch', 'view_aligned']
--- 
---@@ -26,14 +27,15 @@ def position_to_mask(max_seq_length:int,indices:list):
---         o[np.array(indices)%(max_seq_length-2)+1]=1
---     except:
---         pp('position_to_mask',np.array(indices)%(max_seq_length-2)+1)
----        # o[(np.array(indices)%(max_seq_length-2)+1).astype(int)]=1
---+        o[(np.array(indices)%(max_seq_length-2)+1).astype(int)]=1
---     return o
--- 
--- def align_labels_to_mask(mask,labels):
---     '''[0,1,0],[2] -> [0,2,0]'''
---     assert(sum(mask)==len(labels))
----    mask[mask>0]=torch.tensor(labels)
----    return mask.tolist()
---+    m1=mask.copy()
---+    m1[mask>0]=torch.tensor(labels)
---+    return m1.tolist()
--- 
--- def view_aligned(texts,tags,tokenizer,labels_to_ids):
---         return [re.sub(' ##','',' '.join(
---@@ -101,7 +103,9 @@ def chunk_to_len(max_seq_length,tokenizer,tokens,labels=None):
---     split_token_end_idxs=np.array_split(token_end_idxs,breakpoints)
---     split_subwords=np.array_split(subwords,np.arange(max_seq_length-2,len(subwords),max_seq_length-2))
---     ids=[pad_to_len(max_seq_length,tokenizer.convert_tokens_to_ids(['[CLS]']+list(_)+['[SEP]'])) for _ in split_subwords]
----    masks=[position_to_mask(max_seq_length,_) for _ in split_token_end_idxs]
---+    masks=[]
---+    for _ in split_token_end_idxs:
---+        masks.append(position_to_mask(max_seq_length,_).copy())
---     padded_labels=None
---     if labels!=None:
---         split_labels=np.array_split(labels,breakpoints)
---@@ -121,7 +125,7 @@ def chunk_to_len_batch(max_seq_length,tokenizer,tokens,labels=None,labelled=True
---     output = {'input_ids': torch.as_tensor(batch_ids, dtype=torch.long),
---               'attention_mask': torch.as_tensor(batch_ids, dtype=torch.bool),
---               'subtoken_mask': torch.as_tensor(batch_masks,dtype=torch.bool)}
----    output['subtoken_mask']|=(output['input_ids']==101)|(output['input_ids']==102)
---+    output['subtoken_mask']|=((output['input_ids']==101)|(output['input_ids']==102))
---     output['subtoken_mask']&=labelled
---     output['labels']=torch.as_tensor(batch_labels,dtype=torch.long) if labelled==True else torch.zeros_like(output['input_ids'],dtype=torch.long)
---     return output
---diff --git a/experiment/data/punctuation_dataset.py b/experiment/data/punctuation_dataset.py
---index bfd015c..c3d9fb6 100644
------ a/experiment/data/punctuation_dataset.py
---+++ b/experiment/data/punctuation_dataset.py
---@@ -10,6 +10,7 @@ import torch
--- import subprocess
--- from time import time
--- from itertools import cycle
---+import math
--- 
--- class PunctuationDomainDataset(IterableDataset):
--- 
---@@ -242,18 +243,23 @@ class PunctuationInferenceDataset(Dataset):
---             "labels": NeuralType(('B', 'T'), ChannelType()),
---         }
--- 
----    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], degree:int = 0, ):
---+    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], num_samples:int=256, degree:int = 0):
---         """ Initializes BertPunctuationInferDataset. """
---+        self.degree=degree
---+        self.punct_label_ids=punct_label_ids
---         chunked=chunk_examples_with_degree(self.degree, self.punct_label_ids)(queries)
---         features = chunk_to_len_batch(max_seq_length=max_seq_length, tokenizer=tokenizer,tokens=chunked['texts'],labelled=False)
---         self.all_input_ids = features['input_ids']
---         self.all_attention_mask = features['attention_mask']
---         self.all_subtoken_mask = features['subtoken_mask']
---+        self.num_samples=num_samples
--- 
---     def __len__(self):
----        return len(self.all_input_ids)
---+        return math.ceil(len(self.all_input_ids)/self.num_samples)
--- 
---     def __getitem__(self, idx):
----        return {'input_ids':self.all_input_ids[idx],
----                'attention_mask':self.all_attention_mask[idx],
----                'subtoken_mask':self.all_subtoken_mask[idx]}
---+        lower=idx*self.num_samples
---+        upper=min(len(self.all_input_ids),(idx+1)*self.num_samples+1)
---+        return {'input_ids':self.all_input_ids[lower:upper],
---+                'attention_mask':self.all_attention_mask[lower:upper],
---+                'subtoken_mask':self.all_subtoken_mask[lower:upper]}
---diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
---index 4027bb2..b3fe282 100644
------ a/experiment/data/punctuation_dataset_multi.py
---+++ b/experiment/data/punctuation_dataset_multi.py
---@@ -261,18 +261,23 @@ class PunctuationInferenceDataset(Dataset):
---             "labels": NeuralType(('B', 'T'), ChannelType()),
---         }
--- 
----    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], degree:int = 0, ):
---+    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], num_samples:int=256, degree:int = 0):
---         """ Initializes BertPunctuationInferDataset. """
---+        self.degree=degree
---+        self.punct_label_ids=punct_label_ids
---         chunked=chunk_examples_with_degree(self.degree, self.punct_label_ids)(queries)
---         features = chunk_to_len_batch(max_seq_length=max_seq_length, tokenizer=tokenizer,tokens=chunked['texts'],labelled=False)
---         self.all_input_ids = features['input_ids']
---         self.all_attention_mask = features['attention_mask']
---         self.all_subtoken_mask = features['subtoken_mask']
---+        self.num_samples=num_samples
--- 
---     def __len__(self):
----        return len(self.all_input_ids)
---+        return math.ceil(len(self.all_input_ids)/self.num_samples)
--- 
---     def __getitem__(self, idx):
----        return {'input_ids':self.all_input_ids[idx],
----                'attention_mask':self.all_attention_mask[idx],
----                'subtoken_mask':self.all_subtoken_mask[idx]}
---+        lower=idx*self.num_samples
---+        upper=min(len(self.all_input_ids),(idx+1)*self.num_samples+1)
---+        return {'input_ids':self.all_input_ids[lower:upper],
---+                'attention_mask':self.all_attention_mask[lower:upper],
---+                'subtoken_mask':self.all_subtoken_mask[lower:upper]}
---diff --git a/experiment/info.log b/experiment/info.log
---index d9d501b..e69de29 100644
------ a/experiment/info.log
---+++ b/experiment/info.log
---@@ -1,17 +0,0 @@
----[INFO] - shuffling train set
----[INFO] - Optimizer config = AdamW (
----Parameter Group 0
----    amsgrad: False
----    betas: (0.9, 0.999)
----    eps: 1e-08
----    lr: 0.001
----    weight_decay: 0.0
----)
----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f412fde0d90>" 
----will be used during training (effective maximum steps = 80) - 
----Parameters : 
----(warmup_steps: null
----warmup_ratio: 0.1
----last_epoch: -1
----max_steps: 80
----)
---diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
---index c5db7b3..aa05eac 100644
------ a/experiment/models/punctuation_domain_model.py
---+++ b/experiment/models/punctuation_domain_model.py
---@@ -15,9 +15,9 @@ from core.losses import (AggregatorLoss, CrossEntropyLoss, FocalDiceLoss, FocalL
--- from pytorch_lightning.utilities import rank_zero_only
--- from core.optim import get_optimizer, parse_optimizer_args, prepare_lr_scheduler
--- from omegaconf import DictConfig, OmegaConf, open_dict
----from transformers import AutoModel
---+from transformers import AutoModel, AutoTokenizer
--- import torch.utils.data.dataloader as dataloader
----from data import PunctuationDataModule
---+from data import PunctuationDataModule, PunctuationInferenceDataset
--- from os import path
--- import tempfile
--- from core.common import Serialization, FileIO
---@@ -51,6 +51,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---         self._trainer = trainer
--- 
---         self.transformer = AutoModel.from_pretrained(self.hparams.model.transformer_path)
---+        self.tokenizer=AutoTokenizer.from_pretrained(self._cfg.model.transformer_path)
---         self.ids_to_labels = {_[0]: _[1]
---                               for _ in enumerate(self.hparams.model.punct_label_ids)}
---         self.labels_to_ids = {_[1]: _[0]
---@@ -707,4 +708,23 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---             if 'PL_TRAINER_GPUS' in os.environ:
---                 os.environ.pop('PL_TRAINER_GPUS')
--- 
----        super().teardown(stage)
---\ No newline at end of file
---+        super().teardown(stage)
---+
---+    def add_punctuation(self, queries):
---+        infer_ds=PunctuationInferenceDataset(
---+            tokenizer= self._cfg.model.transformer_path,
---+            queries=queries, 
---+            max_seq_length=self.hparams.model.dataset.max_seq_length,
---+            punct_label_ids=self._cfg.model.punct_label_ids)
---+        attention_mask = batch['attention_mask']
---+        subtoken_mask = batch['subtoken_mask']
---+        punct_labels = batch['labels']
---+        domain_labels = batch['domain']
---+        input_ids = batch['input_ids']
---+
---+        labelled_mask=(subtoken_mask[:,0]>0)
---+        test_loss, punct_logits, domain_logits = self._make_step(batch)
---+        # attention_mask = attention_mask > 0.5
---+        punct_preds = self.punctuation_loss.predict(punct_logits[labelled_mask], subtoken_mask[labelled_mask]) \
---+            if self.hparams.model.punct_head.loss == 'crf' else torch.argmax(punct_logits[labelled_mask], axis=-1)[subtoken_mask[labelled_mask]]
---+        return view_aligned(input_ids,punct_preds, self.tokenizer,self.ids_to_labels)
---\ No newline at end of file
---diff --git a/experiment/utils/__init__.py b/experiment/utils/__init__.py
---deleted file mode 100644
---index 9a292b8..0000000
------ a/experiment/utils/__init__.py
---+++ /dev/null
---@@ -1,2 +0,0 @@
----from utils.logging import Logger as _Logger
----logging = _Logger()
---diff --git a/experiment/utils/logging.py b/experiment/utils/logging.py
---deleted file mode 100644
---index 15511fd..0000000
------ a/experiment/utils/logging.py
---+++ /dev/null
---@@ -1,69 +0,0 @@
----import os.path
----import logging
----import traceback
----
----from logging import DEBUG, WARNING, ERROR, INFO
----__all__ = ['Logger']
----
----class Logger(object):
----
----    show_source_location = True
----    # Formats the message as needed and calls the correct logging method
----    # to actually handle it
----    def _raw_log(self, logfn, message, exc_info):
----        cname = ''
----        loc = ''
----        fn = ''
----        tb = traceback.extract_stack()
----        if len(tb) > 2:
----            if self.show_source_location:
----                loc = '(%s:%d):' % (os.path.basename(tb[-3][0]), tb[-3][1])
----            fn = tb[-3][2]
----            if fn != '<module>':
----                if self.__class__.__name__ != Logger.__name__:
----                    fn = self.__class__.__name__ + '.' + fn
----                fn += '()'
----
----        logfn(loc + cname + fn + ': ' + message, exc_info=exc_info)
----
----    def info(self, message, exc_info=False):
----        """
----        Log a info-level message. If exc_info is True, if an exception
----        was caught, show the exception information (message and stack trace).
----        """
----        self._raw_log(logging.info, message, exc_info)
----
----    def debug(self, message, exc_info=False):
----        """
----        Log a debug-level message. If exc_info is True, if an exception
----        was caught, show the exception information (message and stack trace).
----        """
----        self._raw_log(logging.debug, message, exc_info)
----
----    def warning(self, message, exc_info=False):
----        """
----        Log a warning-level message. If exc_info is True, if an exception
----        was caught, show the exception information (message and stack trace).
----        """
----        self._raw_log(logging.warning, message, exc_info)
----
----    def error(self, message, exc_info=False):
----        """
----        Log an error-level message. If exc_info is True, if an exception
----        was caught, show the exception information (message and stack trace).
----        """
----        self._raw_log(logging.error, message, exc_info)
----
----    @staticmethod
----    def basicConfig(level=DEBUG):
----        """
----        Apply a basic logging configuration which outputs the log to the
----        console (stderr). Optionally, the minimum log level can be set, one
----        of DEBUG, WARNING, ERROR (or any of the levels from the logging
----        module). If not set, DEBUG log level is used as minimum.
----        """
----        logging.basicConfig(level=level,
----                format='%(asctime)s %(levelname)s %(message)s',
----                datefmt='%Y-%m-%d %H:%M:%S')
----
----        logger = Logger()
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/hparams.yaml
--deleted file mode 100644
--index cbac11e..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/hparams.yaml
--+++ /dev/null
--@@ -1,110 +0,0 @@
---seed: 42
---trainer:
---  gpus: 1
---  num_nodes: 1
---  max_epochs: 10
---  max_steps: null
---  accumulate_grad_batches: 4
---  gradient_clip_val: 0
---  amp_level: O1
---  precision: 16
---  accelerator: ddp
---  checkpoint_callback: false
---  logger: false
---  log_every_n_steps: 1
---  val_check_interval: 1.0
---  resume_from_checkpoint: null
---exp_manager:
---  exp_dir: /home/nxingyu/project/
---  name: Punctuation_with_Domain_discriminator
---  create_tensorboard_logger: true
---  create_checkpoint_callback: true
---base_path: /home/nxingyu/data
---tmp_path: /home/nxingyu/data/tmp
---model:
---  nemo_path: null
---  transformer_path: google/electra-small-discriminator
---  unfrozen: 0
---  maximum_unfrozen: 1
---  unfreeze_step: 1
---  punct_label_ids:
---  - ''
---  - '!'
---  - ','
---  - '-'
---  - .
---  - ':'
---  - ;
---  - '?'
---  - —
---  - …
---  punct_class_weights: false
---  dataset:
---    data_dir: /home/nxingyu/data
---    labelled:
---    - /home/nxingyu/data/ted_talks_processed
---    unlabelled:
---    - /home/nxingyu/data/open_subtitles_processed
---    max_seq_length: 128
---    pad_label: ''
---    ignore_extra_tokens: false
---    ignore_start_end: false
---    use_cache: false
---    num_workers: 4
---    pin_memory: true
---    drop_last: false
---    num_labels: 10
---    num_domains: 2
---    test_unlabelled: true
---    train_ds:
---      shuffle: true
---      num_samples: -1
---      batch_size: 4
---    validation_ds:
---      shuffle: true
---      num_samples: -1
---      batch_size: 4
---  tokenizer:
---    tokenizer_name: google/electra-small-discriminator
---    vocab_file: null
---    tokenizer_model: null
---    special_tokens: null
---  language_model:
---    pretrained_model_name: google/electra-small-discriminator
---    lm_checkpoint: null
---    config_file: null
---    config: null
---  punct_head:
---    punct_num_fc_layers: 1
---    fc_dropout: 0.1
---    activation: relu
---    log_softmax: false
---    use_transformer_init: true
---    loss: dice
---  domain_head:
---    domain_num_fc_layers: 1
---    fc_dropout: 0.1
---    activation: relu
---    log_softmax: false
---    use_transformer_init: true
---    loss: cel
---    gamma: 0
---    pooling: mean_max
---    idx_conditioned_on: 0
---  dice_loss:
---    epsilon: 0.01
---    alpha: 3
---    macro_average: true
---  focal_loss:
---    gamma: 1
---  optim:
---    name: adamw
---    lr: 0.001
---    weight_decay: 0.0
---    sched:
---      name: WarmupAnnealing
---      warmup_steps: null
---      warmup_ratio: 0.1
---      last_epoch: -1
---      monitor: val_loss
---      reduce_on_plateau: false
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/lightning_logs.txt
--deleted file mode 100644
--index c7d0c2d..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/lightning_logs.txt
--+++ /dev/null
--@@ -1,39 +0,0 @@
---Global seed set to 42
---GPU available: True, used: True
---TPU available: None, using: 0 TPU cores
---LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
---Using native 16bit precision.
---Global seed set to 42
---initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 1.0 K 
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---299 K     Trainable params
---13.2 M    Non-trainable params
---13.5 M    Total params
---Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/lr_find_temp_model.ckpt
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 1.0 K 
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---299 K     Trainable params
---13.2 M    Non-trainable params
---13.5 M    Total params
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_error_log.txt
--deleted file mode 100644
--index 0c8b389..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_error_log.txt
--+++ /dev/null
--@@ -1,10 +0,0 @@
---[NeMo W 2021-02-09 11:10:37 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-09 11:11:08 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-09 11:18:33 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-09 11:24:13 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_log_globalrank-0_localrank-0.txt
--deleted file mode 100644
--index 84d61e5..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_log_globalrank-0_localrank-0.txt
--+++ /dev/null
--@@ -1,12 +0,0 @@
---[NeMo I 2021-02-09 11:10:37 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37
---[NeMo I 2021-02-09 11:10:37 exp_manager:519] TensorboardLogger has been set up
---[NeMo W 2021-02-09 11:10:37 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-09 11:11:08 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-09 11:18:33 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-09 11:24:13 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
--diff --git a/experiment/Untitled.ipynb b/experiment/Untitled.ipynb
--index 1a5526d..f5bb089 100644
----- a/experiment/Untitled.ipynb
--+++ b/experiment/Untitled.ipynb
--@@ -3,33 +3,33 @@
--   {
--    "cell_type": "code",
--    "execution_count": 1,
---   "id": "dense-meaning",
--+   "id": "modern-amplifier",
--    "metadata": {},
--    "outputs": [
--     {
--      "name": "stderr",
--      "output_type": "stream",
--      "text": [
---      "12:16:24.02 LOG:\n"
--+      "14:59:48.20 LOG:\n"
--      ]
--     },
--     {
--      "name": "stdout",
--      "output_type": "stream",
--      "text": [
---      "shuffling <data.punctuation_dataset_multi.PunctuationDomainDataset object at 0x7f86e5bc2220>\n"
--+      "shuffling <data.punctuation_dataset_multi.PunctuationDomainDataset object at 0x7f44ea05e5b0>\n"
--      ]
--     },
--     {
--      "name": "stderr",
--      "output_type": "stream",
--      "text": [
---      "12:16:24.11 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
---      "12:16:24.17 LOG:\n",
---      "12:16:24.48 .... f\"1st {n} encoder layers of transformer frozen\" = '1st 11 encoder layers of transformer frozen'\n",
--+      "14:59:48.28 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
--+      "14:59:48.34 LOG:\n",
--+      "14:59:48.63 .... f\"1st {n} encoder layers of transformer frozen\" = '1st 11 encoder layers of transformer frozen'\n",
--       "GPU available: True, used: False\n",
--       "TPU available: None, using: 0 TPU cores\n",
---      "[NeMo W 2021-02-09 12:16:24 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
--+      "[NeMo W 2021-02-09 14:59:48 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
--       "      warnings.warn(*args, **kwargs)\n",
--       "    \n"
--      ]
--@@ -67,168 +67,27 @@
--     "ids_to_labels = {_[0]:_[1] for _ in enumerate(cfg.model.punct_label_ids)}\n",
--     "\n",
--     "model = PunctuationDomainModel.load_from_checkpoint(\n",
---    "    checkpoint_path=\"/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/checkpoints/Punctuation_with_Domain_discriminator-last.ckpt\")\n",
--+    "    checkpoint_path=\"/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_14-05-14/checkpoints/Punctuation_with_Domain_discriminator-last.ckpt\")\n",
--     "\n",
--     "trainer = pl.Trainer(**cfg.trainer)"
--    ]
--   },
--   {
--    "cell_type": "code",
---   "execution_count": 2,
---   "id": "potential-adrian",
--+   "execution_count": 8,
--+   "id": "hairy-proxy",
--    "metadata": {},
--    "outputs": [
--     {
---     "name": "stderr",
---     "output_type": "stream",
---     "text": [
---      "12:16:24.62 LOG:\n",
---      "12:16:24.69 .... chunk_to_len_batch(max_seq_length, tokenizer,chunked['texts'],chunked['tags'],True) = {'attention_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
---      "12:16:24.69                                                                                                      True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
---      "12:16:24.69                                                                                                      True, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False],\n",
---      "12:16:24.69                                                                                                    [ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False],\n",
---      "12:16:24.69                                                                                                    [ True,  True,  True,  True,  True, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False]]),\n",
---      "12:16:24.69                                                                                             'input_ids': tensor([[  101,  2057,  4149,  2176, 11344,  2028,  7279,  1998,  1037, 14757,\n",
---      "12:16:24.69                                                                                                      2013,  1996,  1050, 17258,  2401,  6718,  3573,  1999,  4203, 10254,\n",
---      "12:16:24.69                                                                                                       102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0],\n",
---      "12:16:24.69                                                                                                    [  101,  2054,  2064,  1045,  2079,  2005,  2017,  2651,   102,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0],\n",
---      "12:16:24.69                                                                                                    [  101,  2129,  2024,  2017,   102,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
---      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0]]),\n",
---      "12:16:24.69                                                                                             'labels': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0],\n",
---      "12:16:24.69                                                                                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0],\n",
---      "12:16:24.69                                                                                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
---      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0]]),\n",
---      "12:16:24.69                                                                                             'subtoken_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
---      "12:16:24.69                                                                                                      True,  True, False, False,  True,  True,  True,  True,  True,  True,\n",
---      "12:16:24.69                                                                                                      True, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False],\n",
---      "12:16:24.69                                                                                                    [ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False],\n",
---      "12:16:24.69                                                                                                    [ True,  True,  True,  True,  True, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
---      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False]])}\n"
---     ]
---    },
---    {
--      "data": {
--       "text/plain": [
---       "['[CLS] we, bought, four- shirts: one, pen, and, a, mug, from, the, nvidia- gear, store, in, santa- clara, [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
---       " '[CLS] what? can— i? do? for? you? today? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
---       " '[CLS] how? are? you? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']"
--+       "[' we bought four shirts, one pen, and a mug from the nvidia gear store in santa clara                                                                                                            ',\n",
--+       " ' what can i do for you today?                                                                                                                        ',\n",
--+       " ' , how are you? ,                                                                                                                           ',\n",
--+       " ' good morning. everyone? how have your weekends been? its a really great day?                                                                                                                  ']"
--       ]
--      },
---     "execution_count": 2,
--+     "execution_count": 8,
--      "metadata": {},
--      "output_type": "execute_result"
--     }
--@@ -238,6 +97,7 @@
--     "    'we bought four shirts one pen and a mug from the nvidia gear store in santa clara',\n",
--     "    'what can i do for you today',\n",
--     "    'how are you',\n",
--+    "    'good morning everyone how have your weekends been its a really great day'\n",
--     "]\n",
--     "inference_results = model.add_punctuation(queries)\n",
--     "inference_results"
--@@ -246,7 +106,7 @@
--   {
--    "cell_type": "code",
--    "execution_count": null,
---   "id": "amateur-production",
--+   "id": "employed-station",
--    "metadata": {},
--    "outputs": [],
--    "source": []
--diff --git a/experiment/config.yaml b/experiment/config.yaml
--index efa7a5d..0aeaa8b 100644
----- a/experiment/config.yaml
--+++ b/experiment/config.yaml
--@@ -41,7 +41,7 @@ tmp_path: /home/nxingyu/data/tmp # /tmp #
-- 
-- model:
--     nemo_path: null
---    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
--+    transformer_path: distilbert-base-uncased #google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
--     unfrozen: 0
--     maximum_unfrozen: 1
--     unfreeze_step: 1
--@@ -67,7 +67,7 @@ model:
--             # - ${base_path}/open_subtitles_processed #  
--         unlabelled:
--             # - ${base_path}/ted_talks_processed #
---            - ${base_path}/open_subtitles_processed #  
--+            # - ${base_path}/open_subtitles_processed #  
--             # parameters for dataset preprocessing
--         max_seq_length: 128
--         pad_label: ''
--@@ -75,11 +75,11 @@ model:
--         ignore_start_end: false
--         use_cache: false
--         # shared among dataloaders
---        num_workers:  4
--+        num_workers:  2
--         pin_memory: true
---        drop_last: false
--+        drop_last: true
--         num_labels: 10
---        num_domains: 2
--+        num_domains: 1
--         test_unlabelled: true
-- 
--         train_ds:
--@@ -114,7 +114,7 @@ model:
--         activation: 'relu'
--         log_softmax: false
--         use_transformer_init: true
---        loss: 'dice'
--+        loss: 'crf'
-- 
--     domain_head:
--         domain_num_fc_layers: 1
--@@ -137,7 +137,7 @@ model:
-- 
--     optim:
--         name: adamw
---        lr: 1e-3
--+        lr: 0.009261935523740748 #1e-3
--         weight_decay: 0.00
--         sched:
--             name: WarmupAnnealing #CyclicLR
--diff --git a/experiment/core/utils.py b/experiment/core/utils.py
--index 4be7503..ce7436b 100644
----- a/experiment/core/utils.py
--+++ b/experiment/core/utils.py
--@@ -38,12 +38,12 @@ def align_labels_to_mask(mask,labels):
--     return m1.tolist()
-- 
-- def view_aligned(texts,tags,tokenizer,labels_to_ids):
---        return [re.sub(' ##','',' '.join(
--+        return [re.sub(r'( ?\[((PAD)|(CLS)|(SEP))\] ?)',' ',re.sub(' ##','',' '.join(
--             [_[0]+_[1] for _ in list(
--                 zip(tokenizer.convert_ids_to_tokens(_[0]),
--                     [labels_to_ids[id] for id in _[1].tolist()])
--             )]
---        )) for _ in zip(texts,tags)]
--+        ))) for _ in zip(texts,tags)]
-- 
-- def text2masks(n, labels_to_ids):
--     def text2masks(text):
--diff --git a/experiment/data/punctuation_datamodule.py b/experiment/data/punctuation_datamodule.py
--index 6978318..20a4093 100644
----- a/experiment/data/punctuation_datamodule.py
--+++ b/experiment/data/punctuation_datamodule.py
--@@ -54,9 +54,10 @@ class PunctuationDataModule(LightningDataModule):
--         self.test_unlabelled=test_unlabelled
--     
--     def reset(self):
---        self.train_dataset.__iter__()
---        self.val_dataset.__iter__()
---        self.test_dataset.__iter__()
--+        # self.setup('fit')
--+        self.train_dataset=iter(self.train_dataset)
--+        self.val_dataset=iter(self.val_dataset)
--+        self.test_dataset=iter(self.test_dataset)
-- 
--     def setup(self, stage=None):
--         if stage=='fit' or stage is None:
--diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
--index 03d661c..88a268f 100644
----- a/experiment/data/punctuation_dataset_multi.py
--+++ b/experiment/data/punctuation_dataset_multi.py
--@@ -167,7 +167,7 @@ class PunctuationDomainDatasets(IterableDataset):
--         self.max_length=max(self.ds_lengths)
--         self.len=int(self.max_length/num_samples)
--         self.per_worker=int(self.max_length/self.num_workers)
---
--+        self.class_weights=None
-- 
--         for i,path in enumerate(labelled):
--             target=os.path.join(tmp_path,os.path.split(path)[1])
--diff --git a/experiment/info.log b/experiment/info.log
--index 69e9a76..e69de29 100644
----- a/experiment/info.log
--+++ b/experiment/info.log
--@@ -1,85 +0,0 @@
---[INFO] - shuffling train set
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.001
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f717ffe5b80>" 
---will be used during training (effective maximum steps = 80) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---last_epoch: -1
---max_steps: 80
---)
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          89.19      22.74      36.24       3012
---! (label_id: 1)                                          0.00       0.00       0.00          1
---, (label_id: 2)                                          7.31      36.21      12.16        243
---- (label_id: 3)                                          2.27      21.43       4.11         28
---. (label_id: 4)                                          1.68       1.65       1.66        182
---: (label_id: 5)                                          0.00       0.00       0.00          5
---; (label_id: 6)                                          0.00       0.00       0.00          3
---? (label_id: 7)                                          0.24      22.22       0.48          9
---— (label_id: 8)                                          0.00       0.00       0.00         10
---… (label_id: 9)                                          0.00       0.00       0.00          1
----------------------
---micro avg                                               22.44      22.44      22.44       3494
---macro avg                                               10.07      10.43       5.47       3494
---weighted avg                                            77.50      22.44      32.21       3494
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                         50.00     100.00      66.67         34
---1 (label_id: 1)                                          0.00       0.00       0.00         34
----------------------
---micro avg                                               50.00      50.00      50.00         68
---macro avg                                               25.00      50.00      33.33         68
---weighted avg                                            25.00      50.00      33.33         68
---
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.007943282347242822
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f717f560fa0>" 
---will be used during training (effective maximum steps = 53050) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---last_epoch: -1
---max_steps: 53050
---)
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          89.19      22.74      36.24       3012
---! (label_id: 1)                                          0.00       0.00       0.00          1
---, (label_id: 2)                                          7.31      36.21      12.16        243
---- (label_id: 3)                                          2.27      21.43       4.11         28
---. (label_id: 4)                                          1.68       1.65       1.66        182
---: (label_id: 5)                                          0.00       0.00       0.00          5
---; (label_id: 6)                                          0.00       0.00       0.00          3
---? (label_id: 7)                                          0.24      22.22       0.48          9
---— (label_id: 8)                                          0.00       0.00       0.00         10
---… (label_id: 9)                                          0.00       0.00       0.00          1
----------------------
---micro avg                                               22.44      22.44      22.44       3494
---macro avg                                               10.07      10.43       5.47       3494
---weighted avg                                            77.50      22.44      32.21       3494
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                         50.00     100.00      66.67         34
---1 (label_id: 1)                                          0.00       0.00       0.00         34
----------------------
---micro avg                                               50.00      50.00      50.00         68
---macro avg                                               25.00      50.00      33.33         68
---weighted avg                                            25.00      50.00      33.33         68
---
--diff --git a/experiment/main.py b/experiment/main.py
--index 6f0a8ea..6b15e25 100644
----- a/experiment/main.py
--+++ b/experiment/main.py
--@@ -39,7 +39,7 @@ def main(cfg: DictConfig)->None:
--     
--     while(model.hparams.model.unfrozen<=cfg.model.maximum_unfrozen and model.hparams.model.unfrozen>=0):
--         trainer.current_epoch=0
---        lr_finder = trainer.tuner.lr_find(model,min_lr=1e-10, max_lr=1e-02, num_training=80, early_stop_threshold=None)
--+        lr_finder = trainer.tuner.lr_find(model,min_lr=1e-8, max_lr=0.5, num_training=80) #, early_stop_threshold=None
--         # Results can be found in
--         pp(lr_finder.results)
--         new_lr = lr_finder.suggestion()
--diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
--index dab395e..999ae0d 100644
----- a/experiment/models/punctuation_domain_model.py
--+++ b/experiment/models/punctuation_domain_model.py
--@@ -130,9 +130,13 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
--         )[0]
--         punct_logits = self.punct_classifier(hidden_states=hidden_states)
--         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
--+        assert not torch.isnan(input_ids).any(), (input_ids,'inputid')
--+        assert not torch.isnan(attention_mask).any(), ('amask',attention_mask)
--+        assert not torch.isnan(hidden_states).any(), (hidden_states,attention_mask.sum(1),'hiddenstate')
--         domain_logits = self.domain_classifier(
--             hidden_states=reverse_grad_hidden_states,
--             attention_mask=attention_mask)
--+        # print(attention_mask.sum(axis=1),domain_logits)
--         return punct_logits, domain_logits
-- 
--     def _make_step(self, batch):
--@@ -157,6 +161,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
--         passed in as `batch`.
--         """
--         p=(self.current_epoch*self.train_size+batch_idx)/(self.train_size*self.hparams.trainer.max_epochs)
--+        if (batch_idx%1000==0):
--+            print('gamma:',p)
--         self.grad_reverse.scale=2/(1+math.exp(-10*p))-1
--         loss, _, _ = self._make_step(batch)
--         lr = self._optimizer.param_groups[0]['lr']
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/hparams.yaml
-deleted file mode 100644
-index 458f253..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/hparams.yaml
-+++ /dev/null
-@@ -1,109 +0,0 @@
--seed: 42
--trainer:
--  gpus: 1
--  num_nodes: 1
--  max_epochs: 10
--  max_steps: null
--  accumulate_grad_batches: 4
--  gradient_clip_val: 0
--  amp_level: O1
--  precision: 16
--  accelerator: ddp
--  checkpoint_callback: false
--  logger: false
--  log_every_n_steps: 1
--  val_check_interval: 1.0
--  resume_from_checkpoint: null
--exp_manager:
--  exp_dir: /home/nxingyu/project/
--  name: Punctuation_with_Domain_discriminator
--  create_tensorboard_logger: true
--  create_checkpoint_callback: true
--base_path: /home/nxingyu/data
--tmp_path: /home/nxingyu/data/tmp
--model:
--  nemo_path: null
--  transformer_path: distilbert-base-uncased
--  unfrozen: 0
--  maximum_unfrozen: 1
--  unfreeze_step: 1
--  punct_label_ids:
--  - ''
--  - '!'
--  - ','
--  - '-'
--  - .
--  - ':'
--  - ;
--  - '?'
--  - —
--  - …
--  punct_class_weights: false
--  dataset:
--    data_dir: /home/nxingyu/data
--    labelled:
--    - /home/nxingyu/data/ted_talks_processed
--    unlabelled: null
--    max_seq_length: 128
--    pad_label: ''
--    ignore_extra_tokens: false
--    ignore_start_end: false
--    use_cache: false
--    num_workers: 2
--    pin_memory: true
--    drop_last: true
--    num_labels: 10
--    num_domains: 1
--    test_unlabelled: true
--    train_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--    validation_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--  tokenizer:
--    tokenizer_name: distilbert-base-uncased
--    vocab_file: null
--    tokenizer_model: null
--    special_tokens: null
--  language_model:
--    pretrained_model_name: distilbert-base-uncased
--    lm_checkpoint: null
--    config_file: null
--    config: null
--  punct_head:
--    punct_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: crf
--  domain_head:
--    domain_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: cel
--    gamma: 0.1
--    pooling: mean_max
--    idx_conditioned_on: 0
--  dice_loss:
--    epsilon: 0.01
--    alpha: 3
--    macro_average: true
--  focal_loss:
--    gamma: 1
--  optim:
--    name: adamw
--    lr: 0.009261935523740748
--    weight_decay: 0.0
--    sched:
--      name: WarmupAnnealing
--      warmup_steps: null
--      warmup_ratio: 0.1
--      last_epoch: -1
--      monitor: val_loss
--      reduce_on_plateau: false
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
-deleted file mode 100644
-index 0f4c210..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
-+++ /dev/null
-@@ -1,91 +0,0 @@
--Global seed set to 42
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
--Using native 16bit precision.
--Global seed set to 42
--initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | DistilBertModel      | 66.4 M
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--3 | punctuation_loss    | LinearChainCRF       | 120   
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--10.9 K    Trainable params
--66.4 M    Non-trainable params
--66.4 M    Total params
--Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lr_find_temp_model.ckpt
--Global seed set to 42
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | DistilBertModel      | 66.4 M
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--3 | punctuation_loss    | LinearChainCRF       | 120   
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--10.9 K    Trainable params
--66.4 M    Non-trainable params
--66.4 M    Total params
--Epoch 0, global step 200: val_loss reached 108.49911 (best 108.49911), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=108.50-epoch=0.ckpt" as top 3
--Epoch 1, global step 400: val_loss reached 155.59180 (best 108.49911), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=155.59-epoch=1.ckpt" as top 3
--Epoch 2, global step 600: val_loss reached 46.96625 (best 46.96625), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=46.97-epoch=2.ckpt" as top 3
--Epoch 3, global step 800: val_loss reached 63.33499 (best 46.96625), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=63.33-epoch=3.ckpt" as top 3
--Epoch 4, global step 1000: val_loss reached 33.09097 (best 33.09097), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=33.09-epoch=4.ckpt" as top 3
--Epoch 5, global step 1200: val_loss reached 30.02221 (best 30.02221), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=30.02-epoch=5.ckpt" as top 3
--Epoch 6, global step 1400: val_loss reached 31.40887 (best 30.02221), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=31.41-epoch=6.ckpt" as top 3
--Epoch 7, step 1600: val_loss was not in top 3
--Epoch 8, global step 1800: val_loss reached 28.61345 (best 28.61345), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=28.61-epoch=8.ckpt" as top 3
--Epoch 9, global step 2000: val_loss reached 26.27475 (best 26.27475), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=26.27-epoch=9.ckpt" as top 3
--Global seed set to 42
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | DistilBertModel      | 66.4 M
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--3 | punctuation_loss    | LinearChainCRF       | 120   
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--7.1 M     Trainable params
--59.3 M    Non-trainable params
--66.4 M    Total params
--LR finder stopped early due to diverging loss.
--Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lr_find_temp_model.ckpt
--Failed to compute suggesting for `lr`. There might not be enough points.
--Traceback (most recent call last):
--  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
--    min_grad = np.gradient(loss).argmin()
--  File "<__array_function__ internals>", line 5, in gradient
--  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
--    raise ValueError(
--ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
--Global seed set to 42
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | DistilBertModel      | 66.4 M
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--3 | punctuation_loss    | LinearChainCRF       | 120   
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--7.1 M     Trainable params
--59.3 M    Non-trainable params
--66.4 M    Total params
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
-deleted file mode 100644
-index cab9655..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
-+++ /dev/null
-@@ -1,42 +0,0 @@
--[NeMo W 2021-02-09 15:09:54 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 15:10:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:10:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:10:09 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:10:11 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-09 15:11:51 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:11:51 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:11:52 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:11:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-09 15:16:02 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce8610> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 15:16:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce40a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 15:57:31 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--      warnings.warn(*args, **kwargs)
--    
--Failed to compute suggesting for `lr`. There might not be enough points.
--Traceback (most recent call last):
--  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
--    min_grad = np.gradient(loss).argmin()
--  File "<__array_function__ internals>", line 5, in gradient
--  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
--    raise ValueError(
--ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
-deleted file mode 100644
-index d19ea43..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
-+++ /dev/null
-@@ -1,36 +0,0 @@
--[NeMo I 2021-02-09 15:09:54 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54
--[NeMo I 2021-02-09 15:09:54 exp_manager:519] TensorboardLogger has been set up
--[NeMo W 2021-02-09 15:09:54 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 15:10:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:10:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:10:09 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:10:11 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-09 15:11:51 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:11:51 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:11:52 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:11:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-09 15:16:02 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce8610> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 15:16:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce40a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 15:57:31 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--      warnings.warn(*args, **kwargs)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/cmd-args.log
-deleted file mode 100644
-index 11a5d8e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/cmd-args.log
-+++ /dev/null
-@@ -1 +0,0 @@
--main.py
-\ No newline at end of file
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/git-info.log
-deleted file mode 100644
-index 4d85d01..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/git-info.log
-+++ /dev/null
-@@ -1,334 +0,0 @@
--commit hash: 836632f0fdebe90f93105efab295fe4cd83af4ca
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0 b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0
--index c3ff071..0e8637d 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0 differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt
--index 7f6eddf..a1b895b 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt
--@@ -39,3 +39,53 @@ Global seed set to 42
-- 66.4 M    Total params
-- Epoch 0, global step 200: val_loss reached 0.26631 (best 0.26631), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=0.ckpt" as top 3
-- Epoch 1, global step 400: val_loss reached 0.26336 (best 0.26336), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=1.ckpt" as top 3
--+Epoch 2, global step 600: val_loss reached 0.26109 (best 0.26109), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=2.ckpt" as top 3
--+Epoch 3, global step 800: val_loss reached 0.25805 (best 0.25805), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=3.ckpt" as top 3
--+Epoch 4, global step 1000: val_loss reached 0.25547 (best 0.25547), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=4.ckpt" as top 3
--+Epoch 5, global step 1200: val_loss reached 0.25339 (best 0.25339), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.25-epoch=5.ckpt" as top 3
--+Epoch 6, global step 1400: val_loss reached 0.24810 (best 0.24810), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.25-epoch=6.ckpt" as top 3
--+Epoch 7, global step 1600: val_loss reached 0.24127 (best 0.24127), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=7.ckpt" as top 3
--+Epoch 8, global step 1800: val_loss reached 0.23864 (best 0.23864), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=8.ckpt" as top 3
--+Epoch 9, global step 2000: val_loss reached 0.23646 (best 0.23646), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=9.ckpt" as top 3
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | DistilBertModel      | 66.4 M
--+1 | punct_classifier    | TokenClassifier      | 7.7 K 
--+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--+3 | punctuation_loss    | FocalDiceLoss        | 0     
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+7.1 M     Trainable params
--+59.3 M    Non-trainable params
--+66.4 M    Total params
--+LR finder stopped early due to diverging loss.
--+Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lr_find_temp_model.ckpt
--+Failed to compute suggesting for `lr`. There might not be enough points.
--+Traceback (most recent call last):
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
--+    min_grad = np.gradient(loss).argmin()
--+  File "<__array_function__ internals>", line 5, in gradient
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
--+    raise ValueError(
--+ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | DistilBertModel      | 66.4 M
--+1 | punct_classifier    | TokenClassifier      | 7.7 K 
--+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--+3 | punctuation_loss    | FocalDiceLoss        | 0     
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+7.1 M     Trainable params
--+59.3 M    Non-trainable params
--+66.4 M    Total params
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt
--index 2d9ccfb..a9e0895 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt
--@@ -14,3 +14,14 @@
-- [NeMo W 2021-02-09 15:11:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fa1136ae0a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 15:25:45 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--+      warnings.warn(*args, **kwargs)
--+    
--+Failed to compute suggesting for `lr`. There might not be enough points.
--+Traceback (most recent call last):
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
--+    min_grad = np.gradient(loss).argmin()
--+  File "<__array_function__ internals>", line 5, in gradient
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
--+    raise ValueError(
--+ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt
--index 80a7030..dfff301 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt
--@@ -16,3 +16,6 @@
-- [NeMo W 2021-02-09 15:11:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fa1136ae0a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 15:25:45 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--+      warnings.warn(*args, **kwargs)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0
--index bb4d846..b06f296 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
--index 2869be1..b9491d6 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
--@@ -37,3 +37,10 @@ Global seed set to 42
-- 10.9 K    Trainable params
-- 66.4 M    Non-trainable params
-- 66.4 M    Total params
--+Epoch 0, global step 200: val_loss reached 108.49911 (best 108.49911), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=108.50-epoch=0.ckpt" as top 3
--+Epoch 1, global step 400: val_loss reached 155.59180 (best 108.49911), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=155.59-epoch=1.ckpt" as top 3
--+Epoch 2, global step 600: val_loss reached 46.96625 (best 46.96625), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=46.97-epoch=2.ckpt" as top 3
--+Epoch 3, global step 800: val_loss reached 63.33499 (best 46.96625), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=63.33-epoch=3.ckpt" as top 3
--+Epoch 4, global step 1000: val_loss reached 33.09097 (best 33.09097), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=33.09-epoch=4.ckpt" as top 3
--+Epoch 5, global step 1200: val_loss reached 30.02221 (best 30.02221), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=30.02-epoch=5.ckpt" as top 3
--+Epoch 6, global step 1400: val_loss reached 31.40887 (best 30.02221), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=31.41-epoch=6.ckpt" as top 3
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
--index 568694f..6bd1322 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
--@@ -23,3 +23,9 @@
-- [NeMo W 2021-02-09 15:11:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--     
--+[NeMo W 2021-02-09 15:16:02 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce8610> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--+[NeMo W 2021-02-09 15:16:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce40a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
--index 7533c2c..e5eaea5 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
--@@ -25,3 +25,9 @@
-- [NeMo W 2021-02-09 15:11:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--     
--+[NeMo W 2021-02-09 15:16:02 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce8610> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--+[NeMo W 2021-02-09 15:16:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce40a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--diff --git a/experiment/config.yaml b/experiment/config.yaml
--index 0aeaa8b..670f79a 100644
----- a/experiment/config.yaml
--+++ b/experiment/config.yaml
--@@ -2,10 +2,10 @@ seed: 42
-- trainer:
--     gpus: 1 # the number of gpus, 0 for CPU
--     num_nodes: 1
---    max_epochs: 10
--+    max_epochs: 2
--     max_steps: null # precedence over max_epochs
--     accumulate_grad_batches: 4 # accumulates grads every k batches
---    gradient_clip_val: 0
--+    gradient_clip_val: 1
--     amp_level: O1 # O1/O2 for mixed precision
--     precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--     accelerator: ddp
--@@ -41,7 +41,7 @@ tmp_path: /home/nxingyu/data/tmp # /tmp #
-- 
-- model:
--     nemo_path: null
---    transformer_path: distilbert-base-uncased #google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
--+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
--     unfrozen: 0
--     maximum_unfrozen: 1
--     unfreeze_step: 1
--@@ -114,7 +114,7 @@ model:
--         activation: 'relu'
--         log_softmax: false
--         use_transformer_init: true
---        loss: 'crf'
--+        loss: 'dice'
-- 
--     domain_head:
--         domain_num_fc_layers: 1
--@@ -129,7 +129,7 @@ model:
--     
--     dice_loss:
--         epsilon: 0.01
---        alpha: 3
--+        alpha: 1
--         macro_average: true
-- 
--     focal_loss: 
--diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
--index 88a268f..31923de 100644
----- a/experiment/data/punctuation_dataset_multi.py
--+++ b/experiment/data/punctuation_dataset_multi.py
--@@ -165,8 +165,8 @@ class PunctuationDomainDatasets(IterableDataset):
--         for path in labelled+unlabelled:
--             self.ds_lengths.append(int(subprocess.Popen(['wc', '-l', f'{path}.{split}.csv'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0]))
--         self.max_length=max(self.ds_lengths)
---        self.len=int(self.max_length/num_samples)
--         self.per_worker=int(self.max_length/self.num_workers)
--+        self.len=int(self.per_worker/num_samples)
--         self.class_weights=None
-- 
--         for i,path in enumerate(labelled):
--diff --git a/experiment/info.log b/experiment/info.log
--index ff2d606..e69de29 100644
----- a/experiment/info.log
--+++ b/experiment/info.log
--@@ -1,133 +0,0 @@
---[INFO] - shuffling train set
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.009261935523740748
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f596c78a3d0>" 
---will be used during training (effective maximum steps = 80) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---last_epoch: -1
---max_steps: 80
---)
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          80.85       4.94       9.30       1540
---! (label_id: 1)                                          0.00       0.00       0.00          2
---, (label_id: 2)                                          7.69       1.41       2.38        142
---- (label_id: 3)                                          1.67      20.00       3.08         20
---. (label_id: 4)                                          6.82      12.50       8.82         96
---: (label_id: 5)                                          0.00       0.00       0.00          0
---; (label_id: 6)                                          0.00       0.00       0.00          0
---? (label_id: 7)                                          0.00       0.00       0.00          4
---— (label_id: 8)                                          0.00       0.00       0.00          0
---… (label_id: 9)                                          0.00       0.00       0.00          0
----------------------
---micro avg                                                5.21       5.21       5.21       1804
---macro avg                                               16.17       6.47       3.93       1804
---weighted avg                                            70.01       5.21       8.63       1804
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00         20
----------------------
---micro avg                                              100.00     100.00     100.00         20
---macro avg                                              100.00     100.00     100.00         20
---weighted avg                                           100.00     100.00     100.00         20
---
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.0024506370946974477
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f596c570f40>" 
---will be used during training (effective maximum steps = 2000) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---last_epoch: -1
---max_steps: 2000
---)
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          80.85       4.94       9.30       1540
---! (label_id: 1)                                          0.00       0.00       0.00          2
---, (label_id: 2)                                          7.69       1.41       2.38        142
---- (label_id: 3)                                          1.67      20.00       3.08         20
---. (label_id: 4)                                          6.82      12.50       8.82         96
---: (label_id: 5)                                          0.00       0.00       0.00          0
---; (label_id: 6)                                          0.00       0.00       0.00          0
---? (label_id: 7)                                          0.00       0.00       0.00          4
---— (label_id: 8)                                          0.00       0.00       0.00          0
---… (label_id: 9)                                          0.00       0.00       0.00          0
----------------------
---micro avg                                                5.21       5.21       5.21       1804
---macro avg                                               16.17       6.47       3.93       1804
---weighted avg                                            70.01       5.21       8.63       1804
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00         20
----------------------
---micro avg                                              100.00     100.00     100.00         20
---macro avg                                              100.00     100.00     100.00         20
---weighted avg                                           100.00     100.00     100.00         20
---
---NFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          90.44      95.02      92.67     179900
---! (label_id: 1)                                          0.00       0.00       0.00         52
---, (label_id: 2)                                         21.92       9.58      13.34      13074
---- (label_id: 3)                                         44.94       6.69      11.64       1062
---. (label_id: 4)                                         28.64      30.78      29.67      11077
---: (label_id: 5)                                          0.00       0.00       0.00        330
---; (label_id: 6)                                          0.00       0.00       0.00        108
---? (label_id: 7)                                          0.00       0.00       0.00        899
---— (label_id: 8)                                          0.00       0.00       0.00        276
---… (label_id: 9)                                          0.00       0.00       0.00         28
----------------------
---micro avg                                               84.95      84.95      84.95     206806
---macro avg                                               18.59      14.21      14.73     206806
---weighted avg                                            81.82      84.95      83.11     206806
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1959
----------------------
---micro avg                                              100.00     100.00     100.00       1959
---macro avg                                              100.00     100.00     100.00       1959
---weighted avg                                           100.00     100.00     100.00       1959
---
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          90.62      94.69      92.61     179900
---! (label_id: 1)                                          0.00       0.00       0.00         52
---, (label_id: 2)                                         21.80      10.36      14.04      13074
---- (label_id: 3)                                         44.94       6.69      11.64       1062
---. (label_id: 4)                                         28.46      32.05      30.15      11077
---: (label_id: 5)                                          0.00       0.00       0.00        330
---; (label_id: 6)                                          0.00       0.00       0.00        108
---? (label_id: 7)                                          0.00       0.00       0.00        899
---— (label_id: 8)                                          0.00       0.00       0.00        276
---… (label_id: 9)                                          0.00       0.00       0.00         28
----------------------
---micro avg                                               84.77      84.77      84.77     206806
---macro avg                                               18.58      14.38      14.84     206806
---weighted avg                                            81.97      84.77      83.12     206806
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1959
----------------------
---micro avg                                              100.00     100.00     100.00       1959
---macro avg                                              100.00     100.00     100.00       1959
---weighted avg                                           100.00     100.00     100.00       1959
---
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/lightning_logs.txt
-deleted file mode 100644
-index 1562e1d..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/lightning_logs.txt
-+++ /dev/null
-@@ -1,22 +0,0 @@
--Global seed set to 42
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
--Using native 16bit precision.
--Global seed set to 42
--initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 513   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--299 K     Trainable params
--13.2 M    Non-trainable params
--13.5 M    Total params
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/nemo_error_log.txt
-deleted file mode 100644
-index 60f6fbd..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/nemo_error_log.txt
-+++ /dev/null
-@@ -1,10 +0,0 @@
--[NeMo W 2021-02-09 15:46:00 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 15:46:12 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:46:12 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:46:13 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/nemo_log_globalrank-0_localrank-0.txt
-deleted file mode 100644
-index 6aba001..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00/nemo_log_globalrank-0_localrank-0.txt
-+++ /dev/null
-@@ -1,12 +0,0 @@
--[NeMo I 2021-02-09 15:46:00 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-46-00
--[NeMo I 2021-02-09 15:46:00 exp_manager:519] TensorboardLogger has been set up
--[NeMo W 2021-02-09 15:46:00 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 15:46:12 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:46:12 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:46:13 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/cmd-args.log
-deleted file mode 100644
-index 11a5d8e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/cmd-args.log
-+++ /dev/null
-@@ -1 +0,0 @@
--main.py
-\ No newline at end of file
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/events.out.tfevents.1612856909.Titan.20492.0 b/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/events.out.tfevents.1612856909.Titan.20492.0
-deleted file mode 100644
-index 603495e..0000000
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/events.out.tfevents.1612856909.Titan.20492.0 and /dev/null differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/git-info.log
-deleted file mode 100644
-index cafa992..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/git-info.log
-+++ /dev/null
-@@ -1,355 +0,0 @@
--commit hash: 836632f0fdebe90f93105efab295fe4cd83af4ca
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0 b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0
--index c3ff071..0e8637d 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0 differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt
--index 7f6eddf..a1b895b 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt
--@@ -39,3 +39,53 @@ Global seed set to 42
-- 66.4 M    Total params
-- Epoch 0, global step 200: val_loss reached 0.26631 (best 0.26631), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=0.ckpt" as top 3
-- Epoch 1, global step 400: val_loss reached 0.26336 (best 0.26336), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=1.ckpt" as top 3
--+Epoch 2, global step 600: val_loss reached 0.26109 (best 0.26109), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=2.ckpt" as top 3
--+Epoch 3, global step 800: val_loss reached 0.25805 (best 0.25805), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=3.ckpt" as top 3
--+Epoch 4, global step 1000: val_loss reached 0.25547 (best 0.25547), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=4.ckpt" as top 3
--+Epoch 5, global step 1200: val_loss reached 0.25339 (best 0.25339), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.25-epoch=5.ckpt" as top 3
--+Epoch 6, global step 1400: val_loss reached 0.24810 (best 0.24810), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.25-epoch=6.ckpt" as top 3
--+Epoch 7, global step 1600: val_loss reached 0.24127 (best 0.24127), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=7.ckpt" as top 3
--+Epoch 8, global step 1800: val_loss reached 0.23864 (best 0.23864), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=8.ckpt" as top 3
--+Epoch 9, global step 2000: val_loss reached 0.23646 (best 0.23646), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=9.ckpt" as top 3
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | DistilBertModel      | 66.4 M
--+1 | punct_classifier    | TokenClassifier      | 7.7 K 
--+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--+3 | punctuation_loss    | FocalDiceLoss        | 0     
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+7.1 M     Trainable params
--+59.3 M    Non-trainable params
--+66.4 M    Total params
--+LR finder stopped early due to diverging loss.
--+Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lr_find_temp_model.ckpt
--+Failed to compute suggesting for `lr`. There might not be enough points.
--+Traceback (most recent call last):
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
--+    min_grad = np.gradient(loss).argmin()
--+  File "<__array_function__ internals>", line 5, in gradient
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
--+    raise ValueError(
--+ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | DistilBertModel      | 66.4 M
--+1 | punct_classifier    | TokenClassifier      | 7.7 K 
--+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--+3 | punctuation_loss    | FocalDiceLoss        | 0     
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+7.1 M     Trainable params
--+59.3 M    Non-trainable params
--+66.4 M    Total params
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt
--index 2d9ccfb..a9e0895 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt
--@@ -14,3 +14,14 @@
-- [NeMo W 2021-02-09 15:11:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fa1136ae0a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 15:25:45 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--+      warnings.warn(*args, **kwargs)
--+    
--+Failed to compute suggesting for `lr`. There might not be enough points.
--+Traceback (most recent call last):
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
--+    min_grad = np.gradient(loss).argmin()
--+  File "<__array_function__ internals>", line 5, in gradient
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
--+    raise ValueError(
--+ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt
--index 80a7030..dfff301 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt
--@@ -16,3 +16,6 @@
-- [NeMo W 2021-02-09 15:11:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fa1136ae0a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 15:25:45 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--+      warnings.warn(*args, **kwargs)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0
--index bb4d846..19d4690 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
--index 2869be1..b9491d6 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
--@@ -37,3 +37,10 @@ Global seed set to 42
-- 10.9 K    Trainable params
-- 66.4 M    Non-trainable params
-- 66.4 M    Total params
--+Epoch 0, global step 200: val_loss reached 108.49911 (best 108.49911), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=108.50-epoch=0.ckpt" as top 3
--+Epoch 1, global step 400: val_loss reached 155.59180 (best 108.49911), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=155.59-epoch=1.ckpt" as top 3
--+Epoch 2, global step 600: val_loss reached 46.96625 (best 46.96625), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=46.97-epoch=2.ckpt" as top 3
--+Epoch 3, global step 800: val_loss reached 63.33499 (best 46.96625), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=63.33-epoch=3.ckpt" as top 3
--+Epoch 4, global step 1000: val_loss reached 33.09097 (best 33.09097), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=33.09-epoch=4.ckpt" as top 3
--+Epoch 5, global step 1200: val_loss reached 30.02221 (best 30.02221), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=30.02-epoch=5.ckpt" as top 3
--+Epoch 6, global step 1400: val_loss reached 31.40887 (best 30.02221), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=31.41-epoch=6.ckpt" as top 3
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
--index 568694f..6bd1322 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
--@@ -23,3 +23,9 @@
-- [NeMo W 2021-02-09 15:11:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--     
--+[NeMo W 2021-02-09 15:16:02 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce8610> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--+[NeMo W 2021-02-09 15:16:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce40a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
--index 7533c2c..e5eaea5 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
--@@ -25,3 +25,9 @@
-- [NeMo W 2021-02-09 15:11:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--     
--+[NeMo W 2021-02-09 15:16:02 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce8610> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--+[NeMo W 2021-02-09 15:16:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce40a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--diff --git a/experiment/config.yaml b/experiment/config.yaml
--index 0aeaa8b..670f79a 100644
----- a/experiment/config.yaml
--+++ b/experiment/config.yaml
--@@ -2,10 +2,10 @@ seed: 42
-- trainer:
--     gpus: 1 # the number of gpus, 0 for CPU
--     num_nodes: 1
---    max_epochs: 10
--+    max_epochs: 2
--     max_steps: null # precedence over max_epochs
--     accumulate_grad_batches: 4 # accumulates grads every k batches
---    gradient_clip_val: 0
--+    gradient_clip_val: 1
--     amp_level: O1 # O1/O2 for mixed precision
--     precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--     accelerator: ddp
--@@ -41,7 +41,7 @@ tmp_path: /home/nxingyu/data/tmp # /tmp #
-- 
-- model:
--     nemo_path: null
---    transformer_path: distilbert-base-uncased #google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
--+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
--     unfrozen: 0
--     maximum_unfrozen: 1
--     unfreeze_step: 1
--@@ -114,7 +114,7 @@ model:
--         activation: 'relu'
--         log_softmax: false
--         use_transformer_init: true
---        loss: 'crf'
--+        loss: 'dice'
-- 
--     domain_head:
--         domain_num_fc_layers: 1
--@@ -129,7 +129,7 @@ model:
--     
--     dice_loss:
--         epsilon: 0.01
---        alpha: 3
--+        alpha: 1
--         macro_average: true
-- 
--     focal_loss: 
--diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
--index 88a268f..31923de 100644
----- a/experiment/data/punctuation_dataset_multi.py
--+++ b/experiment/data/punctuation_dataset_multi.py
--@@ -165,8 +165,8 @@ class PunctuationDomainDatasets(IterableDataset):
--         for path in labelled+unlabelled:
--             self.ds_lengths.append(int(subprocess.Popen(['wc', '-l', f'{path}.{split}.csv'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0]))
--         self.max_length=max(self.ds_lengths)
---        self.len=int(self.max_length/num_samples)
--         self.per_worker=int(self.max_length/self.num_workers)
--+        self.len=int(self.per_worker/num_samples)
--         self.class_weights=None
-- 
--         for i,path in enumerate(labelled):
--diff --git a/experiment/info.log b/experiment/info.log
--index ff2d606..e69de29 100644
----- a/experiment/info.log
--+++ b/experiment/info.log
--@@ -1,133 +0,0 @@
---[INFO] - shuffling train set
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.009261935523740748
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f596c78a3d0>" 
---will be used during training (effective maximum steps = 80) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---last_epoch: -1
---max_steps: 80
---)
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          80.85       4.94       9.30       1540
---! (label_id: 1)                                          0.00       0.00       0.00          2
---, (label_id: 2)                                          7.69       1.41       2.38        142
---- (label_id: 3)                                          1.67      20.00       3.08         20
---. (label_id: 4)                                          6.82      12.50       8.82         96
---: (label_id: 5)                                          0.00       0.00       0.00          0
---; (label_id: 6)                                          0.00       0.00       0.00          0
---? (label_id: 7)                                          0.00       0.00       0.00          4
---— (label_id: 8)                                          0.00       0.00       0.00          0
---… (label_id: 9)                                          0.00       0.00       0.00          0
----------------------
---micro avg                                                5.21       5.21       5.21       1804
---macro avg                                               16.17       6.47       3.93       1804
---weighted avg                                            70.01       5.21       8.63       1804
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00         20
----------------------
---micro avg                                              100.00     100.00     100.00         20
---macro avg                                              100.00     100.00     100.00         20
---weighted avg                                           100.00     100.00     100.00         20
---
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.0024506370946974477
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f596c570f40>" 
---will be used during training (effective maximum steps = 2000) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---last_epoch: -1
---max_steps: 2000
---)
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          80.85       4.94       9.30       1540
---! (label_id: 1)                                          0.00       0.00       0.00          2
---, (label_id: 2)                                          7.69       1.41       2.38        142
---- (label_id: 3)                                          1.67      20.00       3.08         20
---. (label_id: 4)                                          6.82      12.50       8.82         96
---: (label_id: 5)                                          0.00       0.00       0.00          0
---; (label_id: 6)                                          0.00       0.00       0.00          0
---? (label_id: 7)                                          0.00       0.00       0.00          4
---— (label_id: 8)                                          0.00       0.00       0.00          0
---… (label_id: 9)                                          0.00       0.00       0.00          0
----------------------
---micro avg                                                5.21       5.21       5.21       1804
---macro avg                                               16.17       6.47       3.93       1804
---weighted avg                                            70.01       5.21       8.63       1804
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00         20
----------------------
---micro avg                                              100.00     100.00     100.00         20
---macro avg                                              100.00     100.00     100.00         20
---weighted avg                                           100.00     100.00     100.00         20
---
---NFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          90.44      95.02      92.67     179900
---! (label_id: 1)                                          0.00       0.00       0.00         52
---, (label_id: 2)                                         21.92       9.58      13.34      13074
---- (label_id: 3)                                         44.94       6.69      11.64       1062
---. (label_id: 4)                                         28.64      30.78      29.67      11077
---: (label_id: 5)                                          0.00       0.00       0.00        330
---; (label_id: 6)                                          0.00       0.00       0.00        108
---? (label_id: 7)                                          0.00       0.00       0.00        899
---— (label_id: 8)                                          0.00       0.00       0.00        276
---… (label_id: 9)                                          0.00       0.00       0.00         28
----------------------
---micro avg                                               84.95      84.95      84.95     206806
---macro avg                                               18.59      14.21      14.73     206806
---weighted avg                                            81.82      84.95      83.11     206806
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1959
----------------------
---micro avg                                              100.00     100.00     100.00       1959
---macro avg                                              100.00     100.00     100.00       1959
---weighted avg                                           100.00     100.00     100.00       1959
---
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          90.62      94.69      92.61     179900
---! (label_id: 1)                                          0.00       0.00       0.00         52
---, (label_id: 2)                                         21.80      10.36      14.04      13074
---- (label_id: 3)                                         44.94       6.69      11.64       1062
---. (label_id: 4)                                         28.46      32.05      30.15      11077
---: (label_id: 5)                                          0.00       0.00       0.00        330
---; (label_id: 6)                                          0.00       0.00       0.00        108
---? (label_id: 7)                                          0.00       0.00       0.00        899
---— (label_id: 8)                                          0.00       0.00       0.00        276
---… (label_id: 9)                                          0.00       0.00       0.00         28
----------------------
---micro avg                                               84.77      84.77      84.77     206806
---macro avg                                               18.58      14.38      14.84     206806
---weighted avg                                            81.97      84.77      83.12     206806
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1959
----------------------
---micro avg                                              100.00     100.00     100.00       1959
---macro avg                                              100.00     100.00     100.00       1959
---weighted avg                                           100.00     100.00     100.00       1959
---
--diff --git a/experiment/main.py b/experiment/main.py
--index 6b15e25..a779a56 100644
----- a/experiment/main.py
--+++ b/experiment/main.py
--@@ -39,11 +39,11 @@ def main(cfg: DictConfig)->None:
--     
--     while(model.hparams.model.unfrozen<=cfg.model.maximum_unfrozen and model.hparams.model.unfrozen>=0):
--         trainer.current_epoch=0
---        lr_finder = trainer.tuner.lr_find(model,min_lr=1e-8, max_lr=0.5, num_training=80) #, early_stop_threshold=None
---        # Results can be found in
---        pp(lr_finder.results)
---        new_lr = lr_finder.suggestion()
---        model.hparams.model.optim.lr = new_lr
--+        # lr_finder = trainer.tuner.lr_find(model,min_lr=1e-8, max_lr=0.5, num_training=80) #, early_stop_threshold=None
--+        # # Results can be found in
--+        # pp(lr_finder.results)
--+        # new_lr = lr_finder.suggestion()
--+        # model.hparams.model.optim.lr = new_lr
--         model.dm.reset()
--         trainer.current_epoch=0
--         trainer.fit(model)
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/hparams.yaml
-deleted file mode 100644
-index a595688..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/hparams.yaml
-+++ /dev/null
-@@ -1,109 +0,0 @@
--seed: 42
--trainer:
--  gpus: 1
--  num_nodes: 1
--  max_epochs: 2
--  max_steps: null
--  accumulate_grad_batches: 4
--  gradient_clip_val: 1
--  amp_level: O1
--  precision: 16
--  accelerator: ddp
--  checkpoint_callback: false
--  logger: false
--  log_every_n_steps: 1
--  val_check_interval: 1.0
--  resume_from_checkpoint: null
--exp_manager:
--  exp_dir: /home/nxingyu/project/
--  name: Punctuation_with_Domain_discriminator
--  create_tensorboard_logger: true
--  create_checkpoint_callback: true
--base_path: /home/nxingyu/data
--tmp_path: /home/nxingyu/data/tmp
--model:
--  nemo_path: null
--  transformer_path: google/electra-small-discriminator
--  unfrozen: 0
--  maximum_unfrozen: 1
--  unfreeze_step: 1
--  punct_label_ids:
--  - ''
--  - '!'
--  - ','
--  - '-'
--  - .
--  - ':'
--  - ;
--  - '?'
--  - —
--  - …
--  punct_class_weights: false
--  dataset:
--    data_dir: /home/nxingyu/data
--    labelled:
--    - /home/nxingyu/data/ted_talks_processed
--    unlabelled: null
--    max_seq_length: 128
--    pad_label: ''
--    ignore_extra_tokens: false
--    ignore_start_end: false
--    use_cache: false
--    num_workers: 2
--    pin_memory: true
--    drop_last: true
--    num_labels: 10
--    num_domains: 1
--    test_unlabelled: true
--    train_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--    validation_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--  tokenizer:
--    tokenizer_name: google/electra-small-discriminator
--    vocab_file: null
--    tokenizer_model: null
--    special_tokens: null
--  language_model:
--    pretrained_model_name: google/electra-small-discriminator
--    lm_checkpoint: null
--    config_file: null
--    config: null
--  punct_head:
--    punct_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: dice
--  domain_head:
--    domain_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: cel
--    gamma: 0.1
--    pooling: mean_max
--    idx_conditioned_on: 0
--  dice_loss:
--    epsilon: 0.01
--    alpha: 1
--    macro_average: true
--  focal_loss:
--    gamma: 1
--  optim:
--    name: adamw
--    lr: 0.009261935523740748
--    weight_decay: 0.0
--    sched:
--      name: WarmupAnnealing
--      warmup_steps: null
--      warmup_ratio: 0.1
--      last_epoch: -1
--      monitor: val_loss
--      reduce_on_plateau: false
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/lightning_logs.txt
-deleted file mode 100644
-index 84e5133..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/lightning_logs.txt
-+++ /dev/null
-@@ -1,47 +0,0 @@
--Global seed set to 42
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
--Using native 16bit precision.
--Global seed set to 42
--initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 513   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--299 K     Trainable params
--13.2 M    Non-trainable params
--13.5 M    Total params
--Epoch 0, global step 199: val_loss reached 0.28475 (best 0.28475), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.28-epoch=0.ckpt" as top 3
--Epoch 1, global step 399: val_loss reached 0.27252 (best 0.27252), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=1.ckpt" as top 3
--Saving latest checkpoint...
--Global seed set to 42
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 513   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--825 K     Trainable params
--12.7 M    Non-trainable params
--13.5 M    Total params
--Epoch 0, global step 599: val_loss reached 0.27252 (best 0.27252), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=0.ckpt" as top 3
--Epoch 1, global step 799: val_loss reached 0.27252 (best 0.27252), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=1-v0.ckpt" as top 3
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--Using environment variable NODE_RANK for node rank (0).
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/nemo_error_log.txt
-deleted file mode 100644
-index 8fd05fc..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/nemo_error_log.txt
-+++ /dev/null
-@@ -1,25 +0,0 @@
--[NeMo W 2021-02-09 15:48:17 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 15:48:30 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:48:30 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:48:31 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:51:08 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4fa07d5970> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 15:51:22 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4fa9ce8850> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 15:54:23 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-09 15:59:50 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 16:00:08 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4fa9ce85e0> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/nemo_log_globalrank-0_localrank-0.txt
-deleted file mode 100644
-index d2d8b9e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17/nemo_log_globalrank-0_localrank-0.txt
-+++ /dev/null
-@@ -1,27 +0,0 @@
--[NeMo I 2021-02-09 15:48:17 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-48-17
--[NeMo I 2021-02-09 15:48:17 exp_manager:519] TensorboardLogger has been set up
--[NeMo W 2021-02-09 15:48:17 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 15:48:30 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:48:30 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:48:31 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:51:08 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4fa07d5970> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 15:51:22 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4fa9ce8850> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 15:54:23 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-09 15:59:50 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 16:00:08 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4fa9ce85e0> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/cmd-args.log
-deleted file mode 100644
-index 11a5d8e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/cmd-args.log
-+++ /dev/null
-@@ -1 +0,0 @@
--main.py
-\ No newline at end of file
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/events.out.tfevents.1612858892.Titan.27659.0 b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/events.out.tfevents.1612858892.Titan.27659.0
-deleted file mode 100644
-index e8dc27e..0000000
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/events.out.tfevents.1612858892.Titan.27659.0 and /dev/null differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/git-info.log
-deleted file mode 100644
-index 3570852..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/git-info.log
-+++ /dev/null
-@@ -1,449 +0,0 @@
--commit hash: 836632f0fdebe90f93105efab295fe4cd83af4ca
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0 b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0
--index c3ff071..0e8637d 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0 differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt
--index 7f6eddf..a1b895b 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt
--@@ -39,3 +39,53 @@ Global seed set to 42
-- 66.4 M    Total params
-- Epoch 0, global step 200: val_loss reached 0.26631 (best 0.26631), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=0.ckpt" as top 3
-- Epoch 1, global step 400: val_loss reached 0.26336 (best 0.26336), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=1.ckpt" as top 3
--+Epoch 2, global step 600: val_loss reached 0.26109 (best 0.26109), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=2.ckpt" as top 3
--+Epoch 3, global step 800: val_loss reached 0.25805 (best 0.25805), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=3.ckpt" as top 3
--+Epoch 4, global step 1000: val_loss reached 0.25547 (best 0.25547), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=4.ckpt" as top 3
--+Epoch 5, global step 1200: val_loss reached 0.25339 (best 0.25339), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.25-epoch=5.ckpt" as top 3
--+Epoch 6, global step 1400: val_loss reached 0.24810 (best 0.24810), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.25-epoch=6.ckpt" as top 3
--+Epoch 7, global step 1600: val_loss reached 0.24127 (best 0.24127), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=7.ckpt" as top 3
--+Epoch 8, global step 1800: val_loss reached 0.23864 (best 0.23864), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=8.ckpt" as top 3
--+Epoch 9, global step 2000: val_loss reached 0.23646 (best 0.23646), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=9.ckpt" as top 3
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | DistilBertModel      | 66.4 M
--+1 | punct_classifier    | TokenClassifier      | 7.7 K 
--+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--+3 | punctuation_loss    | FocalDiceLoss        | 0     
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+7.1 M     Trainable params
--+59.3 M    Non-trainable params
--+66.4 M    Total params
--+LR finder stopped early due to diverging loss.
--+Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lr_find_temp_model.ckpt
--+Failed to compute suggesting for `lr`. There might not be enough points.
--+Traceback (most recent call last):
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
--+    min_grad = np.gradient(loss).argmin()
--+  File "<__array_function__ internals>", line 5, in gradient
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
--+    raise ValueError(
--+ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | DistilBertModel      | 66.4 M
--+1 | punct_classifier    | TokenClassifier      | 7.7 K 
--+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--+3 | punctuation_loss    | FocalDiceLoss        | 0     
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+7.1 M     Trainable params
--+59.3 M    Non-trainable params
--+66.4 M    Total params
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt
--index 2d9ccfb..a9e0895 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt
--@@ -14,3 +14,14 @@
-- [NeMo W 2021-02-09 15:11:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fa1136ae0a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 15:25:45 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--+      warnings.warn(*args, **kwargs)
--+    
--+Failed to compute suggesting for `lr`. There might not be enough points.
--+Traceback (most recent call last):
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
--+    min_grad = np.gradient(loss).argmin()
--+  File "<__array_function__ internals>", line 5, in gradient
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
--+    raise ValueError(
--+ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt
--index 80a7030..dfff301 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt
--@@ -16,3 +16,6 @@
-- [NeMo W 2021-02-09 15:11:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fa1136ae0a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 15:25:45 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--+      warnings.warn(*args, **kwargs)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0
--index bb4d846..53377de 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
--index 2869be1..0f4c210 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
--@@ -37,3 +37,55 @@ Global seed set to 42
-- 10.9 K    Trainable params
-- 66.4 M    Non-trainable params
-- 66.4 M    Total params
--+Epoch 0, global step 200: val_loss reached 108.49911 (best 108.49911), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=108.50-epoch=0.ckpt" as top 3
--+Epoch 1, global step 400: val_loss reached 155.59180 (best 108.49911), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=155.59-epoch=1.ckpt" as top 3
--+Epoch 2, global step 600: val_loss reached 46.96625 (best 46.96625), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=46.97-epoch=2.ckpt" as top 3
--+Epoch 3, global step 800: val_loss reached 63.33499 (best 46.96625), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=63.33-epoch=3.ckpt" as top 3
--+Epoch 4, global step 1000: val_loss reached 33.09097 (best 33.09097), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=33.09-epoch=4.ckpt" as top 3
--+Epoch 5, global step 1200: val_loss reached 30.02221 (best 30.02221), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=30.02-epoch=5.ckpt" as top 3
--+Epoch 6, global step 1400: val_loss reached 31.40887 (best 30.02221), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=31.41-epoch=6.ckpt" as top 3
--+Epoch 7, step 1600: val_loss was not in top 3
--+Epoch 8, global step 1800: val_loss reached 28.61345 (best 28.61345), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=28.61-epoch=8.ckpt" as top 3
--+Epoch 9, global step 2000: val_loss reached 26.27475 (best 26.27475), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=26.27-epoch=9.ckpt" as top 3
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | DistilBertModel      | 66.4 M
--+1 | punct_classifier    | TokenClassifier      | 7.7 K 
--+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--+3 | punctuation_loss    | LinearChainCRF       | 120   
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+7.1 M     Trainable params
--+59.3 M    Non-trainable params
--+66.4 M    Total params
--+LR finder stopped early due to diverging loss.
--+Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lr_find_temp_model.ckpt
--+Failed to compute suggesting for `lr`. There might not be enough points.
--+Traceback (most recent call last):
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
--+    min_grad = np.gradient(loss).argmin()
--+  File "<__array_function__ internals>", line 5, in gradient
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
--+    raise ValueError(
--+ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | DistilBertModel      | 66.4 M
--+1 | punct_classifier    | TokenClassifier      | 7.7 K 
--+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--+3 | punctuation_loss    | LinearChainCRF       | 120   
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+7.1 M     Trainable params
--+59.3 M    Non-trainable params
--+66.4 M    Total params
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
--index 568694f..cab9655 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
--@@ -23,3 +23,20 @@
-- [NeMo W 2021-02-09 15:11:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--     
--+[NeMo W 2021-02-09 15:16:02 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce8610> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--+[NeMo W 2021-02-09 15:16:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce40a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--+[NeMo W 2021-02-09 15:57:31 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--+      warnings.warn(*args, **kwargs)
--+    
--+Failed to compute suggesting for `lr`. There might not be enough points.
--+Traceback (most recent call last):
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
--+    min_grad = np.gradient(loss).argmin()
--+  File "<__array_function__ internals>", line 5, in gradient
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
--+    raise ValueError(
--+ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
--index 7533c2c..d19ea43 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
--@@ -25,3 +25,12 @@
-- [NeMo W 2021-02-09 15:11:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--     
--+[NeMo W 2021-02-09 15:16:02 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce8610> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--+[NeMo W 2021-02-09 15:16:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce40a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--+[NeMo W 2021-02-09 15:57:31 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--+      warnings.warn(*args, **kwargs)
--+    
--diff --git a/experiment/config.yaml b/experiment/config.yaml
--index 0aeaa8b..7a28782 100644
----- a/experiment/config.yaml
--+++ b/experiment/config.yaml
--@@ -41,7 +41,7 @@ tmp_path: /home/nxingyu/data/tmp # /tmp #
-- 
-- model:
--     nemo_path: null
---    transformer_path: distilbert-base-uncased #google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
--+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
--     unfrozen: 0
--     maximum_unfrozen: 1
--     unfreeze_step: 1
--@@ -75,7 +75,7 @@ model:
--         ignore_start_end: false
--         use_cache: false
--         # shared among dataloaders
---        num_workers:  2
--+        num_workers:  4
--         pin_memory: true
--         drop_last: true
--         num_labels: 10
--@@ -114,7 +114,7 @@ model:
--         activation: 'relu'
--         log_softmax: false
--         use_transformer_init: true
---        loss: 'crf'
--+        loss: 'dice'
-- 
--     domain_head:
--         domain_num_fc_layers: 1
--diff --git a/experiment/data/punctuation_datamodule.py b/experiment/data/punctuation_datamodule.py
--index 20a4093..8711456 100644
----- a/experiment/data/punctuation_datamodule.py
--+++ b/experiment/data/punctuation_datamodule.py
--@@ -108,7 +108,8 @@ class PunctuationDataModule(LightningDataModule):
-- 
--         logging.info(f"shuffling train set")
--         # self.train_dataset.shuffle(randomize=False)
---        self.train_dataset.shuffle(randomize=True, seed=self.seed)
--+        if (self.train_shuffle):
--+            self.train_dataset.shuffle(randomize=True, seed=self.seed)
-- 
--     def train_dataloader(self):
--         return DataLoader(self.train_dataset,batch_size=None,num_workers=self.num_workers,pin_memory=self.pin_memory,drop_last=self.drop_last)
--diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
--index 88a268f..31923de 100644
----- a/experiment/data/punctuation_dataset_multi.py
--+++ b/experiment/data/punctuation_dataset_multi.py
--@@ -165,8 +165,8 @@ class PunctuationDomainDatasets(IterableDataset):
--         for path in labelled+unlabelled:
--             self.ds_lengths.append(int(subprocess.Popen(['wc', '-l', f'{path}.{split}.csv'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0]))
--         self.max_length=max(self.ds_lengths)
---        self.len=int(self.max_length/num_samples)
--         self.per_worker=int(self.max_length/self.num_workers)
--+        self.len=int(self.per_worker/num_samples)
--         self.class_weights=None
-- 
--         for i,path in enumerate(labelled):
--diff --git a/experiment/info.log b/experiment/info.log
--index ff2d606..e69de29 100644
----- a/experiment/info.log
--+++ b/experiment/info.log
--@@ -1,133 +0,0 @@
---[INFO] - shuffling train set
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.009261935523740748
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f596c78a3d0>" 
---will be used during training (effective maximum steps = 80) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---last_epoch: -1
---max_steps: 80
---)
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          80.85       4.94       9.30       1540
---! (label_id: 1)                                          0.00       0.00       0.00          2
---, (label_id: 2)                                          7.69       1.41       2.38        142
---- (label_id: 3)                                          1.67      20.00       3.08         20
---. (label_id: 4)                                          6.82      12.50       8.82         96
---: (label_id: 5)                                          0.00       0.00       0.00          0
---; (label_id: 6)                                          0.00       0.00       0.00          0
---? (label_id: 7)                                          0.00       0.00       0.00          4
---— (label_id: 8)                                          0.00       0.00       0.00          0
---… (label_id: 9)                                          0.00       0.00       0.00          0
----------------------
---micro avg                                                5.21       5.21       5.21       1804
---macro avg                                               16.17       6.47       3.93       1804
---weighted avg                                            70.01       5.21       8.63       1804
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00         20
----------------------
---micro avg                                              100.00     100.00     100.00         20
---macro avg                                              100.00     100.00     100.00         20
---weighted avg                                           100.00     100.00     100.00         20
---
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.0024506370946974477
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f596c570f40>" 
---will be used during training (effective maximum steps = 2000) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---last_epoch: -1
---max_steps: 2000
---)
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          80.85       4.94       9.30       1540
---! (label_id: 1)                                          0.00       0.00       0.00          2
---, (label_id: 2)                                          7.69       1.41       2.38        142
---- (label_id: 3)                                          1.67      20.00       3.08         20
---. (label_id: 4)                                          6.82      12.50       8.82         96
---: (label_id: 5)                                          0.00       0.00       0.00          0
---; (label_id: 6)                                          0.00       0.00       0.00          0
---? (label_id: 7)                                          0.00       0.00       0.00          4
---— (label_id: 8)                                          0.00       0.00       0.00          0
---… (label_id: 9)                                          0.00       0.00       0.00          0
----------------------
---micro avg                                                5.21       5.21       5.21       1804
---macro avg                                               16.17       6.47       3.93       1804
---weighted avg                                            70.01       5.21       8.63       1804
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00         20
----------------------
---micro avg                                              100.00     100.00     100.00         20
---macro avg                                              100.00     100.00     100.00         20
---weighted avg                                           100.00     100.00     100.00         20
---
---NFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          90.44      95.02      92.67     179900
---! (label_id: 1)                                          0.00       0.00       0.00         52
---, (label_id: 2)                                         21.92       9.58      13.34      13074
---- (label_id: 3)                                         44.94       6.69      11.64       1062
---. (label_id: 4)                                         28.64      30.78      29.67      11077
---: (label_id: 5)                                          0.00       0.00       0.00        330
---; (label_id: 6)                                          0.00       0.00       0.00        108
---? (label_id: 7)                                          0.00       0.00       0.00        899
---— (label_id: 8)                                          0.00       0.00       0.00        276
---… (label_id: 9)                                          0.00       0.00       0.00         28
----------------------
---micro avg                                               84.95      84.95      84.95     206806
---macro avg                                               18.59      14.21      14.73     206806
---weighted avg                                            81.82      84.95      83.11     206806
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1959
----------------------
---micro avg                                              100.00     100.00     100.00       1959
---macro avg                                              100.00     100.00     100.00       1959
---weighted avg                                           100.00     100.00     100.00       1959
---
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          90.62      94.69      92.61     179900
---! (label_id: 1)                                          0.00       0.00       0.00         52
---, (label_id: 2)                                         21.80      10.36      14.04      13074
---- (label_id: 3)                                         44.94       6.69      11.64       1062
---. (label_id: 4)                                         28.46      32.05      30.15      11077
---: (label_id: 5)                                          0.00       0.00       0.00        330
---; (label_id: 6)                                          0.00       0.00       0.00        108
---? (label_id: 7)                                          0.00       0.00       0.00        899
---— (label_id: 8)                                          0.00       0.00       0.00        276
---… (label_id: 9)                                          0.00       0.00       0.00         28
----------------------
---micro avg                                               84.77      84.77      84.77     206806
---macro avg                                               18.58      14.38      14.84     206806
---weighted avg                                            81.97      84.77      83.12     206806
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1959
----------------------
---micro avg                                              100.00     100.00     100.00       1959
---macro avg                                              100.00     100.00     100.00       1959
---weighted avg                                           100.00     100.00     100.00       1959
---
--diff --git a/experiment/main.py b/experiment/main.py
--index 6b15e25..1f44193 100644
----- a/experiment/main.py
--+++ b/experiment/main.py
--@@ -26,7 +26,7 @@ def main(cfg: DictConfig)->None:
--     data_id = str(int(time()))
--     def savecounter():
--         # pp(os.system(f'rm -r {cfg.model.dataset.data_dir}/*.{data_id}.csv'))
---        pp(os.system(f'rm -r {cfg.tmp_path}/*.{data_id}.csv'))
--+        pp(os.system(f'rm -r {cfg.tmp_path}/*.{data_id}*'))
--     atexit.register(savecounter)
-- 
--     cfg.model.maximum_unfrozen=max(cfg.model.maximum_unfrozen,cfg.model.unfrozen)
--@@ -37,14 +37,34 @@ def main(cfg: DictConfig)->None:
--     exp_manager(trainer, cfg.exp_manager)
--     model = PunctuationDomainModel(cfg=cfg, trainer=trainer, data_id = data_id)
--     
--+    # lr_finder_dm=PunctuationDataModule(
--+    #         tokenizer= cfg.model.transformer_path,
--+    #         labelled= list(cfg.model.dataset.labelled),
--+    #         unlabelled= list(cfg.model.dataset.unlabelled),
--+    #         punct_label_ids= {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)},
--+    #         train_batch_size= cfg.model.dataset.train_ds.batch_size,
--+    #         max_seq_length= cfg.model.dataset.max_seq_length,
--+    #         val_batch_size= cfg.model.dataset.validation_ds.batch_size,
--+    #         num_workers= 1,
--+    #         pin_memory= False,
--+    #         train_shuffle= True,
--+    #         val_shuffle= False,
--+    #         seed=cfg.seed,
--+    #         data_id=data_id+'lr',
--+    #         tmp_path=cfg.tmp_path,
--+    #         test_unlabelled=False,
--+    #     )
--+    lrs=[1e-4,1e-6]
--     while(model.hparams.model.unfrozen<=cfg.model.maximum_unfrozen and model.hparams.model.unfrozen>=0):
---        trainer.current_epoch=0
---        lr_finder = trainer.tuner.lr_find(model,min_lr=1e-8, max_lr=0.5, num_training=80) #, early_stop_threshold=None
---        # Results can be found in
---        pp(lr_finder.results)
---        new_lr = lr_finder.suggestion()
---        model.hparams.model.optim.lr = new_lr
---        model.dm.reset()
--+        # trainer.current_epoch=0
--+        # lr_finder = trainer.tuner.lr_find(model,datamodule=lr_finder_dm,min_lr=1e-8, max_lr=0.5, num_training=80) #, early_stop_threshold=None
--+        # # Results can be found in
--+        # pp(lr_finder.results)
--+        # new_lr = lr_finder.suggestion()
--+        # model.hparams.model.optim.lr = new_lr
--+        # lr_finder_dm.reset()
--+        # model.dm.reset()
--+        model.hparams.model.optim.lr = lrs[model.hparams.model.unfrozen]
--         trainer.current_epoch=0
--         trainer.fit(model)
--         try:
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/hparams.yaml
-deleted file mode 100644
-index 4922992..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/hparams.yaml
-+++ /dev/null
-@@ -1,109 +0,0 @@
--seed: 42
--trainer:
--  gpus: 1
--  num_nodes: 1
--  max_epochs: 10
--  max_steps: null
--  accumulate_grad_batches: 4
--  gradient_clip_val: 0
--  amp_level: O1
--  precision: 16
--  accelerator: ddp
--  checkpoint_callback: false
--  logger: false
--  log_every_n_steps: 1
--  val_check_interval: 1.0
--  resume_from_checkpoint: null
--exp_manager:
--  exp_dir: /home/nxingyu/project/
--  name: Punctuation_with_Domain_discriminator
--  create_tensorboard_logger: true
--  create_checkpoint_callback: true
--base_path: /home/nxingyu/data
--tmp_path: /home/nxingyu/data/tmp
--model:
--  nemo_path: null
--  transformer_path: google/electra-small-discriminator
--  unfrozen: 0
--  maximum_unfrozen: 1
--  unfreeze_step: 1
--  punct_label_ids:
--  - ''
--  - '!'
--  - ','
--  - '-'
--  - .
--  - ':'
--  - ;
--  - '?'
--  - —
--  - …
--  punct_class_weights: false
--  dataset:
--    data_dir: /home/nxingyu/data
--    labelled:
--    - /home/nxingyu/data/ted_talks_processed
--    unlabelled: null
--    max_seq_length: 128
--    pad_label: ''
--    ignore_extra_tokens: false
--    ignore_start_end: false
--    use_cache: false
--    num_workers: 4
--    pin_memory: true
--    drop_last: true
--    num_labels: 10
--    num_domains: 1
--    test_unlabelled: true
--    train_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--    validation_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--  tokenizer:
--    tokenizer_name: google/electra-small-discriminator
--    vocab_file: null
--    tokenizer_model: null
--    special_tokens: null
--  language_model:
--    pretrained_model_name: google/electra-small-discriminator
--    lm_checkpoint: null
--    config_file: null
--    config: null
--  punct_head:
--    punct_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: dice
--  domain_head:
--    domain_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: cel
--    gamma: 0.1
--    pooling: mean_max
--    idx_conditioned_on: 0
--  dice_loss:
--    epsilon: 0.01
--    alpha: 3
--    macro_average: true
--  focal_loss:
--    gamma: 1
--  optim:
--    name: adamw
--    lr: 0.009261935523740748
--    weight_decay: 0.0
--    sched:
--      name: WarmupAnnealing
--      warmup_steps: null
--      warmup_ratio: 0.1
--      last_epoch: -1
--      monitor: val_loss
--      reduce_on_plateau: false
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/lightning_logs.txt
-deleted file mode 100644
-index 40964a1..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/lightning_logs.txt
-+++ /dev/null
-@@ -1,63 +0,0 @@
--Global seed set to 42
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
--Using native 16bit precision.
--Global seed set to 42
--initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 513   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--299 K     Trainable params
--13.2 M    Non-trainable params
--13.5 M    Total params
--Epoch 0, global step 199: val_loss reached 0.66148 (best 0.66148), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.66-epoch=0.ckpt" as top 3
--Epoch 1, global step 399: val_loss reached 0.23663 (best 0.23663), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=1.ckpt" as top 3
--Epoch 2, global step 599: val_loss reached 0.17288 (best 0.17288), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.17-epoch=2.ckpt" as top 3
--Epoch 3, global step 799: val_loss reached 0.15670 (best 0.15670), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.16-epoch=3.ckpt" as top 3
--Epoch 4, global step 999: val_loss reached 0.14782 (best 0.14782), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.15-epoch=4.ckpt" as top 3
--Epoch 5, global step 1199: val_loss reached 0.14246 (best 0.14246), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=5.ckpt" as top 3
--Epoch 6, global step 1399: val_loss reached 0.13934 (best 0.13934), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=6.ckpt" as top 3
--Epoch 7, global step 1599: val_loss reached 0.13714 (best 0.13714), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=7.ckpt" as top 3
--Epoch 8, global step 1799: val_loss reached 0.13633 (best 0.13633), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=8.ckpt" as top 3
--Epoch 9, global step 1999: val_loss reached 0.13605 (best 0.13605), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=9.ckpt" as top 3
--Saving latest checkpoint...
--Global seed set to 42
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 513   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--825 K     Trainable params
--12.7 M    Non-trainable params
--13.5 M    Total params
--Epoch 0, global step 2199: val_loss reached 0.13605 (best 0.13605), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=0.ckpt" as top 3
--Epoch 1, global step 2399: val_loss reached 0.13605 (best 0.13605), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=1.ckpt" as top 3
--Epoch 2, step 2599: val_loss was not in top 3
--Epoch 3, step 2799: val_loss was not in top 3
--Epoch 4, step 2999: val_loss was not in top 3
--Epoch 5, step 3199: val_loss was not in top 3
--Epoch 6, step 3399: val_loss was not in top 3
--Epoch 7, step 3599: val_loss was not in top 3
--Epoch 8, step 3799: val_loss was not in top 3
--Epoch 9, step 3999: val_loss was not in top 3
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--Using environment variable NODE_RANK for node rank (0).
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_error_log.txt
-deleted file mode 100644
-index 2dc7701..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_error_log.txt
-+++ /dev/null
-@@ -1,16 +0,0 @@
--[NeMo W 2021-02-09 16:21:19 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 16:21:32 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 16:22:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f1a20badbb0> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 16:23:04 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f1a2a0bb850> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 16:36:51 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-09 16:52:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f1a2a0bb5e0> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_log_globalrank-0_localrank-0.txt
-deleted file mode 100644
-index 282de52..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_log_globalrank-0_localrank-0.txt
-+++ /dev/null
-@@ -1,18 +0,0 @@
--[NeMo I 2021-02-09 16:21:19 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19
--[NeMo I 2021-02-09 16:21:19 exp_manager:519] TensorboardLogger has been set up
--[NeMo W 2021-02-09 16:21:19 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 16:21:32 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 16:22:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f1a20badbb0> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 16:23:04 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f1a2a0bb850> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 16:36:51 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-09 16:52:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f1a2a0bb5e0> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/cmd-args.log
-deleted file mode 100644
-index 11a5d8e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/cmd-args.log
-+++ /dev/null
-@@ -1 +0,0 @@
--main.py
-\ No newline at end of file
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/events.out.tfevents.1612860293.Titan.7050.0 b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/events.out.tfevents.1612860293.Titan.7050.0
-deleted file mode 100644
-index 600f922..0000000
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/events.out.tfevents.1612860293.Titan.7050.0 and /dev/null differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/git-info.log
-deleted file mode 100644
-index 76e19ab..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/git-info.log
-+++ /dev/null
-@@ -1,275 +0,0 @@
--commit hash: 22df8b7032fa2ae6a488d957253de2d56042b4d6
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/events.out.tfevents.1612858879.Titan.27601.0 b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/events.out.tfevents.1612858879.Titan.27601.0
--index 4337671..16a35c9 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/events.out.tfevents.1612858879.Titan.27601.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/events.out.tfevents.1612858879.Titan.27601.0 differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/lightning_logs.txt
--index a3fc0fb..201da45 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/lightning_logs.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/lightning_logs.txt
--@@ -21,3 +21,27 @@ initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-- 13.2 M    Non-trainable params
-- 13.5 M    Total params
-- Epoch 0, global step 199: val_loss reached 54.61362 (best 54.61362), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/checkpoints/Punctuation_with_Domain_discriminator---val_loss=54.61-epoch=0.ckpt" as top 3
--+Epoch 1, global step 399: val_loss reached 42.83130 (best 42.83130), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/checkpoints/Punctuation_with_Domain_discriminator---val_loss=42.83-epoch=1.ckpt" as top 3
--+Epoch 2, global step 599: val_loss reached 32.94712 (best 32.94712), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/checkpoints/Punctuation_with_Domain_discriminator---val_loss=32.95-epoch=2.ckpt" as top 3
--+Saving latest checkpoint...
--+Epoch 3, global step 732: val_loss reached 32.94712 (best 32.94712), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/checkpoints/Punctuation_with_Domain_discriminator---val_loss=32.95-epoch=3.ckpt" as top 3
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | ElectraModel         | 13.5 M
--+1 | punct_classifier    | TokenClassifier      | 2.6 K 
--+2 | domain_classifier   | SequenceClassifier   | 513   
--+3 | punctuation_loss    | LinearChainCRF       | 120   
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+825 K     Trainable params
--+12.7 M    Non-trainable params
--+13.5 M    Total params
--+GPU available: True, used: True
--+TPU available: None, using: 0 TPU cores
--+Using environment variable NODE_RANK for node rank (0).
--+LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_error_log.txt
--index f051996..43ff45a 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_error_log.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_error_log.txt
--@@ -11,3 +11,9 @@
-- [NeMo W 2021-02-09 16:25:45 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fe13a7e4820> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 16:37:26 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
--+      warnings.warn(*args, **kwargs)
--+    
--+[NeMo W 2021-02-09 16:37:50 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fe13a7e45b0> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_log_globalrank-0_localrank-0.txt
--index a183522..da0fc8b 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_log_globalrank-0_localrank-0.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_log_globalrank-0_localrank-0.txt
--@@ -13,3 +13,9 @@
-- [NeMo W 2021-02-09 16:25:45 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fe13a7e4820> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 16:37:26 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
--+      warnings.warn(*args, **kwargs)
--+    
--+[NeMo W 2021-02-09 16:37:50 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fe13a7e45b0> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/events.out.tfevents.1612858892.Titan.27659.0 b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/events.out.tfevents.1612858892.Titan.27659.0
--index 5b09bdd..303bdf7 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/events.out.tfevents.1612858892.Titan.27659.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/events.out.tfevents.1612858892.Titan.27659.0 differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/lightning_logs.txt
--index 4f17a12..a2aef56 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/lightning_logs.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/lightning_logs.txt
--@@ -23,3 +23,32 @@ initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-- Epoch 0, global step 199: val_loss reached 0.66148 (best 0.66148), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.66-epoch=0.ckpt" as top 3
-- Epoch 1, global step 399: val_loss reached 0.23663 (best 0.23663), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=1.ckpt" as top 3
-- Epoch 2, global step 599: val_loss reached 0.17288 (best 0.17288), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.17-epoch=2.ckpt" as top 3
--+Epoch 3, global step 799: val_loss reached 0.15670 (best 0.15670), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.16-epoch=3.ckpt" as top 3
--+Epoch 4, global step 999: val_loss reached 0.14782 (best 0.14782), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.15-epoch=4.ckpt" as top 3
--+Epoch 5, global step 1199: val_loss reached 0.14246 (best 0.14246), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=5.ckpt" as top 3
--+Epoch 6, global step 1399: val_loss reached 0.13934 (best 0.13934), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=6.ckpt" as top 3
--+Epoch 7, global step 1599: val_loss reached 0.13714 (best 0.13714), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=7.ckpt" as top 3
--+Epoch 8, global step 1799: val_loss reached 0.13633 (best 0.13633), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=8.ckpt" as top 3
--+Epoch 9, global step 1999: val_loss reached 0.13605 (best 0.13605), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=9.ckpt" as top 3
--+Saving latest checkpoint...
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | ElectraModel         | 13.5 M
--+1 | punct_classifier    | TokenClassifier      | 2.6 K 
--+2 | domain_classifier   | SequenceClassifier   | 513   
--+3 | punctuation_loss    | FocalDiceLoss        | 0     
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+825 K     Trainable params
--+12.7 M    Non-trainable params
--+13.5 M    Total params
--+Epoch 0, global step 2199: val_loss reached 0.13605 (best 0.13605), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=0.ckpt" as top 3
--+Epoch 1, global step 2399: val_loss reached 0.13605 (best 0.13605), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=1.ckpt" as top 3
--+Epoch 2, step 2599: val_loss was not in top 3
--+Epoch 3, step 2799: val_loss was not in top 3
--+Epoch 4, step 2999: val_loss was not in top 3
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_error_log.txt
--index 39ac72a..89932b9 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_error_log.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_error_log.txt
--@@ -8,3 +8,6 @@
-- [NeMo W 2021-02-09 16:23:04 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f1a2a0bb850> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 16:36:51 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--+      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_log_globalrank-0_localrank-0.txt
--index baa087a..cf62f3e 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_log_globalrank-0_localrank-0.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_log_globalrank-0_localrank-0.txt
--@@ -10,3 +10,6 @@
-- [NeMo W 2021-02-09 16:23:04 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f1a2a0bb850> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 16:36:51 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--+      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--+    
--diff --git a/experiment/config.yaml b/experiment/config.yaml
--index 7a28782..fa6c702 100644
----- a/experiment/config.yaml
--+++ b/experiment/config.yaml
--@@ -137,13 +137,20 @@ model:
-- 
--     optim:
--         name: adamw
---        lr: 0.009261935523740748 #1e-3
--+        lr: 1e-3 #1e-3
--         weight_decay: 0.00
--         sched:
---            name: WarmupAnnealing #CyclicLR
--+            # name: CyclicLR
--+            # base_lr: 1e-5
--+            # max_lr: 1e-1
--+            # mode: 'triangular2'
--+            # last_epoch: -1
--+
--+            name: CosineAnnealing #WarmupAnnealing #CyclicLR
--             # Scheduler params
--             warmup_steps: null
--             warmup_ratio: 0.1
--+            min_lr: 1e-10
--             # hold_steps: 6
--             last_epoch: -1
-- 
--diff --git a/experiment/info.log b/experiment/info.log
--index 38d3b4b..e69de29 100644
----- a/experiment/info.log
--+++ b/experiment/info.log
--@@ -1,117 +0,0 @@
---[INFO] - shuffling train set
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.0001
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f1a21b62cd0>" 
---will be used during training (effective maximum steps = 2000) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---last_epoch: -1
---max_steps: 2000
---)
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          86.16      25.06      38.83       1540
---! (label_id: 1)                                          0.00       0.00       0.00          2
---, (label_id: 2)                                          8.60      38.03      14.03        142
---- (label_id: 3)                                          4.62      30.00       8.00         20
---. (label_id: 4)                                          0.00       0.00       0.00         96
---: (label_id: 5)                                          0.00       0.00       0.00          0
---; (label_id: 6)                                          0.00       0.00       0.00          0
---? (label_id: 7)                                          0.00       0.00       0.00          4
---— (label_id: 8)                                          0.00       0.00       0.00          0
---… (label_id: 9)                                          0.00       0.00       0.00          0
----------------------
---micro avg                                               24.72      24.72      24.72       1804
---macro avg                                               16.56      15.52      10.14       1804
---weighted avg                                            74.28      24.72      34.34       1804
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00         20
----------------------
---micro avg                                              100.00     100.00     100.00         20
---macro avg                                              100.00     100.00     100.00         20
---weighted avg                                           100.00     100.00     100.00         20
---
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          86.99     100.00      93.04     179900
---! (label_id: 1)                                          0.00       0.00       0.00         52
---, (label_id: 2)                                          0.00       0.00       0.00      13074
---- (label_id: 3)                                          0.00       0.00       0.00       1062
---. (label_id: 4)                                          0.00       0.00       0.00      11077
---: (label_id: 5)                                          0.00       0.00       0.00        330
---; (label_id: 6)                                          0.00       0.00       0.00        108
---? (label_id: 7)                                          0.00       0.00       0.00        899
---— (label_id: 8)                                          0.00       0.00       0.00        276
---… (label_id: 9)                                          0.00       0.00       0.00         28
----------------------
---micro avg                                               86.99      86.99      86.99     206806
---macro avg                                                8.70      10.00       9.30     206806
---weighted avg                                            75.67      86.99      80.94     206806
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1959
----------------------
---micro avg                                              100.00     100.00     100.00       1959
---macro avg                                              100.00     100.00     100.00       1959
---weighted avg                                           100.00     100.00     100.00       1959
---
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          94.70      97.81      96.23     179900
---! (label_id: 1)                                          0.00       0.00       0.00         52
---, (label_id: 2)                                         48.60      28.63      36.03      13074
---- (label_id: 3)                                         63.84      53.86      58.43       1062
---. (label_id: 4)                                         53.31      59.38      56.18      11077
---: (label_id: 5)                                          0.00       0.00       0.00        330
---; (label_id: 6)                                          0.00       0.00       0.00        108
---? (label_id: 7)                                         20.00       1.22       2.31        899
---— (label_id: 8)                                          0.00       0.00       0.00        276
---… (label_id: 9)                                          0.00       0.00       0.00         28
----------------------
---micro avg                                               90.36      90.36      90.36     206806
---macro avg                                               28.04      24.09      24.92     206806
---weighted avg                                            88.72      90.36      89.31     206806
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1959
----------------------
---micro avg                                              100.00     100.00     100.00       1959
---macro avg                                              100.00     100.00     100.00       1959
---weighted avg                                           100.00     100.00     100.00       1959
---
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          96.35      96.40      96.38     179900
---! (label_id: 1)                                          0.00       0.00       0.00         52
---, (label_id: 2)                                         43.32      40.33      41.77      13074
---- (label_id: 3)                                         64.46      56.87      60.43       1062
---. (label_id: 4)                                         53.28      62.91      57.69      11077
---: (label_id: 5)                                          0.00       0.00       0.00        330
---; (label_id: 6)                                          0.00       0.00       0.00        108
---? (label_id: 7)                                         19.81       9.23      12.59        899
---— (label_id: 8)                                          5.63       4.35       4.91        276
---… (label_id: 9)                                          0.00       0.00       0.00         28
----------------------
---micro avg                                               90.11      90.11      90.11     206806
---macro avg                                               28.29      27.01      27.38     206806
---weighted avg                                            89.83      90.11      89.94     206806
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1959
----------------------
---micro avg                                              100.00     100.00     100.00       1959
---macro avg                                              100.00     100.00     100.00       1959
---weighted avg                                           100.00     100.00     100.00       1959
---
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/hparams.yaml
-deleted file mode 100644
-index b405ca5..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/hparams.yaml
-+++ /dev/null
-@@ -1,110 +0,0 @@
--seed: 42
--trainer:
--  gpus: 1
--  num_nodes: 1
--  max_epochs: 10
--  max_steps: null
--  accumulate_grad_batches: 4
--  gradient_clip_val: 0
--  amp_level: O1
--  precision: 16
--  accelerator: ddp
--  checkpoint_callback: false
--  logger: false
--  log_every_n_steps: 1
--  val_check_interval: 1.0
--  resume_from_checkpoint: null
--exp_manager:
--  exp_dir: /home/nxingyu/project/
--  name: Punctuation_with_Domain_discriminator
--  create_tensorboard_logger: true
--  create_checkpoint_callback: true
--base_path: /home/nxingyu/data
--tmp_path: /home/nxingyu/data/tmp
--model:
--  nemo_path: null
--  transformer_path: google/electra-small-discriminator
--  unfrozen: 0
--  maximum_unfrozen: 1
--  unfreeze_step: 1
--  punct_label_ids:
--  - ''
--  - '!'
--  - ','
--  - '-'
--  - .
--  - ':'
--  - ;
--  - '?'
--  - —
--  - …
--  punct_class_weights: false
--  dataset:
--    data_dir: /home/nxingyu/data
--    labelled:
--    - /home/nxingyu/data/ted_talks_processed
--    unlabelled: null
--    max_seq_length: 128
--    pad_label: ''
--    ignore_extra_tokens: false
--    ignore_start_end: false
--    use_cache: false
--    num_workers: 4
--    pin_memory: true
--    drop_last: true
--    num_labels: 10
--    num_domains: 1
--    test_unlabelled: true
--    train_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--    validation_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--  tokenizer:
--    tokenizer_name: google/electra-small-discriminator
--    vocab_file: null
--    tokenizer_model: null
--    special_tokens: null
--  language_model:
--    pretrained_model_name: google/electra-small-discriminator
--    lm_checkpoint: null
--    config_file: null
--    config: null
--  punct_head:
--    punct_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: dice
--  domain_head:
--    domain_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: cel
--    gamma: 0.1
--    pooling: mean_max
--    idx_conditioned_on: 0
--  dice_loss:
--    epsilon: 0.01
--    alpha: 3
--    macro_average: true
--  focal_loss:
--    gamma: 1
--  optim:
--    name: adamw
--    lr: 0.001
--    weight_decay: 0.0
--    sched:
--      name: CosineAnnealing
--      warmup_steps: null
--      warmup_ratio: 0.1
--      min_lr: 1.0e-10
--      last_epoch: -1
--      monitor: val_loss
--      reduce_on_plateau: false
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/lightning_logs.txt
-deleted file mode 100644
-index 2fe9063..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/lightning_logs.txt
-+++ /dev/null
-@@ -1,63 +0,0 @@
--Global seed set to 42
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
--Using native 16bit precision.
--Global seed set to 42
--initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 513   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--299 K     Trainable params
--13.2 M    Non-trainable params
--13.5 M    Total params
--Epoch 0, global step 199: val_loss reached 0.66148 (best 0.66148), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.66-epoch=0.ckpt" as top 3
--Epoch 1, global step 399: val_loss reached 0.22688 (best 0.22688), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.23-epoch=1.ckpt" as top 3
--Epoch 2, global step 599: val_loss reached 0.16884 (best 0.16884), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.17-epoch=2.ckpt" as top 3
--Epoch 3, global step 799: val_loss reached 0.15254 (best 0.15254), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.15-epoch=3.ckpt" as top 3
--Epoch 4, global step 999: val_loss reached 0.14450 (best 0.14450), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=4.ckpt" as top 3
--Epoch 5, global step 1199: val_loss reached 0.13992 (best 0.13992), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=5.ckpt" as top 3
--Epoch 6, global step 1399: val_loss reached 0.13759 (best 0.13759), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=6.ckpt" as top 3
--Epoch 7, global step 1599: val_loss reached 0.13629 (best 0.13629), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=7.ckpt" as top 3
--Epoch 8, global step 1799: val_loss reached 0.13606 (best 0.13606), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=8.ckpt" as top 3
--Epoch 9, global step 1999: val_loss reached 0.13598 (best 0.13598), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=9.ckpt" as top 3
--Saving latest checkpoint...
--Global seed set to 42
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 513   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--825 K     Trainable params
--12.7 M    Non-trainable params
--13.5 M    Total params
--Epoch 0, global step 2199: val_loss reached 0.13598 (best 0.13598), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=0.ckpt" as top 3
--Epoch 1, global step 2399: val_loss reached 0.13598 (best 0.13598), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=1.ckpt" as top 3
--Epoch 2, global step 2599: val_loss reached 0.13598 (best 0.13598), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=2.ckpt" as top 3
--Epoch 3, global step 2799: val_loss reached 0.13598 (best 0.13598), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=3.ckpt" as top 3
--Epoch 4, global step 2999: val_loss reached 0.13598 (best 0.13598), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=4.ckpt" as top 3
--Epoch 5, step 3199: val_loss was not in top 3
--Epoch 6, step 3399: val_loss was not in top 3
--Epoch 7, step 3599: val_loss was not in top 3
--Epoch 8, step 3799: val_loss was not in top 3
--Epoch 9, step 3999: val_loss was not in top 3
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--Using environment variable NODE_RANK for node rank (0).
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/nemo_error_log.txt
-deleted file mode 100644
-index 955d1d3..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/nemo_error_log.txt
-+++ /dev/null
-@@ -1,16 +0,0 @@
--[NeMo W 2021-02-09 16:44:40 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 16:44:53 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 16:46:18 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fce2633fe20> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 16:46:27 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fce307368e0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 17:00:43 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-09 17:16:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fce30736670> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/nemo_log_globalrank-0_localrank-0.txt
-deleted file mode 100644
-index 006a75e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/nemo_log_globalrank-0_localrank-0.txt
-+++ /dev/null
-@@ -1,18 +0,0 @@
--[NeMo I 2021-02-09 16:44:40 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40
--[NeMo I 2021-02-09 16:44:40 exp_manager:519] TensorboardLogger has been set up
--[NeMo W 2021-02-09 16:44:40 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 16:44:53 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 16:46:18 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fce2633fe20> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 16:46:27 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fce307368e0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 17:00:43 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-09 17:16:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fce30736670> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/cmd-args.log
-deleted file mode 100644
-index 11a5d8e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/cmd-args.log
-+++ /dev/null
-@@ -1 +0,0 @@
--main.py
-\ No newline at end of file
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/events.out.tfevents.1612860903.Titan.12736.0 b/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/events.out.tfevents.1612860903.Titan.12736.0
-deleted file mode 100644
-index f809c63..0000000
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/events.out.tfevents.1612860903.Titan.12736.0 and /dev/null differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/git-info.log
-deleted file mode 100644
-index ed0e8ce..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/git-info.log
-+++ /dev/null
-@@ -1,357 +0,0 @@
--commit hash: 22df8b7032fa2ae6a488d957253de2d56042b4d6
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/events.out.tfevents.1612858879.Titan.27601.0 b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/events.out.tfevents.1612858879.Titan.27601.0
--index 4337671..16a35c9 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/events.out.tfevents.1612858879.Titan.27601.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/events.out.tfevents.1612858879.Titan.27601.0 differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/lightning_logs.txt
--index a3fc0fb..201da45 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/lightning_logs.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/lightning_logs.txt
--@@ -21,3 +21,27 @@ initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-- 13.2 M    Non-trainable params
-- 13.5 M    Total params
-- Epoch 0, global step 199: val_loss reached 54.61362 (best 54.61362), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/checkpoints/Punctuation_with_Domain_discriminator---val_loss=54.61-epoch=0.ckpt" as top 3
--+Epoch 1, global step 399: val_loss reached 42.83130 (best 42.83130), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/checkpoints/Punctuation_with_Domain_discriminator---val_loss=42.83-epoch=1.ckpt" as top 3
--+Epoch 2, global step 599: val_loss reached 32.94712 (best 32.94712), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/checkpoints/Punctuation_with_Domain_discriminator---val_loss=32.95-epoch=2.ckpt" as top 3
--+Saving latest checkpoint...
--+Epoch 3, global step 732: val_loss reached 32.94712 (best 32.94712), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/checkpoints/Punctuation_with_Domain_discriminator---val_loss=32.95-epoch=3.ckpt" as top 3
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | ElectraModel         | 13.5 M
--+1 | punct_classifier    | TokenClassifier      | 2.6 K 
--+2 | domain_classifier   | SequenceClassifier   | 513   
--+3 | punctuation_loss    | LinearChainCRF       | 120   
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+825 K     Trainable params
--+12.7 M    Non-trainable params
--+13.5 M    Total params
--+GPU available: True, used: True
--+TPU available: None, using: 0 TPU cores
--+Using environment variable NODE_RANK for node rank (0).
--+LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_error_log.txt
--index f051996..43ff45a 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_error_log.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_error_log.txt
--@@ -11,3 +11,9 @@
-- [NeMo W 2021-02-09 16:25:45 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fe13a7e4820> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 16:37:26 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
--+      warnings.warn(*args, **kwargs)
--+    
--+[NeMo W 2021-02-09 16:37:50 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fe13a7e45b0> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_log_globalrank-0_localrank-0.txt
--index a183522..da0fc8b 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_log_globalrank-0_localrank-0.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-07/nemo_log_globalrank-0_localrank-0.txt
--@@ -13,3 +13,9 @@
-- [NeMo W 2021-02-09 16:25:45 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fe13a7e4820> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 16:37:26 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
--+      warnings.warn(*args, **kwargs)
--+    
--+[NeMo W 2021-02-09 16:37:50 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fe13a7e45b0> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/events.out.tfevents.1612858892.Titan.27659.0 b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/events.out.tfevents.1612858892.Titan.27659.0
--index 5b09bdd..e8dc27e 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/events.out.tfevents.1612858892.Titan.27659.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/events.out.tfevents.1612858892.Titan.27659.0 differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/lightning_logs.txt
--index 4f17a12..40964a1 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/lightning_logs.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/lightning_logs.txt
--@@ -23,3 +23,41 @@ initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-- Epoch 0, global step 199: val_loss reached 0.66148 (best 0.66148), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.66-epoch=0.ckpt" as top 3
-- Epoch 1, global step 399: val_loss reached 0.23663 (best 0.23663), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=1.ckpt" as top 3
-- Epoch 2, global step 599: val_loss reached 0.17288 (best 0.17288), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.17-epoch=2.ckpt" as top 3
--+Epoch 3, global step 799: val_loss reached 0.15670 (best 0.15670), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.16-epoch=3.ckpt" as top 3
--+Epoch 4, global step 999: val_loss reached 0.14782 (best 0.14782), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.15-epoch=4.ckpt" as top 3
--+Epoch 5, global step 1199: val_loss reached 0.14246 (best 0.14246), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=5.ckpt" as top 3
--+Epoch 6, global step 1399: val_loss reached 0.13934 (best 0.13934), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=6.ckpt" as top 3
--+Epoch 7, global step 1599: val_loss reached 0.13714 (best 0.13714), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=7.ckpt" as top 3
--+Epoch 8, global step 1799: val_loss reached 0.13633 (best 0.13633), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=8.ckpt" as top 3
--+Epoch 9, global step 1999: val_loss reached 0.13605 (best 0.13605), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=9.ckpt" as top 3
--+Saving latest checkpoint...
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | ElectraModel         | 13.5 M
--+1 | punct_classifier    | TokenClassifier      | 2.6 K 
--+2 | domain_classifier   | SequenceClassifier   | 513   
--+3 | punctuation_loss    | FocalDiceLoss        | 0     
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+825 K     Trainable params
--+12.7 M    Non-trainable params
--+13.5 M    Total params
--+Epoch 0, global step 2199: val_loss reached 0.13605 (best 0.13605), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=0.ckpt" as top 3
--+Epoch 1, global step 2399: val_loss reached 0.13605 (best 0.13605), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=1.ckpt" as top 3
--+Epoch 2, step 2599: val_loss was not in top 3
--+Epoch 3, step 2799: val_loss was not in top 3
--+Epoch 4, step 2999: val_loss was not in top 3
--+Epoch 5, step 3199: val_loss was not in top 3
--+Epoch 6, step 3399: val_loss was not in top 3
--+Epoch 7, step 3599: val_loss was not in top 3
--+Epoch 8, step 3799: val_loss was not in top 3
--+Epoch 9, step 3999: val_loss was not in top 3
--+GPU available: True, used: True
--+TPU available: None, using: 0 TPU cores
--+Using environment variable NODE_RANK for node rank (0).
--+LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_error_log.txt
--index 39ac72a..2dc7701 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_error_log.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_error_log.txt
--@@ -8,3 +8,9 @@
-- [NeMo W 2021-02-09 16:23:04 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f1a2a0bb850> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 16:36:51 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--+      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--+    
--+[NeMo W 2021-02-09 16:52:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f1a2a0bb5e0> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_log_globalrank-0_localrank-0.txt
--index baa087a..282de52 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_log_globalrank-0_localrank-0.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-21-19/nemo_log_globalrank-0_localrank-0.txt
--@@ -10,3 +10,9 @@
-- [NeMo W 2021-02-09 16:23:04 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f1a2a0bb850> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 16:36:51 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--+      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--+    
--+[NeMo W 2021-02-09 16:52:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f1a2a0bb5e0> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--diff --git a/experiment/config.yaml b/experiment/config.yaml
--index 7a28782..1911de8 100644
----- a/experiment/config.yaml
--+++ b/experiment/config.yaml
--@@ -2,7 +2,7 @@ seed: 42
-- trainer:
--     gpus: 1 # the number of gpus, 0 for CPU
--     num_nodes: 1
---    max_epochs: 10
--+    max_epochs: 15
--     max_steps: null # precedence over max_epochs
--     accumulate_grad_batches: 4 # accumulates grads every k batches
--     gradient_clip_val: 0
--@@ -63,10 +63,10 @@ model:
--     dataset:
--         data_dir: /home/nxingyu/data # /root/data # 
--         labelled:
---            - ${base_path}/ted_talks_processed #
---            # - ${base_path}/open_subtitles_processed #  
---        unlabelled:
--             # - ${base_path}/ted_talks_processed #
--+            - ${base_path}/open_subtitles_processed #  
--+        unlabelled:
--+            - ${base_path}/ted_talks_processed #
--             # - ${base_path}/open_subtitles_processed #  
--             # parameters for dataset preprocessing
--         max_seq_length: 128
--@@ -79,7 +79,7 @@ model:
--         pin_memory: true
--         drop_last: true
--         num_labels: 10
---        num_domains: 1
--+        num_domains: 2
--         test_unlabelled: true
-- 
--         train_ds:
--@@ -123,7 +123,7 @@ model:
--         log_softmax: false
--         use_transformer_init: true
--         loss: 'cel'
---        gamma: 0.1 #0.1 # coefficient of gradient reversal
--+        gamma: 0.2 #0.1 # coefficient of gradient reversal
--         pooling: 'mean_max' # 'mean' mean_max
--         idx_conditioned_on: 0
--     
--@@ -137,13 +137,20 @@ model:
-- 
--     optim:
--         name: adamw
---        lr: 0.009261935523740748 #1e-3
--+        lr: 1e-2 #1e-3
--         weight_decay: 0.00
--         sched:
---            name: WarmupAnnealing #CyclicLR
--+            # name: CyclicLR
--+            # base_lr: 1e-5
--+            # max_lr: 1e-1
--+            # mode: 'triangular2'
--+            # last_epoch: -1
--+
--+            name: CosineAnnealing #WarmupAnnealing #CyclicLR
--             # Scheduler params
--             warmup_steps: null
--             warmup_ratio: 0.1
--+            min_lr: 1e-10
--             # hold_steps: 6
--             last_epoch: -1
-- 
--diff --git a/experiment/info.log b/experiment/info.log
--index 38d3b4b..e69de29 100644
----- a/experiment/info.log
--+++ b/experiment/info.log
--@@ -1,117 +0,0 @@
---[INFO] - shuffling train set
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.0001
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f1a21b62cd0>" 
---will be used during training (effective maximum steps = 2000) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---last_epoch: -1
---max_steps: 2000
---)
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          86.16      25.06      38.83       1540
---! (label_id: 1)                                          0.00       0.00       0.00          2
---, (label_id: 2)                                          8.60      38.03      14.03        142
---- (label_id: 3)                                          4.62      30.00       8.00         20
---. (label_id: 4)                                          0.00       0.00       0.00         96
---: (label_id: 5)                                          0.00       0.00       0.00          0
---; (label_id: 6)                                          0.00       0.00       0.00          0
---? (label_id: 7)                                          0.00       0.00       0.00          4
---— (label_id: 8)                                          0.00       0.00       0.00          0
---… (label_id: 9)                                          0.00       0.00       0.00          0
----------------------
---micro avg                                               24.72      24.72      24.72       1804
---macro avg                                               16.56      15.52      10.14       1804
---weighted avg                                            74.28      24.72      34.34       1804
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00         20
----------------------
---micro avg                                              100.00     100.00     100.00         20
---macro avg                                              100.00     100.00     100.00         20
---weighted avg                                           100.00     100.00     100.00         20
---
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          86.99     100.00      93.04     179900
---! (label_id: 1)                                          0.00       0.00       0.00         52
---, (label_id: 2)                                          0.00       0.00       0.00      13074
---- (label_id: 3)                                          0.00       0.00       0.00       1062
---. (label_id: 4)                                          0.00       0.00       0.00      11077
---: (label_id: 5)                                          0.00       0.00       0.00        330
---; (label_id: 6)                                          0.00       0.00       0.00        108
---? (label_id: 7)                                          0.00       0.00       0.00        899
---— (label_id: 8)                                          0.00       0.00       0.00        276
---… (label_id: 9)                                          0.00       0.00       0.00         28
----------------------
---micro avg                                               86.99      86.99      86.99     206806
---macro avg                                                8.70      10.00       9.30     206806
---weighted avg                                            75.67      86.99      80.94     206806
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1959
----------------------
---micro avg                                              100.00     100.00     100.00       1959
---macro avg                                              100.00     100.00     100.00       1959
---weighted avg                                           100.00     100.00     100.00       1959
---
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          94.70      97.81      96.23     179900
---! (label_id: 1)                                          0.00       0.00       0.00         52
---, (label_id: 2)                                         48.60      28.63      36.03      13074
---- (label_id: 3)                                         63.84      53.86      58.43       1062
---. (label_id: 4)                                         53.31      59.38      56.18      11077
---: (label_id: 5)                                          0.00       0.00       0.00        330
---; (label_id: 6)                                          0.00       0.00       0.00        108
---? (label_id: 7)                                         20.00       1.22       2.31        899
---— (label_id: 8)                                          0.00       0.00       0.00        276
---… (label_id: 9)                                          0.00       0.00       0.00         28
----------------------
---micro avg                                               90.36      90.36      90.36     206806
---macro avg                                               28.04      24.09      24.92     206806
---weighted avg                                            88.72      90.36      89.31     206806
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1959
----------------------
---micro avg                                              100.00     100.00     100.00       1959
---macro avg                                              100.00     100.00     100.00       1959
---weighted avg                                           100.00     100.00     100.00       1959
---
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          96.35      96.40      96.38     179900
---! (label_id: 1)                                          0.00       0.00       0.00         52
---, (label_id: 2)                                         43.32      40.33      41.77      13074
---- (label_id: 3)                                         64.46      56.87      60.43       1062
---. (label_id: 4)                                         53.28      62.91      57.69      11077
---: (label_id: 5)                                          0.00       0.00       0.00        330
---; (label_id: 6)                                          0.00       0.00       0.00        108
---? (label_id: 7)                                         19.81       9.23      12.59        899
---— (label_id: 8)                                          5.63       4.35       4.91        276
---… (label_id: 9)                                          0.00       0.00       0.00         28
----------------------
---micro avg                                               90.11      90.11      90.11     206806
---macro avg                                               28.29      27.01      27.38     206806
---weighted avg                                            89.83      90.11      89.94     206806
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1959
----------------------
---micro avg                                              100.00     100.00     100.00       1959
---macro avg                                              100.00     100.00     100.00       1959
---weighted avg                                           100.00     100.00     100.00       1959
---
--diff --git a/experiment/main.py b/experiment/main.py
--index 1f44193..4ae03e6 100644
----- a/experiment/main.py
--+++ b/experiment/main.py
--@@ -54,7 +54,7 @@ def main(cfg: DictConfig)->None:
--     #         tmp_path=cfg.tmp_path,
--     #         test_unlabelled=False,
--     #     )
---    lrs=[1e-4,1e-6]
--+    lrs=[1e-2,1e-5]
--     while(model.hparams.model.unfrozen<=cfg.model.maximum_unfrozen and model.hparams.model.unfrozen>=0):
--         # trainer.current_epoch=0
--         # lr_finder = trainer.tuner.lr_find(model,datamodule=lr_finder_dm,min_lr=1e-8, max_lr=0.5, num_training=80) #, early_stop_threshold=None
--diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
--index 999ae0d..5b5d668 100644
----- a/experiment/models/punctuation_domain_model.py
--+++ b/experiment/models/punctuation_domain_model.py
--@@ -163,7 +163,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
--         p=(self.current_epoch*self.train_size+batch_idx)/(self.train_size*self.hparams.trainer.max_epochs)
--         if (batch_idx%1000==0):
--             print('gamma:',p)
---        self.grad_reverse.scale=2/(1+math.exp(-10*p))-1
--+        self.grad_reverse.scale=(2/(1+math.exp(-10*p))-1)*self.hparams.model.domain_head.gamma
--         loss, _, _ = self._make_step(batch)
--         lr = self._optimizer.param_groups[0]['lr']
-- 
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/hparams.yaml
-deleted file mode 100644
-index ccb8491..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/hparams.yaml
-+++ /dev/null
-@@ -1,111 +0,0 @@
--seed: 42
--trainer:
--  gpus: 1
--  num_nodes: 1
--  max_epochs: 15
--  max_steps: null
--  accumulate_grad_batches: 4
--  gradient_clip_val: 0
--  amp_level: O1
--  precision: 16
--  accelerator: ddp
--  checkpoint_callback: false
--  logger: false
--  log_every_n_steps: 1
--  val_check_interval: 1.0
--  resume_from_checkpoint: null
--exp_manager:
--  exp_dir: /home/nxingyu/project/
--  name: Punctuation_with_Domain_discriminator
--  create_tensorboard_logger: true
--  create_checkpoint_callback: true
--base_path: /home/nxingyu/data
--tmp_path: /home/nxingyu/data/tmp
--model:
--  nemo_path: null
--  transformer_path: google/electra-small-discriminator
--  unfrozen: 0
--  maximum_unfrozen: 1
--  unfreeze_step: 1
--  punct_label_ids:
--  - ''
--  - '!'
--  - ','
--  - '-'
--  - .
--  - ':'
--  - ;
--  - '?'
--  - —
--  - …
--  punct_class_weights: false
--  dataset:
--    data_dir: /home/nxingyu/data
--    labelled:
--    - /home/nxingyu/data/open_subtitles_processed
--    unlabelled:
--    - /home/nxingyu/data/ted_talks_processed
--    max_seq_length: 128
--    pad_label: ''
--    ignore_extra_tokens: false
--    ignore_start_end: false
--    use_cache: false
--    num_workers: 4
--    pin_memory: true
--    drop_last: true
--    num_labels: 10
--    num_domains: 2
--    test_unlabelled: true
--    train_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--    validation_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--  tokenizer:
--    tokenizer_name: google/electra-small-discriminator
--    vocab_file: null
--    tokenizer_model: null
--    special_tokens: null
--  language_model:
--    pretrained_model_name: google/electra-small-discriminator
--    lm_checkpoint: null
--    config_file: null
--    config: null
--  punct_head:
--    punct_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: dice
--  domain_head:
--    domain_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: cel
--    gamma: 0.2
--    pooling: mean_max
--    idx_conditioned_on: 0
--  dice_loss:
--    epsilon: 0.01
--    alpha: 3
--    macro_average: true
--  focal_loss:
--    gamma: 1
--  optim:
--    name: adamw
--    lr: 0.01
--    weight_decay: 0.0
--    sched:
--      name: CosineAnnealing
--      warmup_steps: null
--      warmup_ratio: 0.1
--      min_lr: 1.0e-10
--      last_epoch: -1
--      monitor: val_loss
--      reduce_on_plateau: false
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/lightning_logs.txt
-deleted file mode 100644
-index 073ea87..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/lightning_logs.txt
-+++ /dev/null
-@@ -1,23 +0,0 @@
--Global seed set to 42
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
--Using native 16bit precision.
--Global seed set to 42
--initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 1.0 K 
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--299 K     Trainable params
--13.2 M    Non-trainable params
--13.5 M    Total params
--Saving latest checkpoint...
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/nemo_error_log.txt
-deleted file mode 100644
-index 6cc1f9b..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/nemo_error_log.txt
-+++ /dev/null
-@@ -1,7 +0,0 @@
--[NeMo W 2021-02-09 16:54:29 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 16:55:03 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 17:23:50 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
--      warnings.warn(*args, **kwargs)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/nemo_log_globalrank-0_localrank-0.txt
-deleted file mode 100644
-index b5e9717..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/nemo_log_globalrank-0_localrank-0.txt
-+++ /dev/null
-@@ -1,9 +0,0 @@
--[NeMo I 2021-02-09 16:54:29 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29
--[NeMo I 2021-02-09 16:54:29 exp_manager:519] TensorboardLogger has been set up
--[NeMo W 2021-02-09 16:54:29 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 16:55:03 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 17:23:50 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
--      warnings.warn(*args, **kwargs)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/cmd-args.log
-deleted file mode 100644
-index 11a5d8e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/cmd-args.log
-+++ /dev/null
-@@ -1 +0,0 @@
--main.py
-\ No newline at end of file
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/events.out.tfevents.1612862415.Titan.18732.0 b/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/events.out.tfevents.1612862415.Titan.18732.0
-deleted file mode 100644
-index 95812bc..0000000
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/events.out.tfevents.1612862415.Titan.18732.0 and /dev/null differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/git-info.log
-deleted file mode 100644
-index b7eda36..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/git-info.log
-+++ /dev/null
-@@ -1,163 +0,0 @@
--commit hash: 953fa623bede4ee117149aa46c2acf589001ede6
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/events.out.tfevents.1612860293.Titan.7050.0 b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/events.out.tfevents.1612860293.Titan.7050.0
--index c1dbdac..600f922 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/events.out.tfevents.1612860293.Titan.7050.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/events.out.tfevents.1612860293.Titan.7050.0 differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/lightning_logs.txt
--index 085adc8..2fe9063 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/lightning_logs.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/lightning_logs.txt
--@@ -47,3 +47,17 @@ Global seed set to 42
-- 825 K     Trainable params
-- 12.7 M    Non-trainable params
-- 13.5 M    Total params
--+Epoch 0, global step 2199: val_loss reached 0.13598 (best 0.13598), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=0.ckpt" as top 3
--+Epoch 1, global step 2399: val_loss reached 0.13598 (best 0.13598), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=1.ckpt" as top 3
--+Epoch 2, global step 2599: val_loss reached 0.13598 (best 0.13598), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=2.ckpt" as top 3
--+Epoch 3, global step 2799: val_loss reached 0.13598 (best 0.13598), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=3.ckpt" as top 3
--+Epoch 4, global step 2999: val_loss reached 0.13598 (best 0.13598), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.14-epoch=4.ckpt" as top 3
--+Epoch 5, step 3199: val_loss was not in top 3
--+Epoch 6, step 3399: val_loss was not in top 3
--+Epoch 7, step 3599: val_loss was not in top 3
--+Epoch 8, step 3799: val_loss was not in top 3
--+Epoch 9, step 3999: val_loss was not in top 3
--+GPU available: True, used: True
--+TPU available: None, using: 0 TPU cores
--+Using environment variable NODE_RANK for node rank (0).
--+LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/nemo_error_log.txt
--index 21bc8ca..955d1d3 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/nemo_error_log.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/nemo_error_log.txt
--@@ -11,3 +11,6 @@
-- [NeMo W 2021-02-09 17:00:43 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--     
--+[NeMo W 2021-02-09 17:16:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fce30736670> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/nemo_log_globalrank-0_localrank-0.txt
--index 8f639a0..006a75e 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/nemo_log_globalrank-0_localrank-0.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_16-44-40/nemo_log_globalrank-0_localrank-0.txt
--@@ -13,3 +13,6 @@
-- [NeMo W 2021-02-09 17:00:43 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--     
--+[NeMo W 2021-02-09 17:16:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fce30736670> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/events.out.tfevents.1612860903.Titan.12736.0 b/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/events.out.tfevents.1612860903.Titan.12736.0
--index 71d7b9a..b40aed4 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/events.out.tfevents.1612860903.Titan.12736.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_16-54-29/events.out.tfevents.1612860903.Titan.12736.0 differ
--diff --git a/README.md b/README.md
--index ab4fdd1..e08912e 100644
----- a/README.md
--+++ b/README.md
--@@ -459,5 +459,73 @@ testing gamma 0.1 vs just open subtitles:
-- 
-- https://www.aclweb.org/anthology/2020.acl-main.370.pdf uses the formula of 2/(1+e^(-10p))-1 where p varies from 0 to 1. To repeat cycle every unfrozen layer.
-- 
--+### 2021-02-09_16-21-19 warmup ted 
-- 
--+: (label_id: 5)                                         20.69      19.57      20.11        368
--+; (label_id: 6)                                          0.00       0.00       0.00        200
--+? (label_id: 7)                                         22.42      29.15      25.35       1372
--+ (label_id: 8)                                          6.83       9.44       7.93        932
--+… (label_id: 9)                                          0.00       0.00       0.00        124
--+-------------------
--+micro avg                                               89.82      89.82      89.82     300124
--+macro avg                                               31.58      32.56      31.95     300124
--+weighted avg                                            90.46      89.82      90.11     300124
--+
--+[INFO] - Domain report:
--+label                                                precision    recall       f1           support
--+0 (label_id: 0)                                        100.00     100.00     100.00       2744
--+-------------------
--+micro avg                                              100.00     100.00     100.00       2744
--+macro avg                                              100.00     100.00     100.00       2744
--+weighted avg                                           100.00     100.00     100.00       2744
--+
--+Testing: 100%|| 100/100 [00:10<00:00,  9.74it/s]
--+--------------------------------------------------------------------------------
--+DATALOADER:0 TEST RESULTS
--+{'domain_f1': 100.0,
--+ 'domain_precision': 100.0,
--+ 'domain_recall': 100.0,
--+ 'punct_f1': 31.946725845336914,
--+ 'punct_precision': 31.575754165649414,
--+ 'punct_recall': 32.5594596862793,
--+ 'test_loss': 0.23392203450202942}
--+
--+### 2021-02-09_16-44-40 cosine ted around the same:
--+
--+ (label_id: 0)                                          97.17      95.67      96.41     259964
--+! (label_id: 1)                                          0.00       0.00       0.00        152
--+, (label_id: 2)                                         43.51      47.93      45.61      19336
--+- (label_id: 3)                                         69.47      61.49      65.23       1776
--+. (label_id: 4)                                         55.49      62.29      58.69      15900
--+: (label_id: 5)                                         20.45      19.57      20.00        368
--+; (label_id: 6)                                          0.00       0.00       0.00        200
--+? (label_id: 7)                                         22.67      29.74      25.73       1372
--+ (label_id: 8)                                          6.85       9.44       7.94        932
--+… (label_id: 9)                                          0.00       0.00       0.00        124
--+-------------------
--+micro avg                                               89.81      89.81      89.81     300124
--+macro avg                                               31.56      32.61      31.96     300124
--+weighted avg                                            90.47      89.81      90.11     300124
--+
--+[INFO] - Domain report:
--+label                                                precision    recall       f1           support
--+0 (label_id: 0)                                        100.00     100.00     100.00       2744
--+-------------------
--+micro avg                                              100.00     100.00     100.00       2744
--+macro avg                                              100.00     100.00     100.00       2744
--+weighted avg                                           100.00     100.00     100.00       2744
--+
--+Testing: 100%|| 100/100 [00:10<00:00,  9.29it/s]
--+--------------------------------------------------------------------------------
--+DATALOADER:0 TEST RESULTS
--+{'domain_f1': 100.0,
--+ 'domain_precision': 100.0,
--+ 'domain_recall': 100.0,
--+ 'punct_f1': 31.962158203125,
--+ 'punct_precision': 31.560827255249023,
--+ 'punct_recall': 32.61237335205078,
--+ 'test_loss': 0.23370929062366486}
--+
--+ #####################################################################
--+### 2021-02-09_16-54-29 domain adversarial
-- 
--diff --git a/experiment/config.yaml b/experiment/config.yaml
--index 1911de8..8226922 100644
----- a/experiment/config.yaml
--+++ b/experiment/config.yaml
--@@ -66,7 +66,7 @@ model:
--             # - ${base_path}/ted_talks_processed #
--             - ${base_path}/open_subtitles_processed #  
--         unlabelled:
---            - ${base_path}/ted_talks_processed #
--+            # - ${base_path}/ted_talks_processed #
--             # - ${base_path}/open_subtitles_processed #  
--             # parameters for dataset preprocessing
--         max_seq_length: 128
--@@ -79,7 +79,7 @@ model:
--         pin_memory: true
--         drop_last: true
--         num_labels: 10
---        num_domains: 2
--+        num_domains: 1
--         test_unlabelled: true
-- 
--         train_ds:
--@@ -123,7 +123,7 @@ model:
--         log_softmax: false
--         use_transformer_init: true
--         loss: 'cel'
---        gamma: 0.2 #0.1 # coefficient of gradient reversal
--+        gamma: 0 #0.1 # coefficient of gradient reversal
--         pooling: 'mean_max' # 'mean' mean_max
--         idx_conditioned_on: 0
--     
--diff --git a/experiment/info.log b/experiment/info.log
--index bc52a1b..e69de29 100644
--Binary files a/experiment/info.log and b/experiment/info.log differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/hparams.yaml
-deleted file mode 100644
-index 679f833..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/hparams.yaml
-+++ /dev/null
-@@ -1,110 +0,0 @@
--seed: 42
--trainer:
--  gpus: 1
--  num_nodes: 1
--  max_epochs: 15
--  max_steps: null
--  accumulate_grad_batches: 4
--  gradient_clip_val: 0
--  amp_level: O1
--  precision: 16
--  accelerator: ddp
--  checkpoint_callback: false
--  logger: false
--  log_every_n_steps: 1
--  val_check_interval: 1.0
--  resume_from_checkpoint: null
--exp_manager:
--  exp_dir: /home/nxingyu/project/
--  name: Punctuation_with_Domain_discriminator
--  create_tensorboard_logger: true
--  create_checkpoint_callback: true
--base_path: /home/nxingyu/data
--tmp_path: /home/nxingyu/data/tmp
--model:
--  nemo_path: null
--  transformer_path: google/electra-small-discriminator
--  unfrozen: 0
--  maximum_unfrozen: 1
--  unfreeze_step: 1
--  punct_label_ids:
--  - ''
--  - '!'
--  - ','
--  - '-'
--  - .
--  - ':'
--  - ;
--  - '?'
--  - —
--  - …
--  punct_class_weights: false
--  dataset:
--    data_dir: /home/nxingyu/data
--    labelled:
--    - /home/nxingyu/data/open_subtitles_processed
--    unlabelled: null
--    max_seq_length: 128
--    pad_label: ''
--    ignore_extra_tokens: false
--    ignore_start_end: false
--    use_cache: false
--    num_workers: 4
--    pin_memory: true
--    drop_last: true
--    num_labels: 10
--    num_domains: 1
--    test_unlabelled: true
--    train_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--    validation_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--  tokenizer:
--    tokenizer_name: google/electra-small-discriminator
--    vocab_file: null
--    tokenizer_model: null
--    special_tokens: null
--  language_model:
--    pretrained_model_name: google/electra-small-discriminator
--    lm_checkpoint: null
--    config_file: null
--    config: null
--  punct_head:
--    punct_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: dice
--  domain_head:
--    domain_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: cel
--    gamma: 0
--    pooling: mean_max
--    idx_conditioned_on: 0
--  dice_loss:
--    epsilon: 0.01
--    alpha: 3
--    macro_average: true
--  focal_loss:
--    gamma: 1
--  optim:
--    name: adamw
--    lr: 0.01
--    weight_decay: 0.0
--    sched:
--      name: CosineAnnealing
--      warmup_steps: null
--      warmup_ratio: 0.1
--      min_lr: 1.0e-10
--      last_epoch: -1
--      monitor: val_loss
--      reduce_on_plateau: false
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/lightning_logs.txt
-deleted file mode 100644
-index e496097..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/lightning_logs.txt
-+++ /dev/null
-@@ -1,26 +0,0 @@
--Global seed set to 42
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
--Using native 16bit precision.
--Global seed set to 42
--initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 513   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--299 K     Trainable params
--13.2 M    Non-trainable params
--13.5 M    Total params
--Epoch 0, global step 5304: val_loss reached 0.00440 (best 0.00440), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.00-epoch=0.ckpt" as top 3
--Epoch 1, global step 10609: val_loss reached -0.00234 (best -0.00234), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.00-epoch=1.ckpt" as top 3
--Saving latest checkpoint...
--Epoch 2, global step 12262: val_loss reached -0.00234 (best -0.00234), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.00-epoch=2.ckpt" as top 3
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/nemo_error_log.txt
-deleted file mode 100644
-index 1193d63..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/nemo_error_log.txt
-+++ /dev/null
-@@ -1,10 +0,0 @@
--[NeMo W 2021-02-09 17:19:42 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 17:20:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 18:07:34 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f68046f2df0> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 18:14:10 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f680eaeb8b0> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/nemo_log_globalrank-0_localrank-0.txt
-deleted file mode 100644
-index bd490f0..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42/nemo_log_globalrank-0_localrank-0.txt
-+++ /dev/null
-@@ -1,12 +0,0 @@
--[NeMo I 2021-02-09 17:19:42 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_17-19-42
--[NeMo I 2021-02-09 17:19:42 exp_manager:519] TensorboardLogger has been set up
--[NeMo W 2021-02-09 17:19:42 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 17:20:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 18:07:34 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f68046f2df0> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 18:14:10 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f680eaeb8b0> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/events.out.tfevents.1613379232.Titan.2419.0 b/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/events.out.tfevents.1613379232.Titan.2419.0
-index 55e3ccc..dbd3b7a 100644
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/events.out.tfevents.1613379232.Titan.2419.0 and b/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/events.out.tfevents.1613379232.Titan.2419.0 differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/lightning_logs.txt
-index 46e1e6e..74b73ca 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/lightning_logs.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/lightning_logs.txt
-@@ -21,3 +21,28 @@ initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
- 7.1 M     Trainable params
- 108 M     Non-trainable params
- 115 M     Total params
-+Epoch 0, global step 656: val_loss reached 146.20628 (best 146.20628), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=146.21-epoch=0.ckpt" as top 3
-+Epoch 1, global step 1313: val_loss reached 25.70970 (best 25.70970), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=25.71-epoch=1.ckpt" as top 3
-+Epoch 2, global step 1970: val_loss reached 7.76796 (best 7.76796), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=7.77-epoch=2.ckpt" as top 3
-+Epoch 3, global step 2627: val_loss reached 1.31258 (best 1.31258), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=1.31-epoch=3.ckpt" as top 3
-+Epoch 4, global step 3284: val_loss reached 2.27040 (best 1.31258), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=2.27-epoch=4.ckpt" as top 3
-+Epoch 5, global step 3941: val_loss reached 7.67466 (best 1.31258), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=7.67-epoch=5.ckpt" as top 3
-+Saving latest checkpoint...
-+Epoch 6, step 4031: val_loss was not in top 3
-+Global seed set to 42
-+
-+  | Name                | Type                 | Params
-+-------------------------------------------------------------
-+0 | transformer         | ElectraModel         | 108 M 
-+1 | punct_classifier    | TokenClassifier      | 7.7 K 
-+2 | domain_classifier   | SequenceClassifier   | 3.1 K 
-+3 | punctuation_loss    | FocalDiceLoss        | 0     
-+4 | bilstm              | LSTM                 | 7.1 M 
-+5 | domain_loss         | CrossEntropyLoss     | 0     
-+6 | agg_loss            | AggregatorLoss       | 0     
-+7 | punct_class_report  | ClassificationReport | 0     
-+8 | domain_class_report | ClassificationReport | 0     
-+-------------------------------------------------------------
-+14.2 M    Trainable params
-+101 M     Non-trainable params
-+115 M     Total params
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/nemo_error_log.txt
-index 8512f6f..40a936f 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/nemo_error_log.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/nemo_error_log.txt
-@@ -2,3 +2,12 @@
- [NeMo W 2021-02-15 16:53:52 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-       warnings.warn(*args, **kwargs)
-     
-+[NeMo W 2021-02-15 19:19:00 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5700031460> was reported to be 2627 (when accessing len(dataloader)), but 2628 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-+[NeMo W 2021-02-15 19:29:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5700085d30> was reported to be 328 (when accessing len(dataloader)), but 329 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-+[NeMo W 2021-02-16 08:30:30 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-+      warnings.warn(*args, **kwargs)
-+    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/nemo_log_globalrank-0_localrank-0.txt
-index ffe9ee9..bf9ce42 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/nemo_log_globalrank-0_localrank-0.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-15_16-53-18/nemo_log_globalrank-0_localrank-0.txt
-@@ -4,3 +4,12 @@
- [NeMo W 2021-02-15 16:53:52 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-       warnings.warn(*args, **kwargs)
-     
-+[NeMo W 2021-02-15 19:19:00 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5700031460> was reported to be 2627 (when accessing len(dataloader)), but 2628 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-+[NeMo W 2021-02-15 19:29:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5700085d30> was reported to be 328 (when accessing len(dataloader)), but 329 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-+[NeMo W 2021-02-16 08:30:30 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-+      warnings.warn(*args, **kwargs)
-+    
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index 9762568..fb8cee4 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -2,7 +2,7 @@ seed: 42
- trainer:
-     gpus: 1 # the number of gpus, 0 for CPU
-     num_nodes: 1
--    max_epochs: 8
-+    max_epochs: 15
-     max_steps: null # precedence over max_epochs
-     accumulate_grad_batches: 4 # accumulates grads every k batches
-     gradient_clip_val: 0
-@@ -24,7 +24,7 @@ trainer:
-     # amp_level: O0 # O1/O2 for mixed precision
-     # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-     # # accelerator: ddp
--    # checkpoint_callback: false  # Provided by exp_manager
-+    # checkpoint_callback: false # Provided by exp_manager
-     # logger: false #false  # Provided by exp_manager
-     # log_every_n_steps: 1  # Interval of logging.
-     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-@@ -41,11 +41,10 @@ tmp_path: /home/nxingyu/data/tmp # /tmp #
- 
- model:
-     nemo_path: null
--    transformer_path: google/electra-base-discriminator # roberta-base #google/electra-base-discriminator # distilbert-base-uncased # filename to save the model and associated artifacts to .nemo file
-+    transformer_path: google/electra-base-discriminator #google/electra-base-discriminator # roberta-base #google/electra-base-discriminator # distilbert-base-uncased # filename to save the model and associated artifacts to .nemo file
-     unfrozen: 0
--    maximum_unfrozen: 2
-+    maximum_unfrozen: 1
-     unfreeze_step: 1
--    # unfreeze_every: 3
-     punct_label_ids:
-         - ""
-         - "!"
-@@ -58,16 +57,16 @@ model:
-         - "—"
-         - "…"
- 
--    punct_class_weights: false
-+    punct_class_weights: false #false
-     
-     dataset:
-         data_dir: /home/nxingyu/data # /root/data # 
-         labelled:
--            # - ${base_path}/ted_talks_processed #
--            - ${base_path}/open_subtitles_processed #  
--        unlabelled:
-             - ${base_path}/ted_talks_processed #
-             # - ${base_path}/open_subtitles_processed #  
-+        unlabelled:
-+            # - ${base_path}/ted_talks_processed #
-+            # - ${base_path}/open_subtitles_processed #  
-             # parameters for dataset preprocessing
-         max_seq_length: 128
-         pad_label: ''
-@@ -75,18 +74,18 @@ model:
-         ignore_start_end: false
-         use_cache: false
-         # shared among dataloaders
--        num_workers:  12
--        pin_memory: true
-+        num_workers:  4
-+        pin_memory: false
-         drop_last: true
-         num_labels: 10
--        num_domains: 2
-+        num_domains: 1
-         test_unlabelled: true
-         attach_label_to_end: none # false if attach to start none if dont mask
- 
-         train_ds:
-             shuffle: true
-             num_samples: -1
--            batch_size: 32
-+            batch_size: 16
-             manual_len: 0 #default 0 84074
- 
-         validation_ds:
-@@ -113,7 +112,7 @@ model:
-     punct_head:
-         punct_num_fc_layers: 1
-         fc_dropout: 0.1
--        activation: 'relu'
-+        activation: 'gelu'
-         log_softmax: false
-         use_transformer_init: true
-         loss: 'dice'
-@@ -126,7 +125,7 @@ model:
-         log_softmax: false
-         use_transformer_init: true
-         loss: 'cel'
--        gamma: 0.01 #0.1 # coefficient of gradient reversal
-+        gamma: 0.1 #0.1 # coefficient of gradient reversal
-         pooling: 'mean_max' # 'mean' mean_max
-         idx_conditioned_on: 0
-     
-@@ -136,11 +135,11 @@ model:
-         macro_average: true
- 
-     focal_loss: 
--        gamma: 1
-+        gamma: 2
- 
-     frozen_lr:
-         - 1e-2
--        - 5e-3
-+        - 1e-3
- 
-     optim:
-         name: adamw
-diff --git a/experiment/core/layers/multi_layer_perceptron.py b/experiment/core/layers/multi_layer_perceptron.py
-index 63318cc..ba4489f 100644
---- a/experiment/core/layers/multi_layer_perceptron.py
-+++ b/experiment/core/layers/multi_layer_perceptron.py
-@@ -14,6 +14,7 @@
- # limitations under the License.
- 
- import torch
-+from torch import nn
- 
- class MultiLayerPerceptron(torch.nn.Module):
-     """
-@@ -37,10 +38,16 @@ class MultiLayerPerceptron(torch.nn.Module):
-     ):
-         super().__init__()
-         self.layers = 0
-+        activations = {
-+            'relu': nn.ReLU(),
-+            'gelu': nn.GELU(),
-+            'sigmoid': nn.Sigmoid(),
-+            'tanh': nn.Tanh()
-+        }
-         for _ in range(num_layers - 1):
-             layer = torch.nn.Linear(hidden_size, hidden_size)
-             setattr(self, f'layer{self.layers}', layer)
--            setattr(self, f'layer{self.layers + 1}', getattr(torch, activation))
-+            setattr(self, f'layer{self.layers + 1}', activations[activation]) #getattr(torch, activation)
-             self.layers += 2
-         layer = torch.nn.Linear(hidden_size, num_classes)
-         setattr(self, f'layer{self.layers}', layer)
-diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
-index d5e42aa..82b8e1f 100644
---- a/experiment/data/punctuation_dataset_multi.py
-+++ b/experiment/data/punctuation_dataset_multi.py
-@@ -66,12 +66,12 @@ class PunctuationDomainDataset(IterableDataset):
-         self.tmp_path=tmp_path
-         self.attach_label_to_end=attach_label_to_end
-         if not (os.path.exists(self.target_file)):
--            os.system(f'cp {self.csv_file} {self.target_file}')
-+            os.system(f"sed '1d' {self.csv_file} > {self.target_file}")
- 
-     def __iter__(self):
-         self.dataset=iter(pd.read_csv(
-                 self.csv_file,
--                skiprows=(0 % self.len)*self.num_samples+1,
-+                skiprows=(0 % self.len)*self.num_samples,
-                 header=None,
-                 dtype=str,
-                 chunksize=self.num_samples,
-@@ -83,7 +83,7 @@ class PunctuationDomainDataset(IterableDataset):
-         batch = next(self.dataset)[1]
- 
-         l=batch.str.split().map(len).values
--        n=16
-+        n=8
-         a=np.maximum((l-self.max_seq_length*n).clip(min=0),(l*np.random.random(l.__len__())).astype(int))
-         b=np.minimum(l,a+self.max_seq_length*n)
-         batch=pd.DataFrame({'t':batch,'a':a,'b':b}).apply(lambda row: ' '.join(row.t.split()[row.a:row.b]),axis=1)
-diff --git a/experiment/info.log b/experiment/info.log
-index 188b5f1..e69de29 100644
-Binary files a/experiment/info.log and b/experiment/info.log differ
-diff --git a/experiment/main.py b/experiment/main.py
-index 324f489..7f97cb4 100644
---- a/experiment/main.py
-+++ b/experiment/main.py
-@@ -23,6 +23,7 @@ snoop.install()
- 
- @hydra.main(config_name="config")
- def main(cfg: DictConfig)->None:
-+    os.environ["TOKENIZERS_PARALLELISM"] = "false"
-     torch.set_printoptions(sci_mode=False)
-     data_id = str(int(time()))
-     def savecounter():
-@@ -34,7 +35,7 @@ def main(cfg: DictConfig)->None:
- 
-     pp(cfg)
-     pl.seed_everything(cfg.seed)
--    trainer = pl.Trainer(**cfg.trainer)
-+    trainer = pl.Trainer(**cfg.trainer,track_grad_norm=2)
-     exp_manager(trainer, cfg.exp_manager)
-     model = PunctuationDomainModel(cfg=cfg, trainer=trainer, data_id = data_id)
-     
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index 0e46e67..edb0d55 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -245,9 +245,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         }
- 
-     def validation_epoch_end(self, outputs):
--        # print('next epoch:',self.current_epoch+1, (self.current_epoch+1)%self.hparams.model.unfreeze_every)
--        # if ((self.current_epoch+1)%self.hparams.model.unfreeze_every==0):
--        #     self.unfreeze(self.hparams.model.unfreeze_step)
-+        
-         self.dm.train_dataset.shuffle()
-         if outputs is not None and len(outputs) == 0:
-             return {}
-@@ -690,6 +688,10 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         # for last in last_iter:
-         #     continue
-         # set_requires_grad_for_module(last, True)
-+        for name, param in self.transformer.named_parameters():                
-+            if param.requires_grad:
-+                print(name)
-+
- 
-     def freeze(self) -> None:
-         try:
-@@ -701,6 +703,9 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
- 
-         self.frozen = len(encoder.layer)-self.hparams.model.unfrozen
-         self.freeze_transformer_to(self.frozen)
-+        for name, param in encoder.named_parameters(): 
-+            if param.requires_grad: 
-+                print(name, param.data)
- 
-     def unfreeze(self, i: int = 1):
-         self.frozen -= i
-@@ -710,6 +715,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         #     self.frozen+=self.hparams.model.unfrozen-self.hparams.model.maximum_unfrozen
-         #     self.hparams.model.unfrozen=self.hparams.model.maximum_unfrozen
-         self.freeze_transformer_to(max(0, self.frozen))
-+        
- 
-     def teardown(self, stage: str):
-         """
diff --git a/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/hparams.yaml
deleted file mode 100644
index cb47d20..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/hparams.yaml
+++ /dev/null
@@ -1,116 +0,0 @@
-seed: 42
-trainer:
-  gpus: 1
-  num_nodes: 1
-  max_epochs: 15
-  max_steps: null
-  accumulate_grad_batches: 4
-  gradient_clip_val: 0
-  amp_level: O1
-  precision: 16
-  accelerator: ddp
-  checkpoint_callback: false
-  logger: false
-  log_every_n_steps: 1
-  val_check_interval: 1.0
-  resume_from_checkpoint: null
-exp_manager:
-  exp_dir: /home/nxingyu/project/
-  name: Punctuation_with_Domain_discriminator
-  create_tensorboard_logger: true
-  create_checkpoint_callback: true
-base_path: /home/nxingyu/data
-tmp_path: /home/nxingyu/data/tmp
-model:
-  nemo_path: null
-  transformer_path: google/electra-base-discriminator
-  unfrozen: 0
-  maximum_unfrozen: 1
-  unfreeze_step: 1
-  punct_label_ids:
-  - ''
-  - '!'
-  - ','
-  - '-'
-  - .
-  - ':'
-  - ;
-  - '?'
-  - —
-  - …
-  punct_class_weights: false
-  dataset:
-    data_dir: /home/nxingyu/data
-    labelled:
-    - /home/nxingyu/data/ted_talks_processed
-    unlabelled: null
-    max_seq_length: 128
-    pad_label: ''
-    ignore_extra_tokens: false
-    ignore_start_end: false
-    use_cache: false
-    num_workers: 4
-    pin_memory: false
-    drop_last: true
-    num_labels: 10
-    num_domains: 1
-    test_unlabelled: true
-    attach_label_to_end: none
-    train_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 16
-      manual_len: 0
-    validation_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 32
-  tokenizer:
-    tokenizer_name: google/electra-base-discriminator
-    vocab_file: null
-    tokenizer_model: null
-    special_tokens: null
-  language_model:
-    pretrained_model_name: google/electra-base-discriminator
-    lm_checkpoint: null
-    config_file: null
-    config: null
-  punct_head:
-    punct_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: gelu
-    log_softmax: false
-    use_transformer_init: true
-    loss: dice
-    bilstm: true
-  domain_head:
-    domain_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: cel
-    gamma: 0.1
-    pooling: mean_max
-    idx_conditioned_on: 0
-  dice_loss:
-    epsilon: 0.01
-    alpha: 3
-    macro_average: true
-  focal_loss:
-    gamma: 2
-  frozen_lr:
-  - 0.01
-  - 0.001
-  optim:
-    name: adamw
-    lr: 0.01
-    weight_decay: 0.0
-    sched:
-      name: WarmupAnnealing
-      warmup_steps: null
-      warmup_ratio: 0.1
-      min_lr: 1.0e-08
-      last_epoch: -1
-      monitor: val_loss
-      reduce_on_plateau: false
diff --git a/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/lightning_logs.txt
deleted file mode 100644
index dd0e1a5..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/lightning_logs.txt
+++ /dev/null
@@ -1,58 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
-Using native 16bit precision.
-Global seed set to 42
-initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 108 M 
-1 | punct_classifier    | TokenClassifier      | 7.7 K 
-2 | domain_classifier   | SequenceClassifier   | 1.5 K 
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | bilstm              | LSTM                 | 7.1 M 
-5 | domain_loss         | CrossEntropyLoss     | 0     
-6 | agg_loss            | AggregatorLoss       | 0     
-7 | punct_class_report  | ClassificationReport | 0     
-8 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-7.1 M     Trainable params
-108 M     Non-trainable params
-115 M     Total params
-Epoch 0, global step 49: val_loss reached 0.36978 (best 0.36978), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.37-epoch=0.ckpt" as top 3
-Epoch 1, global step 99: val_loss reached 0.35017 (best 0.35017), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.35-epoch=1.ckpt" as top 3
-Epoch 2, global step 149: val_loss reached 0.42292 (best 0.35017), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.42-epoch=2.ckpt" as top 3
-Epoch 3, global step 199: val_loss reached 0.39436 (best 0.35017), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.39-epoch=3.ckpt" as top 3
-Epoch 4, step 249: val_loss was not in top 3
-Epoch 5, global step 299: val_loss reached 0.37925 (best 0.35017), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.38-epoch=5.ckpt" as top 3
-Epoch 6, step 349: val_loss was not in top 3
-Epoch 7, step 399: val_loss was not in top 3
-Epoch 8, step 449: val_loss was not in top 3
-Epoch 9, step 499: val_loss was not in top 3
-Epoch 10, step 549: val_loss was not in top 3
-Epoch 11, step 599: val_loss was not in top 3
-Epoch 12, step 649: val_loss was not in top 3
-Epoch 13, global step 699: val_loss reached 0.37486 (best 0.35017), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.37-epoch=13.ckpt" as top 3
-Epoch 14, global step 749: val_loss reached 0.36074 (best 0.35017), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.36-epoch=14.ckpt" as top 3
-Saving latest checkpoint...
-Global seed set to 42
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 108 M 
-1 | punct_classifier    | TokenClassifier      | 7.7 K 
-2 | domain_classifier   | SequenceClassifier   | 1.5 K 
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | bilstm              | LSTM                 | 7.1 M 
-5 | domain_loss         | CrossEntropyLoss     | 0     
-6 | agg_loss            | AggregatorLoss       | 0     
-7 | punct_class_report  | ClassificationReport | 0     
-8 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-14.2 M    Trainable params
-101 M     Non-trainable params
-115 M     Total params
-Epoch 0, global step 799: val_loss reached 0.36074 (best 0.35017), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.36-epoch=0.ckpt" as top 3
-Epoch 1, step 849: val_loss was not in top 3
diff --git a/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/nemo_error_log.txt
deleted file mode 100644
index 19639a7..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/nemo_error_log.txt
+++ /dev/null
@@ -1,13 +0,0 @@
-[NeMo W 2021-02-16 13:11:12 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-16 13:11:28 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-16 13:13:06 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fb810090f40> was reported to be 199 (when accessing len(dataloader)), but 200 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-16 13:13:18 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fb8210e6f10> was reported to be 12 (when accessing len(dataloader)), but 13 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-16 13:40:08 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 009a860..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,15 +0,0 @@
-[NeMo I 2021-02-16 13:11:12 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-16_13-11-12
-[NeMo I 2021-02-16 13:11:12 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-16 13:11:12 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-16 13:11:28 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-16 13:13:06 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fb810090f40> was reported to be 199 (when accessing len(dataloader)), but 200 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-16 13:13:18 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fb8210e6f10> was reported to be 12 (when accessing len(dataloader)), but 13 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-16 13:40:08 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-    
diff --git a/experiment/config.yaml b/experiment/config.yaml
index f355fd6..4d4d34a 100644
--- a/experiment/config.yaml
+++ b/experiment/config.yaml
@@ -116,7 +116,7 @@ model:
         log_softmax: false
         use_transformer_init: true
         loss: 'dice'
-        bilstm: false
+        bilstm: true
 
     domain_head:
         domain_num_fc_layers: 1
diff --git a/experiment/info.log b/experiment/info.log
index 9d50d24..e69de29 100644
--- a/experiment/info.log
+++ b/experiment/info.log
@@ -1,872 +0,0 @@
-[INFO] - shuffling train set
-[INFO] - Optimizer config = AdamW (
-Parameter Group 0
-    amsgrad: False
-    betas: (0.9, 0.999)
-    eps: 1e-08
-    lr: 0.01
-    weight_decay: 0.0
-)
-[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fb818b2a3d0>" 
-will be used during training (effective maximum steps = 750) - 
-Parameters : 
-(warmup_steps: null
-warmup_ratio: 0.1
-min_lr: 1.0e-08
-last_epoch: -1
-max_steps: 750
-)
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                           0.00       0.00       0.00      38870
-! (label_id: 1)                                          0.00       0.00       0.00         18
-, (label_id: 2)                                          2.75       1.13       1.60       2476
-- (label_id: 3)                                          0.00       0.00       0.00        258
-. (label_id: 4)                                          0.00       0.00       0.00       2242
-: (label_id: 5)                                          0.00       0.00       0.00         46
-; (label_id: 6)                                          0.00       0.00       0.00         16
-? (label_id: 7)                                          0.13       2.41       0.25        166
-— (label_id: 8)                                          0.34      94.44       0.68        144
-… (label_id: 9)                                          0.00       0.00       0.00         14
--------------------
-micro avg                                                0.38       0.38       0.38      44250
-macro avg                                                0.32       9.80       0.25      44250
-weighted avg                                             0.16       0.38       0.09      44250
-
--------------------
- 0.00  0.00  0.00  0.00  2.00  0.00  0.00  2.00  0.00  0.00
-68.00  0.00  4.00  0.00  8.00  0.00  0.00  2.00  0.00  0.00
-946.00  0.00 28.00  6.00 40.00  0.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-42.00  0.00  2.00  4.00  0.00  2.00  0.00  0.00  0.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-94.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-2680.00  0.00 174.00 68.00 74.00  2.00  4.00  4.00  8.00  0.00
-35040.00 18.00 2268.00 180.00 2118.00 42.00 12.00 158.00 136.00 14.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00        378
--------------------
-micro avg                                              100.00     100.00     100.00        378
-macro avg                                              100.00     100.00     100.00        378
-weighted avg                                           100.00     100.00     100.00        378
-
--------------------
-378.00
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.70      95.57      97.11     225872
-! (label_id: 1)                                          2.58      31.58       4.76         76
-, (label_id: 2)                                         46.86      70.17      56.20      15584
-- (label_id: 3)                                         71.78      60.95      65.92       1352
-. (label_id: 4)                                         79.30      67.00      72.63      13588
-: (label_id: 5)                                         11.42      37.18      17.47        312
-; (label_id: 6)                                          0.00       0.00       0.00        148
-? (label_id: 7)                                         62.90      63.41      63.16        984
-— (label_id: 8)                                         10.90      15.67      12.85        868
-… (label_id: 9)                                          0.00       0.00       0.00         88
--------------------
-micro avg                                               91.80      91.80      91.80     258872
-macro avg                                               38.44      44.15      39.01     258872
-weighted avg                                            93.77      91.80      92.58     258872
-
--------------------
-215876.00  4.00 1836.00 396.00 448.00 32.00  4.00 32.00 72.00 20.00
-112.00 24.00 224.00 12.00 436.00  4.00 16.00 60.00 36.00  8.00
-8896.00 20.00 10936.00 100.00 2756.00 80.00 40.00 84.00 396.00 28.00
-300.00  0.00  8.00 824.00 16.00  0.00  0.00  0.00  0.00  0.00
-316.00 20.00 1584.00 12.00 9104.00 52.00 68.00 160.00 152.00 12.00
-104.00  8.00 304.00  4.00 376.00 116.00 16.00 20.00 68.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-88.00  0.00 120.00  0.00 144.00  4.00  4.00 624.00  8.00  0.00
-180.00  0.00 572.00  4.00 308.00 24.00  0.00  4.00 136.00 20.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       2236
--------------------
-micro avg                                              100.00     100.00     100.00       2236
-macro avg                                              100.00     100.00     100.00       2236
-weighted avg                                           100.00     100.00     100.00       2236
-
--------------------
-2236.00
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.68      96.17      97.41     225872
-! (label_id: 1)                                          2.01      42.11       3.84         76
-, (label_id: 2)                                         51.22      70.71      59.41      15584
-- (label_id: 3)                                         73.86      66.86      70.19       1352
-. (label_id: 4)                                         78.60      71.56      74.92      13588
-: (label_id: 5)                                         31.67      24.36      27.54        312
-; (label_id: 6)                                          0.00       0.00       0.00        148
-? (label_id: 7)                                         64.82      66.67      65.73        984
-— (label_id: 8)                                         14.72      13.36      14.01        868
-… (label_id: 9)                                          0.00       0.00       0.00         88
--------------------
-micro avg                                               92.62      92.62      92.62     258872
-macro avg                                               41.56      45.18      41.30     258872
-weighted avg                                            94.03      92.62      93.20     258872
-
--------------------
-217232.00  4.00 1952.00 348.00 408.00 36.00  8.00 36.00 84.00 20.00
-208.00 32.00 408.00  4.00 776.00 12.00 24.00 60.00 52.00 16.00
-7432.00 20.00 11020.00 88.00 2312.00 104.00 40.00 64.00 404.00 32.00
-316.00  0.00  0.00 904.00  4.00  0.00  0.00  0.00  0.00  0.00
-336.00 20.00 1800.00  8.00 9724.00 68.00 68.00 148.00 192.00  8.00
-28.00  0.00 60.00  0.00 64.00 76.00  0.00  4.00  8.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-108.00  0.00 100.00  0.00 128.00  8.00  0.00 656.00 12.00  0.00
-212.00  0.00 244.00  0.00 172.00  8.00  8.00 16.00 116.00 12.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       2236
--------------------
-micro avg                                              100.00     100.00     100.00       2236
-macro avg                                              100.00     100.00     100.00       2236
-weighted avg                                           100.00     100.00     100.00       2236
-
--------------------
-2236.0[I[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          96.90      99.04      97.96     225872
-! (label_id: 1)                                          0.00       0.00       0.00         76
-, (label_id: 2)                                         46.04      64.81      53.84      15584
-- (label_id: 3)                                         82.61      61.83      70.73       1352
-. (label_id: 4)                                         88.96      26.82      41.21      13588
-: (label_id: 5)                                         30.51      23.08      26.28        312
-; (label_id: 6)                                          0.00       0.00       0.00        148
-? (label_id: 7)                                         86.83      58.94      70.22        984
-— (label_id: 8)                                         66.67       3.69       6.99        868
-… (label_id: 9)                                          0.00       0.00       0.00         88
--------------------
-micro avg                                               92.31      92.31      92.31     258872
-macro avg                                               49.85      33.82      36.72     258872
-weighted avg                                            93.01      92.31      91.57     258872
-
--------------------
-223712.00 12.00 5200.00 460.00 1096.00 88.00 16.00 80.00 172.00 24.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-1824.00 56.00 10100.00 56.00 8728.00 132.00 132.00 252.00 612.00 44.00
-172.00  0.00  0.00 836.00  4.00  0.00  0.00  0.00  0.00  0.00
-112.00  8.00 200.00  0.00 3644.00 16.00  0.00 68.00 32.00 16.00
-16.00  0.00 48.00  0.00 84.00 72.00  0.00  4.00 12.00  0.00
- 0.00  0.00  0.00  0.00 12.00  0.00  0.00  0.00  4.00  0.00
-36.00  0.00 32.00  0.00 12.00  4.00  0.00 580.00  4.00  0.00
- 0.00  0.00  4.00  0.00  8.00  0.00  0.00  0.00 32.00  4.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       2236
--------------------
-micro avg                                              100.00     100.00     100.00       2236
-macro avg                                              100.00     100.00     100.00       2236
-weighted avg                                           100.00     100.00     100.00       2236
-
--------------------
-2236.00
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.15      98.08      98.12     225872
-! (label_id: 1)                                          0.00       0.00       0.00         76
-, (label_id: 2)                                         61.53      67.71      64.48      15584
-- (label_id: 3)                                         80.65      66.57      72.93       1352
-. (label_id: 4)                                         78.81      75.10      76.91      13588
-: (label_id: 5)                                         14.21      35.90      20.36        312
-; (label_id: 6)                                          0.00       0.00       0.00        148
-? (label_id: 7)                                         74.04      70.73      72.35        984
-— (label_id: 8)                                         37.50       9.68      15.38        868
-… (label_id: 9)                                          0.00       0.00       0.00         88
--------------------
-micro avg                                               94.29      94.29      94.29     258872
-macro avg                                               44.49      42.38      42.05     258872
-weighted avg                                            94.33      94.29      94.26     258872
-
--------------------
-221536.00  8.00 2912.00 376.00 624.00 60.00 12.00 56.00 104.00 20.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-3416.00 32.00 10552.00 68.00 2400.00 80.00 72.00 72.00 412.00 44.00
-208.00  0.00  4.00 900.00  4.00  0.00  0.00  0.00  0.00  0.00
-528.00 32.00 1712.00  8.00 10204.00 56.00 60.00 124.00 204.00 20.00
-68.00  4.00 244.00  0.00 264.00 112.00  4.00 36.00 56.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-76.00  0.00 92.00  0.00 64.00  4.00  0.00 696.00  8.00  0.00
-40.00  0.00 68.00  0.00 28.00  0.00  0.00  0.00 84.00  4.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       2236
--------------------
-micro avg                                              100.00     100.00     100.00       2236
-macro avg                                              100.00     100.00     100.00       2236
-weighted avg                                           100.00     100.00     100.00       2236
-
--------------------
-2236.00
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.79      98.40      98.10     225872
-! (label_id: 1)                                          0.00       0.00       0.00         76
-, (label_id: 2)                                         68.28      56.70      61.95      15584
-- (label_id: 3)                                         61.96      72.78      66.94       1352
-. (label_id: 4)                                         73.34      83.81      78.23      13588
-: (label_id: 5)                                         39.53      21.79      28.10        312
-; (label_id: 6)                                          0.00       0.00       0.00        148
-? (label_id: 7)                                         65.11      73.58      69.08        984
-— (label_id: 8)                                         35.00       9.68      15.16        868
-… (label_id: 9)                                          0.00       0.00       0.00         88
--------------------
-micro avg                                               94.39      94.39      94.39     258872
-macro avg                                               44.10      41.67      41.76     258872
-weighted avg                                            94.02      94.39      94.12     258872
-
--------------------
-222268.00 12.00 3740.00 280.00 652.00 80.00 12.00 84.00 140.00 24.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-2172.00 28.00 8836.00 80.00 1296.00 68.00 52.00 28.00 340.00 40.00
-592.00  0.00  8.00 984.00  4.00  0.00  0.00  0.00  0.00  0.00
-752.00 32.00 2756.00  8.00 11388.00 76.00 76.00 136.00 284.00 20.00
-12.00  0.00 32.00  0.00 44.00 68.00  0.00  8.00  8.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-60.00  4.00 120.00  0.00 172.00 16.00  4.00 724.00 12.00  0.00
-16.00  0.00 92.00  0.00 32.00  4.00  4.00  4.00 84.00  4.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       2236
--------------------
-micro avg                                              100.00     100.00     100.00       2236
-macro avg                                              100.00     100.00     100.00       2236
-weighted avg                                           100.00     100.00     100.00       2236
-
--------------------
-2236.00
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.37      97.46      97.91     225872
-! (label_id: 1)                                          0.00       0.00       0.00         76
-, (label_id: 2)                                         55.23      75.03      63.63      15584
-- (label_id: 3)                                         69.55      73.67      71.55       1352
-. (label_id: 4)                                         84.21      67.06      74.66      13588
-: (label_id: 5)                                         57.58      24.36      34.23        312
-; (label_id: 6)                                          0.00       0.00       0.00        148
-? (label_id: 7)                                         71.89      72.76      72.32        984
-— (label_id: 8)                                         26.09      16.59      20.28        868
-… (label_id: 9)                                          0.00       0.00       0.00         88
--------------------
-micro avg                                               93.81      93.81      93.81     258872
-macro avg                                               46.29      42.69      43.46     258872
-weighted avg                                            94.37      93.81      93.94     258872
-
--------------------
-220124.00  4.00 2568.00 268.00 560.00 56.00  0.00 60.00 112.00 20.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-4928.00 44.00 11692.00 84.00 3564.00 120.00 84.00 108.00 492.00 52.00
-416.00  0.00 16.00 996.00  4.00  0.00  0.00  0.00  0.00  0.00
-332.00 24.00 1052.00  4.00 9112.00 40.00 48.00 88.00 108.00 12.00
- 0.00  0.00  8.00  0.00 40.00 76.00  0.00  4.00  4.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-64.00  0.00 72.00  0.00 124.00  8.00  4.00 716.00  8.00  0.00
- 8.00  4.00 176.00  0.00 184.00 12.00 12.00  8.00 144.00  4.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       2236
--------------------
-micro avg                                              100.00     100.00     100.00       2236
-macro avg                                              100.00     100.00     100.00       2236
-weighted avg                                           100.00     100.00     100.00       2236
-
--------------------
-2236.00
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.38      97.04      97.71     225872
-! (label_id: 1)                                          0.00       0.00       0.00         76
-, (label_id: 2)                                         51.99      78.29      62.48      15584
-- (label_id: 3)                                         71.08      69.82      70.45       1352
-. (label_id: 4)                                         85.99      64.12      73.46      13588
-: (label_id: 5)                                         60.00      19.23      29.13        312
-; (label_id: 6)                                          0.00       0.00       0.00        148
-? (label_id: 7)                                         80.37      69.92      74.78        984
-— (label_id: 8)                                         37.78       7.83      12.98        868
-… (label_id: 9)                                          0.00       0.00       0.00         88
--------------------
-micro avg                                               93.43      93.43      93.43     258872
-macro avg                                               48.56      40.62      42.10     258872
-weighted avg                                            94.35      93.43      93.60     258872
-
--------------------
-219192.00  4.00 2408.00 288.00 648.00 64.00  4.00 48.00 132.00 20.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-6068.00 44.00 12200.00 120.00 4088.00 132.00 104.00 152.00 512.00 48.00
-376.00  0.00  4.00 944.00  4.00  0.00  0.00  0.00  0.00  0.00
-192.00 28.00 860.00  0.00 8712.00 52.00 32.00 96.00 148.00 12.00
- 0.00  0.00  4.00  0.00 32.00 60.00  0.00  0.00  4.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-44.00  0.00 60.00  0.00 56.00  0.00  4.00 688.00  4.00  0.00
- 0.00  0.00 48.00  0.00 48.00  4.00  4.00  0.00 68.00  8.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       2236
--------------------
-micro avg                                              100.00     100.00     100.00       2236
-macro avg                                              100.00     100.00     100.00       2236
-weighted avg                                           100.00     100.00     100.00       2236
-
--------------------
-2236.00
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.53      98.70      98.11     225872
-! (label_id: 1)                                          0.00       0.00       0.00         76
-, (label_id: 2)                                         62.00      65.40      63.65      15584
-- (label_id: 3)                                         81.02      65.68      72.55       1352
-. (label_id: 4)                                         82.92      70.62      76.28      13588
-: (label_id: 5)                                         55.56      12.82      20.83        312
-; (label_id: 6)                                          0.00       0.00       0.00        148
-? (label_id: 7)                                         73.88      73.58      73.73        984
-— (label_id: 8)                                         36.36       1.84       3.51        868
-… (label_id: 9)                                          0.00       0.00       0.00         88
--------------------
-micro avg                                               94.41      94.41      94.41     258872
-macro avg                                               48.93      38.86      40.87     258872
-weighted avg                                            94.07      94.41      94.14     258872
-
--------------------
-222940.00 12.00 4044.00 400.00 844.00 72.00  4.00 72.00 180.00 28.00
-16.00  0.00 36.00  0.00  8.00  8.00  0.00  4.00  0.00  0.00
-2324.00 36.00 10192.00 60.00 2984.00 136.00 96.00 88.00 484.00 40.00
-200.00  0.00  4.00 888.00  0.00  0.00  0.00  0.00  4.00  0.00
-340.00 28.00 1224.00  4.00 9596.00 52.00 44.00 96.00 172.00 16.00
- 0.00  0.00  4.00  0.00 24.00 40.00  0.00  0.00  4.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-52.00  0.00 68.00  0.00 120.00  4.00  4.00 724.00  8.00  0.00
- 0.00  0.00 12.00  0.00 12.00  0.00  0.00  0.00 16.00  4.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       2236
--------------------
-micro avg                                              100.00     100.00     100.00       2236
-macro avg                                              100.00     100.00     100.00       2236
-weighted avg                                           100.00     100.00     100.00       2236
-
--------------------
-2236.00
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.22      97.92      98.07     225872
-! (label_id: 1)                                          0.00       0.00       0.00         76
-, (label_id: 2)                                         64.06      63.83      63.95      15584
-- (label_id: 3)                                         82.22      65.68      73.03       1352
-. (label_id: 4)                                         73.23      83.81      78.16      13588
-: (label_id: 5)                                         48.57      21.79      30.09        312
-; (label_id: 6)                                          0.00       0.00       0.00        148
-? (label_id: 7)                                         62.30      77.24      68.97        984
-— (label_id: 8)                                         36.17       7.83      12.88        868
-… (label_id: 9)                                          0.00       0.00       0.00         88
--------------------
-micro avg                                               94.36      94.36      94.36     258872
-macro avg                                               46.48      41.81      42.51     258872
-weighted avg                                            94.25      94.36      94.24     258872
-
--------------------
-221164.00  8.00 2680.00 384.00 648.00 68.00  4.00 52.00 132.00 24.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-3600.00 24.00 9948.00 60.00 1364.00 68.00 40.00 40.00 352.00 32.00
-188.00  0.00  4.00 888.00  0.00  0.00  0.00  0.00  0.00  0.00
-744.00 40.00 2724.00 16.00 11388.00 92.00 96.00 132.00 300.00 20.00
-16.00  0.00 16.00  0.00 32.00 68.00  0.00  0.00  8.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-144.00  4.00 148.00  4.00 136.00  8.00  4.00 760.00  8.00  4.00
-16.00  0.00 64.00  0.00 20.00  8.00  4.00  0.00 68.00  8.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       2236
--------------------
-micro avg                                              100.00     100.00     100.00       2236
-macro avg                                              100.00     100.00     100.00       2236
-weighted avg                                           100.00     100.00     100.00       2236
-
--------------------
-2236.00
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.95      98.40      98.17     225872
-! (label_id: 1)                                          0.00       0.00       0.00         76
-, (label_id: 2)                                         59.98      71.36      65.17      15584
-- (label_id: 3)                                         80.21      67.16      73.11       1352
-. (label_id: 4)                                         83.40      68.62      75.29      13588
-: (label_id: 5)                                         66.67      23.08      34.29        312
-; (label_id: 6)                                          0.00       0.00       0.00        148
-? (label_id: 7)                                         79.30      73.17      76.11        984
-— (label_id: 8)                                         40.74       5.07       9.02        868
-… (label_id: 9)                                          0.00       0.00       0.00         88
--------------------
-micro avg                                               94.42      94.42      94.42     258872
-macro avg                                               50.82      40.68      43.12     258872
-weighted avg                                            94.39      94.42      94.28     258872
-
--------------------
-222248.00  8.00 3268.00 368.00 708.00 64.00  4.00 60.00 148.00 20.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-2988.00 40.00 11120.00 72.00 3440.00 108.00 96.00 96.00 536.00 44.00
-216.00  0.00  4.00 908.00  4.00  0.00  0.00  0.00  0.00  0.00
-356.00 28.00 1120.00  4.00 9324.00 60.00 40.00 104.00 128.00 16.00
- 0.00  0.00  4.00  0.00 28.00 72.00  0.00  0.00  4.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-60.00  0.00 48.00  0.00 64.00  4.00  4.00 720.00  8.00  0.00
- 4.00  0.00 20.00  0.00 20.00  4.00  4.00  4.00 44.00  8.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       2236
--------------------
-micro avg                                              100.00     100.00     100.00       2236
-macro avg                                              100.00     100.00     100.00       2236
-weighted avg                                           100.00     100.00     100.00       2236
-
--------------------
-2236.00
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.10      98.28      98.19     225872
-! (label_id: 1)                                          0.00       0.00       0.00         76
-, (label_id: 2)                                         61.25      70.64      65.61      15584
-- (label_id: 3)                                         81.07      67.16      73.46       1352
-. (label_id: 4)                                         80.94      73.77      77.19      13588
-: (label_id: 5)                                         66.67      23.08      34.29        312
-; (label_id: 6)                                          0.00       0.00       0.00        148
-? (label_id: 7)                                         78.45      73.98      76.15        984
-— (label_id: 8)                                         50.00       5.07       9.21        868
-… (label_id: 9)                                          0.00       0.00       0.00         88
--------------------
-micro avg                                               94.55      94.55      94.55     258872
-macro avg                                               51.65      41.20      43.41     258872
-weighted avg                                            94.50      94.55      94.42     258872
-
--------------------
-221976.00  4.00 3008.00 356.00 636.00 64.00  0.00 56.00 152.00 20.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-3272.00 40.00 11008.00 76.00 2800.00 100.00 76.00 104.00 456.00 40.00
-208.00  0.00  0.00 908.00  4.00  0.00  0.00  0.00  0.00  0.00
-364.00 28.00 1504.00 12.00 10024.00 60.00 68.00 96.00 204.00 24.00
- 0.00  0.00  4.00  0.00 28.00 72.00  0.00  0.00  4.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-48.00  4.00 48.00  0.00 80.00  8.00  4.00 728.00  8.00  0.00
- 4.00  0.00 12.00  0.00 16.00  8.00  0.00  0.00 44.00  4.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       2236
--------------------
-micro avg                                              100.00     100.00     100.00       2236
-macro avg                                              100.00     100.00     100.00       2236
-weighted avg                                           100.00     100.00     100.00       2236
-
--------------------
-2236.00
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.20      98.13      98.16     225872
-! (label_id: 1)                                          0.00       0.00       0.00         76
-, (label_id: 2)                                         60.37      72.38      65.83      15584
-- (label_id: 3)                                         81.63      68.34      74.40       1352
-. (label_id: 4)                                         81.60      73.36      77.26      13588
-: (label_id: 5)                                         73.08      24.36      36.54        312
-; (label_id: 6)                                          0.00       0.00       0.00        148
-? (label_id: 7)                                         81.08      73.17      76.92        984
-— (label_id: 8)                                         46.88       6.91      12.05        868
-… (label_id: 9)                                          0.00       0.00       0.00         88
--------------------
-micro avg                                               94.52      94.52      94.52     258872
-macro avg                                               52.28      41.67      44.12     258872
-weighted avg                                            94.58      94.52      94.43     258872
-
--------------------
-221652.00  4.00 2820.00 336.00 612.00 64.00  4.00 56.00 152.00 20.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-3612.00 40.00 11280.00 84.00 2916.00 88.00 60.00 96.00 464.00 44.00
-208.00  0.00  0.00 924.00  0.00  0.00  0.00  0.00  0.00  0.00
-348.00 32.00 1400.00  8.00 9968.00 72.00 76.00 112.00 180.00 20.00
- 0.00  0.00  0.00  0.00 24.00 76.00  0.00  0.00  4.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-48.00  0.00 52.00  0.00 52.00  4.00  4.00 720.00  8.00  0.00
- 4.00  0.00 32.00  0.00 16.00  8.00  4.00  0.00 60.00  4.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       2236
--------------------
-micro avg                                              100.00     100.00     100.00       2236
-macro avg                                              100.00     100.00     100.00       2236
-weighted avg                                           100.00     100.00     100.00       2236
-
--------------------
-2236.00
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.15      98.16      98.16     225872
-! (label_id: 1)                                          0.00       0.00       0.00         76
-, (label_id: 2)                                         63.10      67.76      65.35      15584
-- (label_id: 3)                                         83.78      64.20      72.70       1352
-. (label_id: 4)                                         77.59      79.01      78.30      13588
-: (label_id: 5)                                         67.86      24.36      35.85        312
-; (label_id: 6)                                          0.00       0.00       0.00        148
-? (label_id: 7)                                         78.02      73.58      75.73        984
-— (label_id: 8)                                         32.10      11.98      17.45        868
-… (label_id: 9)                                          0.00       0.00       0.00         88
--------------------
-micro avg                                               94.56      94.56      94.56     258872
-macro avg                                               50.06      41.91      44.35     258872
-weighted avg                                            94.43      94.56      94.46     258872
-
--------------------
-221720.00  4.00 2856.00 392.00 632.00 60.00  4.00 56.00 156.00 20.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-3384.00 32.00 10560.00 80.00 2068.00 88.00 56.00 64.00 364.00 40.00
-168.00  0.00  0.00 868.00  0.00  0.00  0.00  0.00  0.00  0.00
-528.00 40.00 1976.00 12.00 10736.00 76.00 80.00 128.00 236.00 24.00
- 0.00  0.00  0.00  0.00 28.00 76.00  0.00  4.00  4.00  0.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-60.00  0.00 80.00  0.00 56.00  0.00  4.00 724.00  4.00  0.00
-12.00  0.00 112.00  0.00 68.00 12.00  4.00  8.00 104.00  4.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       2236
--------------------
-micro avg                                              100.00     100.00     100.00       2236
-macro avg                                              100.00     100.00     100.00       2236
-weighted avg                                           100.00     100.00     100.00       2236
-
--------------------
-2236.00
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.96      98.43      98.20     225872
-! (label_id: 1)                                          0.00       0.00       0.00         76
-, (label_id: 2)                                         62.35      70.23      66.06      15584
-- (label_id: 3)                                         82.94      61.83      70.85       1352
-. (label_id: 4)                                         81.72      72.77      76.99      13588
-: (label_id: 5)                                         55.26      26.92      36.21        312
-; (label_id: 6)                                         50.00       2.70       5.13        148
-? (label_id: 7)                                         80.72      73.17      76.76        984
-— (label_id: 8)                                         40.74      10.14      16.24        868
-… (label_id: 9)                                          0.00       0.00       0.00         88
--------------------
-micro avg                                               94.60      94.60      94.60     258872
-macro avg                                               55.17      41.62      44.64     258872
-weighted avg                                            94.49      94.60      94.46     258872
-
--------------------
-222324.00  8.00 3176.00 428.00 692.00 68.00  8.00 56.00 160.00 24.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-2912.00 36.00 10944.00 80.00 2864.00 80.00 72.00 96.00 432.00 36.00
-172.00  0.00  0.00 836.00  0.00  0.00  0.00  0.00  0.00  0.00
-404.00 32.00 1336.00  8.00 9888.00 68.00 56.00 108.00 176.00 24.00
- 4.00  0.00  4.00  0.00 44.00 84.00  4.00  4.00  8.00  0.00
- 0.00  0.00  4.00  0.00  0.00  0.00  4.00  0.00  0.00  0.00
-48.00  0.00 56.00  0.00 56.00  4.00  4.00 720.00  4.00  0.00
- 8.00  0.00 64.00  0.00 44.00  8.00  0.00  0.00 88.00  4.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       2236
--------------------
-micro avg                                              100.00     100.00     100.00       2236
-macro avg                                              100.00     100.00     100.00       2236
-weighted avg                                           100.00     100.00     100.00       2236
-
--------------------
-2236.00
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.98      98.44      98.21     225872
-! (label_id: 1)                                          0.00       0.00       0.00         76
-, (label_id: 2)                                         65.60      65.35      65.48      15584
-- (label_id: 3)                                         80.57      67.46      73.43       1352
-. (label_id: 4)                                         78.74      78.30      78.52      13588
-: (label_id: 5)                                         57.50      29.49      38.98        312
-; (label_id: 6)                                         33.33       2.70       5.00        148
-? (label_id: 7)                                         79.65      73.17      76.27        984
-— (label_id: 8)                                         23.12      18.43      20.51        868
-… (label_id: 9)                                          0.00       0.00       0.00         88
--------------------
-micro avg                                               94.67      94.67      94.67     258872
-macro avg                                               51.65      43.33      45.64     258872
-weighted avg                                            94.46      94.67      94.55     258872
-
--------------------
-222356.00  8.00 3192.00 356.00 700.00 72.00  8.00 60.00 164.00 20.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-2712.00 32.00 10184.00 68.00 1992.00 68.00 36.00 60.00 336.00 36.00
-216.00  0.00  4.00 912.00  0.00  0.00  0.00  0.00  0.00  0.00
-512.00 36.00 1816.00 16.00 10640.00 68.00 76.00 128.00 200.00 20.00
- 8.00  0.00  4.00  0.00 48.00 92.00  0.00  4.00  4.00  0.00
- 0.00  0.00  4.00  0.00  4.00  0.00  4.00  0.00  0.00  0.00
-48.00  0.00 80.00  0.00 48.00  0.00  4.00 720.00  4.00  0.00
-20.00  0.00 300.00  0.00 156.00 12.00 20.00 12.00 160.00 12.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       2236
--------------------
-micro avg                                              100.00     100.00     100.00       2236
-macro avg                                              100.00     100.00     100.00       2236
-weighted avg                                           100.00     100.00     100.00       2236
-
--------------------
-2236.00
-[INFO] - Optimizer config = AdamW (
-Parameter Group 0
-    amsgrad: False
-    betas: (0.9, 0.999)
-    eps: 1e-08
-    lr: 0.001
-    weight_decay: 0.0
-)
-[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fb818b76c10>" 
-will be used during training (effective maximum steps = 750) - 
-Parameters : 
-(warmup_steps: null
-warmup_ratio: 0.1
-min_lr: 1.0e-08
-last_epoch: -1
-max_steps: 750
-)
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.11      98.52      98.32      38870
-! (label_id: 1)                                          0.00       0.00       0.00         18
-, (label_id: 2)                                         64.43      65.11      64.76       2476
-- (label_id: 3)                                         79.57      57.36      66.67        258
-. (label_id: 4)                                         79.55      79.48      79.52       2242
-: (label_id: 5)                                         50.00      21.74      30.30         46
-; (label_id: 6)                                          0.00       0.00       0.00         16
-? (label_id: 7)                                         81.71      80.72      81.21        166
-— (label_id: 8)                                         17.65      12.50      14.63        144
-… (label_id: 9)                                          0.00       0.00       0.00         14
--------------------
-micro avg                                               94.92      94.92      94.92      44250
-macro avg                                               47.10      41.54      43.54      44250
-weighted avg                                            94.70      94.92      94.79      44250
-
--------------------
-38296.00  2.00 498.00 90.00 92.00  4.00  0.00 12.00 36.00  2.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-450.00  8.00 1612.00 18.00 324.00 18.00  6.00  8.00 52.00  6.00
-38.00  0.00  0.00 148.00  0.00  0.00  0.00  0.00  0.00  0.00
-66.00  8.00 312.00  2.00 1782.00 12.00 10.00  8.00 36.00  4.00
- 2.00  0.00  0.00  0.00  8.00 10.00  0.00  0.00  0.00  0.00
- 0.00  0.00  2.00  0.00  2.00  0.00  0.00  0.00  0.00  0.00
-14.00  0.00 12.00  0.00  2.00  0.00  0.00 134.00  2.00  0.00
- 4.00  0.00 40.00  0.00 32.00  2.00  0.00  4.00 18.00  2.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00        378
--------------------
-micro avg                                              100.00     100.00     100.00        378
-macro avg                                              100.00     100.00     100.00        378
-weighted avg                                           100.00     100.00     100.00        378
-
--------------------
-378.00
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.98      98.44      98.21     225872
-! (label_id: 1)                                          0.00       0.00       0.00         76
-, (label_id: 2)                                         65.60      65.35      65.48      15584
-- (label_id: 3)                                         80.57      67.46      73.43       1352
-. (label_id: 4)                                         78.74      78.30      78.52      13588
-: (label_id: 5)                                         57.50      29.49      38.98        312
-; (label_id: 6)                                         33.33       2.70       5.00        148
-? (label_id: 7)                                         79.65      73.17      76.27        984
-— (label_id: 8)                                         23.12      18.43      20.51        868
-… (label_id: 9)                                          0.00       0.00       0.00         88
--------------------
-micro avg                                               94.67      94.67      94.67     258872
-macro avg                                               51.65      43.33      45.64     258872
-weighted avg                                            94.46      94.67      94.55     258872
-
--------------------
-222356.00  8.00 3192.00 356.00 700.00 72.00  8.00 60.00 164.00 20.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-2712.00 32.00 10184.00 68.00 1992.00 68.00 36.00 60.00 336.00 36.00
-216.00  0.00  4.00 912.00  0.00  0.00  0.00  0.00  0.00  0.00
-512.00 36.00 1816.00 16.00 10640.00 68.00 76.00 128.00 200.00 20.00
- 8.00  0.00  4.00  0.00 48.00 92.00  0.00  4.00  4.00  0.00
- 0.00  0.00  4.00  0.00  4.00  0.00  4.00  0.00  0.00  0.00
-48.00  0.00 80.00  0.00 48.00  0.00  4.00 720.00  4.00  0.00
-20.00  0.00 300.00  0.00 156.00 12.00 20.00 12.00 160.00 12.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       2236
--------------------
-micro avg                                              100.00     100.00     100.00       2236
-macro avg                                              100.00     100.00     100.00       2236
-weighted avg                                           100.00     100.00     100.00       2236
-
--------------------
-2236.00
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.98      98.44      98.21     225872
-! (label_id: 1)                                          0.00       0.00       0.00         76
-, (label_id: 2)                                         65.60      65.35      65.48      15584
-- (label_id: 3)                                         80.57      67.46      73.43       1352
-. (label_id: 4)                                         78.74      78.30      78.52      13588
-: (label_id: 5)                                         57.50      29.49      38.98        312
-; (label_id: 6)                                         33.33       2.70       5.00        148
-? (label_id: 7)                                         79.65      73.17      76.27        984
-— (label_id: 8)                                         23.12      18.43      20.51        868
-… (label_id: 9)                                          0.00       0.00       0.00         88
--------------------
-micro avg                                               94.67      94.67      94.67     258872
-macro avg                                               51.65      43.33      45.64     258872
-weighted avg                                            94.46      94.67      94.55     258872
-
--------------------
-222356.00  8.00 3192.00 356.00 700.00 72.00  8.00 60.00 164.00 20.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-2712.00 32.00 10184.00 68.00 1992.00 68.00 36.00 60.00 336.00 36.00
-216.00  0.00  4.00 912.00  0.00  0.00  0.00  0.00  0.00  0.00
-512.00 36.00 1816.00 16.00 10640.00 68.00 76.00 128.00 200.00 20.00
- 8.00  0.00  4.00  0.00 48.00 92.00  0.00  4.00  4.00  0.00
- 0.00  0.00  4.00  0.00  4.00  0.00  4.00  0.00  0.00  0.00
-48.00  0.00 80.00  0.00 48.00  0.00  4.00 720.00  4.00  0.00
-20.00  0.00 300.00  0.00 156.00 12.00 20.00 12.00 160.00 12.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       2236
--------------------
-micro avg                                              100.00     100.00     100.00       2236
-macro avg                                              100.00     100.00     100.00       2236
-weighted avg                                           100.00     100.00     100.00       2236
-
--------------------
-2236.00
---------------
-2236.00
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.84      97.75      97.79     225872
-! (label_id: 1)                                          0.00       0.00       0.00         76
-, (label_id: 2)                                         59.73      59.24      59.48      15584
-- (label_id: 3)                                         80.74      64.50      71.71       1352
-. (label_id: 4)                                         67.57      76.74      71.87      13588
-: (label_id: 5)                                         53.12      21.79      30.91        312
-; (label_id: 6)                                          0.00       0.00       0.00        148
-? (label_id: 7)                                         64.83      38.21      48.08        984
-— (label_id: 8)                                         15.79       9.68      12.00        868
-… (label_id: 9)                                          0.00       0.00       0.00         88
--------------------
-micro avg                                               93.42      93.42      93.42     258872
-macro avg                                               43.96      36.79      39.18     258872
-weighted avg                                            93.30      93.42      93.31     258872
-
--------------------
-220780.00  4.00 3300.00 404.00 872.00 48.00  8.00 68.00 148.00 20.00
- 0.00  0.00  8.00  0.00  0.00  0.00  0.00  0.00  0.00  4.00
-3424.00 32.00 9232.00 60.00 2060.00 96.00 68.00 108.00 344.00 32.00
-200.00  0.00  8.00 872.00  0.00  0.00  0.00  0.00  0.00  0.00
-1384.00 36.00 2692.00 16.00 10428.00 84.00 68.00 424.00 276.00 24.00
- 0.00  0.00  4.00  0.00 40.00 68.00  0.00  4.00  8.00  4.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-40.00  0.00 60.00  0.00 96.00  0.00  0.00 376.00  8.00  0.00
-44.00  4.00 280.00  0.00 92.00 16.00  4.00  4.00 84.00  4.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       2236
--------------------
-micro avg                                              100.00     100.00     100.00       2236
-macro avg                                              100.00     100.00     100.00       2236
-weighted avg                                           100.00     100.00     100.00       2236
-
--------------------
-2236.00
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.84      97.75      97.79     225872
-! (label_id: 1)                                          0.00       0.00       0.00         76
-, (label_id: 2)                                         59.73      59.24      59.48      15584
-- (label_id: 3)                                         80.74      64.50      71.71       1352
-. (label_id: 4)                                         67.57      76.74      71.87      13588
-: (label_id: 5)                                         53.12      21.79      30.91        312
-; (label_id: 6)                                          0.00       0.00       0.00        148
-? (label_id: 7)                                         64.83      38.21      48.08        984
-— (label_id: 8)                                         15.79       9.68      12.00        868
-… (label_id: 9)                                          0.00       0.00       0.00         88
--------------------
-micro avg                                               93.42      93.42      93.42     258872
-macro avg                                               43.96      36.79      39.18     258872
-weighted avg                                            93.30      93.42      93.31     258872
-
--------------------
-220780.00  4.00 3300.00 404.00 872.00 48.00  8.00 68.00 148.00 20.00
- 0.00  0.00  8.00  0.00  0.00  0.00  0.00  0.00  0.00  4.00
-3424.00 32.00 9232.00 60.00 2060.00 96.00 68.00 108.00 344.00 32.00
-200.00  0.00  8.00 872.00  0.00  0.00  0.00  0.00  0.00  0.00
-1384.00 36.00 2692.00 16.00 10428.00 84.00 68.00 424.00 276.00 24.00
- 0.00  0.00  4.00  0.00 40.00 68.00  0.00  4.00  8.00  4.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-40.00  0.00 60.00  0.00 96.00  0.00  0.00 376.00  8.00  0.00
-44.00  4.00 280.00  0.00 92.00 16.00  4.00  4.00 84.00  4.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       2236
--------------------
-micro avg                                              100.00     100.00     100.00       2236
-macro avg                                              100.00     100.00     100.00       2236
-weighted avg                                           100.00     100.00     100.00       2236
-
--------------------
-2236.00
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.84      97.75      97.79     225872
-! (label_id: 1)                                          0.00       0.00       0.00         76
-, (label_id: 2)                                         59.73      59.24      59.48      15584
-- (label_id: 3)                                         80.74      64.50      71.71       1352
-. (label_id: 4)                                         67.57      76.74      71.87      13588
-: (label_id: 5)                                         53.12      21.79      30.91        312
-; (label_id: 6)                                          0.00       0.00       0.00        148
-? (label_id: 7)                                         64.83      38.21      48.08        984
-— (label_id: 8)                                         15.79       9.68      12.00        868
-… (label_id: 9)                                          0.00       0.00       0.00         88
--------------------
-micro avg                                               93.42      93.42      93.42     258872
-macro avg                                               43.96      36.79      39.18     258872
-weighted avg                                            93.30      93.42      93.31     258872
-
--------------------
-220780.00  4.00 3300.00 404.00 872.00 48.00  8.00 68.00 148.00 20.00
- 0.00  0.00  8.00  0.00  0.00  0.00  0.00  0.00  0.00  4.00
-3424.00 32.00 9232.00 60.00 2060.00 96.00 68.00 108.00 344.00 32.00
-200.00  0.00  8.00 872.00  0.00  0.00  0.00  0.00  0.00  0.00
-1384.00 36.00 2692.00 16.00 10428.00 84.00 68.00 424.00 276.00 24.00
- 0.00  0.00  4.00  0.00 40.00 68.00  0.00  4.00  8.00  4.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-40.00  0.00 60.00  0.00 96.00  0.00  0.00 376.00  8.00  0.00
-44.00  4.00 280.00  0.00 92.00 16.00  4.00  4.00 84.00  4.00
- 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       2236
--------------------
-micro avg                                              100.00     100.00     100.00       2236
-macro avg                                              100.00     100.00     100.00       2236
-weighted avg                                           100.00     100.00     100.00       2236
-
--------------------
-2236.00
