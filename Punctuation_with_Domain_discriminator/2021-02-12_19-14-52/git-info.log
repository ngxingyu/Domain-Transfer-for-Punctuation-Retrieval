commit hash: 68c7776c6ae820211b0383957eb39d9ce7d7d7a7
diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/events.out.tfevents.1613010499.intern-instance.11236.0 b/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/events.out.tfevents.1613010499.intern-instance.11236.0
index 243ac86..7ff3150 100644
Binary files a/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/events.out.tfevents.1613010499.intern-instance.11236.0 and b/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/events.out.tfevents.1613010499.intern-instance.11236.0 differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/lightning_logs.txt
index 3a982ac..83d2823 100644
--- a/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/lightning_logs.txt
+++ b/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/lightning_logs.txt
@@ -21,3 +21,50 @@ initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
 Epoch 0, global step 5254: val_loss reached 0.01068 (best 0.01068), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.01-epoch=0.ckpt" as top 3
 Epoch 1, global step 10509: val_loss reached 0.01570 (best 0.01068), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.02-epoch=1.ckpt" as top 3
 Epoch 2, global step 15764: val_loss reached 0.01145 (best 0.01068), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.01-epoch=2.ckpt" as top 3
+Epoch 3, global step 21019: val_loss reached 0.00733 (best 0.00733), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.01-epoch=3.ckpt" as top 3
+Epoch 4, global step 26274: val_loss reached 0.00962 (best 0.00733), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.01-epoch=4.ckpt" as top 3
+Epoch 5, global step 31529: val_loss reached 0.01048 (best 0.00733), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.01-epoch=5.ckpt" as top 3
+Epoch 6, step 36784: val_loss was not in top 3
+Epoch 7, global step 42039: val_loss reached 0.00450 (best 0.00450), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.00-epoch=7.ckpt" as top 3
+Epoch 8, global step 47294: val_loss reached 0.00307 (best 0.00307), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.00-epoch=8.ckpt" as top 3
+Epoch 9, global step 52549: val_loss reached -0.00113 (best -0.00113), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.00-epoch=9.ckpt" as top 3
+Epoch 10, global step 57804: val_loss reached -0.00221 (best -0.00221), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.00-epoch=10.ckpt" as top 3
+Epoch 11, global step 63059: val_loss reached -0.00430 (best -0.00430), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.00-epoch=11.ckpt" as top 3
+Epoch 12, global step 68314: val_loss reached -0.00639 (best -0.00639), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.01-epoch=12.ckpt" as top 3
+Epoch 13, global step 73569: val_loss reached -0.00726 (best -0.00726), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.01-epoch=13.ckpt" as top 3
+Epoch 14, global step 78824: val_loss reached -0.00660 (best -0.00726), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.01-epoch=14.ckpt" as top 3
+Saving latest checkpoint...
+
+  | Name                | Type                 | Params
+-------------------------------------------------------------
+0 | transformer         | DistilBertModel      | 66.4 M
+1 | punct_classifier    | TokenClassifier      | 7.7 K 
+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
+3 | punctuation_loss    | FocalDiceLoss        | 0     
+4 | domain_loss         | CrossEntropyLoss     | 0     
+5 | agg_loss            | AggregatorLoss       | 0     
+6 | punct_class_report  | ClassificationReport | 0     
+7 | domain_class_report | ClassificationReport | 0     
+-------------------------------------------------------------
+7.1 M     Trainable params
+59.3 M    Non-trainable params
+66.4 M    Total params
+Epoch 0, global step 84079: val_loss reached -0.01453 (best -0.01453), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.01-epoch=0.ckpt" as top 3
+Epoch 1, global step 89334: val_loss reached -0.02339 (best -0.02339), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.02-epoch=1.ckpt" as top 3
+Epoch 2, global step 94589: val_loss reached -0.02879 (best -0.02879), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.03-epoch=2.ckpt" as top 3
+Epoch 3, global step 99844: val_loss reached -0.03249 (best -0.03249), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.03-epoch=3.ckpt" as top 3
+Epoch 4, global step 105099: val_loss reached -0.03651 (best -0.03651), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.04-epoch=4.ckpt" as top 3
+Epoch 5, global step 110354: val_loss reached -0.03725 (best -0.03725), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.04-epoch=5.ckpt" as top 3
+Epoch 6, global step 115609: val_loss reached -0.04035 (best -0.04035), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.04-epoch=6.ckpt" as top 3
+Epoch 7, global step 120864: val_loss reached -0.04093 (best -0.04093), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.04-epoch=7.ckpt" as top 3
+Epoch 8, global step 126119: val_loss reached -0.04200 (best -0.04200), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.04-epoch=8.ckpt" as top 3
+Epoch 9, global step 131374: val_loss reached -0.04282 (best -0.04282), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.04-epoch=9.ckpt" as top 3
+Epoch 10, global step 136629: val_loss reached -0.04352 (best -0.04352), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.04-epoch=10.ckpt" as top 3
+Epoch 11, global step 141884: val_loss reached -0.04396 (best -0.04396), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.04-epoch=11.ckpt" as top 3
+Epoch 12, global step 147139: val_loss reached -0.04385 (best -0.04396), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.04-epoch=12.ckpt" as top 3
+Epoch 13, global step 152394: val_loss reached -0.04389 (best -0.04396), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.04-epoch=13.ckpt" as top 3
+Epoch 14, step 157649: val_loss was not in top 3
+GPU available: True, used: True
+TPU available: None, using: 0 TPU cores
+Using environment variable NODE_RANK for node rank (0).
+LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/nemo_error_log.txt
index 864dfb4..83e5ef3 100644
--- a/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/nemo_error_log.txt
+++ b/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/nemo_error_log.txt
@@ -8,3 +8,6 @@
 [NeMo W 2021-02-11 11:12:14 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f0d466c9b50> was reported to be 2627 (when accessing len(dataloader)), but 2628 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
       warnings.warn(warn_msg)
     
+[NeMo W 2021-02-12 17:56:43 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f0d466c9a00> was reported to be 2627 (when accessing len(dataloader)), but 2628 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/nemo_log_globalrank-0_localrank-0.txt
index fc03a49..5f25b7a 100644
--- a/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/nemo_log_globalrank-0_localrank-0.txt
+++ b/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/nemo_log_globalrank-0_localrank-0.txt
@@ -10,3 +10,6 @@
 [NeMo W 2021-02-11 11:12:14 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f0d466c9b50> was reported to be 2627 (when accessing len(dataloader)), but 2628 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
       warnings.warn(warn_msg)
     
+[NeMo W 2021-02-12 17:56:43 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f0d466c9a00> was reported to be 2627 (when accessing len(dataloader)), but 2628 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/events.out.tfevents.1613015290.intern-instance.24189.0 b/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/events.out.tfevents.1613015290.intern-instance.24189.0
deleted file mode 100644
index 237ef93..0000000
Binary files a/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/events.out.tfevents.1613015290.intern-instance.24189.0 and /dev/null differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/git-info.log
deleted file mode 100644
index de9e9e7..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/git-info.log
+++ /dev/null
@@ -1,962 +0,0 @@
-commit hash: e06c976eca0f0f1ece5dd302964c113374b09762
-diff --git a/README.md b/README.md
-index 8b23590..e1fff55 100644
---- a/README.md
-+++ b/README.md
-@@ -619,3 +619,22 @@ weighted avg                                            90.70      90.25      90
-  'punct_precision': tensor(33.9651),
-  'punct_recall': tensor(33.8733),
-  'test_loss': tensor(0.2265)}
-+
-+TED end
-+ 'punct_f1': tensor(32.2363, device='cuda:0'),
-+ 'punct_precision': tensor(30.6842, device='cuda:0'),
-+ 'punct_recall': tensor(36.3651, device='cuda:0'),
-+ 'test_loss': tensor(0.2000, device='cuda:0')}
-+
-+TED start
-+'punct_f1': tensor(32.0951, device='cuda:0'),
-+'punct_precision': tensor(30.3402, device='cuda:0'),
-+'punct_recall': tensor(36.4819, device='cuda:0'),
-+'test_loss': tensor(0.1911, device='cuda:0')}
-+
-+
-+TED None
-+{'punct_f1': tensor(35.3044, device='cuda:0'),
-+ 'punct_precision': tensor(34.8901, device='cuda:0'),
-+ 'punct_recall': tensor(35.7895, device='cuda:0'),
-+ 'test_loss': tensor(0.2175, device='cuda:0')}
-diff --git a/experiment/Inference.ipynb b/experiment/Inference.ipynb
-index 99790f8..3756810 100644
---- a/experiment/Inference.ipynb
-+++ b/experiment/Inference.ipynb
-@@ -2,15 +2,34 @@
-  "cells": [
-   {
-    "cell_type": "code",
--   "execution_count": 1,
-+   "execution_count": 10,
-    "id": "modern-amplifier",
-    "metadata": {},
-    "outputs": [
-     {
--     "output_type": "stream",
-      "name": "stdout",
-+     "output_type": "stream",
-+     "text": [
-+      "/home/nxingyu2/project/experiment\n",
-+      "shuffling <data.punctuation_dataset_multi.PunctuationDomainDataset object at 0x7fb2c71e8790>\n"
-+     ]
-+    },
-+    {
-+     "name": "stderr",
-+     "output_type": "stream",
-      "text": [
--      "/home/nxingyu2/project/experiment\n"
-+      "11:23:21.60 LOG:\n",
-+      "11:23:21.61 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
-+      "11:23:21.62 LOG:\n",
-+      "11:23:21.63 .... f\"1st {n} encoder layers of transformer frozen\" = '1st 11 encoder layers of transformer frozen'\n",
-+      "GPU available: True, used: True\n",
-+      "INFO:lightning:GPU available: True, used: True\n",
-+      "TPU available: None, using: 0 TPU cores\n",
-+      "INFO:lightning:TPU available: None, using: 0 TPU cores\n",
-+      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
-+      "INFO:lightning:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
-+      "Using native 16bit precision.\n",
-+      "INFO:lightning:Using native 16bit precision.\n"
-      ]
-     }
-    ],
-@@ -37,29 +56,191 @@
-     "\n",
-     "from hydra.experimental import initialize, initialize_config_module, initialize_config_dir, compose\n",
-     "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
--    "initialize(config_path=\"project/experiment\")\n",
-+    "# folder=\"TEDend2021-02-11_07-57-33\"\n",
-+    "folder=\"TEDstart2021-02-11_07-55-58\"\n",
-+    "# folder=\"TEDnone2021-02-11_10-14-34\"\n",
-+    "initialize(config_path=\"../Punctuation_with_Domain_discriminator/\"+folder) #config_path=\"project/experiment\"\n",
-     "!pwd\n",
--    "\n",
-     "cfg=compose(\n",
--    "    config_name=\"test_config.yaml\", \n",
-+    "    config_name=\"hparams.yaml\", \n",
-     ")\n",
-     "cfg.model.punct_label_ids=OmegaConf.create(sorted(cfg.model.punct_label_ids))\n",
--    "labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
--    "ids_to_labels = {_[0]:_[1] for _ in enumerate(cfg.model.punct_label_ids)}\n",
-+    "# labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
-+    "# ids_to_labels = {_[0]:_[1] for _ in enumerate(cfg.model.punct_label_ids)}\n",
-+    "model = PunctuationDomainModel.load_from_checkpoint( #TEDend2021-02-11_07-57-33  # TEDstart2021-02-11_07-55-58\n",
-+    "    checkpoint_path=f\"/home/nxingyu2/project/Punctuation_with_Domain_discriminator/{folder}/checkpoints/Punctuation_with_Domain_discriminator-last.ckpt\")\n",
-+    "trainer = pl.Trainer(**cfg.trainer)"
-+   ]
-+  },
-+  {
-+   "cell_type": "code",
-+   "execution_count": 9,
-+   "id": "hairy-proxy",
-+   "metadata": {},
-+   "outputs": [
-+    {
-+     "data": {
-+      "text/plain": [
-+       "[' hooooooo,                                                                                                                           ',\n",
-+       " ' what can i do for you today?                                                                                                                        ',\n",
-+       " ' ? how are you?                                                                                                                            ',\n",
-+       " ' in guadeloupe or marti nique, it also brin gs into ques tion budgetary po licy. because the europe, an union is after all, making a present of ecu 1, 9 billion to three multinationals. where are the financial interests of the european union— firstly, development policy— in africa. in any case, in the acp, countries employ ment policy in madeira, the canaries— guadel, oupe, martinique and crete, regional, pol icy in the ultra- peripheral areas— human rights, which mr barthet mayer mentioned earlier, since dollar bananas are after all, slavery. bana nas, the product of human exploitation by three multinat ionals, payments of ec ',\n",
-+       " ' u 50 per mon th. instead of ecu 50 per day                                                                                                                   ',\n",
-+       " ' plans for this weekend, include turning wine into water. the small white buoys marked the location of hundreds of crab pots. he said he was not there yesterday. however, many people saw him there. today. arrived with a crash of my car through the garage door. the lyrics of the song sounded like fingernails on a chalkboard. the guinea fowl flies through the air with all the grace of a turtle. they ran around the corner, to find that they had traveled back in time,                                      ',\n",
-+       " ' good morning. everyone? how have your weekends been? its a really great day? thank you.                                                                                                                ',\n",
-+       " ' first of all, i too agree that tourism- related action must include employment training and education. as you know, after the european conference on tourism and employment in luxemborg, we set up a high- level group, whose mission was to examine how best tourism could contribute towards employment— the first stage,                                                                          ']"
-+      ]
-+     },
-+     "execution_count": 9,
-+     "metadata": {},
-+     "output_type": "execute_result"
-+    }
-+   ],
-+   "source": [
-     "\n",
--    "# model = PunctuationDomainModel.load_from_checkpoint(\n",
--    "#     checkpoint_path=\"/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator-last.ckpt\")\n",
-+    "queries = [\n",
-+    "    'Hooooooo!',\n",
-+    "    'what can i do for you today',\n",
-+    "    'how are you',\n",
-+    "    ' in guadeloupe or marti nique it also brin gs into ques tion budgetary po licy because the europe an union is after all making a present of ecu 1.9 billion to three multinationals where are the financial interests of the european union firstly development policy in africa in any case in the acp countries employ ment policy in madeira the canaries guadel oupe martinique and crete regional pol icy in the ultra-peripheral areas human rights which mr barthet mayer mentioned earlier since dollar bananas are after all slavery bana nas the product of human exploitation by three multinat ionals payments of ecu 50 per mon th instead of ecu 50 per day',\n",
-+    "    '''Plans for this weekend include turning wine into water.\n",
-+    "The small white buoys marked the location of hundreds of crab pots.\n",
-+    "He said he was not there yesterday; however, many people saw him there.\n",
-+    "Today arrived with a crash of my car through the garage door.\n",
-+    "The lyrics of the song sounded like fingernails on a chalkboard.\n",
-+    "The Guinea fowl flies through the air with all the grace of a turtle.\n",
-+    "They ran around the corner to find that they had traveled back in time.''',\n",
-+    "    'good morning everyone how have your weekends been its a really great day thank you',\n",
-+    "    'first of all i too agree that tourism related action must include employment training and education as you know after the european conference on tourism and employment in luxemborg we set up a high-level group whose mission was to examine how best tourism could contribute towards employment the first stage'\n",
-+    "]\n",
-+    "inference_results = model.add_punctuation(queries)\n",
-+    "inference_results"
-+   ]
-+  },
-+  {
-+   "cell_type": "code",
-+   "execution_count": 20,
-+   "id": "magnetic-approach",
-+   "metadata": {},
-+   "outputs": [
-+    {
-+     "name": "stdout",
-+     "output_type": "stream",
-+     "text": [
-+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
-+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
-+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
-+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
-+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
-+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
-+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
-+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
-+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
-+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n"
-+     ]
-+    }
-+   ],
-+   "source": [
-+    "for i in torch.zeros((10,10)):\n",
-+    "    print(' '.join('{:5.2f}'.format(x) for x in i))"
-+   ]
-+  },
-+  {
-+   "cell_type": "code",
-+   "execution_count": 12,
-+   "id": "dietary-violin",
-+   "metadata": {},
-+   "outputs": [
-+    {
-+     "data": {
-+      "text/plain": [
-+       "[' hooooooo                                                                                                                           ',\n",
-+       " ' what can i do for you today?                                                                                                                        ',\n",
-+       " ' , how? are? you?                                                                                                                            ',\n",
-+       " ' in guadeloupe or marti nique it also br-in gs into ques tion budgetary po. licy because the europe, an union is after all, making a present of ecu 1, 9 billion to three multinationals where are the financial interests of the european union? firstly development policy in africa. in any case, in the ac.p countries, employ men-t policy in madeira. the canaries guadel ou.pe martinique and crete, regional, pol, icy in the ultra- peripheral areas— human rights, which mr barthet mayer mentioned earlier, since dollar bananas are after all. slavery. bana nas. the product of human exploitation, by three multi-nat iona.ls payments of ec. ',\n",
-+       " ' u 50 per mon, th. instead of ecu 50 per day                                                                                                                   ',\n",
-+       " ' plans for this weekend, include turning wine into water. the small white bu.oys marked the location of hundreds of crab pots. he said he was not there yesterday. however many people saw him there today, arrived with a crash of my car through the garage door. the lyrics of the song sounded like fingernails on a chalkboard the guinea f-owl flies through the air with all the grace of a turtle. they ran around the corner, to find that they had traveled back in time,                                      ',\n",
-+       " ' good morning? everyone? how have your weekends? been? its a really great day? thank you.                                                                                                                ',\n",
-+       " ' first of all, i too agree that tourism- related action must include employment, training and education. as you know, after the european conference on tourism and employment in luxemborg we set up a high- level group, whose mission was to examine how best tourism could contribute towards employment— the first stage,                                                                          ']"
-+      ]
-+     },
-+     "execution_count": 12,
-+     "metadata": {},
-+     "output_type": "execute_result"
-+    }
-+   ],
-+   "source": [
-+    "\n",
-+    "queries = [\n",
-+    "    'Hooooooo!',\n",
-+    "    'what can i do for you today',\n",
-+    "    'how are you',\n",
-+    "    ' in guadeloupe or marti nique it also brin gs into ques tion budgetary po licy because the europe an union is after all making a present of ecu 1.9 billion to three multinationals where are the financial interests of the european union firstly development policy in africa in any case in the acp countries employ ment policy in madeira the canaries guadel oupe martinique and crete regional pol icy in the ultra-peripheral areas human rights which mr barthet mayer mentioned earlier since dollar bananas are after all slavery bana nas the product of human exploitation by three multinat ionals payments of ecu 50 per mon th instead of ecu 50 per day',\n",
-+    "    '''Plans for this weekend include turning wine into water.\n",
-+    "The small white buoys marked the location of hundreds of crab pots.\n",
-+    "He said he was not there yesterday; however, many people saw him there.\n",
-+    "Today arrived with a crash of my car through the garage door.\n",
-+    "The lyrics of the song sounded like fingernails on a chalkboard.\n",
-+    "The Guinea fowl flies through the air with all the grace of a turtle.\n",
-+    "They ran around the corner to find that they had traveled back in time.''',\n",
-+    "    'good morning everyone how have your weekends been its a really great day thank you',\n",
-+    "    'first of all i too agree that tourism related action must include employment training and education as you know after the european conference on tourism and employment in luxemborg we set up a high-level group whose mission was to examine how best tourism could contribute towards employment the first stage'\n",
-+    "]\n",
-+    "inference_results = model.add_punctuation(queries)\n",
-+    "inference_results"
-+   ]
-+  },
-+  {
-+   "cell_type": "code",
-+   "execution_count": 9,
-+   "id": "residential-scene",
-+   "metadata": {},
-+   "outputs": [
-+    {
-+     "data": {
-+      "text/plain": [
-+       "[' hooooooo,                                                                                                                           ',\n",
-+       " ' what can i do for you today?                                                                                                                        ',\n",
-+       " ' ? how are you?                                                                                                                            ',\n",
-+       " ' firstly, development policy in africa. in any case, in the acp, countries, employment policy in madeira, the canaries— guadeloupe, martinique and crete, regional policy in the ultra- peripheral areas— human rights, which mr barthet mayer, mentioned earlier, since dollar bananas are after all, slavery. bananas, the product of human exploitation by three multinationals, payments of ecu 50 per month, instead of ecu 50 per day in guadeloupe or martinique. it also brings into question budgetary policy. because the european union is after all, making a present of ecu 1, 9 billion to three multinationals. where are the financial interests of the european union?         ',\n",
-+       " ' plans for this weekend, include turning wine into water. the small white buoys marked the location of hundreds of crab pots. he said he was not there yesterday. however, many people saw him there. today. arrived with a crash of my car through the garage door. the lyrics of the song sounded like fingernails on a chalkboard. the guinea fowl flies through the air with all the grace of a turtle. they ran around the corner, to find that they had traveled back in time,                                      ',\n",
-+       " ' good morning. everyone? how have your weekends been? its a really great day? thank you.                                                                                                                ',\n",
-+       " ' first of all, i too agree that tourism- related action must include employment training and education. as you know, after the european conference on tourism and employment in luxemborg, we set up a high- level group, whose mission was to examine how best tourism could contribute towards employment— the first stage,                                                                          ']"
-+      ]
-+     },
-+     "execution_count": 9,
-+     "metadata": {},
-+     "output_type": "execute_result"
-+    }
-+   ],
-+   "source": [
-     "\n",
--    "# trainer = pl.Trainer(**cfg.trainer)"
-+    "queries = [\n",
-+    "    'Hooooooo!',\n",
-+    "    'what can i do for you today',\n",
-+    "    'how are you',\n",
-+    "    'firstly development policy in africa in any case in the acp countries employment policy in madeira the canaries guadeloupe martinique and crete regional policy in the ultra-peripheral areas human rights which mr barthet-mayer mentioned earlier since dollar bananas are after all slavery bananas the product of human exploitation by three multinationals payments of ecu 50 per month instead of ecu 50 per day in guadeloupe or martinique it also brings into question budgetary policy because the european union is after all making a present of ecu 1.9 billion to three multinationals where are the financial interests of the european union',\n",
-+    "    '''Plans for this weekend include turning wine into water.\n",
-+    "The small white buoys marked the location of hundreds of crab pots.\n",
-+    "He said he was not there yesterday; however, many people saw him there.\n",
-+    "Today arrived with a crash of my car through the garage door.\n",
-+    "The lyrics of the song sounded like fingernails on a chalkboard.\n",
-+    "The Guinea fowl flies through the air with all the grace of a turtle.\n",
-+    "They ran around the corner to find that they had traveled back in time.''',\n",
-+    "    'good morning everyone how have your weekends been its a really great day thank you',\n",
-+    "    'first of all i too agree that tourism related action must include employment training and education as you know after the european conference on tourism and employment in luxemborg we set up a high-level group whose mission was to examine how best tourism could contribute towards employment the first stage'\n",
-+    "]\n",
-+    "inference_results = model.add_punctuation(queries)\n",
-+    "inference_results"
-    ]
-   },
-   {
-    "cell_type": "code",
--   "execution_count": 11,
-+   "execution_count": 3,
-+   "id": "loose-assignment",
-    "metadata": {},
-    "outputs": [
-     {
--     "output_type": "execute_result",
-      "data": {
-       "text/plain": [
-        "{'input_ids': tensor([[ 101, 7570, 9541, 9541, 9541,  102,    0,    0,    0,    0,    0,    0,\n",
-@@ -86,7 +267,7 @@
-        "          False, False, False, False, False, False, False, False, False, False,\n",
-        "          False, False, False, False, False, False, False, False, False, False,\n",
-        "          False, False, False, False, False, False, False, False]]),\n",
--       " 'subtoken_mask': tensor([[ True, False, False, False,  True,  True, False, False, False, False,\n",
-+       " 'subtoken_mask': tensor([[ True,  True, False, False, False, False, False, False, False, False,\n",
-        "          False, False, False, False, False, False, False, False, False, False,\n",
-        "          False, False, False, False, False, False, False, False, False, False,\n",
-        "          False, False, False, False, False, False, False, False, False, False,\n",
-@@ -99,7 +280,7 @@
-        "          False, False, False, False, False, False, False, False, False, False,\n",
-        "          False, False, False, False, False, False, False, False, False, False,\n",
-        "          False, False, False, False, False, False, False, False]]),\n",
--       " 'labels': tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-+       " 'labels': tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-        "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-        "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-        "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-@@ -107,11 +288,15 @@
-        "          0, 0, 0, 0, 0, 0, 0, 0]])}"
-       ]
-      },
-+     "execution_count": 3,
-      "metadata": {},
--     "execution_count": 11
-+     "output_type": "execute_result"
-     }
-    ],
-    "source": [
-+    "# cfg.model.punct_label_ids=OmegaConf.create(sorted(cfg.model.punct_label_ids))\n",
-+    "# labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
-+    "\n",
-     "queries = [\n",
-     "    'Hooooooo!',\n",
-     "    # 'what can i do for you today',\n",
-@@ -120,36 +305,10 @@
-     "]\n",
-     "cfg.model.punct_label_ids=OmegaConf.create(sorted(cfg.model.punct_label_ids))\n",
-     "labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
--    "ds=PunctuationInferenceDataset(tokenizer=AutoTokenizer.from_pretrained('google/electra-small-discriminator'), queries=queries, max_seq_length=128, punct_label_ids=labels_to_ids, degree=0, attach_label_to_end=True)\n",
-+    "ds=PunctuationInferenceDataset(tokenizer=AutoTokenizer.from_pretrained('google/electra-small-discriminator'), queries=queries, max_seq_length=128, punct_label_ids=labels_to_ids, degree=0, attach_label_to_end=False)\n",
-     "ds[0]"
-    ]
-   },
--  {
--   "cell_type": "code",
--   "execution_count": 8,
--   "id": "hairy-proxy",
--   "metadata": {},
--   "outputs": [
--    {
--     "data": {
--      "text/plain": [
--       "[' we bought four shirts, one pen, and a mug from the nvidia gear store in santa clara                                                                                                            ',\n",
--       " ' what can i do for you today?                                                                                                                        ',\n",
--       " ' , how are you? ,                                                                                                                           ',\n",
--       " ' good morning. everyone? how have your weekends been? its a really great day?                                                                                                                  ']"
--      ]
--     },
--     "execution_count": 8,
--     "metadata": {},
--     "output_type": "execute_result"
--    }
--   ],
--   "source": [
--    "\n",
--    "inference_results = model.add_punctuation(queries)\n",
--    "inference_results"
--   ]
--  },
-   {
-    "cell_type": "code",
-    "execution_count": null,
-@@ -175,9 +334,9 @@
-    "name": "python",
-    "nbconvert_exporter": "python",
-    "pygments_lexer": "ipython3",
--   "version": "3.8.5-final"
-+   "version": "3.8.5"
-   }
-  },
-  "nbformat": 4,
-  "nbformat_minor": 5
--}
-\ No newline at end of file
-+}
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index 2aa798f..9cfe485 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,36 +1,36 @@
- seed: 42
- trainer:
--    # gpus: 1 # the number of gpus, 0 for CPU
--    # num_nodes: 1
--    # max_epochs: 15
--    # max_steps: null # precedence over max_epochs
--    # accumulate_grad_batches: 4 # accumulates grads every k batches
--    # gradient_clip_val: 0
--    # amp_level: O1 # O1/O2 for mixed precision
--    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
--    # checkpoint_callback: false  # Provided by exp_manager
--    # logger: false #false  # Provided by exp_manager
--    # log_every_n_steps: 1  # Interval of logging.
--    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # resume_from_checkpoint: null
--
--    gpus: 0 # the number of gpus, 0 for CPU
-+    gpus: 1 # the number of gpus, 0 for CPU
-     num_nodes: 1
--    max_epochs: 8
-+    max_epochs: 15
-     max_steps: null # precedence over max_epochs
-     accumulate_grad_batches: 4 # accumulates grads every k batches
-     gradient_clip_val: 0
--    amp_level: O0 # O1/O2 for mixed precision
--    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
-+    amp_level: O1 # O1/O2 for mixed precision
-+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    accelerator: ddp
-     checkpoint_callback: false  # Provided by exp_manager
-     logger: false #false  # Provided by exp_manager
-     log_every_n_steps: 1  # Interval of logging.
-     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    reload_dataloaders_every_epoch: true
-     resume_from_checkpoint: null
- 
-+    # gpus: 0 # the number of gpus, 0 for CPU
-+    # num_nodes: 1
-+    # max_epochs: 8
-+    # max_steps: null # precedence over max_epochs
-+    # accumulate_grad_batches: 4 # accumulates grads every k batches
-+    # gradient_clip_val: 0
-+    # amp_level: O0 # O1/O2 for mixed precision
-+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # # accelerator: ddp
-+    # checkpoint_callback: false  # Provided by exp_manager
-+    # logger: false #false  # Provided by exp_manager
-+    # log_every_n_steps: 1  # Interval of logging.
-+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    # reload_dataloaders_every_epoch: true
-+    # resume_from_checkpoint: null
-+
- exp_manager:
-     exp_dir: /home/nxingyu2/project/ # /root/project # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-@@ -41,7 +41,7 @@ tmp_path: /home/nxingyu2/data/tmp # /tmp #
- 
- model:
-     nemo_path: null
--    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-+    transformer_path: distilbert-base-uncased # google/electra-small-discriminator #  filename to save the model and associated artifacts to .nemo file
-     unfrozen: 0
-     maximum_unfrozen: 1
-     unfreeze_step: 1
-@@ -81,7 +81,7 @@ model:
-         num_labels: 10
-         num_domains: 1
-         test_unlabelled: true
--        attach_label_to_end: false # false if attach to start
-+        attach_label_to_end: none # false if attach to start none if dont mask
- 
-         train_ds:
-             shuffle: true
-diff --git a/experiment/core/classification_report.py b/experiment/core/classification_report.py
-index 7ff6282..ca758d1 100644
---- a/experiment/core/classification_report.py
-+++ b/experiment/core/classification_report.py
-@@ -15,6 +15,7 @@
- from typing import Any, Dict, Optional
- 
- import torch
-+from torch.nn.functional import one_hot
- from pytorch_lightning.metrics import Metric
- from pytorch_lightning.metrics.utils import METRIC_EPS
- 
-@@ -82,11 +83,14 @@ class ClassificationReport(Metric):
-         self.add_state(
-             "num_examples_per_class", default=torch.zeros(num_classes), dist_reduce_fx='sum', persistent=False
-         )
-+        self.add_state("cm", default=torch.zeros((num_classes,num_classes)), dist_reduce_fx="sum", persistent=False)
- 
-     def update(self, predictions: torch.Tensor, labels: torch.Tensor):
-         TP = []
-         FN = []
-         FP = []
-+        CM = torch.zeros((self.num_classes,self.num_classes),dtype=torch.long).to(predictions.device)
-+        CM.index_add_(0, predictions, one_hot(labels,num_classes=self.num_classes))
-         for label_id in range(self.num_classes):
-             current_label = labels == label_id
-             label_predicted = predictions == label_id
-@@ -98,11 +102,13 @@ class ClassificationReport(Metric):
-         tp = torch.tensor(TP).to(predictions.device)
-         fn = torch.tensor(FN).to(predictions.device)
-         fp = torch.tensor(FP).to(predictions.device)
-+        # cm = torch.tensor(CM).to(predictions.device)
-         num_examples_per_class = tp + fn
- 
-         self.tp += tp
-         self.fn += fn
-         self.fp += fp
-+        self.cm += CM
-         self.num_examples_per_class += num_examples_per_class
- 
-     def compute(self):
-@@ -156,16 +162,20 @@ class ClassificationReport(Metric):
-             + '\n'
-         )
- 
-+        report += "\n-------------------\n"
-+        for i in self.cm:
-+            report+=(' '.join('{:5.2f}'.format(x) for x in i)) + '\n'
-+        report += "\n-------------------\n"
-         self.total_examples = total_examples
- 
-         if self.mode == 'macro':
--            return macro_precision, macro_recall, macro_f1, report
-+            return macro_precision, macro_recall, macro_f1, report, self.cm
-         elif self.mode == 'weighted':
--            return weighted_precision, weighted_recall, weighted_f1, report
-+            return weighted_precision, weighted_recall, weighted_f1, report, self.cm
-         elif self.mode == 'micro':
--            return micro_precision, micro_recall, micro_f1, report
-+            return micro_precision, micro_recall, micro_f1, report, self.cm
-         elif self.mode == 'all':
--            return precision, recall, f1, report
-+            return precision, recall, f1, report, self.cm
-         else:
-             raise ValueError(
-                 f'{self.mode} mode is not supported. Choose "macro" to get aggregated numbers \
-diff --git a/experiment/core/utils.py b/experiment/core/utils.py
-index c9470b8..7f288e1 100644
---- a/experiment/core/utils.py
-+++ b/experiment/core/utils.py
-@@ -38,7 +38,7 @@ def align_labels_to_mask(mask,labels):
-     return m1.tolist()
- 
- def view_aligned(texts,tags,tokenizer,labels_to_ids):
--        return [re.sub(r'( ?\[((PAD)|(CLS)|(SEP))\] ?)',' ',re.sub(' ##','',' '.join(
-+        return [re.sub(r'( ?\[((PAD)|(CLS)|(SEP))\] ?)',' ',re.sub(' +##','',' '.join( #[.?!,;:\-—… ]+
-             [_[0]+_[1] for _ in list(
-                 zip(tokenizer.convert_ids_to_tokens(_[0]),
-                     [labels_to_ids[id] for id in _[1].tolist()])
-@@ -113,7 +113,11 @@ def chunk_to_len(max_seq_length,tokenizer,attach_label_to_end,tokens,labels=None
-         padded_labels=[pad_to_len(max_seq_length,align_labels_to_mask(*_)) for _ in zip(masks,split_labels)]
-     return ids,masks,padded_labels
-     
--def chunk_to_len_batch(max_seq_length,tokenizer,tokens,labels=None,labelled=True,ignore_index=-100, attach_label_to_end=True):
-+def chunk_to_len_batch(max_seq_length,tokenizer,tokens,labels=None,labelled=True,ignore_index=-100, attach_label_to_end=None):
-+    no_mask=False
-+    if attach_label_to_end is None:
-+        no_mask=True
-+        attach_label_to_end=True
-     batch_ids=[]
-     batch_masks=[]
-     batch_labels=[]
-@@ -126,8 +130,11 @@ def chunk_to_len_batch(max_seq_length,tokenizer,tokens,labels=None,labelled=True
-     output = {'input_ids': torch.as_tensor(batch_ids, dtype=torch.long),
-               'attention_mask': torch.as_tensor(batch_ids, dtype=torch.bool),
-               'subtoken_mask': torch.as_tensor(batch_masks,dtype=torch.bool)}
--    output['subtoken_mask']|=((output['input_ids']==101)|(output['input_ids']==102))
--    output['subtoken_mask']&=labelled
-+    if no_mask:
-+        output['subtoken_mask']=output['attention_mask']&(output['input_ids']!=102)
-+    else:
-+        output['subtoken_mask']|=(output['input_ids']==101)  # dont want end token |(output['input_ids']==102)
-+        output['subtoken_mask']&=labelled
-     output['labels']=torch.as_tensor(batch_labels,dtype=torch.long) if labelled==True else torch.zeros_like(output['input_ids'],dtype=torch.long)
-     return output
- 
-diff --git a/experiment/data/punctuation_datamodule.py b/experiment/data/punctuation_datamodule.py
-index fb69299..71a1f7f 100644
---- a/experiment/data/punctuation_datamodule.py
-+++ b/experiment/data/punctuation_datamodule.py
-@@ -25,7 +25,7 @@ class PunctuationDataModule(LightningDataModule):
-             data_id: str = '',
-             tmp_path:str = '~/data/tmp',
-             test_unlabelled:bool = True,
--            attach_label_to_end:bool = True,
-+            attach_label_to_end:bool = None,
-             ):
-         #unlabelled=[], batch_size = 256, max_seq_length = 256, num_workers=1):
-         super().__init__()
-diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
-index 97f2fb2..4c9f67a 100644
---- a/experiment/data/punctuation_dataset_multi.py
-+++ b/experiment/data/punctuation_dataset_multi.py
-@@ -39,7 +39,7 @@ class PunctuationDomainDataset(IterableDataset):
-         tmp_path='~/data/tmp',
-         start=0,
-         end=-1,
--        attach_label_to_end=True,
-+        attach_label_to_end=None,
-     ):
-         if not (os.path.exists(csv_file)):
-             raise FileNotFoundError(
-@@ -154,7 +154,7 @@ class PunctuationDomainDatasets(IterableDataset):
-                  randomize:bool=True,
-                  data_id='',
-                  tmp_path='~/data/tmp',
--                 attach_label_to_end=True,
-+                 attach_label_to_end=None,
-                  ):
-         worker_info = get_worker_info()
-         self.num_workers=1 if worker_info is None else worker_info.num_workers
-@@ -266,7 +266,7 @@ class PunctuationInferenceDataset(Dataset):
-             "labels": NeuralType(('B', 'T'), ChannelType()),
-         }
- 
--    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], num_samples:int=256, degree:int = 0, attach_label_to_end:bool=True):
-+    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], num_samples:int=256, degree:int = 0, attach_label_to_end:bool=None):
-         """ Initializes BertPunctuationInferDataset. """
-         self.degree=degree
-         self.punct_label_ids=punct_label_ids
-diff --git a/experiment/info.log b/experiment/info.log
-index 85bc297..104c5dd 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,192 +1,4 @@
--[INFO] - GPU available: True, used: False
-+[INFO] - GPU available: True, used: True
- [INFO] - TPU available: None, using: 0 TPU cores
--[INFO] - shuffling train set
--[INFO] - Global seed set to 42
--[INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.01
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.CosineAnnealing object at 0x7f13e0b714f0>" 
--will be used during training (effective maximum steps = 1600) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--min_lr: 1.0e-10
--last_epoch: -1
--max_steps: 1600
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 513   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--299 K     Trainable params
--13.2 M    Non-trainable params
--13.5 M    Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          91.10      19.21      31.73       3196
--! (label_id: 1)                                          0.00       0.00       0.00          0
--, (label_id: 2)                                          5.61      37.37       9.76        198
--- (label_id: 3)                                          0.79       9.09       1.46         22
--. (label_id: 4)                                          0.00       0.00       0.00        212
--: (label_id: 5)                                          0.00       0.00       0.00          4
--; (label_id: 6)                                          0.00       0.00       0.00          4
--? (label_id: 7)                                          0.96      62.50       1.88         16
--— (label_id: 8)                                          0.00       0.00       0.00         20
--… (label_id: 9)                                          0.00       0.00       0.00          0
---------------------
--micro avg                                               19.06      19.06      19.06       3672
--macro avg                                               12.31      16.02       5.60       3672
--weighted avg                                            79.60      19.06      28.16       3672
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00         34
---------------------
--micro avg                                              100.00     100.00     100.00         34
--macro avg                                              100.00     100.00     100.00         34
--weighted avg                                           100.00     100.00     100.00         34
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          97.76      93.83      95.75     303856
--! (label_id: 1)                                          0.00       0.00       0.00        155
--, (label_id: 2)                                         38.85      59.39      46.98      23143
--- (label_id: 3)                                         72.00      53.11      61.13       1830
--. (label_id: 4)                                         58.36      60.43      59.38      20164
--: (label_id: 5)                                          0.00       0.00       0.00        439
--; (label_id: 6)                                          0.00       0.00       0.00        176
--? (label_id: 7)                                         24.31      36.42      29.15       1590
--— (label_id: 8)                                          6.84       6.23       6.52       1509
--… (label_id: 9)                                          0.00       0.00       0.00        111
---------------------
--micro avg                                               88.58      88.58      88.58     352973
--macro avg                                               29.81      30.94      29.89     352973
--weighted avg                                            90.55      88.58      89.38     352973
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       3181
---------------------
--micro avg                                              100.00     100.00     100.00       3181
--macro avg                                              100.00     100.00     100.00       3181
--weighted avg                                           100.00     100.00     100.00       3181
--
--[INFO] - Epoch 0, global step 199: val_loss reached 0.27337 (best 0.27337), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=0.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          97.63      94.98      96.29     303856
--! (label_id: 1)                                          0.00       0.00       0.00        155
--, (label_id: 2)                                         43.70      57.68      49.73      23143
--- (label_id: 3)                                         75.76      51.58      61.38       1830
--. (label_id: 4)                                         58.49      63.93      61.09      20164
--: (label_id: 5)                                          0.00       0.00       0.00        439
--; (label_id: 6)                                          0.00       0.00       0.00        176
--? (label_id: 7)                                         35.78      37.42      36.58       1590
--— (label_id: 8)                                          5.88       7.22       6.48       1509
--… (label_id: 9)                                          0.00       0.00       0.00        111
---------------------
--micro avg                                               89.67      89.67      89.67     352973
--macro avg                                               31.72      31.28      31.15     352973
--weighted avg                                            90.83      89.67      90.15     352973
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       3181
---------------------
--micro avg                                              100.00     100.00     100.00       3181
--macro avg                                              100.00     100.00     100.00       3181
--weighted avg                                           100.00     100.00     100.00       3181
--
--[INFO] - Epoch 1, global step 399: val_loss reached 0.26732 (best 0.26732), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=1.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          97.23      95.39      96.30     303856
--! (label_id: 1)                                          0.00       0.00       0.00        155
--, (label_id: 2)                                         47.20      48.78      47.98      23143
--- (label_id: 3)                                         69.68      55.25      61.63       1830
--. (label_id: 4)                                         55.03      67.55      60.65      20164
--: (label_id: 5)                                          0.00       0.00       0.00        439
--; (label_id: 6)                                          0.00       0.00       0.00        176
--? (label_id: 7)                                         28.81      45.91      35.40       1590
--— (label_id: 8)                                          8.17      11.93       9.70       1509
--… (label_id: 9)                                          0.00       0.00       0.00        111
---------------------
--micro avg                                               89.72      89.72      89.72     352973
--macro avg                                               30.61      32.48      31.17     352973
--weighted avg                                            90.47      89.72      90.03     352973
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       3181
---------------------
--micro avg                                              100.00     100.00     100.00       3181
--macro avg                                              100.00     100.00     100.00       3181
--weighted avg                                           100.00     100.00     100.00       3181
--
--[INFO] - Epoch 2, global step 599: val_loss reached 0.26594 (best 0.26594), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=2.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          97.60      95.36      96.46     303856
--! (label_id: 1)                                          0.00       0.00       0.00        155
--, (label_id: 2)                                         45.38      58.10      50.96      23143
--- (label_id: 3)                                         70.17      59.40      64.34       1830
--. (label_id: 4)                                         60.79      63.42      62.08      20164
--: (label_id: 5)                                          0.00       0.00       0.00        439
--; (label_id: 6)                                          0.00       0.00       0.00        176
--? (label_id: 7)                                         30.77      51.57      38.54       1590
--— (label_id: 8)                                         10.69       8.48       9.46       1509
--… (label_id: 9)                                          0.00       0.00       0.00        111
---------------------
--micro avg                                               90.10      90.10      90.10     352973
--macro avg                                               31.54      33.63      32.18     352973
--weighted avg                                            91.01      90.10      90.48     352973
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       3181
---------------------
--micro avg                                              100.00     100.00     100.00       3181
--macro avg                                              100.00     100.00     100.00       3181
--weighted avg                                           100.00     100.00     100.00       3181
--
--[INFO] - Epoch 3, global step 799: val_loss reached 0.25942 (best 0.25942), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=3.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          97.63      95.55      96.58     303856
--! (label_id: 1)                                          0.00       0.00       0.00        155
--, (label_id: 2)                                         46.84      57.30      51.54      23143
--- (label_id: 3)                                         75.90      57.49      65.42       1830
--. (label_id: 4)                                         59.34      67.15      63.00      20164
--: (label_id: 5)                                          0.00       0.00       0.00        439
--; (label_id: 6)                                          0.00       0.00       0.00        176
--? (label_id: 7)                                         33.55      48.62      39.70       1590
--— (label_id: 8)                                         12.65       6.36       8.47       1509
--… (label_id: 9)                                          0.00       0.00       0.00        111
---------------------
--micro avg                                               90.39      90.39      90.39     352973
--macro avg                                               32.59      33.25      32.47     352973
--weighted avg                                            91.10      90.39      90.67     352973
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       3181
---------------------
--micro avg                                              100.00     100.00     100.00       3181
--macro avg                                              100.00     100.00     100.00       3181
--weighted avg                                           100.00     100.00     100.00       3181
--
--[INFO] - Epoch 4, global step 999: val_loss reached 0.26189 (best 0.25942), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=4.ckpt" as top 3
-+[INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
-+[INFO] - Using native 16bit precision.
-diff --git a/experiment/main.py b/experiment/main.py
-index 4ae03e6..51abddd 100644
---- a/experiment/main.py
-+++ b/experiment/main.py
-@@ -23,6 +23,7 @@ snoop.install()
- 
- @hydra.main(config_name="config")
- def main(cfg: DictConfig)->None:
-+    torch.set_printoptions(sci_mode=False)
-     data_id = str(int(time()))
-     def savecounter():
-         # pp(os.system(f'rm -r {cfg.model.dataset.data_dir}/*.{data_id}.csv'))
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index 20e9816..d893ed1 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -171,7 +171,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
- 
-         self.log('lr', lr, prog_bar=True)
-         self.log('train_loss', loss)
--        self.log('gamma', self.grad_reverse.scale)
-+        self.log('gamma', self.grad_reverse.scale,logger=True)
- 
-         return {'loss': loss, 'lr': lr}
- 
-@@ -262,11 +262,11 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
- 
-         # calculate metrics and log classification report for Punctuation task
--        punct_precision, punct_recall, punct_f1, punct_report = self.punct_class_report.compute()
-+        punct_precision, punct_recall, punct_f1, punct_report, punctuation_cm = self.punct_class_report.compute()
-         logging.info(f'Punctuation report: {punct_report}')
- 
-         # calculate metrics and log classification report for domainalization task
--        domain_precision, domain_recall, domain_f1, domain_report = self.domain_class_report.compute()
-+        domain_precision, domain_recall, domain_f1, domain_report, domain_cm = self.domain_class_report.compute()
-         logging.info(f'Domain report: {domain_report}')
- 
-         self.log('val_loss', avg_loss, prog_bar=True)
-@@ -276,6 +276,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         self.log('domain_precision', domain_precision)
-         self.log('domain_f1', domain_f1)
-         self.log('domain_recall', domain_recall)
-+        self.log('punctuation_cm', punctuation_cm.__str__())
-+        self.log('domain_cm', domain_cm.__str__())
- 
-     def test_epoch_end(self, outputs):
-         if outputs is not None and len(outputs) == 0:
-@@ -298,11 +300,11 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()
- 
-         # calculate metrics and log classification report for Punctuation task
--        punct_precision, punct_recall, punct_f1, punct_report = self.punct_class_report.compute()
-+        punct_precision, punct_recall, punct_f1, punct_report, punct_cm = self.punct_class_report.compute()
-         logging.info(f'Punctuation report: {punct_report}')
- 
-         # calculate metrics and log classification report for domainalization task
--        domain_precision, domain_recall, domain_f1, domain_report = self.domain_class_report.compute()
-+        domain_precision, domain_recall, domain_f1, domain_report, domain_cm = self.domain_class_report.compute()
-         logging.info(f'Domain report: {domain_report}')
- 
-         self.log('test_loss', avg_loss, prog_bar=True)
-@@ -312,6 +314,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         self.log('domain_precision', domain_precision)
-         self.log('domain_f1', domain_f1)
-         self.log('domain_recall', domain_recall)
-+        self.log('punctuation_cm', punct_cm.__str__())
-+        self.log('domain_cm', domain_cm.__str__())
- 
-     def setup_optimization(self, optim_config: Optional[Union[DictConfig, Dict]] = None):
-         """
-@@ -724,7 +728,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-             tokenizer= self.tokenizer,
-             queries=queries, 
-             max_seq_length=self.hparams.model.dataset.max_seq_length,
--            punct_label_ids=self._cfg.model.punct_label_ids,
-+            punct_label_ids=self.labels_to_ids,
-             attach_label_to_end=self._cfg.model.dataset.attach_label_to_end)
-         batch=ds[0]
-         attention_mask = batch['attention_mask']
-diff --git a/experiment/testing.py b/experiment/testing.py
-index 667f776..272edb8 100644
---- a/experiment/testing.py
-+++ b/experiment/testing.py
-@@ -21,50 +21,9 @@ from copy import deepcopy
- import snoop
- snoop.install()
- 
--# @hydra.main(config_name="config")
--# def main(cfg: DictConfig)->None:
--#     data_id = str(int(time()))
--#     def savecounter():
--#         # pp(os.system(f'rm -r {cfg.model.dataset.data_dir}/*.{data_id}.csv'))
--#         pp(os.system(f'rm -r {cfg.tmp_path}/*.{data_id}.csv'))
--#     atexit.register(savecounter)
--
--#     cfg.model.maximum_unfrozen=max(cfg.model.maximum_unfrozen,cfg.model.unfrozen)
--
--#     pp(cfg)
--#     pl.seed_everything(cfg.seed)
--#     trainer = pl.Trainer(**cfg.trainer)
--#     exp_manager(trainer, cfg.exp_manager)
--#     model = PunctuationDomainModel(cfg=cfg, trainer=trainer, data_id = data_id)
--    
--#     while(model.hparams.model.unfrozen<=cfg.model.maximum_unfrozen and model.hparams.model.unfrozen>=0):
--#         trainer.current_epoch=0
--#         lr_finder = trainer.tuner.lr_find(model,min_lr=1e-10, max_lr=1e-02, num_training=80, early_stop_threshold=None)
--#         # Results can be found in
--#         pp(lr_finder.results)
--#         new_lr = lr_finder.suggestion()
--#         model.hparams.model.optim.lr = new_lr
--#         model.dm.reset()
--#         trainer.current_epoch=0
--#         trainer.fit(model)
--#         try:
--#             model.unfreeze(cfg.model.unfreeze_step)
--#         except:
--#             pp('training complete.')
--#             break
--#     if cfg.model.nemo_path:
--#         model.save_to(cfg.model.nemo_path)
--
--    
--    
--#     gpu = 1 if cfg.trainer.gpus != 0 else 0
--#     # model.dm.setup('test')
--#     trainer = pl.Trainer(gpus=gpu)
--#     trainer.test(model,ckpt_path=None)
--
--
--@hydra.main(config_name="test_config")
-+@hydra.main(config_path="../Punctuation_with_Domain_discriminator/TEDstart2021-02-11_07-55-58/",config_name="hparams.yaml")
- def main(cfg : DictConfig) -> None:
-+    torch.set_printoptions(sci_mode=False)
-     # trainer=pl.Trainer(**cfg.trainer)
-     # exp_manager(trainer, cfg.get("exp_manager", None))
-     # do_training = False
-@@ -74,19 +33,22 @@ def main(cfg : DictConfig) -> None:
-     #     if cfg.model.nemo_path:
-     #         model.save_to(cfg.model.nemo_path)
-     # gpu = 1 if cfg.trainer.gpus != 0 else 0
--    model = PunctuationDomainModel.restore_from(restore_path=cfg.exp_manager.restore_path, override_config_path=cfg.exp_manager.override_config_path, )
--    trainer = pl.Trainer(gpus=gpu)
--    model.set_trainer(trainer)
--    queries = [
--        'we bought four shirts one pen and a mug from the nvidia gear store in santa clara',
--        'what can i do for you today',
--        'how are you',
--    ]
--    inference_results = model.add_punctuation_capitalization(queries)
--
--    for query, result in zip(queries, inference_results):
--        logging.info(f'Query : {query}')
--        logging.info(f'Result: {result.strip()}\n')
-+    # model = PunctuationDomainModel.restore_from(restore_path=cfg.exp_manager.restore_path, override_config_path=cfg.exp_manager.override_config_path, )
-+    model = PunctuationDomainModel.load_from_checkpoint( #TEDend2021-02-11_07-57-33  # TEDstart2021-02-11_07-55-58
-+    checkpoint_path="/home/nxingyu2/project/Punctuation_with_Domain_discriminator/TEDstart2021-02-11_07-55-58/checkpoints/Punctuation_with_Domain_discriminator-last.ckpt")
-+    trainer = pl.Trainer(**cfg.trainer)
-+    # trainer = pl.Trainer(gpus=gpu)
-+    trainer.test(model,ckpt_path=None)
-+    # queries = [
-+    #     'we bought four shirts one pen and a mug from the nvidia gear store in santa clara',
-+    #     'what can i do for you today',
-+    #     'how are you',
-+    # ]
-+    # inference_results = model.add_punctuation_capitalization(queries)
-+
-+    # for query, result in zip(queries, inference_results):
-+    #     logging.info(f'Query : {query}')
-+    #     logging.info(f'Result: {result.strip()}\n')
- 
- if __name__ == "__main__":
-     main()
-\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/hparams.yaml
deleted file mode 100644
index d14b3a5..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/hparams.yaml
+++ /dev/null
@@ -1,111 +0,0 @@
-seed: 42
-trainer:
-  gpus: 1
-  num_nodes: 1
-  max_epochs: 15
-  max_steps: null
-  accumulate_grad_batches: 4
-  gradient_clip_val: 0
-  amp_level: O1
-  precision: 16
-  accelerator: ddp
-  checkpoint_callback: false
-  logger: false
-  log_every_n_steps: 1
-  val_check_interval: 1.0
-  resume_from_checkpoint: null
-exp_manager:
-  exp_dir: /home/nxingyu2/project/
-  name: Punctuation_with_Domain_discriminator
-  create_tensorboard_logger: true
-  create_checkpoint_callback: true
-base_path: /home/nxingyu2/data
-tmp_path: /home/nxingyu2/data/tmp
-model:
-  nemo_path: null
-  transformer_path: distilbert-base-uncased
-  unfrozen: 0
-  maximum_unfrozen: 1
-  unfreeze_step: 1
-  punct_label_ids:
-  - ''
-  - '!'
-  - ','
-  - '-'
-  - .
-  - ':'
-  - ;
-  - '?'
-  - —
-  - …
-  punct_class_weights: false
-  dataset:
-    data_dir: /home/nxingyu2/data
-    labelled:
-    - /home/nxingyu2/data/ted_talks_processed
-    unlabelled: null
-    max_seq_length: 128
-    pad_label: ''
-    ignore_extra_tokens: false
-    ignore_start_end: false
-    use_cache: false
-    num_workers: 4
-    pin_memory: true
-    drop_last: true
-    num_labels: 10
-    num_domains: 1
-    test_unlabelled: true
-    attach_label_to_end: none
-    train_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 4
-    validation_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 4
-  tokenizer:
-    tokenizer_name: distilbert-base-uncased
-    vocab_file: null
-    tokenizer_model: null
-    special_tokens: null
-  language_model:
-    pretrained_model_name: distilbert-base-uncased
-    lm_checkpoint: null
-    config_file: null
-    config: null
-  punct_head:
-    punct_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: dice
-  domain_head:
-    domain_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: cel
-    gamma: 0
-    pooling: mean_max
-    idx_conditioned_on: 0
-  dice_loss:
-    epsilon: 0.01
-    alpha: 3
-    macro_average: true
-  focal_loss:
-    gamma: 1
-  optim:
-    name: adamw
-    lr: 0.01
-    weight_decay: 0.0
-    sched:
-      name: CosineAnnealing
-      warmup_steps: null
-      warmup_ratio: 0.1
-      min_lr: 1.0e-10
-      last_epoch: -1
-      monitor: val_loss
-      reduce_on_plateau: false
diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/lightning_logs.txt
deleted file mode 100644
index 180b7db..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/lightning_logs.txt
+++ /dev/null
@@ -1,23 +0,0 @@
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
-Using native 16bit precision.
-initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | DistilBertModel      | 66.4 M
-1 | punct_classifier    | TokenClassifier      | 7.7 K 
-2 | domain_classifier   | SequenceClassifier   | 1.5 K 
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-10.8 K    Trainable params
-66.4 M    Non-trainable params
-66.4 M    Total params
-Epoch 0, global step 199: val_loss reached 0.30744 (best 0.30744), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.31-epoch=0.ckpt" as top 3
-Saving latest checkpoint...
-Epoch 0, global step 198: val_loss reached 0.30744 (best 0.30744), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.31-epoch=0-v0.ckpt" as top 3
diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/nemo_error_log.txt
deleted file mode 100644
index 13088e0..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/nemo_error_log.txt
+++ /dev/null
@@ -1,10 +0,0 @@
-[NeMo W 2021-02-11 11:48:02 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-11 11:48:10 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-11 11:49:21 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fa57cf4f3d0> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-11 11:49:30 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fa57cf4cac0> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index d2dd215..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,12 +0,0 @@
-[NeMo I 2021-02-11 11:48:02 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02
-[NeMo I 2021-02-11 11:48:02 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-11 11:48:02 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-11 11:48:10 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-11 11:49:21 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fa57cf4f3d0> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-11 11:49:30 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fa57cf4cac0> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/events.out.tfevents.1613018787.intern-instance.26577.0 b/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/events.out.tfevents.1613018787.intern-instance.26577.0
index 7733536..d29916d 100644
Binary files a/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/events.out.tfevents.1613018787.intern-instance.26577.0 and b/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/events.out.tfevents.1613018787.intern-instance.26577.0 differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/lightning_logs.txt
index d732323..1982832 100644
--- a/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/lightning_logs.txt
+++ b/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/lightning_logs.txt
@@ -18,3 +18,10 @@ initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
 2.4 M     Trainable params
 106 M     Non-trainable params
 108 M     Total params
+Epoch 0, global step 5254: val_loss reached 0.12680 (best 0.12680), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.13-epoch=0.ckpt" as top 3
+Epoch 1, global step 10509: val_loss reached 0.99380 (best 0.12680), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.99-epoch=1.ckpt" as top 3
+Epoch 2, global step 15764: val_loss reached 0.48886 (best 0.12680), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.49-epoch=2.ckpt" as top 3
+Epoch 3, global step 21019: val_loss reached 0.12802 (best 0.12680), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.13-epoch=3.ckpt" as top 3
+Epoch 4, global step 26274: val_loss reached 0.18930 (best 0.12680), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.19-epoch=4.ckpt" as top 3
+Epoch 5, step 31529: val_loss was not in top 3
+Epoch 6, global step 36784: val_loss reached 0.17587 (best 0.12680), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.18-epoch=6.ckpt" as top 3
diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/nemo_error_log.txt
index c5d946d..03caccf 100644
--- a/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/nemo_error_log.txt
+++ b/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/nemo_error_log.txt
@@ -2,3 +2,9 @@
 [NeMo W 2021-02-11 12:46:27 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
       warnings.warn(*args, **kwargs)
     
+[NeMo W 2021-02-11 15:27:02 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f2ae415a5b0> was reported to be 21018 (when accessing len(dataloader)), but 21019 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
+[NeMo W 2021-02-11 15:49:15 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f2ae4116dc0> was reported to be 2627 (when accessing len(dataloader)), but 2628 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/nemo_log_globalrank-0_localrank-0.txt
index c5ed369..e00bdaf 100644
--- a/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/nemo_log_globalrank-0_localrank-0.txt
+++ b/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/nemo_log_globalrank-0_localrank-0.txt
@@ -4,3 +4,9 @@
 [NeMo W 2021-02-11 12:46:27 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
       warnings.warn(*args, **kwargs)
     
+[NeMo W 2021-02-11 15:27:02 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f2ae415a5b0> was reported to be 21018 (when accessing len(dataloader)), but 21019 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
+[NeMo W 2021-02-11 15:49:15 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f2ae4116dc0> was reported to be 2627 (when accessing len(dataloader)), but 2628 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
diff --git a/README.md b/README.md
index 9c03a29..6407311 100644
--- a/README.md
+++ b/README.md
@@ -648,4 +648,42 @@ electra-small can't train opensubtitles
  'test_loss': tensor(0.2392, device='cuda:0')}
 
 
-114802 distilbert opensub
\ No newline at end of file
+114802 distilbert opensub
+ (label_id: 0)                                          97.41      94.81      96.09   13849820
+! (label_id: 1)                                         28.56      40.82      33.60     228399
+, (label_id: 2)                                         45.53      48.81      47.12     981518
+- (label_id: 3)                                         58.45      42.81      49.42      85993
+. (label_id: 4)                                         58.33      66.02      61.94    1857888
+: (label_id: 5)                                         92.71      54.91      68.97       3983
+; (label_id: 6)                                          0.00       0.00       0.00       1353
+? (label_id: 7)                                         58.06      55.83      56.93     474152
+— (label_id: 8)                                          0.00       0.00       0.00        512
+… (label_id: 9)                                         22.95      23.05      23.00     161832
+-------------------
+micro avg                                               86.54      86.54      86.54   17645450
+macro avg                                               46.20      42.71      43.71   17645450
+weighted avg                                            87.57      86.54      86.99   17645450
+
+-------------------
+tensor([[ 13130599.,  11431.,   99043.,  18848.,   130364.,  332.,  164.,   45236.,    56.,  44072.],
+        [    31464.,  93226.,   54383.,   2660.,   123020.,   96.,   40.,   13890.,    80.,   7584.],
+        [   178120.,  39328.,  479117.,   4423.,   279274.,  308.,  152.,   44966.,    76.,  26535.],
+        [    16123.,    980.,    2488.,  36816.,     4584.,   36.,    4.,     424.,     0.,   1536.],
+        [   389548.,  69723.,  268286.,   9439.,  1226576.,  884.,  922.,   99381.,   204.,  37687.],
+        [       20.,      0.,      36.,     56.,       56., 2187.,    0.,       4.,     0.,      0.],
+        [        0.,      0.,       0.,      0.,        0.,    0.,    0.,       0.,     0.,      0.],
+        [    67051.,  10711.,   36473.,   1870.,    67834.,   64.,   47.,  264740.,    72.,   7108.],
+        [        0.,      0.,       0.,      0.,        0.,    0.,    0.,       0.,     0.,      0.],
+        [    36895.,   3000.,   41692.,  11881.,    26180.,   76.,   24.,    5511.,    24.,  37310.]], device='cuda:0')
+
+
+ 'punct_f1': tensor(43.7067, device='cuda:0'),
+ 'punct_precision': tensor(46.1993, device='cuda:0'),
+ 'punct_recall': tensor(42.7068, device='cuda:0'),
+ 'test_loss': tensor(-0.0626, device='cuda:0')}
+
+
+electra base domain adversarial gamma 0
+
+
+electra base domain adversarial gamma 0.05
\ No newline at end of file
diff --git a/experiment/config.yaml b/experiment/config.yaml
index ef10a04..3cf8809 100644
--- a/experiment/config.yaml
+++ b/experiment/config.yaml
@@ -124,7 +124,7 @@ model:
         log_softmax: false
         use_transformer_init: true
         loss: 'cel'
-        gamma: 0 #0.1 # coefficient of gradient reversal
+        gamma: 0.1 #0.1 # coefficient of gradient reversal
         pooling: 'mean_max' # 'mean' mean_max
         idx_conditioned_on: 0
     
diff --git a/experiment/info.log b/experiment/info.log
index d45bd15..dc33065 100644
--- a/experiment/info.log
+++ b/experiment/info.log
@@ -1,78 +1,4 @@
 [INFO] - GPU available: True, used: True
 [INFO] - TPU available: None, using: 0 TPU cores
-[INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
+[INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
 [INFO] - Using native 16bit precision.
-[INFO] - shuffling train set
-[INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-[INFO] - Optimizer config = AdamW (
-Parameter Group 0
-    amsgrad: False
-    betas: (0.9, 0.999)
-    eps: 1e-08
-    lr: 0.01
-    weight_decay: 0.0
-)
-[INFO] - Scheduler "<core.optim.lr_scheduler.CosineAnnealing object at 0x7f2aadd1dd30>" 
-will be used during training (effective maximum steps = 78825) - 
-Parameters : 
-(warmup_steps: null
-warmup_ratio: 0.1
-min_lr: 1.0e-10
-last_epoch: -1
-max_steps: 78825
-)
-[INFO] - 
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 108 M 
-1 | punct_classifier    | TokenClassifier      | 7.7 K 
-2 | domain_classifier   | SequenceClassifier   | 3.1 K 
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-2.4 M     Trainable params
-106 M     Non-trainable params
-108 M     Total params
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.31       1.05       2.09      17162
-! (label_id: 1)                                          0.28      10.34       0.54         58
-, (label_id: 2)                                          3.49      10.34       5.22        532
-- (label_id: 3)                                          0.00       0.00       0.00         27
-. (label_id: 4)                                          7.95       2.53       3.84       1028
-: (label_id: 5)                                          0.00       0.00       0.00          0
-; (label_id: 6)                                          0.03      50.00       0.06          4
-? (label_id: 7)                                          0.37       2.07       0.63        145
-— (label_id: 8)                                          0.00       0.00       0.00          0
-… (label_id: 9)                                          0.52      36.00       1.03         75
--------------------
-micro avg                                                1.58       1.58       1.58      19031
-macro avg                                               13.74      14.04       1.68      19031
-weighted avg                                            88.29       1.58       2.25      19031
-
--------------------
-181.00  0.00  0.00  1.00  4.00  0.00  0.00  0.00  0.00  0.00
-2048.00  6.00 37.00  4.00 59.00  0.00  1.00  9.00  0.00 10.00
-1435.00 10.00 55.00  0.00 68.00  0.00  0.00  5.00  0.00  1.00
-80.00  0.00  3.00  0.00  2.00  0.00  0.00  0.00  0.00  1.00
-273.00  1.00 17.00  0.00 26.00  0.00  0.00  8.00  0.00  2.00
-2232.00  7.00 48.00  6.00 81.00  0.00  1.00 12.00  0.00  1.00
-5515.00 19.00 217.00 10.00 431.00  0.00  2.00 72.00  0.00 30.00
-754.00  0.00 13.00  2.00 28.00  0.00  0.00  3.00  0.00  3.00
-15.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
-4629.00 15.00 142.00  4.00 329.00  0.00  0.00 36.00  0.00 27.00
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                          0.00       0.00       0.00         80
-1 (label_id: 1)                                         50.00     100.00      66.67         80
--------------------
-micro avg                                               50.00      50.00      50.00        160
-macro avg                                               25.00      50.00      33.33        160
-weighted avg                                            25.00      50.00      33.33        160
-
--------------------
- 0.00  0.00
-80.00 80.00
