Global seed set to 42
GPU available: True, used: True
TPU available: None, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
Using native 16bit precision.
Global seed set to 42
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1

  | Name                | Type                 | Params
-------------------------------------------------------------
0 | transformer         | DistilBertModel      | 66.4 M
1 | punct_classifier    | TokenClassifier      | 7.7 K 
2 | domain_classifier   | SequenceClassifier   | 1.5 K 
3 | punctuation_loss    | FocalDiceLoss        | 0     
4 | domain_loss         | CrossEntropyLoss     | 0     
5 | agg_loss            | AggregatorLoss       | 0     
6 | punct_class_report  | ClassificationReport | 0     
7 | domain_class_report | ClassificationReport | 0     
-------------------------------------------------------------
10.8 K    Trainable params
66.4 M    Non-trainable params
66.4 M    Total params
Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_14-46-00/lr_find_temp_model.ckpt
Global seed set to 42

  | Name                | Type                 | Params
-------------------------------------------------------------
0 | transformer         | DistilBertModel      | 66.4 M
1 | punct_classifier    | TokenClassifier      | 7.7 K 
2 | domain_classifier   | SequenceClassifier   | 1.5 K 
3 | punctuation_loss    | FocalDiceLoss        | 0     
4 | domain_loss         | CrossEntropyLoss     | 0     
5 | agg_loss            | AggregatorLoss       | 0     
6 | punct_class_report  | ClassificationReport | 0     
7 | domain_class_report | ClassificationReport | 0     
-------------------------------------------------------------
10.8 K    Trainable params
66.4 M    Non-trainable params
66.4 M    Total params
Epoch 0, global step 200: val_loss reached 0.26631 (best 0.26631), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_14-46-00/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=0.ckpt" as top 3
Epoch 1, global step 400: val_loss reached 0.26336 (best 0.26336), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_14-46-00/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=1.ckpt" as top 3
Epoch 2, global step 600: val_loss reached 0.26109 (best 0.26109), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_14-46-00/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=2.ckpt" as top 3
Epoch 3, global step 800: val_loss reached 0.25805 (best 0.25805), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_14-46-00/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=3.ckpt" as top 3
Epoch 4, global step 1000: val_loss reached 0.25547 (best 0.25547), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_14-46-00/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=4.ckpt" as top 3
Epoch 5, global step 1200: val_loss reached 0.25339 (best 0.25339), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_14-46-00/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.25-epoch=5.ckpt" as top 3
Epoch 6, global step 1400: val_loss reached 0.24810 (best 0.24810), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_14-46-00/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.25-epoch=6.ckpt" as top 3
Epoch 7, global step 1600: val_loss reached 0.24127 (best 0.24127), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_14-46-00/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=7.ckpt" as top 3
Epoch 8, global step 1800: val_loss reached 0.23864 (best 0.23864), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_14-46-00/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=8.ckpt" as top 3
Epoch 9, global step 2000: val_loss reached 0.23646 (best 0.23646), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_14-46-00/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=9.ckpt" as top 3
Global seed set to 42

  | Name                | Type                 | Params
-------------------------------------------------------------
0 | transformer         | DistilBertModel      | 66.4 M
1 | punct_classifier    | TokenClassifier      | 7.7 K 
2 | domain_classifier   | SequenceClassifier   | 1.5 K 
3 | punctuation_loss    | FocalDiceLoss        | 0     
4 | domain_loss         | CrossEntropyLoss     | 0     
5 | agg_loss            | AggregatorLoss       | 0     
6 | punct_class_report  | ClassificationReport | 0     
7 | domain_class_report | ClassificationReport | 0     
-------------------------------------------------------------
7.1 M     Trainable params
59.3 M    Non-trainable params
66.4 M    Total params
LR finder stopped early due to diverging loss.
Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_14-46-00/lr_find_temp_model.ckpt
Failed to compute suggesting for `lr`. There might not be enough points.
Traceback (most recent call last):
  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
    min_grad = np.gradient(loss).argmin()
  File "<__array_function__ internals>", line 5, in gradient
  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
    raise ValueError(
ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
Global seed set to 42

  | Name                | Type                 | Params
-------------------------------------------------------------
0 | transformer         | DistilBertModel      | 66.4 M
1 | punct_classifier    | TokenClassifier      | 7.7 K 
2 | domain_classifier   | SequenceClassifier   | 1.5 K 
3 | punctuation_loss    | FocalDiceLoss        | 0     
4 | domain_loss         | CrossEntropyLoss     | 0     
5 | agg_loss            | AggregatorLoss       | 0     
6 | punct_class_report  | ClassificationReport | 0     
7 | domain_class_report | ClassificationReport | 0     
-------------------------------------------------------------
7.1 M     Trainable params
59.3 M    Non-trainable params
66.4 M    Total params
