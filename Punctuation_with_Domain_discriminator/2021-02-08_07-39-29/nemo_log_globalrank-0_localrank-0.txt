[NeMo I 2021-02-08 07:39:29 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29
[NeMo I 2021-02-08 07:39:29 exp_manager:519] TensorboardLogger has been set up
[NeMo W 2021-02-08 07:39:29 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
[NeMo W 2021-02-08 07:39:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
      warnings.warn(*args, **kwargs)
    
[NeMo W 2021-02-08 07:39:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
      warnings.warn(*args, **kwargs)
    
[NeMo W 2021-02-08 07:39:40 nemo_logging:349] /home/nxingyu/project/experiment/core/classification_report.py:116: UserWarning: This overload of nonzero is deprecated:
    	nonzero(Tensor input, *, Tensor out)
    Consider using one of the following signatures instead:
    	nonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629411241/work/torch/csrc/utils/python_arg_parser.cpp:766.)
      num_non_empty_classes = torch.nonzero(self.num_examples_per_class).size(0)
    
[NeMo W 2021-02-08 07:39:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
      warnings.warn(*args, **kwargs)
    
[NeMo W 2021-02-08 07:43:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
      warnings.warn(*args, **kwargs)
    
[NeMo W 2021-02-08 07:43:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
      warnings.warn(*args, **kwargs)
    
[NeMo W 2021-02-08 07:44:01 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
      warnings.warn(*args, **kwargs)
    
[NeMo W 2021-02-08 07:44:05 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:118: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
    
[NeMo W 2021-02-08 07:48:43 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f3e417748e0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
      warnings.warn(warn_msg)
    
[NeMo W 2021-02-08 07:49:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f3e4bb7b490> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
      warnings.warn(warn_msg)
    
[NeMo W 2021-02-08 07:59:34 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
      warnings.warn(*args, **kwargs)
    
[NeMo W 2021-02-08 08:28:12 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
      warnings.warn(*args, **kwargs)
    
[NeMo W 2021-02-08 08:28:39 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f3e4bb7b5b0> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
      warnings.warn(warn_msg)
    
