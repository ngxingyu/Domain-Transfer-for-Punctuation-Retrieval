[NeMo W 2021-03-02 10:33:17 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
[NeMo W 2021-03-02 10:34:11 nemo_logging:349] /home/nxingyu/project/experiment/models/punctuation_domain_model.py:68: SyntaxWarning: assertion is always true, perhaps remove parentheses?
      assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
    
[NeMo W 2021-03-02 10:34:11 nemo_logging:349] /home/nxingyu/project/experiment/models/punctuation_domain_model.py:68: SyntaxWarning: assertion is always true, perhaps remove parentheses?
      assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
    
[NeMo W 2021-03-02 10:34:14 nemo_logging:349] /home/nxingyu/project/experiment/models/punctuation_domain_model.py:68: SyntaxWarning: assertion is always true, perhaps remove parentheses?
      assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
    
[NeMo W 2021-03-02 10:34:14 nemo_logging:349] /home/nxingyu/project/experiment/models/punctuation_domain_model.py:68: SyntaxWarning: assertion is always true, perhaps remove parentheses?
      assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
    
[NeMo W 2021-03-02 10:34:14 nemo_logging:349] /home/nxingyu/project/experiment/models/punctuation_domain_model.py:68: SyntaxWarning: assertion is always true, perhaps remove parentheses?
      assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
    
[NeMo W 2021-03-02 10:34:16 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
      warnings.warn(*args, **kwargs)
    
[NeMo W 2021-03-02 10:36:03 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f48ba5d8190> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
      warnings.warn(warn_msg)
    
[NeMo W 2021-03-02 10:36:18 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f48ba5d7670> was reported to be 12 (when accessing len(dataloader)), but 13 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
      warnings.warn(warn_msg)
    
[NeMo W 2021-03-02 10:51:36 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
    
[NeMo W 2021-03-02 11:30:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f48ba52f2b0> was reported to be 12 (when accessing len(dataloader)), but 13 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
      warnings.warn(warn_msg)
    
