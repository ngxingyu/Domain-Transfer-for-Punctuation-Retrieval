commit hash: 66d59fddd871d29e1ceab6de212c3812501bf786
diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/events.out.tfevents.1612493730.intern-instance.20106.0 b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/events.out.tfevents.1612493730.intern-instance.20106.0
deleted file mode 100644
index 25acd93..0000000
Binary files a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/events.out.tfevents.1612493730.intern-instance.20106.0 and /dev/null differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/git-info.log
deleted file mode 100644
index 2bf17c1..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/git-info.log
+++ /dev/null
@@ -1,521 +0,0 @@
-commit hash: 5b55597d5aa99268a9f2ee637608d94abc88934b
-diff --git a/experiment/Nemo2Lightning.ipynb b/experiment/Nemo2Lightning.ipynb
-index ebb0800..d2ec988 100644
---- a/experiment/Nemo2Lightning.ipynb
-+++ b/experiment/Nemo2Lightning.ipynb
-@@ -74,79 +74,79 @@
-   },
-   {
-    "cell_type": "code",
--   "execution_count": 11,
-+   "execution_count": 13,
-    "metadata": {},
-    "outputs": [
-     {
-      "name": "stderr",
-      "output_type": "stream",
-      "text": [
--      "09:00:02.50 LOG:\n",
--      "09:00:02.52 .... 'cel none' = 'cel none'\n",
--      "09:00:02.53 .... output = tensor([1.5514, 1.5514, 0.5514, 0.5514], grad_fn=<NllLossBackward>)\n",
--      "09:00:02.53 LOG:\n",
--      "09:00:02.53 .... 'cel mean' = 'cel mean'\n",
--      "09:00:02.53 .... output = tensor(1.0514, grad_fn=<NllLossBackward>)\n",
--      "09:00:02.53 LOG:\n",
--      "09:00:02.53 .... 'cel sum' = 'cel sum'\n",
--      "09:00:02.53 .... output = tensor(4.2058, grad_fn=<NllLossBackward>)\n",
--      "09:00:02.53 LOG:\n",
--      "09:00:02.54 .... 'focal sum' = 'focal sum'\n",
--      "09:00:02.54 .... loss(inp, tar) = tensor(6.7352, grad_fn=<SumBackward0>)\n",
--      "09:00:02.54 LOG:\n",
--      "09:00:02.54 .... 'focal mean' = 'focal mean'\n",
--      "09:00:02.54 .... loss(inp, tar) = tensor(0.4210, grad_fn=<MeanBackward0>)\n",
--      "09:00:02.54 LOG:\n",
--      "09:00:02.54 .... 'focal none' = 'focal none'\n",
--      "09:00:02.55 .... loss(inp, tar) = tensor([0.9635, 0.9635, 0.0991, 0.0991], grad_fn=<MulBackward0>)\n",
--      "09:00:02.55 LOG:\n",
--      "09:00:02.55 .... 'focal none' = 'focal none'\n",
--      "09:00:02.55 .... loss(inp, tar) = tensor([0.4716, 0.4716, 0.0075, 0.0075], grad_fn=<MulBackward0>)\n",
--      "09:00:02.55 LOG:\n",
--      "09:00:02.55 .... 'crf,none' = 'crf,none'\n",
--      "09:00:02.56 .... output = tensor([4.2377], grad_fn=<NegBackward>)\n",
--      "09:00:02.56 LOG:\n",
--      "09:00:02.56 .... 'crf,mean' = 'crf,mean'\n",
--      "09:00:02.56 .... output = tensor(4.3934, grad_fn=<NegBackward>)\n",
--      "09:00:02.56 LOG:\n",
--      "09:00:02.56 .... 'crf,sum' = 'crf,sum'\n",
--      "09:00:02.57 .... output = tensor(4.3557, grad_fn=<NegBackward>)\n",
--      "09:00:02.57 LOG:\n",
--      "09:00:02.57 .... 'crf,token_mean' = 'crf,token_mean'\n",
--      "09:00:02.57 .... output = tensor(1.0820, grad_fn=<DivBackward0>)\n",
--      "09:00:02.57 LOG:\n",
--      "09:00:02.57 .... 'dice none,micro' = 'dice none,micro'\n",
--      "09:00:02.57 .... output = tensor(0.1876, grad_fn=<PowBackward0>)\n",
--      "09:00:02.58 LOG:\n",
--      "09:00:02.58 .... 'dice mean,micro' = 'dice mean,micro'\n",
--      "09:00:02.58 .... output = tensor(0.0625, grad_fn=<DivBackward0>)\n",
--      "09:00:02.58 LOG:\n",
--      "09:00:02.58 .... 'dice sum,micro' = 'dice sum,micro'\n",
--      "09:00:02.58 .... output = tensor(0.1876, grad_fn=<SumBackward0>)\n",
--      "09:00:02.58 LOG:\n",
--      "09:00:02.58 .... 'dice sum,micro' = 'dice sum,micro'\n",
--      "09:00:02.59 .... output = tensor(0.0812, grad_fn=<SumBackward0>)\n",
--      "09:00:02.59 LOG:\n",
--      "09:00:02.59 .... 'dice none,macro' = 'dice none,macro'\n",
--      "09:00:02.59 .... loss(inp, tar) = tensor([5.9547, 4.4872, 2.0551], grad_fn=<MulBackward0>)\n",
--      "09:00:02.59 LOG:\n",
--      "09:00:02.59 .... 'dice none,macro' = 'dice none,macro'\n",
--      "09:00:02.60 .... loss(inp, tar) = tensor([0.0042, 0.1202, 0.0027], grad_fn=<MulBackward0>)\n",
--      "09:00:02.60 LOG:\n",
--      "09:00:02.60 .... 'dice none,macro' = 'dice none,macro'\n",
--      "09:00:02.60 .... loss(inp, tar) = tensor([0.1116, 0.4285, 0.0935], grad_fn=<MulBackward0>)\n",
--      "09:00:02.60 LOG:\n",
--      "09:00:02.60 .... 'dice mean,macro' = 'dice mean,macro'\n",
--      "09:00:02.60 .... loss(inp, tar) = tensor(0.2112, grad_fn=<DivBackward0>)\n",
--      "09:00:02.61 LOG:\n",
--      "09:00:02.61 .... 'dice sum,macro' = 'dice sum,macro'\n",
--      "09:00:02.61 .... loss(inp, tar) = tensor(0.6335, grad_fn=<SumBackward0>)\n",
--      "09:00:02.61 LOG:\n",
--      "09:00:02.61 .... 'dice sum,macro' = 'dice sum,macro'\n",
--      "09:00:02.61 .... output = tensor([0.5989, 0.7696, 0.2411], grad_fn=<MulBackward0>)\n",
--      "09:00:02.61 LOG:\n",
--      "09:00:02.62 .... 'dice sum,macro' = 'dice sum,macro'\n",
--      "09:00:02.62 .... output = tensor([0.2148, 0.4559, 0.0140], grad_fn=<MulBackward0>)\n"
-+      "09:01:12.28 LOG:\n",
-+      "09:01:12.30 .... 'cel none' = 'cel none'\n",
-+      "09:01:12.31 .... output = tensor([1.5514, 1.5514, 0.5514, 0.5514], grad_fn=<NllLossBackward>)\n",
-+      "09:01:12.31 LOG:\n",
-+      "09:01:12.31 .... 'cel mean' = 'cel mean'\n",
-+      "09:01:12.31 .... output = tensor(1.0514, grad_fn=<NllLossBackward>)\n",
-+      "09:01:12.31 LOG:\n",
-+      "09:01:12.31 .... 'cel sum' = 'cel sum'\n",
-+      "09:01:12.31 .... output = tensor(4.2058, grad_fn=<NllLossBackward>)\n",
-+      "09:01:12.31 LOG:\n",
-+      "09:01:12.32 .... 'focal sum' = 'focal sum'\n",
-+      "09:01:12.32 .... loss(inp, tar) = tensor(6.7352, grad_fn=<SumBackward0>)\n",
-+      "09:01:12.32 LOG:\n",
-+      "09:01:12.32 .... 'focal mean' = 'focal mean'\n",
-+      "09:01:12.32 .... loss(inp, tar) = tensor(0.4210, grad_fn=<MeanBackward0>)\n",
-+      "09:01:12.32 LOG:\n",
-+      "09:01:12.33 .... 'focal none' = 'focal none'\n",
-+      "09:01:12.33 .... loss(inp, tar) = tensor([0.9635, 0.9635, 0.0991, 0.0991], grad_fn=<MulBackward0>)\n",
-+      "09:01:12.33 LOG:\n",
-+      "09:01:12.33 .... 'focal none' = 'focal none'\n",
-+      "09:01:12.33 .... loss(inp, tar) = tensor([0.4716, 0.4716, 0.0075, 0.0075], grad_fn=<MulBackward0>)\n",
-+      "09:01:12.33 LOG:\n",
-+      "09:01:12.34 .... 'crf,none' = 'crf,none'\n",
-+      "09:01:12.34 .... output = tensor([4.2927], grad_fn=<NegBackward>)\n",
-+      "09:01:12.34 LOG:\n",
-+      "09:01:12.34 .... 'crf,mean' = 'crf,mean'\n",
-+      "09:01:12.34 .... output = tensor(4.3138, grad_fn=<NegBackward>)\n",
-+      "09:01:12.34 LOG:\n",
-+      "09:01:12.35 .... 'crf,sum' = 'crf,sum'\n",
-+      "09:01:12.35 .... output = tensor(4.2588, grad_fn=<NegBackward>)\n",
-+      "09:01:12.35 LOG:\n",
-+      "09:01:12.35 .... 'crf,token_mean' = 'crf,token_mean'\n",
-+      "09:01:12.35 .... output = tensor(1.0429, grad_fn=<DivBackward0>)\n",
-+      "09:01:12.35 LOG:\n",
-+      "09:01:12.35 .... 'dice none,micro' = 'dice none,micro'\n",
-+      "09:01:12.36 .... output = tensor(0.1876, grad_fn=<PowBackward0>)\n",
-+      "09:01:12.36 LOG:\n",
-+      "09:01:12.36 .... 'dice mean,micro' = 'dice mean,micro'\n",
-+      "09:01:12.36 .... output = tensor(0.0625, grad_fn=<DivBackward0>)\n",
-+      "09:01:12.36 LOG:\n",
-+      "09:01:12.36 .... 'dice sum,micro' = 'dice sum,micro'\n",
-+      "09:01:12.36 .... output = tensor(0.1876, grad_fn=<SumBackward0>)\n",
-+      "09:01:12.37 LOG:\n",
-+      "09:01:12.37 .... 'dice sum,micro' = 'dice sum,micro'\n",
-+      "09:01:12.37 .... output = tensor(0.0812, grad_fn=<SumBackward0>)\n",
-+      "09:01:12.37 LOG:\n",
-+      "09:01:12.37 .... 'dice none,macro' = 'dice none,macro'\n",
-+      "09:01:12.37 .... loss(inp, tar) = tensor([5.9547, 4.4872, 2.0551], grad_fn=<MulBackward0>)\n",
-+      "09:01:12.37 LOG:\n",
-+      "09:01:12.38 .... 'dice mean,macro' = 'dice mean,macro'\n",
-+      "09:01:12.38 .... loss(inp, tar) = tensor(0.2112, grad_fn=<DivBackward0>)\n",
-+      "09:01:12.38 LOG:\n",
-+      "09:01:12.38 .... 'dice sum,macro' = 'dice sum,macro'\n",
-+      "09:01:12.38 .... loss(inp, tar) = tensor(0.6335, grad_fn=<SumBackward0>)\n",
-+      "09:01:12.38 LOG:\n",
-+      "09:01:12.38 .... 'dice none,macro' = 'dice none,macro'\n",
-+      "09:01:12.39 .... loss(inp, tar) = tensor([0.3340, 0.6546, 0.3057], grad_fn=<MulBackward0>)\n",
-+      "09:01:12.39 LOG:\n",
-+      "09:01:12.39 .... 'dice none,macro' = 'dice none,macro'\n",
-+      "09:01:12.39 .... loss(inp, tar) = tensor([0.0373, 0.2805, 0.0286], grad_fn=<MulBackward0>)\n",
-+      "09:01:12.39 LOG:\n",
-+      "09:01:12.39 .... 'dice sum,macro' = 'dice sum,macro'\n",
-+      "09:01:12.40 .... output = tensor([0.5989, 0.7696, 0.2411], grad_fn=<MulBackward0>)\n",
-+      "09:01:12.40 LOG:\n",
-+      "09:01:12.40 .... 'dice sum,macro' = 'dice sum,macro'\n",
-+      "09:01:12.40 .... output = tensor([0.2148, 0.4559, 0.0140], grad_fn=<MulBackward0>)\n"
-      ]
-     },
-     {
-@@ -155,7 +155,7 @@
-        "('dice sum,macro', tensor([0.2148, 0.4559, 0.0140], grad_fn=<MulBackward0>))"
-       ]
-      },
--     "execution_count": 11,
-+     "execution_count": 13,
-      "metadata": {},
-      "output_type": "execute_result"
-     }
-@@ -249,25 +249,25 @@
-     "# output.backward()\n",
-     "pp('dice none,macro',loss(inp, tar))\n",
-     "\n",
--    "loss = FocalDiceLoss(reduction='none',macro_average=True,alpha=5.0)\n",
-+    "loss = FocalDiceLoss(reduction='mean',macro_average=True)\n",
-     "output = loss(inp, tar)\n",
-     "# output.backward()\n",
--    "pp('dice none,macro',loss(inp, tar))\n",
-+    "pp('dice mean,macro',loss(inp, tar))\n",
-     "\n",
--    "loss = FocalDiceLoss(reduction='none',macro_average=True)\n",
-+    "loss = FocalDiceLoss(reduction='sum',macro_average=True)\n",
-     "output = loss(inp, tar)\n",
-     "# output.backward()\n",
--    "pp('dice none,macro',loss(inp, tar))\n",
-+    "pp('dice sum,macro',loss(inp, tar))\n",
-     "\n",
--    "loss = FocalDiceLoss(reduction='mean',macro_average=True)\n",
-+    "loss = FocalDiceLoss(reduction='none',macro_average=True,alpha=1.0)\n",
-     "output = loss(inp, tar)\n",
-     "# output.backward()\n",
--    "pp('dice mean,macro',loss(inp, tar))\n",
-+    "pp('dice none,macro',loss(inp, tar))\n",
-     "\n",
--    "loss = FocalDiceLoss(reduction='sum',macro_average=True)\n",
-+    "loss = FocalDiceLoss(reduction='none',macro_average=True, alpha=3)\n",
-     "output = loss(inp, tar)\n",
-     "# output.backward()\n",
--    "pp('dice sum,macro',loss(inp, tar))\n",
-+    "pp('dice none,macro',loss(inp, tar))\n",
-     "\n",
-     "inp = torch.tensor([[[0,1,0],[1,0,1],[0,0,1],[0,1,0]]],dtype=torch.float, requires_grad=True)\n",
-     "tar = torch.tensor([[0,1,2,0]],dtype=torch.long)\n",
-@@ -284,78 +284,6 @@
-     "pp('dice sum,macro',output)"
-    ]
-   },
--  {
--   "cell_type": "code",
--   "execution_count": null,
--   "metadata": {},
--   "outputs": [],
--   "source": [
--    "# loss = LinearChainCRF(num_labels=5,reduction='none')\n",
--    "# output = loss(inp, tar,mask)\n",
--    "# # output.backward()\n",
--    "# ic('crf,none',output)\n",
--    "\n",
--    "# loss = LinearChainCRF(num_labels=5,reduction='mean')\n",
--    "# output = loss(inp, tar,mask)\n",
--    "# # output.backward()\n",
--    "# ic('crf,mean',output)\n",
--    "\n",
--    "# loss = LinearChainCRF(num_labels=5,reduction='sum')\n",
--    "# output = loss(inp, tar,mask)\n",
--    "# # output.backward()\n",
--    "# ic('crf,sum',output)\n",
--    "\n",
--    "# loss = LinearChainCRF(num_labels=5,reduction='token_mean')\n",
--    "# output = loss(inp, tar,mask)\n",
--    "# # output.backward()\n",
--    "# ic('crf,token_mean',output)\n",
--    "\n",
--    "# loss = FocalDiceLoss(reduction='none',macro_average=True)\n",
--    "# output = loss(inp, tar)\n",
--    "# # output.backward()\n",
--    "# pp('focal none,macro',loss(inp, tar))\n",
--    "\n",
--    "# loss = FocalDiceLoss(reduction='mean',macro_average=True)\n",
--    "# output = loss(inp, tar)\n",
--    "# # output.backward()\n",
--    "# pp('focal mean,macro',loss(inp, tar))\n",
--    "\n",
--    "# loss = FocalDiceLoss(reduction='sum',macro_average=True)\n",
--    "# output = loss(inp, tar)\n",
--    "# # output.backward()\n",
--    "# pp('focal sum,macro',loss(inp, tar))\n",
--    "\n",
--    "# loss = FocalDiceLoss(reduction='none', macro_average=False)\n",
--    "# output = loss(inp, tar)\n",
--    "# # output.backward()\n",
--    "# pp('focal none,micro',output)\n",
--    "\n",
--    "# loss = FocalDiceLoss(reduction='mean', macro_average=False)\n",
--    "# output = loss(inp, tar)\n",
--    "# # output.backward()\n",
--    "# pp('focal mean,micro',output)\n",
--    "\n",
--    "# loss = FocalDiceLoss(reduction='sum', macro_average=False)\n",
--    "# output = loss(inp, tar)\n",
--    "# # output.backward()\n",
--    "# pp('focal sum,micro',output)\n",
--    "\n",
--    "# loss = CrossEntropyLoss(reduction='none')\n",
--    "# output = loss(inp, tar)\n",
--    "# # output.backward()\n",
--    "# pp('cel none',output)\n",
--    "\n",
--    "# loss = CrossEntropyLoss(reduction='mean')\n",
--    "# output = loss(inp, tar)\n",
--    "# # output.backward()\n",
--    "# pp('cel mean',output)\n",
--    "\n",
--    "# loss = CrossEntropyLoss(reduction='sum')\n",
--    "# output = loss(inp, tar)\n",
--    "# # output.backward()\n",
--    "# pp('cel sum',output)"
--   ]
--  },
-   {
-    "cell_type": "code",
-    "execution_count": 2,
-@@ -365,15 +293,20 @@
-      "name": "stderr",
-      "output_type": "stream",
-      "text": [
--      "ic| os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M')): 0\n",
--      "ic| os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M')): 0\n",
--      "ic| max(len(d) for d in self.datasets): 12\n"
-+      "10:05:46.40 LOG:\n",
-+      "10:05:46.46 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
-+      "10:05:46.66 LOG:\n",
-+      "10:05:46.66 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
-+      "10:06:04.19 LOG:\n",
-+      "10:06:04.20 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
-+      "10:06:04.34 LOG:\n",
-+      "10:06:04.34 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n"
-      ]
-     },
-     {
-      "data": {
-       "text/plain": [
--       "12"
-+       "10609"
-       ]
-      },
-      "execution_count": 2,
-@@ -408,6 +341,35 @@
-     "len(dm.train_dataset)"
-    ]
-   },
-+  {
-+   "cell_type": "code",
-+   "execution_count": 4,
-+   "metadata": {},
-+   "outputs": [
-+    {
-+     "data": {
-+      "text/plain": [
-+       "10609"
-+      ]
-+     },
-+     "execution_count": 4,
-+     "metadata": {},
-+     "output_type": "execute_result"
-+    }
-+   ],
-+   "source": [
-+    "# it=dm.train_dataset\n",
-+    "# ni=next(it)\n",
-+    "# it=dm.train_dataset.datasets[0]\n",
-+    "dm.train_dataset.__len__()#determine_class_weights()\n",
-+    "# ct=torch.zeros(10)\n",
-+    "# for _ in range(64):\n",
-+    "#     print('.',end='')\n",
-+    "#     ni=next(it)\n",
-+    "#     ct+=torch.bincount(ni['labels'].view(-1))\n",
-+    "# return ct/sum(ct)"
-+   ]
-+  },
-   {
-    "cell_type": "code",
-    "execution_count": 19,
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index 9d99993..7417f08 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,42 +1,42 @@
- seed: 42
- trainer:
--    gpus: 1 # the number of gpus, 0 for CPU
-+    # gpus: 1 # the number of gpus, 0 for CPU
-+    # num_nodes: 1
-+    # max_epochs: 8
-+    # max_steps: null # precedence over max_epochs
-+    # accumulate_grad_batches: 4 # accumulates grads every k batches
-+    # gradient_clip_val: 0
-+    # amp_level: O1 # O1/O2 for mixed precision
-+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-+    # checkpoint_callback: false  # Provided by exp_manager
-+    # logger: false #false  # Provided by exp_manager
-+    # log_every_n_steps: 1  # Interval of logging.
-+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    # resume_from_checkpoint: null
-+
-+    gpus: 0 # the number of gpus, 0 for CPU
-     num_nodes: 1
-     max_epochs: 8
-     max_steps: null # precedence over max_epochs
-     accumulate_grad_batches: 4 # accumulates grads every k batches
-     gradient_clip_val: 0
--    amp_level: O1 # O1/O2 for mixed precision
--    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    accelerator: ddp
-+    amp_level: O0 # O1/O2 for mixed precision
-+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-     checkpoint_callback: false  # Provided by exp_manager
-     logger: false #false  # Provided by exp_manager
-     log_every_n_steps: 1  # Interval of logging.
-     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-     resume_from_checkpoint: null
- 
--    # gpus: 0 # the number of gpus, 0 for CPU
--    # num_nodes: 1
--    # max_epochs: 3
--    # max_steps: null # precedence over max_epochs
--    # accumulate_grad_batches: 8 # accumulates grads every k batches
--    # gradient_clip_val: 0.5
--    # amp_level: O0 # O1/O2 for mixed precision
--    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # # accelerator: ddp
--    # checkpoint_callback: false  # Provided by exp_manager
--    # logger: false #false  # Provided by exp_manager
--    # log_every_n_steps: 1  # Interval of logging.
--    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # resume_from_checkpoint: null
--
- exp_manager:
--    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /root/data # /home/nxingyu2/data # 
--tmp_path: /tmp # /home/nxingyu2/data/tmp # 
-+base_path: /home/nxingyu2/data # /root/data # 
-+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
-@@ -57,7 +57,7 @@ model:
-     punct_class_weights: true
-     
-     dataset:
--        data_dir: /root/data # /home/nxingyu2/data # 
-+        data_dir: /home/nxingyu2/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -108,7 +108,7 @@ model:
-         activation: 'relu'
-         log_softmax: false
-         use_transformer_init: true
--        loss: 'cel'
-+        loss: 'dice'
- 
-     domain_head:
-         domain_num_fc_layers: 1
-@@ -121,7 +121,7 @@ model:
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 3
-+        alpha: 4
-         macro_average: true
- 
-     focal_loss: 
-diff --git a/experiment/info.log b/experiment/info.log
-index 6f9e06b..5024c4f 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,83 +1,2 @@
--[INFO] - shuffling train set
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f13d3dd1400>" 
--will be used during training (effective maximum steps = 60) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 60
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.003162277660168378
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f13c79baa30>" 
--will be used during training (effective maximum steps = 800) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 800
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
-+[INFO] - GPU available: True, used: False
-+[INFO] - TPU available: None, using: 0 TPU cores
diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/hparams.yaml
deleted file mode 100644
index 650b6cc..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/hparams.yaml
+++ /dev/null
@@ -1,104 +0,0 @@
-seed: 42
-trainer:
-  gpus: 0
-  num_nodes: 1
-  max_epochs: 8
-  max_steps: null
-  accumulate_grad_batches: 4
-  gradient_clip_val: 0
-  amp_level: O0
-  precision: 32
-  checkpoint_callback: false
-  logger: false
-  log_every_n_steps: 1
-  val_check_interval: 1.0
-  resume_from_checkpoint: null
-exp_manager:
-  exp_dir: /home/nxingyu2/project/
-  name: Punctuation_with_Domain_discriminator
-  create_tensorboard_logger: true
-  create_checkpoint_callback: true
-base_path: /home/nxingyu2/data
-tmp_path: /home/nxingyu2/data/tmp
-model:
-  nemo_path: null
-  transformer_path: google/electra-small-discriminator
-  initial_unfrozen: 1
-  punct_label_ids:
-  - ''
-  - '!'
-  - ','
-  - '-'
-  - .
-  - ':'
-  - ;
-  - '?'
-  - —
-  - …
-  punct_class_weights: true
-  dataset:
-    data_dir: /home/nxingyu2/data
-    labelled:
-    - /home/nxingyu2/data/ted_talks_processed
-    unlabelled: null
-    max_seq_length: 128
-    pad_label: ''
-    ignore_extra_tokens: false
-    ignore_start_end: false
-    use_cache: false
-    num_workers: 0
-    pin_memory: true
-    drop_last: false
-    num_labels: 10
-    num_domains: 1
-    test_unlabelled: true
-    train_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 8
-    validation_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 8
-  tokenizer:
-    tokenizer_name: google/electra-small-discriminator
-    vocab_file: null
-    tokenizer_model: null
-    special_tokens: null
-  language_model:
-    pretrained_model_name: google/electra-small-discriminator
-    lm_checkpoint: null
-    config_file: null
-    config: null
-  punct_head:
-    punct_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: dice
-  domain_head:
-    domain_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: cel
-    gamma: 0.1
-  dice_loss:
-    epsilon: 0.01
-    alpha: 4
-    macro_average: true
-  focal_loss:
-    gamma: 5
-  optim:
-    name: adamw
-    lr: 0.001
-    weight_decay: 0.0
-    sched:
-      name: WarmupAnnealing
-      warmup_steps: null
-      warmup_ratio: 0.1
-      last_epoch: -1
-      monitor: val_loss
-      reduce_on_plateau: false
diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/lightning_logs.txt
deleted file mode 100644
index 756258b..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/lightning_logs.txt
+++ /dev/null
@@ -1,42 +0,0 @@
-GPU available: True, used: False
-TPU available: None, using: 0 TPU cores
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-825 K     Trainable params
-12.7 M    Non-trainable params
-13.5 M    Total params
-Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/lr_find_temp_model.ckpt
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-825 K     Trainable params
-12.7 M    Non-trainable params
-13.5 M    Total params
-Epoch 1, global step 100: val_loss reached 0.71243 (best 0.71243), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.71-epoch=1.ckpt" as top 3
-Epoch 2, global step 200: val_loss reached 0.60420 (best 0.60420), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.60-epoch=2.ckpt" as top 3
-Epoch 3, global step 300: val_loss reached 0.54977 (best 0.54977), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.55-epoch=3.ckpt" as top 3
-Epoch 4, global step 400: val_loss reached 0.58447 (best 0.54977), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.58-epoch=4.ckpt" as top 3
-Epoch 5, global step 500: val_loss reached 0.58260 (best 0.54977), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.58-epoch=5.ckpt" as top 3
-Epoch 6, global step 600: val_loss reached 0.53657 (best 0.53657), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.54-epoch=6.ckpt" as top 3
-Epoch 7, global step 700: val_loss reached 0.56844 (best 0.53657), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.57-epoch=7.ckpt" as top 3
-GPU available: True, used: False
-TPU available: None, using: 0 TPU cores
diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_error_log.txt
deleted file mode 100644
index e6499a0..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_error_log.txt
+++ /dev/null
@@ -1,28 +0,0 @@
-[NeMo W 2021-02-05 10:48:12 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 10:48:12 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-05 10:48:24 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 10:48:24 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 10:48:29 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 11:08:02 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f1b0f34dac0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-05 11:08:35 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f1b0f34d5b0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-05 12:25:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 12:25:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 12:26:20 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f1b0f34d130> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index e5bb9e8..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,30 +0,0 @@
-[NeMo W 2021-02-05 10:48:12 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo I 2021-02-05 10:48:12 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12
-[NeMo I 2021-02-05 10:48:12 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-05 10:48:12 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-05 10:48:24 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 10:48:24 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 10:48:29 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 11:08:02 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f1b0f34dac0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-05 11:08:35 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f1b0f34d5b0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-05 12:25:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 12:25:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 12:26:20 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f1b0f34d130> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/events.out.tfevents.1612499897.intern-instance.31879.0 b/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/events.out.tfevents.1612499897.intern-instance.31879.0
deleted file mode 100644
index 658293c..0000000
Binary files a/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/events.out.tfevents.1612499897.intern-instance.31879.0 and /dev/null differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/git-info.log
deleted file mode 100644
index 1104b94..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/git-info.log
+++ /dev/null
@@ -1,321 +0,0 @@
-commit hash: ed7303d1e763178a0ec2d13f917d7de6c8553488
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/events.out.tfevents.1612493730.intern-instance.20106.0 b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/events.out.tfevents.1612493730.intern-instance.20106.0
-index af71914..25acd93 100644
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/events.out.tfevents.1612493730.intern-instance.20106.0 and b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/events.out.tfevents.1612493730.intern-instance.20106.0 differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/lightning_logs.txt
-index 95f4866..756258b 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/lightning_logs.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/lightning_logs.txt
-@@ -35,3 +35,8 @@ Epoch 1, global step 100: val_loss reached 0.71243 (best 0.71243), saving model
- Epoch 2, global step 200: val_loss reached 0.60420 (best 0.60420), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.60-epoch=2.ckpt" as top 3
- Epoch 3, global step 300: val_loss reached 0.54977 (best 0.54977), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.55-epoch=3.ckpt" as top 3
- Epoch 4, global step 400: val_loss reached 0.58447 (best 0.54977), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.58-epoch=4.ckpt" as top 3
-+Epoch 5, global step 500: val_loss reached 0.58260 (best 0.54977), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.58-epoch=5.ckpt" as top 3
-+Epoch 6, global step 600: val_loss reached 0.53657 (best 0.53657), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.54-epoch=6.ckpt" as top 3
-+Epoch 7, global step 700: val_loss reached 0.56844 (best 0.53657), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.57-epoch=7.ckpt" as top 3
-+GPU available: True, used: False
-+TPU available: None, using: 0 TPU cores
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_error_log.txt
-index 7732b8a..e6499a0 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_error_log.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_error_log.txt
-@@ -17,3 +17,12 @@
- [NeMo W 2021-02-05 11:08:35 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f1b0f34d5b0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
-       warnings.warn(warn_msg)
-     
-+[NeMo W 2021-02-05 12:25:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-+      warnings.warn(*args, **kwargs)
-+    
-+[NeMo W 2021-02-05 12:25:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-+      warnings.warn(*args, **kwargs)
-+    
-+[NeMo W 2021-02-05 12:26:20 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f1b0f34d130> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
-+      warnings.warn(warn_msg)
-+    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_log_globalrank-0_localrank-0.txt
-index cb6d529..e5bb9e8 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_log_globalrank-0_localrank-0.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_log_globalrank-0_localrank-0.txt
-@@ -19,3 +19,12 @@
- [NeMo W 2021-02-05 11:08:35 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f1b0f34d5b0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
-       warnings.warn(warn_msg)
-     
-+[NeMo W 2021-02-05 12:25:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-+      warnings.warn(*args, **kwargs)
-+    
-+[NeMo W 2021-02-05 12:25:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-+      warnings.warn(*args, **kwargs)
-+    
-+[NeMo W 2021-02-05 12:26:20 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f1b0f34d130> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
-+      warnings.warn(warn_msg)
-+    
-diff --git a/README.md b/README.md
-index ca7075a..ae03334 100644
---- a/README.md
-+++ b/README.md
-@@ -182,4 +182,26 @@ label                                                precision    recall       f
- Electra small cel weighted ted l 1 unfrozen 0.0031622776 lr adamw accgrad4 bs8
- 
- 
--
-+### elsmall dice alpha 4 weighted ted-l unfrozen 0.003162277660 lr adamw accgrad4 bbs8
-+
-+label                 |   precision  |  recall |    f1    |      support
-+---|---|---|---|---
-+ (label_id: 0)        |      79.50   |   29.94 |   43.50  |    5026
-+! (label_id: 1)       |       6.84   |   20.59 |   10.27  |     102
-+, (label_id: 2)       |      50.70   |   60.09 |   55.00  |   17571
-+- (label_id: 3)       |      64.45   |   82.11 |   72.22  |    1526
-+. (label_id: 4)       |      57.40   |   49.43 |   53.12  |   14767
-+: (label_id: 5)       |      17.86   |   31.83 |   22.89  |     289
-+; (label_id: 6)       |       1.50   |    5.88 |    2.39  |      85
-+? (label_id: 7)       |      37.02   |   61.32 |   46.17  |    1228
-+— (label_id: 8)       |       6.44   |    7.34 |    6.86  |     763
-+… (label_id: 9)       |       0.00   |    0.00 |    0.00  |      80
-+-------------------||||
-+micro avg             |      51.99   |   51.99 |   51.99  |   41437
-+macro avg             |      32.17   |   34.85 |   31.24  |   41437
-+weighted avg          |      55.33   |   51.99 |   51.87  |   41437
-+
-+{'punct_f1': tensor(31.2411),
-+ 'punct_precision': tensor(32.1728),
-+ 'punct_recall': tensor(34.8539),
-+ 'test_loss': tensor(0.6303)}
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index 7417f08..ace920a 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -121,7 +121,7 @@ model:
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 4
-+        alpha: 1
-         macro_average: true
- 
-     focal_loss: 
-diff --git a/experiment/info.log b/experiment/info.log
-index ccfd4ae..5024c4f 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,220 +1,2 @@
- [INFO] - GPU available: True, used: False
- [INFO] - TPU available: None, using: 0 TPU cores
--[INFO] - shuffling train set
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f1b0f3334c0>" 
--will be used during training (effective maximum steps = 60) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 60
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 257   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--825 K     Trainable params
--12.7 M    Non-trainable params
--13.5 M    Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          17.26      96.74      29.30        184
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                         58.62       2.86       5.46        594
--- (label_id: 3)                                          7.14       8.47       7.75         59
--. (label_id: 4)                                         48.80      15.46      23.48        524
--: (label_id: 5)                                          1.12       5.56       1.87         18
--; (label_id: 6)                                          0.00       0.00       0.00         13
--? (label_id: 7)                                         14.89       7.37       9.86         95
--— (label_id: 8)                                          0.00       0.00       0.00         12
--… (label_id: 9)                                          0.00       0.00       0.00          0
---------------------
--micro avg                                               19.23      19.23      19.23       1503
--macro avg                                               16.43      15.16       8.64       1503
--weighted avg                                            43.53      19.23      14.88       1503
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00         92
---------------------
--micro avg                                              100.00     100.00     100.00         92
--macro avg                                              100.00     100.00     100.00         92
--weighted avg                                           100.00     100.00     100.00         92
--
--[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/lr_find_temp_model.ckpt
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.003162277660168378
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f1b00306f10>" 
--will be used during training (effective maximum steps = 800) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 800
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 257   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--825 K     Trainable params
--12.7 M    Non-trainable params
--13.5 M    Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          17.82      95.16      30.03        124
--! (label_id: 1)                                          0.00       0.00       0.00          3
--, (label_id: 2)                                         70.59       3.01       5.77        399
--- (label_id: 3)                                          2.56       3.23       2.86         31
--. (label_id: 4)                                         48.76      17.40      25.65        339
--: (label_id: 5)                                          1.79       9.09       2.99         11
--; (label_id: 6)                                          0.00       0.00       0.00          1
--? (label_id: 7)                                         15.62       7.58      10.20         66
--— (label_id: 8)                                          0.00       0.00       0.00         11
--… (label_id: 9)                                          0.00       0.00       0.00          0
---------------------
--micro avg                                               19.90      19.90      19.90        985
--macro avg                                               17.46      15.05       8.61        985
--weighted avg                                            48.77      19.90      15.75        985
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00         62
---------------------
--micro avg                                              100.00     100.00     100.00         62
--macro avg                                              100.00     100.00     100.00         62
--weighted avg                                           100.00     100.00     100.00         62
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                         100.00       0.05       0.09       4284
--! (label_id: 1)                                          0.00       0.00       0.00        151
--, (label_id: 2)                                         44.08      77.25      56.13      14795
--- (label_id: 3)                                         65.97      71.00      68.39       1286
--. (label_id: 4)                                         55.76       5.49       9.99      12169
--: (label_id: 5)                                         16.64      34.86      22.53        350
--; (label_id: 6)                                          3.61       8.48       5.06        165
--? (label_id: 7)                                         18.27      69.05      28.90       1021
--— (label_id: 8)                                          2.51       7.36       3.74        421
--… (label_id: 9)                                          0.00       0.00       0.00         85
---------------------
--micro avg                                               39.98      39.98      39.98      34727
--macro avg                                               30.68      27.35      19.48      34727
--weighted avg                                            53.85      39.98      31.11      34727
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2142
---------------------
--micro avg                                              100.00     100.00     100.00       2142
--macro avg                                              100.00     100.00     100.00       2142
--weighted avg                                           100.00     100.00     100.00       2142
--
--[INFO] - Epoch 1, global step 100: val_loss reached 0.71243 (best 0.71243), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.71-epoch=1.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       4044
--! (label_id: 1)                                          0.00       0.00       0.00        145
--, (label_id: 2)                                         43.32      43.32      43.32      13833
--- (label_id: 3)                                         59.53      75.81      66.69       1174
--. (label_id: 4)                                         51.13      53.22      52.15      11588
--: (label_id: 5)                                         32.11      24.69      27.92        320
--; (label_id: 6)                                          3.27      10.76       5.01        158
--? (label_id: 7)                                         22.43      67.01      33.61       1067
--— (label_id: 8)                                          3.45      11.46       5.30        445
--… (label_id: 9)                                          0.00       0.00       0.00         74
---------------------
--micro avg                                               42.35      42.35      42.35      32848
--macro avg                                               21.52      28.63      23.40      32848
--weighted avg                                            39.51      42.35      40.48      32848
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2022
---------------------
--micro avg                                              100.00     100.00     100.00       2022
--macro avg                                              100.00     100.00     100.00       2022
--weighted avg                                           100.00     100.00     100.00       2022
--
--[INFO] - Epoch 2, global step 200: val_loss reached 0.60420 (best 0.60420), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.60-epoch=2.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          84.44       0.89       1.75       4292
--! (label_id: 1)                                          7.41       9.02       8.14        133
--, (label_id: 2)                                         43.74      44.01      43.88      14694
--- (label_id: 3)                                         52.78      80.86      63.87       1327
--. (label_id: 4)                                         52.94      50.45      51.66      12213
--: (label_id: 5)                                         18.73      39.23      25.36        339
--; (label_id: 6)                                          3.05      21.52       5.35        158
--? (label_id: 7)                                         27.21      55.73      36.57       1055
--— (label_id: 8)                                          3.02      13.20       4.92        485
--… (label_id: 9)                                          0.00       0.00       0.00         68
---------------------
--micro avg                                               41.91      41.91      41.91      34764
--macro avg                                               29.33      31.49      24.15      34764
--weighted avg                                            50.62      41.91      40.83      34764
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2146
---------------------
--micro avg                                              100.00     100.00     100.00       2146
--macro avg                                              100.00     100.00     100.00       2146
--weighted avg                                           100.00     100.00     100.00       2146
--
--[INFO] - Epoch 3, global step 300: val_loss reached 0.54977 (best 0.54977), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.55-epoch=3.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          53.66       2.71       5.15       4066
--! (label_id: 1)                                         10.99       6.90       8.47        145
--, (label_id: 2)                                         43.36      35.48      39.02      14051
--- (label_id: 3)                                         63.28      71.71      67.23       1276
--. (label_id: 4)                                         50.84      73.36      60.06      11507
--: (label_id: 5)                                         23.79      39.88      29.80        321
--; (label_id: 6)                                          3.17       2.67       2.90        150
--? (label_id: 7)                                         44.77      47.00      45.86        983
--— (label_id: 8)                                          2.80      10.36       4.41        386
--… (label_id: 9)                                          0.00       0.00       0.00         83
---------------------
--micro avg                                               45.79      45.79      45.79      32968
--macro avg                                               29.67      29.01      26.29      32968
--weighted avg                                            46.95      45.79      42.59      32968
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2033
---------------------
--micro avg                                              100.00     100.00     100.00       2033
--macro avg                                              100.00     100.00     100.00       2033
--weighted avg                                           100.00     100.00     100.00       2033
--
--[INFO] - Epoch 4, global step 400: val_loss reached 0.58447 (best 0.54977), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.58-epoch=4.ckpt" as top 3
diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/hparams.yaml
deleted file mode 100644
index a7e1e19..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/hparams.yaml
+++ /dev/null
@@ -1,104 +0,0 @@
-seed: 42
-trainer:
-  gpus: 0
-  num_nodes: 1
-  max_epochs: 8
-  max_steps: null
-  accumulate_grad_batches: 4
-  gradient_clip_val: 0
-  amp_level: O0
-  precision: 32
-  checkpoint_callback: false
-  logger: false
-  log_every_n_steps: 1
-  val_check_interval: 1.0
-  resume_from_checkpoint: null
-exp_manager:
-  exp_dir: /home/nxingyu2/project/
-  name: Punctuation_with_Domain_discriminator
-  create_tensorboard_logger: true
-  create_checkpoint_callback: true
-base_path: /home/nxingyu2/data
-tmp_path: /home/nxingyu2/data/tmp
-model:
-  nemo_path: null
-  transformer_path: google/electra-small-discriminator
-  initial_unfrozen: 1
-  punct_label_ids:
-  - ''
-  - '!'
-  - ','
-  - '-'
-  - .
-  - ':'
-  - ;
-  - '?'
-  - —
-  - …
-  punct_class_weights: true
-  dataset:
-    data_dir: /home/nxingyu2/data
-    labelled:
-    - /home/nxingyu2/data/ted_talks_processed
-    unlabelled: null
-    max_seq_length: 128
-    pad_label: ''
-    ignore_extra_tokens: false
-    ignore_start_end: false
-    use_cache: false
-    num_workers: 0
-    pin_memory: true
-    drop_last: false
-    num_labels: 10
-    num_domains: 1
-    test_unlabelled: true
-    train_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 8
-    validation_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 8
-  tokenizer:
-    tokenizer_name: google/electra-small-discriminator
-    vocab_file: null
-    tokenizer_model: null
-    special_tokens: null
-  language_model:
-    pretrained_model_name: google/electra-small-discriminator
-    lm_checkpoint: null
-    config_file: null
-    config: null
-  punct_head:
-    punct_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: dice
-  domain_head:
-    domain_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: cel
-    gamma: 0.1
-  dice_loss:
-    epsilon: 0.01
-    alpha: 1
-    macro_average: true
-  focal_loss:
-    gamma: 5
-  optim:
-    name: adamw
-    lr: 0.001
-    weight_decay: 0.0
-    sched:
-      name: WarmupAnnealing
-      warmup_steps: null
-      warmup_ratio: 0.1
-      last_epoch: -1
-      monitor: val_loss
-      reduce_on_plateau: false
diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/lightning_logs.txt
deleted file mode 100644
index 6c6e9b9..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/lightning_logs.txt
+++ /dev/null
@@ -1,42 +0,0 @@
-GPU available: True, used: False
-TPU available: None, using: 0 TPU cores
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-825 K     Trainable params
-12.7 M    Non-trainable params
-13.5 M    Total params
-Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/lr_find_temp_model.ckpt
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-825 K     Trainable params
-12.7 M    Non-trainable params
-13.5 M    Total params
-Epoch 1, global step 100: val_loss reached 0.37278 (best 0.37278), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.37-epoch=1.ckpt" as top 3
-Epoch 2, global step 200: val_loss reached 0.26551 (best 0.26551), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=2.ckpt" as top 3
-Epoch 3, global step 300: val_loss reached 0.37096 (best 0.26551), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.37-epoch=3.ckpt" as top 3
-Epoch 4, global step 400: val_loss reached 0.25577 (best 0.25577), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=4.ckpt" as top 3
-Epoch 5, global step 500: val_loss reached 0.35675 (best 0.25577), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.36-epoch=5.ckpt" as top 3
-Epoch 6, global step 600: val_loss reached 0.30581 (best 0.25577), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.31-epoch=6.ckpt" as top 3
-Epoch 7, global step 700: val_loss reached 0.24315 (best 0.24315), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=7.ckpt" as top 3
-GPU available: True, used: False
-TPU available: None, using: 0 TPU cores
diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/nemo_error_log.txt
deleted file mode 100644
index e628500..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/nemo_error_log.txt
+++ /dev/null
@@ -1,28 +0,0 @@
-[NeMo W 2021-02-05 12:30:41 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 12:30:41 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-05 12:30:53 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 12:30:53 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 12:30:58 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 12:50:21 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f65f74ef100> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-05 12:50:55 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f65f7515220> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-05 14:06:12 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 14:06:12 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 14:06:48 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f65f7515040> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index f8349d0..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,30 +0,0 @@
-[NeMo W 2021-02-05 12:30:41 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo I 2021-02-05 12:30:41 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41
-[NeMo I 2021-02-05 12:30:41 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-05 12:30:41 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-05 12:30:53 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 12:30:53 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 12:30:58 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 12:50:21 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f65f74ef100> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-05 12:50:55 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f65f7515220> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-05 14:06:12 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 14:06:12 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 14:06:48 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f65f7515040> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/events.out.tfevents.1612505959.intern-instance.12714.0 b/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/events.out.tfevents.1612505959.intern-instance.12714.0
deleted file mode 100644
index 0303750..0000000
Binary files a/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/events.out.tfevents.1612505959.intern-instance.12714.0 and /dev/null differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/git-info.log
deleted file mode 100644
index ae1343a..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/git-info.log
+++ /dev/null
@@ -1,412 +0,0 @@
-commit hash: ed7303d1e763178a0ec2d13f917d7de6c8553488
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/events.out.tfevents.1612493730.intern-instance.20106.0 b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/events.out.tfevents.1612493730.intern-instance.20106.0
-index af71914..25acd93 100644
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/events.out.tfevents.1612493730.intern-instance.20106.0 and b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/events.out.tfevents.1612493730.intern-instance.20106.0 differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/lightning_logs.txt
-index 95f4866..756258b 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/lightning_logs.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/lightning_logs.txt
-@@ -35,3 +35,8 @@ Epoch 1, global step 100: val_loss reached 0.71243 (best 0.71243), saving model
- Epoch 2, global step 200: val_loss reached 0.60420 (best 0.60420), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.60-epoch=2.ckpt" as top 3
- Epoch 3, global step 300: val_loss reached 0.54977 (best 0.54977), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.55-epoch=3.ckpt" as top 3
- Epoch 4, global step 400: val_loss reached 0.58447 (best 0.54977), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.58-epoch=4.ckpt" as top 3
-+Epoch 5, global step 500: val_loss reached 0.58260 (best 0.54977), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.58-epoch=5.ckpt" as top 3
-+Epoch 6, global step 600: val_loss reached 0.53657 (best 0.53657), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.54-epoch=6.ckpt" as top 3
-+Epoch 7, global step 700: val_loss reached 0.56844 (best 0.53657), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.57-epoch=7.ckpt" as top 3
-+GPU available: True, used: False
-+TPU available: None, using: 0 TPU cores
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_error_log.txt
-index 7732b8a..e6499a0 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_error_log.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_error_log.txt
-@@ -17,3 +17,12 @@
- [NeMo W 2021-02-05 11:08:35 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f1b0f34d5b0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
-       warnings.warn(warn_msg)
-     
-+[NeMo W 2021-02-05 12:25:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-+      warnings.warn(*args, **kwargs)
-+    
-+[NeMo W 2021-02-05 12:25:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-+      warnings.warn(*args, **kwargs)
-+    
-+[NeMo W 2021-02-05 12:26:20 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f1b0f34d130> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
-+      warnings.warn(warn_msg)
-+    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_log_globalrank-0_localrank-0.txt
-index cb6d529..e5bb9e8 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_log_globalrank-0_localrank-0.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_log_globalrank-0_localrank-0.txt
-@@ -19,3 +19,12 @@
- [NeMo W 2021-02-05 11:08:35 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f1b0f34d5b0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
-       warnings.warn(warn_msg)
-     
-+[NeMo W 2021-02-05 12:25:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-+      warnings.warn(*args, **kwargs)
-+    
-+[NeMo W 2021-02-05 12:25:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-+      warnings.warn(*args, **kwargs)
-+    
-+[NeMo W 2021-02-05 12:26:20 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f1b0f34d130> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
-+      warnings.warn(warn_msg)
-+    
-diff --git a/README.md b/README.md
-index ca7075a..1f40c55 100644
---- a/README.md
-+++ b/README.md
-@@ -182,4 +182,100 @@ label                                                precision    recall       f
- Electra small cel weighted ted l 1 unfrozen 0.0031622776 lr adamw accgrad4 bs8
- 
- 
-+### elsmall dice alpha 4 weighted ted-l unfrozen 0.003162277660 lr adamw accgrad4 bbs8
-+
-+label                 |   precision  |  recall |    f1    |      support
-+---|---|---|---|---
-+ (label_id: 0)        |      79.50   |   29.94 |   43.50  |    5026
-+! (label_id: 1)       |       6.84   |   20.59 |   10.27  |     102
-+, (label_id: 2)       |      50.70   |   60.09 |   55.00  |   17571
-+- (label_id: 3)       |      64.45   |   82.11 |   72.22  |    1526
-+. (label_id: 4)       |      57.40   |   49.43 |   53.12  |   14767
-+: (label_id: 5)       |      17.86   |   31.83 |   22.89  |     289
-+; (label_id: 6)       |       1.50   |    5.88 |    2.39  |      85
-+? (label_id: 7)       |      37.02   |   61.32 |   46.17  |    1228
-+— (label_id: 8)       |       6.44   |    7.34 |    6.86  |     763
-+… (label_id: 9)       |       0.00   |    0.00 |    0.00  |      80
-+-------------------||||
-+micro avg             |      51.99   |   51.99 |   51.99  |   41437
-+macro avg             |      32.17   |   34.85 |   31.24  |   41437
-+weighted avg          |      55.33   |   51.99 |   51.87  |   41437
-+
-+{'punct_f1': tensor(31.2411),
-+ 'punct_precision': tensor(32.1728),
-+ 'punct_recall': tensor(34.8539),
-+ 'test_loss': tensor(0.6303)}
-+
-+
-+### elsmall dice alpha 1 weighted ted-l unfrozen 0.007943282347 lr adamw accgrad4 bbs7
-+
-+label                 |   precision  |  recall |    f1    |      support
-+---|---|---|---|---
-+ (label_id: 0)             |     0.00  |  0.00 |   0.00  |  5026
-+! (label_id: 1)            |     0.00  |  0.00 |   0.00  |   102
-+, (label_id: 2)            |    42.79  | 47.54 |  45.04  | 17571
-+- (label_id: 3)            |    73.63  | 80.87 |  77.08  |  1526
-+. (label_id: 4)            |    47.36  | 55.16 |  50.96  | 14767
-+: (label_id: 5)            |    10.88  | 27.68 |  15.62  |   289
-+; (label_id: 6)            |     0.00  |  0.00 |   0.00  |    85
-+? (label_id: 7)            |    43.18  | 60.10 |  50.26  |  1228
-+— (label_id: 8)            |     3.03  |  2.36 |   2.65  |   763
-+… (label_id: 9)            |     0.00  |  0.00 |   0.00  |    80
-+-------------------||||
-+micro avg                  |    44.81  | 44.81 |  44.81  | 41437
-+macro avg                  |    22.09  | 27.37 |  24.16  | 41437
-+weighted avg               |    39.14  | 44.81 |  41.75  | 41437
-+
-+{'punct_f1': tensor(24.1611),
-+ 'punct_precision': tensor(22.0869),
-+ 'punct_recall': tensor(27.3705),
-+ 'test_loss': tensor(0.4047)}
-+
-+
-+### elsmall crf ted-l unfrozen 0.005011872336272719 lr adamw accgrad4 bbs8
-+
-+label                  |  precision | recall |   f1   |     support
-+---|---|---|---|---
-+ (label_id: 0)         |     59.35  |  52.35 |  55.63 |   7314
-+! (label_id: 1)        |      0.00  |   0.00 |   0.00 |    154
-+, (label_id: 2)        |     44.15  |  82.80 |  57.59 |  28180
-+- (label_id: 3)        |      3.91  |   2.02 |   2.66 |   1933
-+. (label_id: 4)        |     39.91  |  11.64 |  18.02 |  24624
-+: (label_id: 5)        |      0.00  |   0.00 |   0.00 |    522
-+; (label_id: 6)        |      0.00  |   0.00 |   0.00 |    485
-+? (label_id: 7)        |      0.00  |   0.00 |   0.00 |   2096
-+— (label_id: 8)        |      0.00  |   0.00 |   0.00 |   2055
-+… (label_id: 9)        |      0.00  |   0.00 |   0.00 |    123
-+-------------------||||
-+micro avg              |     44.55  |  44.55 |  44.55 |  67486
-+macro avg              |     14.73  |  14.88 |  13.39 |  67486
-+weighted avg           |     39.54  |  44.55 |  36.73 |  67486
-+
-+{'punct_f1': 13.390362739562988,
-+ 'punct_precision': 14.73101806640625,
-+ 'punct_recall': 14.881169319152832,
-+ 'test_loss': 11.328206062316895}
-+
-+### elsmall dice alpha 3 no weight ted-l unfrozen 0.005011872336272719 lr adamw accgrad4 bbs8
-+label                |  precision | recall |   f1   |    support
-+---|---|---|---|---
-+ (label_id: 0)       |  62.32   | 99.78 |  76.72  |   7314
-+! (label_id: 1)      |   0.00   |  0.00 |   0.00  |    154
-+, (label_id: 2)      |  49.81   |  4.72 |   8.62  |  28180
-+- (label_id: 3)      |   5.91   | 28.35 |   9.78  |   1933
-+. (label_id: 4)      |  41.80   | 52.40 |  46.50  |  24624
-+: (label_id: 5)      |   0.94   |  4.02 |   1.53  |    522
-+; (label_id: 6)      |   0.00   |  0.00 |   0.00  |    485
-+? (label_id: 7)      |   4.92   | 24.86 |   8.22  |   2096
-+— (label_id: 8)      |   0.00   |  0.00 |   0.00  |   2055
-+… (label_id: 9)      |   0.00   |  0.00 |   0.00  |    123
-+-------------------||||
-+micro avg            |  33.52   | 33.52 |  33.52  |  67486
-+macro avg            |  16.57   | 21.41 |  15.14  |  67486
-+weighted avg         |  43.14   | 33.52 |  29.43  |  67486
-+
-+'punct_f1': 15.136445999145508,
-+ 'punct_precision': 16.57059097290039,
-+ 'punct_recall': 21.41229820251465,
-+ 'test_loss': 0.6608337163925171}
- 
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index 7417f08..4e38aa0 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -41,7 +41,7 @@ tmp_path: /home/nxingyu2/data/tmp # /tmp #
- model:
-     nemo_path: null
-     transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
--    initial_unfrozen: 1
-+    initial_unfrozen: 0
-     punct_label_ids:
-         - ""
-         - "!"
-@@ -108,7 +108,7 @@ model:
-         activation: 'relu'
-         log_softmax: false
-         use_transformer_init: true
--        loss: 'dice'
-+        loss: 'crf'
- 
-     domain_head:
-         domain_num_fc_layers: 1
-@@ -121,7 +121,7 @@ model:
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 4
-+        alpha: 1
-         macro_average: true
- 
-     focal_loss: 
-diff --git a/experiment/info.log b/experiment/info.log
-index ccfd4ae..5024c4f 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,220 +1,2 @@
- [INFO] - GPU available: True, used: False
- [INFO] - TPU available: None, using: 0 TPU cores
--[INFO] - shuffling train set
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f1b0f3334c0>" 
--will be used during training (effective maximum steps = 60) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 60
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 257   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--825 K     Trainable params
--12.7 M    Non-trainable params
--13.5 M    Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          17.26      96.74      29.30        184
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                         58.62       2.86       5.46        594
--- (label_id: 3)                                          7.14       8.47       7.75         59
--. (label_id: 4)                                         48.80      15.46      23.48        524
--: (label_id: 5)                                          1.12       5.56       1.87         18
--; (label_id: 6)                                          0.00       0.00       0.00         13
--? (label_id: 7)                                         14.89       7.37       9.86         95
--— (label_id: 8)                                          0.00       0.00       0.00         12
--… (label_id: 9)                                          0.00       0.00       0.00          0
---------------------
--micro avg                                               19.23      19.23      19.23       1503
--macro avg                                               16.43      15.16       8.64       1503
--weighted avg                                            43.53      19.23      14.88       1503
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00         92
---------------------
--micro avg                                              100.00     100.00     100.00         92
--macro avg                                              100.00     100.00     100.00         92
--weighted avg                                           100.00     100.00     100.00         92
--
--[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/lr_find_temp_model.ckpt
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.003162277660168378
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f1b00306f10>" 
--will be used during training (effective maximum steps = 800) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 800
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 257   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--825 K     Trainable params
--12.7 M    Non-trainable params
--13.5 M    Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          17.82      95.16      30.03        124
--! (label_id: 1)                                          0.00       0.00       0.00          3
--, (label_id: 2)                                         70.59       3.01       5.77        399
--- (label_id: 3)                                          2.56       3.23       2.86         31
--. (label_id: 4)                                         48.76      17.40      25.65        339
--: (label_id: 5)                                          1.79       9.09       2.99         11
--; (label_id: 6)                                          0.00       0.00       0.00          1
--? (label_id: 7)                                         15.62       7.58      10.20         66
--— (label_id: 8)                                          0.00       0.00       0.00         11
--… (label_id: 9)                                          0.00       0.00       0.00          0
---------------------
--micro avg                                               19.90      19.90      19.90        985
--macro avg                                               17.46      15.05       8.61        985
--weighted avg                                            48.77      19.90      15.75        985
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00         62
---------------------
--micro avg                                              100.00     100.00     100.00         62
--macro avg                                              100.00     100.00     100.00         62
--weighted avg                                           100.00     100.00     100.00         62
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                         100.00       0.05       0.09       4284
--! (label_id: 1)                                          0.00       0.00       0.00        151
--, (label_id: 2)                                         44.08      77.25      56.13      14795
--- (label_id: 3)                                         65.97      71.00      68.39       1286
--. (label_id: 4)                                         55.76       5.49       9.99      12169
--: (label_id: 5)                                         16.64      34.86      22.53        350
--; (label_id: 6)                                          3.61       8.48       5.06        165
--? (label_id: 7)                                         18.27      69.05      28.90       1021
--— (label_id: 8)                                          2.51       7.36       3.74        421
--… (label_id: 9)                                          0.00       0.00       0.00         85
---------------------
--micro avg                                               39.98      39.98      39.98      34727
--macro avg                                               30.68      27.35      19.48      34727
--weighted avg                                            53.85      39.98      31.11      34727
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2142
---------------------
--micro avg                                              100.00     100.00     100.00       2142
--macro avg                                              100.00     100.00     100.00       2142
--weighted avg                                           100.00     100.00     100.00       2142
--
--[INFO] - Epoch 1, global step 100: val_loss reached 0.71243 (best 0.71243), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.71-epoch=1.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       4044
--! (label_id: 1)                                          0.00       0.00       0.00        145
--, (label_id: 2)                                         43.32      43.32      43.32      13833
--- (label_id: 3)                                         59.53      75.81      66.69       1174
--. (label_id: 4)                                         51.13      53.22      52.15      11588
--: (label_id: 5)                                         32.11      24.69      27.92        320
--; (label_id: 6)                                          3.27      10.76       5.01        158
--? (label_id: 7)                                         22.43      67.01      33.61       1067
--— (label_id: 8)                                          3.45      11.46       5.30        445
--… (label_id: 9)                                          0.00       0.00       0.00         74
---------------------
--micro avg                                               42.35      42.35      42.35      32848
--macro avg                                               21.52      28.63      23.40      32848
--weighted avg                                            39.51      42.35      40.48      32848
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2022
---------------------
--micro avg                                              100.00     100.00     100.00       2022
--macro avg                                              100.00     100.00     100.00       2022
--weighted avg                                           100.00     100.00     100.00       2022
--
--[INFO] - Epoch 2, global step 200: val_loss reached 0.60420 (best 0.60420), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.60-epoch=2.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          84.44       0.89       1.75       4292
--! (label_id: 1)                                          7.41       9.02       8.14        133
--, (label_id: 2)                                         43.74      44.01      43.88      14694
--- (label_id: 3)                                         52.78      80.86      63.87       1327
--. (label_id: 4)                                         52.94      50.45      51.66      12213
--: (label_id: 5)                                         18.73      39.23      25.36        339
--; (label_id: 6)                                          3.05      21.52       5.35        158
--? (label_id: 7)                                         27.21      55.73      36.57       1055
--— (label_id: 8)                                          3.02      13.20       4.92        485
--… (label_id: 9)                                          0.00       0.00       0.00         68
---------------------
--micro avg                                               41.91      41.91      41.91      34764
--macro avg                                               29.33      31.49      24.15      34764
--weighted avg                                            50.62      41.91      40.83      34764
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2146
---------------------
--micro avg                                              100.00     100.00     100.00       2146
--macro avg                                              100.00     100.00     100.00       2146
--weighted avg                                           100.00     100.00     100.00       2146
--
--[INFO] - Epoch 3, global step 300: val_loss reached 0.54977 (best 0.54977), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.55-epoch=3.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          53.66       2.71       5.15       4066
--! (label_id: 1)                                         10.99       6.90       8.47        145
--, (label_id: 2)                                         43.36      35.48      39.02      14051
--- (label_id: 3)                                         63.28      71.71      67.23       1276
--. (label_id: 4)                                         50.84      73.36      60.06      11507
--: (label_id: 5)                                         23.79      39.88      29.80        321
--; (label_id: 6)                                          3.17       2.67       2.90        150
--? (label_id: 7)                                         44.77      47.00      45.86        983
--— (label_id: 8)                                          2.80      10.36       4.41        386
--… (label_id: 9)                                          0.00       0.00       0.00         83
---------------------
--micro avg                                               45.79      45.79      45.79      32968
--macro avg                                               29.67      29.01      26.29      32968
--weighted avg                                            46.95      45.79      42.59      32968
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2033
---------------------
--micro avg                                              100.00     100.00     100.00       2033
--macro avg                                              100.00     100.00     100.00       2033
--weighted avg                                           100.00     100.00     100.00       2033
--
--[INFO] - Epoch 4, global step 400: val_loss reached 0.58447 (best 0.54977), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.58-epoch=4.ckpt" as top 3
diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/hparams.yaml
deleted file mode 100644
index 28a5d44..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/hparams.yaml
+++ /dev/null
@@ -1,104 +0,0 @@
-seed: 42
-trainer:
-  gpus: 0
-  num_nodes: 1
-  max_epochs: 8
-  max_steps: null
-  accumulate_grad_batches: 4
-  gradient_clip_val: 0
-  amp_level: O0
-  precision: 32
-  checkpoint_callback: false
-  logger: false
-  log_every_n_steps: 1
-  val_check_interval: 1.0
-  resume_from_checkpoint: null
-exp_manager:
-  exp_dir: /home/nxingyu2/project/
-  name: Punctuation_with_Domain_discriminator
-  create_tensorboard_logger: true
-  create_checkpoint_callback: true
-base_path: /home/nxingyu2/data
-tmp_path: /home/nxingyu2/data/tmp
-model:
-  nemo_path: null
-  transformer_path: google/electra-small-discriminator
-  initial_unfrozen: 0
-  punct_label_ids:
-  - ''
-  - '!'
-  - ','
-  - '-'
-  - .
-  - ':'
-  - ;
-  - '?'
-  - —
-  - …
-  punct_class_weights: true
-  dataset:
-    data_dir: /home/nxingyu2/data
-    labelled:
-    - /home/nxingyu2/data/ted_talks_processed
-    unlabelled: null
-    max_seq_length: 128
-    pad_label: ''
-    ignore_extra_tokens: false
-    ignore_start_end: false
-    use_cache: false
-    num_workers: 0
-    pin_memory: true
-    drop_last: false
-    num_labels: 10
-    num_domains: 1
-    test_unlabelled: true
-    train_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 8
-    validation_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 8
-  tokenizer:
-    tokenizer_name: google/electra-small-discriminator
-    vocab_file: null
-    tokenizer_model: null
-    special_tokens: null
-  language_model:
-    pretrained_model_name: google/electra-small-discriminator
-    lm_checkpoint: null
-    config_file: null
-    config: null
-  punct_head:
-    punct_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: crf
-  domain_head:
-    domain_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: cel
-    gamma: 0.1
-  dice_loss:
-    epsilon: 0.01
-    alpha: 1
-    macro_average: true
-  focal_loss:
-    gamma: 5
-  optim:
-    name: adamw
-    lr: 0.001
-    weight_decay: 0.0
-    sched:
-      name: WarmupAnnealing
-      warmup_steps: null
-      warmup_ratio: 0.1
-      last_epoch: -1
-      monitor: val_loss
-      reduce_on_plateau: false
diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/lightning_logs.txt
deleted file mode 100644
index 9bbaa3a..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/lightning_logs.txt
+++ /dev/null
@@ -1,42 +0,0 @@
-GPU available: True, used: False
-TPU available: None, using: 0 TPU cores
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | LinearChainCRF       | 120   
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-36.0 K    Trainable params
-13.4 M    Non-trainable params
-13.5 M    Total params
-Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/lr_find_temp_model.ckpt
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | LinearChainCRF       | 120   
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-36.0 K    Trainable params
-13.4 M    Non-trainable params
-13.5 M    Total params
-Epoch 1, global step 100: val_loss reached 4.50218 (best 4.50218), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/checkpoints/Punctuation_with_Domain_discriminator---val_loss=4.50-epoch=1.ckpt" as top 3
-Epoch 2, global step 200: val_loss reached -7.95181 (best -7.95181), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-7.95-epoch=2.ckpt" as top 3
-Epoch 3, global step 300: val_loss reached -17.86337 (best -17.86337), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-17.86-epoch=3.ckpt" as top 3
-Epoch 4, global step 400: val_loss reached -25.60198 (best -25.60198), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-25.60-epoch=4.ckpt" as top 3
-Epoch 5, global step 500: val_loss reached -31.27675 (best -31.27675), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-31.28-epoch=5.ckpt" as top 3
-Epoch 6, global step 600: val_loss reached -35.57878 (best -35.57878), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-35.58-epoch=6.ckpt" as top 3
-Epoch 7, global step 700: val_loss reached -38.22509 (best -38.22509), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-38.23-epoch=7.ckpt" as top 3
-GPU available: True, used: False
-TPU available: None, using: 0 TPU cores
diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/nemo_error_log.txt
deleted file mode 100644
index 6cd04a9..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/nemo_error_log.txt
+++ /dev/null
@@ -1,28 +0,0 @@
-[NeMo W 2021-02-05 14:11:35 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 14:11:35 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-05 14:11:37 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 14:11:37 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 14:11:42 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 14:31:49 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f44c1de1ac0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-05 14:32:24 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f44c1de15b0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-05 15:49:26 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 15:49:26 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 15:50:05 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f44c1de1130> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index cad8ec2..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,30 +0,0 @@
-[NeMo W 2021-02-05 14:11:35 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo I 2021-02-05 14:11:35 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35
-[NeMo I 2021-02-05 14:11:35 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-05 14:11:35 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-05 14:11:37 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 14:11:37 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 14:11:42 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 14:31:49 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f44c1de1ac0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-05 14:32:24 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f44c1de15b0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-05 15:49:26 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 15:49:26 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-05 15:50:05 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f44c1de1130> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/git-info.log
deleted file mode 100644
index d760651..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/git-info.log
+++ /dev/null
@@ -1,690 +0,0 @@
-commit hash: cce540d4c95ad9ed32010552a7aa1fb2486658c8
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index 3d27dd7..f13ae33 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,43 +1,43 @@
- seed: 42
- trainer:
--    gpus: 1 # the number of gpus, 0 for CPU
--    num_nodes: 1
--    max_epochs: 2
--    max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 4 # accumulates grads every k batches
--    gradient_clip_val: 0
--    amp_level: O1 # O1/O2 for mixed precision
--    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    accelerator: ddp
--    checkpoint_callback: false  # Provided by exp_manager
--    logger: false #false  # Provided by exp_manager
--    log_every_n_steps: 1  # Interval of logging.
--    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    resume_from_checkpoint: null
--
--    # gpus: 0 # the number of gpus, 0 for CPU
-+    # gpus: 1 # the number of gpus, 0 for CPU
-     # num_nodes: 1
--    # max_epochs: 10
-+    # max_epochs: 2
-     # max_steps: null # precedence over max_epochs
-     # accumulate_grad_batches: 4 # accumulates grads every k batches
-     # gradient_clip_val: 0
--    # amp_level: O0 # O1/O2 for mixed precision
--    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # # accelerator: ddp
-+    # amp_level: O1 # O1/O2 for mixed precision
-+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-     # checkpoint_callback: false  # Provided by exp_manager
-     # logger: false #false  # Provided by exp_manager
-     # log_every_n_steps: 1  # Interval of logging.
-     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # reload_dataloaders_every_epoch: true
-     # resume_from_checkpoint: null
- 
-+    gpus: 0 # the number of gpus, 0 for CPU
-+    num_nodes: 1
-+    max_epochs: 8
-+    max_steps: null # precedence over max_epochs
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-+    gradient_clip_val: 0
-+    amp_level: O0 # O1/O2 for mixed precision
-+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-+    checkpoint_callback: false  # Provided by exp_manager
-+    logger: false #false  # Provided by exp_manager
-+    log_every_n_steps: 1  # Interval of logging.
-+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    reload_dataloaders_every_epoch: true
-+    resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /root/data # /home/nxingyu2/data # 
--tmp_path: /tmp # /home/nxingyu2/data/tmp # 
-+base_path: /home/nxingyu2/data # /root/data # 
-+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
-@@ -61,7 +61,7 @@ model:
-     punct_class_weights: true
-     
-     dataset:
--        data_dir: /root/data # /home/nxingyu2/data # 
-+        data_dir: /home/nxingyu2/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -125,14 +125,14 @@ model:
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 4
-+        alpha: 3
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: adamw
-+        name: ranger
-         lr: 1e-3
-         weight_decay: 0.00
- 
-@@ -142,10 +142,12 @@ model:
-             warmup_steps: null
-             warmup_ratio: 0.1
-             last_epoch: -1
-+            delay_epochs: 4
-+
- 
-             # pytorch lightning args
-             monitor: val_loss
--            reduce_on_plateau: true
-+            reduce_on_plateau: false
- hydra:
-     run:
-         dir: .
-diff --git a/experiment/core/optim/lr_scheduler.py b/experiment/core/optim/lr_scheduler.py
-index a17ebf8..a24b059 100644
---- a/experiment/core/optim/lr_scheduler.py
-+++ b/experiment/core/optim/lr_scheduler.py
-@@ -347,6 +347,39 @@ class PolynomialHoldDecayAnnealing(WarmupHoldPolicy):
-         return new_lrs
- 
- 
-+class DelayerScheduler(_LRScheduler):
-+	""" Starts with a flat lr schedule until it reaches N epochs the applies a scheduler 
-+	Args:
-+		optimizer (Optimizer): Wrapped optimizer.
-+		delay_epochs: number of epochs to keep the initial lr until starting aplying the scheduler
-+		after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)
-+	"""
-+
-+	def __init__(self, optimizer, delay_epochs, after_scheduler):
-+		self.delay_epochs = delay_epochs
-+		self.after_scheduler = after_scheduler
-+		self.finished = False
-+		super().__init__(optimizer)
-+
-+	def get_lr(self):
-+		if self.last_epoch >= self.delay_epochs:
-+			if not self.finished:
-+				self.after_scheduler.base_lrs = self.base_lrs
-+				self.finished = True
-+			return self.after_scheduler.get_lr()
-+
-+		return self.base_lrs
-+
-+	def step(self, epoch=None):
-+		if self.finished:
-+			if epoch is None:
-+				self.after_scheduler.step(None)
-+			else:
-+				self.after_scheduler.step(epoch - self.delay_epochs)
-+		else:
-+			return super(DelayerScheduler, self).step(epoch)
-+
-+
- def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: SchedulerParams):
-     """
-     Checks if the scheduler name exists in the registry, and if it doesnt, adds it.
-@@ -365,7 +398,7 @@ def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: Sch
-     register_scheduler_params(name=sched_name, scheduler_params=scheduler_params)
- 
- 
--def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
-+def get_scheduler(name: str, optimizer, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
-     """
-     Convenience method to obtain an _LRScheduler class and partially instantiate it with optimizer kwargs.
-     Args:
-@@ -380,7 +413,9 @@ def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler
-         )
- 
-     scheduler_cls = AVAILABLE_SCHEDULERS[name]
--    scheduler = partial(scheduler_cls, **kwargs)
-+    scheduler = scheduler_cls(optimizer=optimizer, **kwargs)
-+    if delay_epochs>0:
-+        scheduler = DelayerScheduler(optimizer=optimizer,delay_epochs=delay_epochs,after_scheduler=scheduler)
-     return scheduler
- 
- 
-@@ -584,10 +619,10 @@ def prepare_lr_scheduler(
-         scheduler_args['max_steps'] = max_steps
- 
-     # Get the scheduler class from the config
--    scheduler_cls = get_scheduler(scheduler_name, **scheduler_args)
-+    scheduler = get_scheduler(scheduler_name, optimizer **scheduler_args)
- 
-     # Instantiate the LR schedule
--    schedule = scheduler_cls(optimizer, **scheduler_args)
-+    # schedule = scheduler_cls(optimizer, **scheduler_args)
- 
-     logging.info(
-         'Scheduler "%s" \nwill be used during training (effective maximum steps = %d) - \nParameters : \n(%s)',
-diff --git a/experiment/core/optim/optimizers.py b/experiment/core/optim/optimizers.py
-index 5afc4fd..f15686e 100644
---- a/experiment/core/optim/optimizers.py
-+++ b/experiment/core/optim/optimizers.py
-@@ -7,6 +7,7 @@ import torch.optim as optim
- from omegaconf import DictConfig, OmegaConf
- from torch.optim import adadelta, adagrad, adamax, rmsprop, rprop
- from torch.optim.optimizer import Optimizer
-+from torchtools.optim import Ranger
- 
- from core.config import OptimizerParams, get_optimizer_config, register_optimizer_params
- from core.optim.novograd import Novograd
-@@ -24,6 +25,7 @@ AVAILABLE_OPTIMIZERS = {
-     'rmsprop': rmsprop.RMSprop,
-     'rprop': rprop.Rprop,
-     'novograd': Novograd,
-+    'ranger': Ranger,
- }
- 
- 
-diff --git a/experiment/info.log b/experiment/info.log
-index b211d42..5024c4f 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,473 +1,2 @@
--[INFO] - shuffling train set
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4da7f8e0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.008413951416451957
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e67aa00>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          22.42      11.12      14.86       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                         38.62       7.46      12.50      19530
--- (label_id: 3)                                          6.60      15.75       9.30       1746
--. (label_id: 4)                                         33.33       0.01       0.01      17976
--: (label_id: 5)                                          0.79      19.15       1.52        376
--; (label_id: 6)                                          0.45       2.94       0.78        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.17      50.56       0.33         89
---------------------
--micro avg                                                5.14       5.14       5.14      48233
--macro avg                                               10.24      10.70       3.93      48233
--weighted avg                                            30.92       5.14       7.15      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          22.09      11.27      14.93       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                         35.39       6.74      11.32      19813
--- (label_id: 3)                                          6.03      16.28       8.79       1708
--. (label_id: 4)                                        100.00       0.01       0.02      18084
--: (label_id: 5)                                          0.78      20.69       1.51        348
--; (label_id: 6)                                          0.45       2.55       0.77        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.19      58.62       0.38         87
---------------------
--micro avg                                                4.89       4.89       4.89      48633
--macro avg                                               16.49      11.62       3.77      48633
--weighted avg                                            54.38       4.89       6.67      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.008413951416451957
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e74d910>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          19.61      14.42      16.62        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         38.84       6.82      11.60        689
--- (label_id: 3)                                          4.50      15.52       6.98         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.59      20.00       1.14         10
--; (label_id: 6)                                          2.56      16.67       4.44          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.25      50.00       0.51          4
---------------------
--micro avg                                                5.55       5.55       5.55       1641
--macro avg                                                6.64      12.34       4.13       1641
--weighted avg                                            18.97       5.55       7.25       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.63      98.91       7.00       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          3.96       1.76       2.44       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.63       3.63       3.63      48233
--macro avg                                                0.76      10.07       0.94      48233
--weighted avg                                             0.25       3.63       0.33      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e68b8b0>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          19.61      14.42      16.62        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         38.84       6.82      11.60        689
--- (label_id: 3)                                          4.50      15.52       6.98         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.59      20.00       1.14         10
--; (label_id: 6)                                          2.56      16.67       4.44          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.25      50.00       0.51          4
---------------------
--micro avg                                                5.55       5.55       5.55       1641
--macro avg                                                6.64      12.34       4.13       1641
--weighted avg                                            18.97       5.55       7.25       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                          0.00       0.00       0.00      19813
--- (label_id: 3)                                          3.51     100.00       6.79       1708
--. (label_id: 4)                                          0.00       0.00       0.00      18084
--: (label_id: 5)                                          0.00       0.00       0.00        348
--; (label_id: 6)                                          0.00       0.00       0.00        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.00       0.00       0.00         87
---------------------
--micro avg                                                3.51       3.51       3.51      48633
--macro avg                                                0.35      10.00       0.68      48633
--weighted avg                                             0.12       3.51       0.24      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75eaf0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                          0.00       0.00       0.00        689
--- (label_id: 3)                                          3.53     100.00       6.83         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.00       0.00       0.00         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                3.53       3.53       3.53       1641
--macro avg                                                0.35      10.00       0.68       1641
--weighted avg                                             0.12       3.53       0.24       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75e160>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                          0.00       0.00       0.00        689
--- (label_id: 3)                                          3.53     100.00       6.83         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.00       0.00       0.00         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                3.53       3.53       3.53       1641
--macro avg                                                0.35      10.00       0.68       1641
--weighted avg                                             0.12       3.53       0.24       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                          0.00       0.00       0.00      19813
--- (label_id: 3)                                          3.51     100.00       6.79       1708
--. (label_id: 4)                                          0.00       0.00       0.00      18084
--: (label_id: 5)                                          0.00       0.00       0.00        348
--; (label_id: 6)                                          0.00       0.00       0.00        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.00       0.00       0.00         87
---------------------
--micro avg                                                3.51       3.51       3.51      48633
--macro avg                                                0.35      10.00       0.68      48633
--weighted avg                                             0.12       3.51       0.24      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       7470
--! (label_id: 1)                                          0.00       0.00       0.00        148
--, (label_id: 2)                                          0.00       0.00       0.00      28513
--- (label_id: 3)                                          3.02     100.00       5.86       2074
--. (label_id: 4)                                          0.00       0.00       0.00      25120
--: (label_id: 5)                                          0.00       0.00       0.00        570
--; (label_id: 6)                                          0.00       0.00       0.00        534
--? (label_id: 7)                                          0.00       0.00       0.00       2085
--— (label_id: 8)                                          0.00       0.00       0.00       2073
--… (label_id: 9)                                          0.00       0.00       0.00        142
---------------------
--micro avg                                                3.02       3.02       3.02      68729
--macro avg                                                0.30      10.00       0.59      68729
--weighted avg                                             0.09       3.02       0.18      68729
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       3735
---------------------
--micro avg                                              100.00     100.00     100.00       3735
--macro avg                                              100.00     100.00     100.00       3735
--weighted avg                                           100.00     100.00     100.00       3735
--
--[INFO] - Internal process exited
-+[INFO] - GPU available: True, used: False
-+[INFO] - TPU available: None, using: 0 TPU cores
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/lightning_logs.txt
deleted file mode 100644
index 01e1664..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/lightning_logs.txt
+++ /dev/null
@@ -1,2 +0,0 @@
-GPU available: True, used: False
-TPU available: None, using: 0 TPU cores
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/nemo_error_log.txt
deleted file mode 100644
index ec4f502..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/nemo_error_log.txt
+++ /dev/null
@@ -1,4 +0,0 @@
-[NeMo W 2021-02-06 09:03:49 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 09:03:49 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 8b41f44..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,6 +0,0 @@
-[NeMo W 2021-02-06 09:03:49 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo I 2021-02-06 09:03:49 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49
-[NeMo I 2021-02-06 09:03:49 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-06 09:03:49 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/git-info.log
deleted file mode 100644
index 43bd866..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/git-info.log
+++ /dev/null
@@ -1,690 +0,0 @@
-commit hash: cce540d4c95ad9ed32010552a7aa1fb2486658c8
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index 3d27dd7..f13ae33 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,43 +1,43 @@
- seed: 42
- trainer:
--    gpus: 1 # the number of gpus, 0 for CPU
--    num_nodes: 1
--    max_epochs: 2
--    max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 4 # accumulates grads every k batches
--    gradient_clip_val: 0
--    amp_level: O1 # O1/O2 for mixed precision
--    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    accelerator: ddp
--    checkpoint_callback: false  # Provided by exp_manager
--    logger: false #false  # Provided by exp_manager
--    log_every_n_steps: 1  # Interval of logging.
--    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    resume_from_checkpoint: null
--
--    # gpus: 0 # the number of gpus, 0 for CPU
-+    # gpus: 1 # the number of gpus, 0 for CPU
-     # num_nodes: 1
--    # max_epochs: 10
-+    # max_epochs: 2
-     # max_steps: null # precedence over max_epochs
-     # accumulate_grad_batches: 4 # accumulates grads every k batches
-     # gradient_clip_val: 0
--    # amp_level: O0 # O1/O2 for mixed precision
--    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # # accelerator: ddp
-+    # amp_level: O1 # O1/O2 for mixed precision
-+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-     # checkpoint_callback: false  # Provided by exp_manager
-     # logger: false #false  # Provided by exp_manager
-     # log_every_n_steps: 1  # Interval of logging.
-     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # reload_dataloaders_every_epoch: true
-     # resume_from_checkpoint: null
- 
-+    gpus: 0 # the number of gpus, 0 for CPU
-+    num_nodes: 1
-+    max_epochs: 8
-+    max_steps: null # precedence over max_epochs
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-+    gradient_clip_val: 0
-+    amp_level: O0 # O1/O2 for mixed precision
-+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-+    checkpoint_callback: false  # Provided by exp_manager
-+    logger: false #false  # Provided by exp_manager
-+    log_every_n_steps: 1  # Interval of logging.
-+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    reload_dataloaders_every_epoch: true
-+    resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /root/data # /home/nxingyu2/data # 
--tmp_path: /tmp # /home/nxingyu2/data/tmp # 
-+base_path: /home/nxingyu2/data # /root/data # 
-+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
-@@ -61,7 +61,7 @@ model:
-     punct_class_weights: true
-     
-     dataset:
--        data_dir: /root/data # /home/nxingyu2/data # 
-+        data_dir: /home/nxingyu2/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -125,14 +125,14 @@ model:
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 4
-+        alpha: 3
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: adamw
-+        name: ranger
-         lr: 1e-3
-         weight_decay: 0.00
- 
-@@ -142,10 +142,12 @@ model:
-             warmup_steps: null
-             warmup_ratio: 0.1
-             last_epoch: -1
-+            delay_epochs: 4
-+
- 
-             # pytorch lightning args
-             monitor: val_loss
--            reduce_on_plateau: true
-+            reduce_on_plateau: false
- hydra:
-     run:
-         dir: .
-diff --git a/experiment/core/optim/lr_scheduler.py b/experiment/core/optim/lr_scheduler.py
-index a17ebf8..ed029f3 100644
---- a/experiment/core/optim/lr_scheduler.py
-+++ b/experiment/core/optim/lr_scheduler.py
-@@ -347,6 +347,39 @@ class PolynomialHoldDecayAnnealing(WarmupHoldPolicy):
-         return new_lrs
- 
- 
-+class DelayerScheduler(_LRScheduler):
-+	""" Starts with a flat lr schedule until it reaches N epochs the applies a scheduler 
-+	Args:
-+		optimizer (Optimizer): Wrapped optimizer.
-+		delay_epochs: number of epochs to keep the initial lr until starting aplying the scheduler
-+		after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)
-+	"""
-+
-+	def __init__(self, optimizer, delay_epochs, after_scheduler):
-+		self.delay_epochs = delay_epochs
-+		self.after_scheduler = after_scheduler
-+		self.finished = False
-+		super().__init__(optimizer)
-+
-+	def get_lr(self):
-+		if self.last_epoch >= self.delay_epochs:
-+			if not self.finished:
-+				self.after_scheduler.base_lrs = self.base_lrs
-+				self.finished = True
-+			return self.after_scheduler.get_lr()
-+
-+		return self.base_lrs
-+
-+	def step(self, epoch=None):
-+		if self.finished:
-+			if epoch is None:
-+				self.after_scheduler.step(None)
-+			else:
-+				self.after_scheduler.step(epoch - self.delay_epochs)
-+		else:
-+			return super(DelayerScheduler, self).step(epoch)
-+
-+
- def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: SchedulerParams):
-     """
-     Checks if the scheduler name exists in the registry, and if it doesnt, adds it.
-@@ -365,7 +398,7 @@ def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: Sch
-     register_scheduler_params(name=sched_name, scheduler_params=scheduler_params)
- 
- 
--def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
-+def get_scheduler(name: str, optimizer, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
-     """
-     Convenience method to obtain an _LRScheduler class and partially instantiate it with optimizer kwargs.
-     Args:
-@@ -380,7 +413,9 @@ def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler
-         )
- 
-     scheduler_cls = AVAILABLE_SCHEDULERS[name]
--    scheduler = partial(scheduler_cls, **kwargs)
-+    scheduler = scheduler_cls(optimizer=optimizer, **kwargs)
-+    if delay_epochs>0:
-+        scheduler = DelayerScheduler(optimizer=optimizer,delay_epochs=delay_epochs,after_scheduler=scheduler)
-     return scheduler
- 
- 
-@@ -584,10 +619,10 @@ def prepare_lr_scheduler(
-         scheduler_args['max_steps'] = max_steps
- 
-     # Get the scheduler class from the config
--    scheduler_cls = get_scheduler(scheduler_name, **scheduler_args)
-+    scheduler = get_scheduler(scheduler_name, optimizer, **scheduler_args)
- 
-     # Instantiate the LR schedule
--    schedule = scheduler_cls(optimizer, **scheduler_args)
-+    # schedule = scheduler_cls(optimizer, **scheduler_args)
- 
-     logging.info(
-         'Scheduler "%s" \nwill be used during training (effective maximum steps = %d) - \nParameters : \n(%s)',
-diff --git a/experiment/core/optim/optimizers.py b/experiment/core/optim/optimizers.py
-index 5afc4fd..f15686e 100644
---- a/experiment/core/optim/optimizers.py
-+++ b/experiment/core/optim/optimizers.py
-@@ -7,6 +7,7 @@ import torch.optim as optim
- from omegaconf import DictConfig, OmegaConf
- from torch.optim import adadelta, adagrad, adamax, rmsprop, rprop
- from torch.optim.optimizer import Optimizer
-+from torchtools.optim import Ranger
- 
- from core.config import OptimizerParams, get_optimizer_config, register_optimizer_params
- from core.optim.novograd import Novograd
-@@ -24,6 +25,7 @@ AVAILABLE_OPTIMIZERS = {
-     'rmsprop': rmsprop.RMSprop,
-     'rprop': rprop.Rprop,
-     'novograd': Novograd,
-+    'ranger': Ranger,
- }
- 
- 
-diff --git a/experiment/info.log b/experiment/info.log
-index b211d42..5024c4f 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,473 +1,2 @@
--[INFO] - shuffling train set
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4da7f8e0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.008413951416451957
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e67aa00>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          22.42      11.12      14.86       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                         38.62       7.46      12.50      19530
--- (label_id: 3)                                          6.60      15.75       9.30       1746
--. (label_id: 4)                                         33.33       0.01       0.01      17976
--: (label_id: 5)                                          0.79      19.15       1.52        376
--; (label_id: 6)                                          0.45       2.94       0.78        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.17      50.56       0.33         89
---------------------
--micro avg                                                5.14       5.14       5.14      48233
--macro avg                                               10.24      10.70       3.93      48233
--weighted avg                                            30.92       5.14       7.15      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          22.09      11.27      14.93       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                         35.39       6.74      11.32      19813
--- (label_id: 3)                                          6.03      16.28       8.79       1708
--. (label_id: 4)                                        100.00       0.01       0.02      18084
--: (label_id: 5)                                          0.78      20.69       1.51        348
--; (label_id: 6)                                          0.45       2.55       0.77        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.19      58.62       0.38         87
---------------------
--micro avg                                                4.89       4.89       4.89      48633
--macro avg                                               16.49      11.62       3.77      48633
--weighted avg                                            54.38       4.89       6.67      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.008413951416451957
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e74d910>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          19.61      14.42      16.62        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         38.84       6.82      11.60        689
--- (label_id: 3)                                          4.50      15.52       6.98         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.59      20.00       1.14         10
--; (label_id: 6)                                          2.56      16.67       4.44          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.25      50.00       0.51          4
---------------------
--micro avg                                                5.55       5.55       5.55       1641
--macro avg                                                6.64      12.34       4.13       1641
--weighted avg                                            18.97       5.55       7.25       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.63      98.91       7.00       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          3.96       1.76       2.44       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.63       3.63       3.63      48233
--macro avg                                                0.76      10.07       0.94      48233
--weighted avg                                             0.25       3.63       0.33      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e68b8b0>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          19.61      14.42      16.62        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         38.84       6.82      11.60        689
--- (label_id: 3)                                          4.50      15.52       6.98         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.59      20.00       1.14         10
--; (label_id: 6)                                          2.56      16.67       4.44          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.25      50.00       0.51          4
---------------------
--micro avg                                                5.55       5.55       5.55       1641
--macro avg                                                6.64      12.34       4.13       1641
--weighted avg                                            18.97       5.55       7.25       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                          0.00       0.00       0.00      19813
--- (label_id: 3)                                          3.51     100.00       6.79       1708
--. (label_id: 4)                                          0.00       0.00       0.00      18084
--: (label_id: 5)                                          0.00       0.00       0.00        348
--; (label_id: 6)                                          0.00       0.00       0.00        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.00       0.00       0.00         87
---------------------
--micro avg                                                3.51       3.51       3.51      48633
--macro avg                                                0.35      10.00       0.68      48633
--weighted avg                                             0.12       3.51       0.24      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75eaf0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                          0.00       0.00       0.00        689
--- (label_id: 3)                                          3.53     100.00       6.83         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.00       0.00       0.00         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                3.53       3.53       3.53       1641
--macro avg                                                0.35      10.00       0.68       1641
--weighted avg                                             0.12       3.53       0.24       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75e160>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                          0.00       0.00       0.00        689
--- (label_id: 3)                                          3.53     100.00       6.83         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.00       0.00       0.00         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                3.53       3.53       3.53       1641
--macro avg                                                0.35      10.00       0.68       1641
--weighted avg                                             0.12       3.53       0.24       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                          0.00       0.00       0.00      19813
--- (label_id: 3)                                          3.51     100.00       6.79       1708
--. (label_id: 4)                                          0.00       0.00       0.00      18084
--: (label_id: 5)                                          0.00       0.00       0.00        348
--; (label_id: 6)                                          0.00       0.00       0.00        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.00       0.00       0.00         87
---------------------
--micro avg                                                3.51       3.51       3.51      48633
--macro avg                                                0.35      10.00       0.68      48633
--weighted avg                                             0.12       3.51       0.24      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       7470
--! (label_id: 1)                                          0.00       0.00       0.00        148
--, (label_id: 2)                                          0.00       0.00       0.00      28513
--- (label_id: 3)                                          3.02     100.00       5.86       2074
--. (label_id: 4)                                          0.00       0.00       0.00      25120
--: (label_id: 5)                                          0.00       0.00       0.00        570
--; (label_id: 6)                                          0.00       0.00       0.00        534
--? (label_id: 7)                                          0.00       0.00       0.00       2085
--— (label_id: 8)                                          0.00       0.00       0.00       2073
--… (label_id: 9)                                          0.00       0.00       0.00        142
---------------------
--micro avg                                                3.02       3.02       3.02      68729
--macro avg                                                0.30      10.00       0.59      68729
--weighted avg                                             0.09       3.02       0.18      68729
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       3735
---------------------
--micro avg                                              100.00     100.00     100.00       3735
--macro avg                                              100.00     100.00     100.00       3735
--weighted avg                                           100.00     100.00     100.00       3735
--
--[INFO] - Internal process exited
-+[INFO] - GPU available: True, used: False
-+[INFO] - TPU available: None, using: 0 TPU cores
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/lightning_logs.txt
deleted file mode 100644
index 01e1664..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/lightning_logs.txt
+++ /dev/null
@@ -1,2 +0,0 @@
-GPU available: True, used: False
-TPU available: None, using: 0 TPU cores
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/nemo_error_log.txt
deleted file mode 100644
index 9c11000..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/nemo_error_log.txt
+++ /dev/null
@@ -1,4 +0,0 @@
-[NeMo W 2021-02-06 09:08:50 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 09:08:50 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 75b41a8..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,6 +0,0 @@
-[NeMo W 2021-02-06 09:08:50 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo I 2021-02-06 09:08:50 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50
-[NeMo I 2021-02-06 09:08:50 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-06 09:08:50 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/git-info.log
deleted file mode 100644
index 8c7a2cc..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/git-info.log
+++ /dev/null
@@ -1,720 +0,0 @@
-commit hash: cce540d4c95ad9ed32010552a7aa1fb2486658c8
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index 3d27dd7..6d84830 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,43 +1,43 @@
- seed: 42
- trainer:
--    gpus: 1 # the number of gpus, 0 for CPU
--    num_nodes: 1
--    max_epochs: 2
--    max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 4 # accumulates grads every k batches
--    gradient_clip_val: 0
--    amp_level: O1 # O1/O2 for mixed precision
--    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    accelerator: ddp
--    checkpoint_callback: false  # Provided by exp_manager
--    logger: false #false  # Provided by exp_manager
--    log_every_n_steps: 1  # Interval of logging.
--    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    resume_from_checkpoint: null
--
--    # gpus: 0 # the number of gpus, 0 for CPU
-+    # gpus: 1 # the number of gpus, 0 for CPU
-     # num_nodes: 1
--    # max_epochs: 10
-+    # max_epochs: 2
-     # max_steps: null # precedence over max_epochs
-     # accumulate_grad_batches: 4 # accumulates grads every k batches
-     # gradient_clip_val: 0
--    # amp_level: O0 # O1/O2 for mixed precision
--    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # # accelerator: ddp
-+    # amp_level: O1 # O1/O2 for mixed precision
-+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-     # checkpoint_callback: false  # Provided by exp_manager
-     # logger: false #false  # Provided by exp_manager
-     # log_every_n_steps: 1  # Interval of logging.
-     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # reload_dataloaders_every_epoch: true
-     # resume_from_checkpoint: null
- 
-+    gpus: 0 # the number of gpus, 0 for CPU
-+    num_nodes: 1
-+    max_epochs: 8
-+    max_steps: null # precedence over max_epochs
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-+    gradient_clip_val: 0
-+    amp_level: O0 # O1/O2 for mixed precision
-+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-+    checkpoint_callback: false  # Provided by exp_manager
-+    logger: false #false  # Provided by exp_manager
-+    log_every_n_steps: 1  # Interval of logging.
-+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    reload_dataloaders_every_epoch: true
-+    resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /root/data # /home/nxingyu2/data # 
--tmp_path: /tmp # /home/nxingyu2/data/tmp # 
-+base_path: /home/nxingyu2/data # /root/data # 
-+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
-@@ -61,7 +61,7 @@ model:
-     punct_class_weights: true
-     
-     dataset:
--        data_dir: /root/data # /home/nxingyu2/data # 
-+        data_dir: /home/nxingyu2/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -125,17 +125,18 @@ model:
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 4
-+        alpha: 3
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: adamw
-+        name: ranger
-         lr: 1e-3
-         weight_decay: 0.00
- 
-+        delay_epochs: 4
-         sched:
-             name: WarmupAnnealing #CyclicLR
-             # Scheduler params
-@@ -145,7 +146,7 @@ model:
- 
-             # pytorch lightning args
-             monitor: val_loss
--            reduce_on_plateau: true
-+            reduce_on_plateau: false
- hydra:
-     run:
-         dir: .
-diff --git a/experiment/core/optim/lr_scheduler.py b/experiment/core/optim/lr_scheduler.py
-index a17ebf8..4944555 100644
---- a/experiment/core/optim/lr_scheduler.py
-+++ b/experiment/core/optim/lr_scheduler.py
-@@ -347,6 +347,39 @@ class PolynomialHoldDecayAnnealing(WarmupHoldPolicy):
-         return new_lrs
- 
- 
-+class DelayerScheduler(_LRScheduler):
-+	""" Starts with a flat lr schedule until it reaches N epochs the applies a scheduler 
-+	Args:
-+		optimizer (Optimizer): Wrapped optimizer.
-+		delay_epochs: number of epochs to keep the initial lr until starting aplying the scheduler
-+		after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)
-+	"""
-+
-+	def __init__(self, optimizer, delay_epochs, after_scheduler):
-+		self.delay_epochs = delay_epochs
-+		self.after_scheduler = after_scheduler
-+		self.finished = False
-+		super().__init__(optimizer)
-+
-+	def get_lr(self):
-+		if self.last_epoch >= self.delay_epochs:
-+			if not self.finished:
-+				self.after_scheduler.base_lrs = self.base_lrs
-+				self.finished = True
-+			return self.after_scheduler.get_lr()
-+
-+		return self.base_lrs
-+
-+	def step(self, epoch=None):
-+		if self.finished:
-+			if epoch is None:
-+				self.after_scheduler.step(None)
-+			else:
-+				self.after_scheduler.step(epoch - self.delay_epochs)
-+		else:
-+			return super(DelayerScheduler, self).step(epoch)
-+
-+
- def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: SchedulerParams):
-     """
-     Checks if the scheduler name exists in the registry, and if it doesnt, adds it.
-@@ -365,7 +398,7 @@ def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: Sch
-     register_scheduler_params(name=sched_name, scheduler_params=scheduler_params)
- 
- 
--def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
-+def get_scheduler(name: str, optimizer, delay_epochs, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
-     """
-     Convenience method to obtain an _LRScheduler class and partially instantiate it with optimizer kwargs.
-     Args:
-@@ -380,12 +413,15 @@ def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler
-         )
- 
-     scheduler_cls = AVAILABLE_SCHEDULERS[name]
--    scheduler = partial(scheduler_cls, **kwargs)
-+    scheduler = scheduler_cls(optimizer=optimizer, **kwargs)
-+    if delay_epochs>0:
-+        scheduler = DelayerScheduler(optimizer=optimizer,delay_epochs=delay_epochs,after_scheduler=scheduler)
-     return scheduler
- 
- 
- def prepare_lr_scheduler(
-     optimizer: optim.Optimizer,
-+    delay_epochs: int,
-     scheduler_config: Union[Dict[str, Any], DictConfig],
-     train_dataloader: Optional[Dict[str,int]] = None,
- 
-@@ -584,10 +620,10 @@ def prepare_lr_scheduler(
-         scheduler_args['max_steps'] = max_steps
- 
-     # Get the scheduler class from the config
--    scheduler_cls = get_scheduler(scheduler_name, **scheduler_args)
-+    scheduler = get_scheduler(scheduler_name, optimizer, delay_epochs, **scheduler_args)
- 
-     # Instantiate the LR schedule
--    schedule = scheduler_cls(optimizer, **scheduler_args)
-+    # schedule = scheduler_cls(optimizer, **scheduler_args)
- 
-     logging.info(
-         'Scheduler "%s" \nwill be used during training (effective maximum steps = %d) - \nParameters : \n(%s)',
-diff --git a/experiment/core/optim/optimizers.py b/experiment/core/optim/optimizers.py
-index 5afc4fd..f15686e 100644
---- a/experiment/core/optim/optimizers.py
-+++ b/experiment/core/optim/optimizers.py
-@@ -7,6 +7,7 @@ import torch.optim as optim
- from omegaconf import DictConfig, OmegaConf
- from torch.optim import adadelta, adagrad, adamax, rmsprop, rprop
- from torch.optim.optimizer import Optimizer
-+from torchtools.optim import Ranger
- 
- from core.config import OptimizerParams, get_optimizer_config, register_optimizer_params
- from core.optim.novograd import Novograd
-@@ -24,6 +25,7 @@ AVAILABLE_OPTIMIZERS = {
-     'rmsprop': rmsprop.RMSprop,
-     'rprop': rprop.Rprop,
-     'novograd': Novograd,
-+    'ranger': Ranger,
- }
- 
- 
-diff --git a/experiment/info.log b/experiment/info.log
-index b211d42..5024c4f 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,473 +1,2 @@
--[INFO] - shuffling train set
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4da7f8e0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.008413951416451957
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e67aa00>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          22.42      11.12      14.86       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                         38.62       7.46      12.50      19530
--- (label_id: 3)                                          6.60      15.75       9.30       1746
--. (label_id: 4)                                         33.33       0.01       0.01      17976
--: (label_id: 5)                                          0.79      19.15       1.52        376
--; (label_id: 6)                                          0.45       2.94       0.78        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.17      50.56       0.33         89
---------------------
--micro avg                                                5.14       5.14       5.14      48233
--macro avg                                               10.24      10.70       3.93      48233
--weighted avg                                            30.92       5.14       7.15      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          22.09      11.27      14.93       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                         35.39       6.74      11.32      19813
--- (label_id: 3)                                          6.03      16.28       8.79       1708
--. (label_id: 4)                                        100.00       0.01       0.02      18084
--: (label_id: 5)                                          0.78      20.69       1.51        348
--; (label_id: 6)                                          0.45       2.55       0.77        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.19      58.62       0.38         87
---------------------
--micro avg                                                4.89       4.89       4.89      48633
--macro avg                                               16.49      11.62       3.77      48633
--weighted avg                                            54.38       4.89       6.67      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.008413951416451957
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e74d910>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          19.61      14.42      16.62        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         38.84       6.82      11.60        689
--- (label_id: 3)                                          4.50      15.52       6.98         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.59      20.00       1.14         10
--; (label_id: 6)                                          2.56      16.67       4.44          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.25      50.00       0.51          4
---------------------
--micro avg                                                5.55       5.55       5.55       1641
--macro avg                                                6.64      12.34       4.13       1641
--weighted avg                                            18.97       5.55       7.25       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.63      98.91       7.00       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          3.96       1.76       2.44       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.63       3.63       3.63      48233
--macro avg                                                0.76      10.07       0.94      48233
--weighted avg                                             0.25       3.63       0.33      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e68b8b0>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          19.61      14.42      16.62        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         38.84       6.82      11.60        689
--- (label_id: 3)                                          4.50      15.52       6.98         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.59      20.00       1.14         10
--; (label_id: 6)                                          2.56      16.67       4.44          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.25      50.00       0.51          4
---------------------
--micro avg                                                5.55       5.55       5.55       1641
--macro avg                                                6.64      12.34       4.13       1641
--weighted avg                                            18.97       5.55       7.25       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                          0.00       0.00       0.00      19813
--- (label_id: 3)                                          3.51     100.00       6.79       1708
--. (label_id: 4)                                          0.00       0.00       0.00      18084
--: (label_id: 5)                                          0.00       0.00       0.00        348
--; (label_id: 6)                                          0.00       0.00       0.00        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.00       0.00       0.00         87
---------------------
--micro avg                                                3.51       3.51       3.51      48633
--macro avg                                                0.35      10.00       0.68      48633
--weighted avg                                             0.12       3.51       0.24      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75eaf0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                          0.00       0.00       0.00        689
--- (label_id: 3)                                          3.53     100.00       6.83         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.00       0.00       0.00         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                3.53       3.53       3.53       1641
--macro avg                                                0.35      10.00       0.68       1641
--weighted avg                                             0.12       3.53       0.24       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75e160>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                          0.00       0.00       0.00        689
--- (label_id: 3)                                          3.53     100.00       6.83         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.00       0.00       0.00         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                3.53       3.53       3.53       1641
--macro avg                                                0.35      10.00       0.68       1641
--weighted avg                                             0.12       3.53       0.24       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                          0.00       0.00       0.00      19813
--- (label_id: 3)                                          3.51     100.00       6.79       1708
--. (label_id: 4)                                          0.00       0.00       0.00      18084
--: (label_id: 5)                                          0.00       0.00       0.00        348
--; (label_id: 6)                                          0.00       0.00       0.00        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.00       0.00       0.00         87
---------------------
--micro avg                                                3.51       3.51       3.51      48633
--macro avg                                                0.35      10.00       0.68      48633
--weighted avg                                             0.12       3.51       0.24      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       7470
--! (label_id: 1)                                          0.00       0.00       0.00        148
--, (label_id: 2)                                          0.00       0.00       0.00      28513
--- (label_id: 3)                                          3.02     100.00       5.86       2074
--. (label_id: 4)                                          0.00       0.00       0.00      25120
--: (label_id: 5)                                          0.00       0.00       0.00        570
--; (label_id: 6)                                          0.00       0.00       0.00        534
--? (label_id: 7)                                          0.00       0.00       0.00       2085
--— (label_id: 8)                                          0.00       0.00       0.00       2073
--… (label_id: 9)                                          0.00       0.00       0.00        142
---------------------
--micro avg                                                3.02       3.02       3.02      68729
--macro avg                                                0.30      10.00       0.59      68729
--weighted avg                                             0.09       3.02       0.18      68729
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       3735
---------------------
--micro avg                                              100.00     100.00     100.00       3735
--macro avg                                              100.00     100.00     100.00       3735
--weighted avg                                           100.00     100.00     100.00       3735
--
--[INFO] - Internal process exited
-+[INFO] - GPU available: True, used: False
-+[INFO] - TPU available: None, using: 0 TPU cores
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index 782fbfa..c51b05b 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -374,6 +374,11 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             scheduler_config = None
- 
-+        if 'delay_epochs' in optim_config:
-+            delay_epochs = optim_config.pop('delay_epochs')
-+        else:
-+            delay_epochs = 0
-+
-         # Check if caller provided optimizer name, default to Adam otherwise
-         optimizer_cls = optim_config.get('_target_', None)
- 
-@@ -455,7 +460,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
- 
-         # Try to instantiate scheduler for optimizer
-         self._scheduler = prepare_lr_scheduler(
--            optimizer=self._optimizer, scheduler_config=scheduler_config,
-+            optimizer=self._optimizer, delay_epoch=delay_epochs,scheduler_config=scheduler_config,
-             train_dataloader=pp({'num_samples' : self.train_size*self.hparams.model.dataset.train_ds.batch_size, 
-             'batch_size': self.hparams.model.dataset.train_ds.batch_size,
-             'drop_last' : self.hparams.model.dataset.drop_last})
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/lightning_logs.txt
deleted file mode 100644
index 01e1664..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/lightning_logs.txt
+++ /dev/null
@@ -1,2 +0,0 @@
-GPU available: True, used: False
-TPU available: None, using: 0 TPU cores
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/nemo_error_log.txt
deleted file mode 100644
index bef3c22..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/nemo_error_log.txt
+++ /dev/null
@@ -1,4 +0,0 @@
-[NeMo W 2021-02-06 09:14:08 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 09:14:08 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index ff808b1..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,6 +0,0 @@
-[NeMo W 2021-02-06 09:14:08 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo I 2021-02-06 09:14:08 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08
-[NeMo I 2021-02-06 09:14:08 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-06 09:14:08 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/git-info.log
deleted file mode 100644
index 04842e2..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/git-info.log
+++ /dev/null
@@ -1,720 +0,0 @@
-commit hash: cce540d4c95ad9ed32010552a7aa1fb2486658c8
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index 3d27dd7..6d84830 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,43 +1,43 @@
- seed: 42
- trainer:
--    gpus: 1 # the number of gpus, 0 for CPU
--    num_nodes: 1
--    max_epochs: 2
--    max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 4 # accumulates grads every k batches
--    gradient_clip_val: 0
--    amp_level: O1 # O1/O2 for mixed precision
--    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    accelerator: ddp
--    checkpoint_callback: false  # Provided by exp_manager
--    logger: false #false  # Provided by exp_manager
--    log_every_n_steps: 1  # Interval of logging.
--    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    resume_from_checkpoint: null
--
--    # gpus: 0 # the number of gpus, 0 for CPU
-+    # gpus: 1 # the number of gpus, 0 for CPU
-     # num_nodes: 1
--    # max_epochs: 10
-+    # max_epochs: 2
-     # max_steps: null # precedence over max_epochs
-     # accumulate_grad_batches: 4 # accumulates grads every k batches
-     # gradient_clip_val: 0
--    # amp_level: O0 # O1/O2 for mixed precision
--    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # # accelerator: ddp
-+    # amp_level: O1 # O1/O2 for mixed precision
-+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-     # checkpoint_callback: false  # Provided by exp_manager
-     # logger: false #false  # Provided by exp_manager
-     # log_every_n_steps: 1  # Interval of logging.
-     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # reload_dataloaders_every_epoch: true
-     # resume_from_checkpoint: null
- 
-+    gpus: 0 # the number of gpus, 0 for CPU
-+    num_nodes: 1
-+    max_epochs: 8
-+    max_steps: null # precedence over max_epochs
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-+    gradient_clip_val: 0
-+    amp_level: O0 # O1/O2 for mixed precision
-+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-+    checkpoint_callback: false  # Provided by exp_manager
-+    logger: false #false  # Provided by exp_manager
-+    log_every_n_steps: 1  # Interval of logging.
-+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    reload_dataloaders_every_epoch: true
-+    resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /root/data # /home/nxingyu2/data # 
--tmp_path: /tmp # /home/nxingyu2/data/tmp # 
-+base_path: /home/nxingyu2/data # /root/data # 
-+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
-@@ -61,7 +61,7 @@ model:
-     punct_class_weights: true
-     
-     dataset:
--        data_dir: /root/data # /home/nxingyu2/data # 
-+        data_dir: /home/nxingyu2/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -125,17 +125,18 @@ model:
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 4
-+        alpha: 3
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: adamw
-+        name: ranger
-         lr: 1e-3
-         weight_decay: 0.00
- 
-+        delay_epochs: 4
-         sched:
-             name: WarmupAnnealing #CyclicLR
-             # Scheduler params
-@@ -145,7 +146,7 @@ model:
- 
-             # pytorch lightning args
-             monitor: val_loss
--            reduce_on_plateau: true
-+            reduce_on_plateau: false
- hydra:
-     run:
-         dir: .
-diff --git a/experiment/core/optim/lr_scheduler.py b/experiment/core/optim/lr_scheduler.py
-index a17ebf8..4944555 100644
---- a/experiment/core/optim/lr_scheduler.py
-+++ b/experiment/core/optim/lr_scheduler.py
-@@ -347,6 +347,39 @@ class PolynomialHoldDecayAnnealing(WarmupHoldPolicy):
-         return new_lrs
- 
- 
-+class DelayerScheduler(_LRScheduler):
-+	""" Starts with a flat lr schedule until it reaches N epochs the applies a scheduler 
-+	Args:
-+		optimizer (Optimizer): Wrapped optimizer.
-+		delay_epochs: number of epochs to keep the initial lr until starting aplying the scheduler
-+		after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)
-+	"""
-+
-+	def __init__(self, optimizer, delay_epochs, after_scheduler):
-+		self.delay_epochs = delay_epochs
-+		self.after_scheduler = after_scheduler
-+		self.finished = False
-+		super().__init__(optimizer)
-+
-+	def get_lr(self):
-+		if self.last_epoch >= self.delay_epochs:
-+			if not self.finished:
-+				self.after_scheduler.base_lrs = self.base_lrs
-+				self.finished = True
-+			return self.after_scheduler.get_lr()
-+
-+		return self.base_lrs
-+
-+	def step(self, epoch=None):
-+		if self.finished:
-+			if epoch is None:
-+				self.after_scheduler.step(None)
-+			else:
-+				self.after_scheduler.step(epoch - self.delay_epochs)
-+		else:
-+			return super(DelayerScheduler, self).step(epoch)
-+
-+
- def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: SchedulerParams):
-     """
-     Checks if the scheduler name exists in the registry, and if it doesnt, adds it.
-@@ -365,7 +398,7 @@ def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: Sch
-     register_scheduler_params(name=sched_name, scheduler_params=scheduler_params)
- 
- 
--def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
-+def get_scheduler(name: str, optimizer, delay_epochs, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
-     """
-     Convenience method to obtain an _LRScheduler class and partially instantiate it with optimizer kwargs.
-     Args:
-@@ -380,12 +413,15 @@ def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler
-         )
- 
-     scheduler_cls = AVAILABLE_SCHEDULERS[name]
--    scheduler = partial(scheduler_cls, **kwargs)
-+    scheduler = scheduler_cls(optimizer=optimizer, **kwargs)
-+    if delay_epochs>0:
-+        scheduler = DelayerScheduler(optimizer=optimizer,delay_epochs=delay_epochs,after_scheduler=scheduler)
-     return scheduler
- 
- 
- def prepare_lr_scheduler(
-     optimizer: optim.Optimizer,
-+    delay_epochs: int,
-     scheduler_config: Union[Dict[str, Any], DictConfig],
-     train_dataloader: Optional[Dict[str,int]] = None,
- 
-@@ -584,10 +620,10 @@ def prepare_lr_scheduler(
-         scheduler_args['max_steps'] = max_steps
- 
-     # Get the scheduler class from the config
--    scheduler_cls = get_scheduler(scheduler_name, **scheduler_args)
-+    scheduler = get_scheduler(scheduler_name, optimizer, delay_epochs, **scheduler_args)
- 
-     # Instantiate the LR schedule
--    schedule = scheduler_cls(optimizer, **scheduler_args)
-+    # schedule = scheduler_cls(optimizer, **scheduler_args)
- 
-     logging.info(
-         'Scheduler "%s" \nwill be used during training (effective maximum steps = %d) - \nParameters : \n(%s)',
-diff --git a/experiment/core/optim/optimizers.py b/experiment/core/optim/optimizers.py
-index 5afc4fd..f15686e 100644
---- a/experiment/core/optim/optimizers.py
-+++ b/experiment/core/optim/optimizers.py
-@@ -7,6 +7,7 @@ import torch.optim as optim
- from omegaconf import DictConfig, OmegaConf
- from torch.optim import adadelta, adagrad, adamax, rmsprop, rprop
- from torch.optim.optimizer import Optimizer
-+from torchtools.optim import Ranger
- 
- from core.config import OptimizerParams, get_optimizer_config, register_optimizer_params
- from core.optim.novograd import Novograd
-@@ -24,6 +25,7 @@ AVAILABLE_OPTIMIZERS = {
-     'rmsprop': rmsprop.RMSprop,
-     'rprop': rprop.Rprop,
-     'novograd': Novograd,
-+    'ranger': Ranger,
- }
- 
- 
-diff --git a/experiment/info.log b/experiment/info.log
-index b211d42..5024c4f 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,473 +1,2 @@
--[INFO] - shuffling train set
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4da7f8e0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.008413951416451957
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e67aa00>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          22.42      11.12      14.86       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                         38.62       7.46      12.50      19530
--- (label_id: 3)                                          6.60      15.75       9.30       1746
--. (label_id: 4)                                         33.33       0.01       0.01      17976
--: (label_id: 5)                                          0.79      19.15       1.52        376
--; (label_id: 6)                                          0.45       2.94       0.78        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.17      50.56       0.33         89
---------------------
--micro avg                                                5.14       5.14       5.14      48233
--macro avg                                               10.24      10.70       3.93      48233
--weighted avg                                            30.92       5.14       7.15      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          22.09      11.27      14.93       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                         35.39       6.74      11.32      19813
--- (label_id: 3)                                          6.03      16.28       8.79       1708
--. (label_id: 4)                                        100.00       0.01       0.02      18084
--: (label_id: 5)                                          0.78      20.69       1.51        348
--; (label_id: 6)                                          0.45       2.55       0.77        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.19      58.62       0.38         87
---------------------
--micro avg                                                4.89       4.89       4.89      48633
--macro avg                                               16.49      11.62       3.77      48633
--weighted avg                                            54.38       4.89       6.67      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.008413951416451957
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e74d910>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          19.61      14.42      16.62        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         38.84       6.82      11.60        689
--- (label_id: 3)                                          4.50      15.52       6.98         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.59      20.00       1.14         10
--; (label_id: 6)                                          2.56      16.67       4.44          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.25      50.00       0.51          4
---------------------
--micro avg                                                5.55       5.55       5.55       1641
--macro avg                                                6.64      12.34       4.13       1641
--weighted avg                                            18.97       5.55       7.25       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.63      98.91       7.00       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          3.96       1.76       2.44       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.63       3.63       3.63      48233
--macro avg                                                0.76      10.07       0.94      48233
--weighted avg                                             0.25       3.63       0.33      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e68b8b0>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          19.61      14.42      16.62        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         38.84       6.82      11.60        689
--- (label_id: 3)                                          4.50      15.52       6.98         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.59      20.00       1.14         10
--; (label_id: 6)                                          2.56      16.67       4.44          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.25      50.00       0.51          4
---------------------
--micro avg                                                5.55       5.55       5.55       1641
--macro avg                                                6.64      12.34       4.13       1641
--weighted avg                                            18.97       5.55       7.25       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                          0.00       0.00       0.00      19813
--- (label_id: 3)                                          3.51     100.00       6.79       1708
--. (label_id: 4)                                          0.00       0.00       0.00      18084
--: (label_id: 5)                                          0.00       0.00       0.00        348
--; (label_id: 6)                                          0.00       0.00       0.00        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.00       0.00       0.00         87
---------------------
--micro avg                                                3.51       3.51       3.51      48633
--macro avg                                                0.35      10.00       0.68      48633
--weighted avg                                             0.12       3.51       0.24      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75eaf0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                          0.00       0.00       0.00        689
--- (label_id: 3)                                          3.53     100.00       6.83         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.00       0.00       0.00         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                3.53       3.53       3.53       1641
--macro avg                                                0.35      10.00       0.68       1641
--weighted avg                                             0.12       3.53       0.24       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75e160>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                          0.00       0.00       0.00        689
--- (label_id: 3)                                          3.53     100.00       6.83         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.00       0.00       0.00         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                3.53       3.53       3.53       1641
--macro avg                                                0.35      10.00       0.68       1641
--weighted avg                                             0.12       3.53       0.24       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                          0.00       0.00       0.00      19813
--- (label_id: 3)                                          3.51     100.00       6.79       1708
--. (label_id: 4)                                          0.00       0.00       0.00      18084
--: (label_id: 5)                                          0.00       0.00       0.00        348
--; (label_id: 6)                                          0.00       0.00       0.00        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.00       0.00       0.00         87
---------------------
--micro avg                                                3.51       3.51       3.51      48633
--macro avg                                                0.35      10.00       0.68      48633
--weighted avg                                             0.12       3.51       0.24      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       7470
--! (label_id: 1)                                          0.00       0.00       0.00        148
--, (label_id: 2)                                          0.00       0.00       0.00      28513
--- (label_id: 3)                                          3.02     100.00       5.86       2074
--. (label_id: 4)                                          0.00       0.00       0.00      25120
--: (label_id: 5)                                          0.00       0.00       0.00        570
--; (label_id: 6)                                          0.00       0.00       0.00        534
--? (label_id: 7)                                          0.00       0.00       0.00       2085
--— (label_id: 8)                                          0.00       0.00       0.00       2073
--… (label_id: 9)                                          0.00       0.00       0.00        142
---------------------
--micro avg                                                3.02       3.02       3.02      68729
--macro avg                                                0.30      10.00       0.59      68729
--weighted avg                                             0.09       3.02       0.18      68729
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       3735
---------------------
--micro avg                                              100.00     100.00     100.00       3735
--macro avg                                              100.00     100.00     100.00       3735
--weighted avg                                           100.00     100.00     100.00       3735
--
--[INFO] - Internal process exited
-+[INFO] - GPU available: True, used: False
-+[INFO] - TPU available: None, using: 0 TPU cores
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index 782fbfa..464595d 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -374,6 +374,11 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             scheduler_config = None
- 
-+        if 'delay_epochs' in optim_config:
-+            delay_epochs = optim_config.pop('delay_epochs')
-+        else:
-+            delay_epochs = 0
-+
-         # Check if caller provided optimizer name, default to Adam otherwise
-         optimizer_cls = optim_config.get('_target_', None)
- 
-@@ -455,7 +460,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
- 
-         # Try to instantiate scheduler for optimizer
-         self._scheduler = prepare_lr_scheduler(
--            optimizer=self._optimizer, scheduler_config=scheduler_config,
-+            optimizer=self._optimizer, delay_epochs=delay_epochs,scheduler_config=scheduler_config,
-             train_dataloader=pp({'num_samples' : self.train_size*self.hparams.model.dataset.train_ds.batch_size, 
-             'batch_size': self.hparams.model.dataset.train_ds.batch_size,
-             'drop_last' : self.hparams.model.dataset.drop_last})
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/lightning_logs.txt
deleted file mode 100644
index 01e1664..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/lightning_logs.txt
+++ /dev/null
@@ -1,2 +0,0 @@
-GPU available: True, used: False
-TPU available: None, using: 0 TPU cores
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/nemo_error_log.txt
deleted file mode 100644
index 13f2117..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/nemo_error_log.txt
+++ /dev/null
@@ -1,4 +0,0 @@
-[NeMo W 2021-02-06 09:14:51 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 09:14:51 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 6f21c5c..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,6 +0,0 @@
-[NeMo W 2021-02-06 09:14:51 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo I 2021-02-06 09:14:51 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51
-[NeMo I 2021-02-06 09:14:51 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-06 09:14:51 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/events.out.tfevents.1612574759.intern-instance.19385.0 b/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/events.out.tfevents.1612574759.intern-instance.19385.0
deleted file mode 100644
index 2abef90..0000000
Binary files a/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/events.out.tfevents.1612574759.intern-instance.19385.0 and /dev/null differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/git-info.log
deleted file mode 100644
index 0d43585..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/git-info.log
+++ /dev/null
@@ -1,720 +0,0 @@
-commit hash: cce540d4c95ad9ed32010552a7aa1fb2486658c8
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index 3d27dd7..6d84830 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,43 +1,43 @@
- seed: 42
- trainer:
--    gpus: 1 # the number of gpus, 0 for CPU
--    num_nodes: 1
--    max_epochs: 2
--    max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 4 # accumulates grads every k batches
--    gradient_clip_val: 0
--    amp_level: O1 # O1/O2 for mixed precision
--    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    accelerator: ddp
--    checkpoint_callback: false  # Provided by exp_manager
--    logger: false #false  # Provided by exp_manager
--    log_every_n_steps: 1  # Interval of logging.
--    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    resume_from_checkpoint: null
--
--    # gpus: 0 # the number of gpus, 0 for CPU
-+    # gpus: 1 # the number of gpus, 0 for CPU
-     # num_nodes: 1
--    # max_epochs: 10
-+    # max_epochs: 2
-     # max_steps: null # precedence over max_epochs
-     # accumulate_grad_batches: 4 # accumulates grads every k batches
-     # gradient_clip_val: 0
--    # amp_level: O0 # O1/O2 for mixed precision
--    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # # accelerator: ddp
-+    # amp_level: O1 # O1/O2 for mixed precision
-+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-     # checkpoint_callback: false  # Provided by exp_manager
-     # logger: false #false  # Provided by exp_manager
-     # log_every_n_steps: 1  # Interval of logging.
-     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # reload_dataloaders_every_epoch: true
-     # resume_from_checkpoint: null
- 
-+    gpus: 0 # the number of gpus, 0 for CPU
-+    num_nodes: 1
-+    max_epochs: 8
-+    max_steps: null # precedence over max_epochs
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-+    gradient_clip_val: 0
-+    amp_level: O0 # O1/O2 for mixed precision
-+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-+    checkpoint_callback: false  # Provided by exp_manager
-+    logger: false #false  # Provided by exp_manager
-+    log_every_n_steps: 1  # Interval of logging.
-+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    reload_dataloaders_every_epoch: true
-+    resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /root/data # /home/nxingyu2/data # 
--tmp_path: /tmp # /home/nxingyu2/data/tmp # 
-+base_path: /home/nxingyu2/data # /root/data # 
-+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
-@@ -61,7 +61,7 @@ model:
-     punct_class_weights: true
-     
-     dataset:
--        data_dir: /root/data # /home/nxingyu2/data # 
-+        data_dir: /home/nxingyu2/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -125,17 +125,18 @@ model:
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 4
-+        alpha: 3
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: adamw
-+        name: ranger
-         lr: 1e-3
-         weight_decay: 0.00
- 
-+        delay_epochs: 4
-         sched:
-             name: WarmupAnnealing #CyclicLR
-             # Scheduler params
-@@ -145,7 +146,7 @@ model:
- 
-             # pytorch lightning args
-             monitor: val_loss
--            reduce_on_plateau: true
-+            reduce_on_plateau: false
- hydra:
-     run:
-         dir: .
-diff --git a/experiment/core/optim/lr_scheduler.py b/experiment/core/optim/lr_scheduler.py
-index a17ebf8..a1bae5e 100644
---- a/experiment/core/optim/lr_scheduler.py
-+++ b/experiment/core/optim/lr_scheduler.py
-@@ -347,6 +347,39 @@ class PolynomialHoldDecayAnnealing(WarmupHoldPolicy):
-         return new_lrs
- 
- 
-+class DelayerScheduler(_LRScheduler):
-+	""" Starts with a flat lr schedule until it reaches N epochs the applies a scheduler 
-+	Args:
-+		optimizer (Optimizer): Wrapped optimizer.
-+		delay_epochs: number of epochs to keep the initial lr until starting aplying the scheduler
-+		after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)
-+	"""
-+
-+	def __init__(self, optimizer, delay_epochs, after_scheduler):
-+		self.delay_epochs = delay_epochs
-+		self.after_scheduler = after_scheduler
-+		self.finished = False
-+		super().__init__(optimizer)
-+
-+	def get_lr(self):
-+		if self.last_epoch >= self.delay_epochs:
-+			if not self.finished:
-+				self.after_scheduler.base_lrs = self.base_lrs
-+				self.finished = True
-+			return self.after_scheduler.get_lr()
-+
-+		return self.base_lrs
-+
-+	def step(self, epoch=None):
-+		if self.finished:
-+			if epoch is None:
-+				self.after_scheduler.step(None)
-+			else:
-+				self.after_scheduler.step(epoch - self.delay_epochs)
-+		else:
-+			return super(DelayerScheduler, self).step(epoch)
-+
-+
- def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: SchedulerParams):
-     """
-     Checks if the scheduler name exists in the registry, and if it doesnt, adds it.
-@@ -365,7 +398,7 @@ def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: Sch
-     register_scheduler_params(name=sched_name, scheduler_params=scheduler_params)
- 
- 
--def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
-+def get_scheduler(name: str, optimizer, delay_epochs, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
-     """
-     Convenience method to obtain an _LRScheduler class and partially instantiate it with optimizer kwargs.
-     Args:
-@@ -380,12 +413,15 @@ def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler
-         )
- 
-     scheduler_cls = AVAILABLE_SCHEDULERS[name]
--    scheduler = partial(scheduler_cls, **kwargs)
-+    scheduler = scheduler_cls(optimizer=optimizer, **kwargs)
-+    if delay_epochs>0:
-+        scheduler = DelayerScheduler(optimizer=optimizer,delay_epochs=delay_epochs,after_scheduler=scheduler)
-     return scheduler
- 
- 
- def prepare_lr_scheduler(
-     optimizer: optim.Optimizer,
-+    delay_epochs: int,
-     scheduler_config: Union[Dict[str, Any], DictConfig],
-     train_dataloader: Optional[Dict[str,int]] = None,
- 
-@@ -584,10 +620,10 @@ def prepare_lr_scheduler(
-         scheduler_args['max_steps'] = max_steps
- 
-     # Get the scheduler class from the config
--    scheduler_cls = get_scheduler(scheduler_name, **scheduler_args)
-+    schedule = get_scheduler(scheduler_name, optimizer, delay_epochs, **scheduler_args)
- 
-     # Instantiate the LR schedule
--    schedule = scheduler_cls(optimizer, **scheduler_args)
-+    # schedule = scheduler_cls(optimizer, **scheduler_args)
- 
-     logging.info(
-         'Scheduler "%s" \nwill be used during training (effective maximum steps = %d) - \nParameters : \n(%s)',
-diff --git a/experiment/core/optim/optimizers.py b/experiment/core/optim/optimizers.py
-index 5afc4fd..f15686e 100644
---- a/experiment/core/optim/optimizers.py
-+++ b/experiment/core/optim/optimizers.py
-@@ -7,6 +7,7 @@ import torch.optim as optim
- from omegaconf import DictConfig, OmegaConf
- from torch.optim import adadelta, adagrad, adamax, rmsprop, rprop
- from torch.optim.optimizer import Optimizer
-+from torchtools.optim import Ranger
- 
- from core.config import OptimizerParams, get_optimizer_config, register_optimizer_params
- from core.optim.novograd import Novograd
-@@ -24,6 +25,7 @@ AVAILABLE_OPTIMIZERS = {
-     'rmsprop': rmsprop.RMSprop,
-     'rprop': rprop.Rprop,
-     'novograd': Novograd,
-+    'ranger': Ranger,
- }
- 
- 
-diff --git a/experiment/info.log b/experiment/info.log
-index b211d42..5024c4f 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,473 +1,2 @@
--[INFO] - shuffling train set
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4da7f8e0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.008413951416451957
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e67aa00>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          22.42      11.12      14.86       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                         38.62       7.46      12.50      19530
--- (label_id: 3)                                          6.60      15.75       9.30       1746
--. (label_id: 4)                                         33.33       0.01       0.01      17976
--: (label_id: 5)                                          0.79      19.15       1.52        376
--; (label_id: 6)                                          0.45       2.94       0.78        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.17      50.56       0.33         89
---------------------
--micro avg                                                5.14       5.14       5.14      48233
--macro avg                                               10.24      10.70       3.93      48233
--weighted avg                                            30.92       5.14       7.15      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          22.09      11.27      14.93       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                         35.39       6.74      11.32      19813
--- (label_id: 3)                                          6.03      16.28       8.79       1708
--. (label_id: 4)                                        100.00       0.01       0.02      18084
--: (label_id: 5)                                          0.78      20.69       1.51        348
--; (label_id: 6)                                          0.45       2.55       0.77        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.19      58.62       0.38         87
---------------------
--micro avg                                                4.89       4.89       4.89      48633
--macro avg                                               16.49      11.62       3.77      48633
--weighted avg                                            54.38       4.89       6.67      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.008413951416451957
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e74d910>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          19.61      14.42      16.62        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         38.84       6.82      11.60        689
--- (label_id: 3)                                          4.50      15.52       6.98         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.59      20.00       1.14         10
--; (label_id: 6)                                          2.56      16.67       4.44          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.25      50.00       0.51          4
---------------------
--micro avg                                                5.55       5.55       5.55       1641
--macro avg                                                6.64      12.34       4.13       1641
--weighted avg                                            18.97       5.55       7.25       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.63      98.91       7.00       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          3.96       1.76       2.44       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.63       3.63       3.63      48233
--macro avg                                                0.76      10.07       0.94      48233
--weighted avg                                             0.25       3.63       0.33      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e68b8b0>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          19.61      14.42      16.62        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         38.84       6.82      11.60        689
--- (label_id: 3)                                          4.50      15.52       6.98         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.59      20.00       1.14         10
--; (label_id: 6)                                          2.56      16.67       4.44          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.25      50.00       0.51          4
---------------------
--micro avg                                                5.55       5.55       5.55       1641
--macro avg                                                6.64      12.34       4.13       1641
--weighted avg                                            18.97       5.55       7.25       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                          0.00       0.00       0.00      19813
--- (label_id: 3)                                          3.51     100.00       6.79       1708
--. (label_id: 4)                                          0.00       0.00       0.00      18084
--: (label_id: 5)                                          0.00       0.00       0.00        348
--; (label_id: 6)                                          0.00       0.00       0.00        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.00       0.00       0.00         87
---------------------
--micro avg                                                3.51       3.51       3.51      48633
--macro avg                                                0.35      10.00       0.68      48633
--weighted avg                                             0.12       3.51       0.24      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75eaf0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                          0.00       0.00       0.00        689
--- (label_id: 3)                                          3.53     100.00       6.83         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.00       0.00       0.00         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                3.53       3.53       3.53       1641
--macro avg                                                0.35      10.00       0.68       1641
--weighted avg                                             0.12       3.53       0.24       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75e160>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                          0.00       0.00       0.00        689
--- (label_id: 3)                                          3.53     100.00       6.83         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.00       0.00       0.00         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                3.53       3.53       3.53       1641
--macro avg                                                0.35      10.00       0.68       1641
--weighted avg                                             0.12       3.53       0.24       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                          0.00       0.00       0.00      19813
--- (label_id: 3)                                          3.51     100.00       6.79       1708
--. (label_id: 4)                                          0.00       0.00       0.00      18084
--: (label_id: 5)                                          0.00       0.00       0.00        348
--; (label_id: 6)                                          0.00       0.00       0.00        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.00       0.00       0.00         87
---------------------
--micro avg                                                3.51       3.51       3.51      48633
--macro avg                                                0.35      10.00       0.68      48633
--weighted avg                                             0.12       3.51       0.24      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       7470
--! (label_id: 1)                                          0.00       0.00       0.00        148
--, (label_id: 2)                                          0.00       0.00       0.00      28513
--- (label_id: 3)                                          3.02     100.00       5.86       2074
--. (label_id: 4)                                          0.00       0.00       0.00      25120
--: (label_id: 5)                                          0.00       0.00       0.00        570
--; (label_id: 6)                                          0.00       0.00       0.00        534
--? (label_id: 7)                                          0.00       0.00       0.00       2085
--— (label_id: 8)                                          0.00       0.00       0.00       2073
--… (label_id: 9)                                          0.00       0.00       0.00        142
---------------------
--micro avg                                                3.02       3.02       3.02      68729
--macro avg                                                0.30      10.00       0.59      68729
--weighted avg                                             0.09       3.02       0.18      68729
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       3735
---------------------
--micro avg                                              100.00     100.00     100.00       3735
--macro avg                                              100.00     100.00     100.00       3735
--weighted avg                                           100.00     100.00     100.00       3735
--
--[INFO] - Internal process exited
-+[INFO] - GPU available: True, used: False
-+[INFO] - TPU available: None, using: 0 TPU cores
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index 782fbfa..464595d 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -374,6 +374,11 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             scheduler_config = None
- 
-+        if 'delay_epochs' in optim_config:
-+            delay_epochs = optim_config.pop('delay_epochs')
-+        else:
-+            delay_epochs = 0
-+
-         # Check if caller provided optimizer name, default to Adam otherwise
-         optimizer_cls = optim_config.get('_target_', None)
- 
-@@ -455,7 +460,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
- 
-         # Try to instantiate scheduler for optimizer
-         self._scheduler = prepare_lr_scheduler(
--            optimizer=self._optimizer, scheduler_config=scheduler_config,
-+            optimizer=self._optimizer, delay_epochs=delay_epochs,scheduler_config=scheduler_config,
-             train_dataloader=pp({'num_samples' : self.train_size*self.hparams.model.dataset.train_ds.batch_size, 
-             'batch_size': self.hparams.model.dataset.train_ds.batch_size,
-             'drop_last' : self.hparams.model.dataset.drop_last})
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/hparams.yaml
deleted file mode 100644
index 34d5300..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/hparams.yaml
+++ /dev/null
@@ -1,108 +0,0 @@
-seed: 42
-trainer:
-  gpus: 0
-  num_nodes: 1
-  max_epochs: 8
-  max_steps: null
-  accumulate_grad_batches: 4
-  gradient_clip_val: 0
-  amp_level: O0
-  precision: 32
-  checkpoint_callback: false
-  logger: false
-  log_every_n_steps: 1
-  val_check_interval: 1.0
-  reload_dataloaders_every_epoch: true
-  resume_from_checkpoint: null
-exp_manager:
-  exp_dir: /home/nxingyu2/project/
-  name: Punctuation_with_Domain_discriminator
-  create_tensorboard_logger: true
-  create_checkpoint_callback: true
-base_path: /home/nxingyu2/data
-tmp_path: /home/nxingyu2/data/tmp
-model:
-  nemo_path: null
-  transformer_path: google/electra-small-discriminator
-  unfrozen: 0
-  maximum_unfrozen: 2
-  unfreeze_step: 1
-  punct_label_ids:
-  - ''
-  - '!'
-  - ','
-  - '-'
-  - .
-  - ':'
-  - ;
-  - '?'
-  - —
-  - …
-  punct_class_weights: true
-  dataset:
-    data_dir: /home/nxingyu2/data
-    labelled:
-    - /home/nxingyu2/data/ted_talks_processed
-    unlabelled: null
-    max_seq_length: 128
-    pad_label: ''
-    ignore_extra_tokens: false
-    ignore_start_end: false
-    use_cache: false
-    num_workers: 0
-    pin_memory: true
-    drop_last: false
-    num_labels: 10
-    num_domains: 1
-    test_unlabelled: true
-    train_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 8
-    validation_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 8
-  tokenizer:
-    tokenizer_name: google/electra-small-discriminator
-    vocab_file: null
-    tokenizer_model: null
-    special_tokens: null
-  language_model:
-    pretrained_model_name: google/electra-small-discriminator
-    lm_checkpoint: null
-    config_file: null
-    config: null
-  punct_head:
-    punct_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: dice
-  domain_head:
-    domain_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: cel
-    gamma: 0.1
-  dice_loss:
-    epsilon: 0.01
-    alpha: 3
-    macro_average: true
-  focal_loss:
-    gamma: 5
-  optim:
-    name: ranger
-    lr: 0.001
-    weight_decay: 0.0
-    delay_epochs: 4
-    sched:
-      name: WarmupAnnealing
-      warmup_steps: null
-      warmup_ratio: 0.1
-      last_epoch: -1
-      monitor: val_loss
-      reduce_on_plateau: false
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/lightning_logs.txt
deleted file mode 100644
index fd892f0..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/lightning_logs.txt
+++ /dev/null
@@ -1,74 +0,0 @@
-GPU available: True, used: False
-TPU available: None, using: 0 TPU cores
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-35.9 K    Trainable params
-13.4 M    Non-trainable params
-13.5 M    Total params
-Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/lr_find_temp_model.ckpt
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-35.9 K    Trainable params
-13.4 M    Non-trainable params
-13.5 M    Total params
-Epoch 0, global step 100: val_loss reached 0.85947 (best 0.85947), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-825 K     Trainable params
-12.7 M    Non-trainable params
-13.5 M    Total params
-LR finder stopped early due to diverging loss.
-Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/lr_find_temp_model.ckpt
-Failed to compute suggesting for `lr`. There might not be enough points.
-Traceback (most recent call last):
-  File "/home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 353, in suggestion
-    min_grad = np.gradient(loss).argmin()
-  File "<__array_function__ internals>", line 5, in gradient
-  File "/home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
-    raise ValueError(
-ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-825 K     Trainable params
-12.7 M    Non-trainable params
-13.5 M    Total params
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/nemo_error_log.txt
deleted file mode 100644
index 94214f5..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/nemo_error_log.txt
+++ /dev/null
@@ -1,42 +0,0 @@
-[NeMo W 2021-02-06 09:16:57 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 09:16:57 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-06 09:17:08 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 09:17:08 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 09:17:12 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 09:17:21 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtools/optim/radam.py:62: UserWarning: This overload of addcmul_ is deprecated:
-    	addcmul_(Number value, Tensor tensor1, Tensor tensor2)
-    Consider using one of the following signatures instead:
-    	addcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/torch/csrc/utils/python_arg_parser.cpp:882.)
-      exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
-    
-[NeMo W 2021-02-06 09:26:31 nemo_logging:349] /home/nxingyu2/project/experiment/core/optim/lr_scheduler.py:49: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
-      warnings.warn(
-    
-[NeMo W 2021-02-06 09:36:51 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f56bb2964c0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-06 09:37:19 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f56bb296220> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-06 09:38:03 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 09:38:05 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
-      warnings.warn(*args, **kwargs)
-    
-Failed to compute suggesting for `lr`. There might not be enough points.
-Traceback (most recent call last):
-  File "/home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 353, in suggestion
-    min_grad = np.gradient(loss).argmin()
-  File "<__array_function__ internals>", line 5, in gradient
-  File "/home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
-    raise ValueError(
-ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 82d30d0..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,36 +0,0 @@
-[NeMo W 2021-02-06 09:16:57 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo I 2021-02-06 09:16:57 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57
-[NeMo I 2021-02-06 09:16:57 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-06 09:16:57 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-06 09:17:08 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 09:17:08 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 09:17:12 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 09:17:21 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtools/optim/radam.py:62: UserWarning: This overload of addcmul_ is deprecated:
-    	addcmul_(Number value, Tensor tensor1, Tensor tensor2)
-    Consider using one of the following signatures instead:
-    	addcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/torch/csrc/utils/python_arg_parser.cpp:882.)
-      exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
-    
-[NeMo W 2021-02-06 09:26:31 nemo_logging:349] /home/nxingyu2/project/experiment/core/optim/lr_scheduler.py:49: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
-      warnings.warn(
-    
-[NeMo W 2021-02-06 09:36:51 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f56bb2964c0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-06 09:37:19 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f56bb296220> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-06 09:38:03 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 09:38:05 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/git-info.log
deleted file mode 100644
index 35bebc4..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/git-info.log
+++ /dev/null
@@ -1,725 +0,0 @@
-commit hash: cce540d4c95ad9ed32010552a7aa1fb2486658c8
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index 3d27dd7..a30ba33 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,43 +1,43 @@
- seed: 42
- trainer:
--    gpus: 1 # the number of gpus, 0 for CPU
--    num_nodes: 1
--    max_epochs: 2
--    max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 4 # accumulates grads every k batches
--    gradient_clip_val: 0
--    amp_level: O1 # O1/O2 for mixed precision
--    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    accelerator: ddp
--    checkpoint_callback: false  # Provided by exp_manager
--    logger: false #false  # Provided by exp_manager
--    log_every_n_steps: 1  # Interval of logging.
--    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    resume_from_checkpoint: null
--
--    # gpus: 0 # the number of gpus, 0 for CPU
-+    # gpus: 1 # the number of gpus, 0 for CPU
-     # num_nodes: 1
--    # max_epochs: 10
-+    # max_epochs: 2
-     # max_steps: null # precedence over max_epochs
-     # accumulate_grad_batches: 4 # accumulates grads every k batches
-     # gradient_clip_val: 0
--    # amp_level: O0 # O1/O2 for mixed precision
--    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # # accelerator: ddp
-+    # amp_level: O1 # O1/O2 for mixed precision
-+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-     # checkpoint_callback: false  # Provided by exp_manager
-     # logger: false #false  # Provided by exp_manager
-     # log_every_n_steps: 1  # Interval of logging.
-     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # reload_dataloaders_every_epoch: true
-     # resume_from_checkpoint: null
- 
-+    gpus: 0 # the number of gpus, 0 for CPU
-+    num_nodes: 1
-+    max_epochs: 8
-+    max_steps: null # precedence over max_epochs
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-+    gradient_clip_val: 0
-+    amp_level: O0 # O1/O2 for mixed precision
-+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-+    checkpoint_callback: false  # Provided by exp_manager
-+    logger: false #false  # Provided by exp_manager
-+    log_every_n_steps: 1  # Interval of logging.
-+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    reload_dataloaders_every_epoch: true
-+    resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /root/data # /home/nxingyu2/data # 
--tmp_path: /tmp # /home/nxingyu2/data/tmp # 
-+base_path: /home/nxingyu2/data # /root/data # 
-+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
-@@ -61,7 +61,7 @@ model:
-     punct_class_weights: true
-     
-     dataset:
--        data_dir: /root/data # /home/nxingyu2/data # 
-+        data_dir: /home/nxingyu2/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -125,27 +125,28 @@ model:
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 4
-+        alpha: 3
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: adamw
-+        name: ranger
-         lr: 1e-3
-         weight_decay: 0.00
- 
-+        delay_epochs: 6
-         sched:
--            name: WarmupAnnealing #CyclicLR
-+            name: CosineAnnealing #CyclicLR
-             # Scheduler params
--            warmup_steps: null
--            warmup_ratio: 0.1
-+            # warmup_steps: null
-+            # warmup_ratio: 0.1
-             last_epoch: -1
- 
-             # pytorch lightning args
-             monitor: val_loss
--            reduce_on_plateau: true
-+            reduce_on_plateau: false
- hydra:
-     run:
-         dir: .
-diff --git a/experiment/core/optim/lr_scheduler.py b/experiment/core/optim/lr_scheduler.py
-index a17ebf8..3def2ff 100644
---- a/experiment/core/optim/lr_scheduler.py
-+++ b/experiment/core/optim/lr_scheduler.py
-@@ -347,6 +347,39 @@ class PolynomialHoldDecayAnnealing(WarmupHoldPolicy):
-         return new_lrs
- 
- 
-+class DelayerScheduler(_LRScheduler):
-+	""" Starts with a flat lr schedule until it reaches N epochs the applies a scheduler 
-+	Args:
-+		optimizer (Optimizer): Wrapped optimizer.
-+		delay_epochs: number of epochs to keep the initial lr until starting aplying the scheduler
-+		after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)
-+	"""
-+
-+	def __init__(self, optimizer, delay_epochs, after_scheduler):
-+		self.delay_epochs = delay_epochs
-+		self.after_scheduler = after_scheduler
-+		self.finished = False
-+		super().__init__(optimizer, last_epoch)
-+
-+	def get_lr(self):
-+		if self.last_epoch >= self.delay_epochs:
-+			if not self.finished:
-+				self.after_scheduler.base_lrs = self.base_lrs
-+				self.finished = True
-+			return self.after_scheduler._get_lr()
-+
-+		return self.base_lrs
-+
-+	def step(self, epoch=None):
-+		if self.finished:
-+			if epoch is None:
-+				self.after_scheduler.step(None)
-+			else:
-+				self.after_scheduler.step(epoch - self.delay_epochs)
-+		else:
-+			return super(DelayerScheduler, self).step(epoch)
-+
-+
- def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: SchedulerParams):
-     """
-     Checks if the scheduler name exists in the registry, and if it doesnt, adds it.
-@@ -365,7 +398,7 @@ def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: Sch
-     register_scheduler_params(name=sched_name, scheduler_params=scheduler_params)
- 
- 
--def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
-+def get_scheduler(name: str, optimizer, delay_epochs, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
-     """
-     Convenience method to obtain an _LRScheduler class and partially instantiate it with optimizer kwargs.
-     Args:
-@@ -380,12 +413,15 @@ def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler
-         )
- 
-     scheduler_cls = AVAILABLE_SCHEDULERS[name]
--    scheduler = partial(scheduler_cls, **kwargs)
-+    scheduler = scheduler_cls(optimizer=optimizer, **kwargs)
-+    if delay_epochs>0:
-+        scheduler = DelayerScheduler(optimizer=optimizer,delay_epochs=delay_epochs,after_scheduler=scheduler)
-     return scheduler
- 
- 
- def prepare_lr_scheduler(
-     optimizer: optim.Optimizer,
-+    delay_epochs: int,
-     scheduler_config: Union[Dict[str, Any], DictConfig],
-     train_dataloader: Optional[Dict[str,int]] = None,
- 
-@@ -584,10 +620,10 @@ def prepare_lr_scheduler(
-         scheduler_args['max_steps'] = max_steps
- 
-     # Get the scheduler class from the config
--    scheduler_cls = get_scheduler(scheduler_name, **scheduler_args)
-+    schedule = get_scheduler(scheduler_name, optimizer, delay_epochs, **scheduler_args)
- 
-     # Instantiate the LR schedule
--    schedule = scheduler_cls(optimizer, **scheduler_args)
-+    # schedule = scheduler_cls(optimizer, **scheduler_args)
- 
-     logging.info(
-         'Scheduler "%s" \nwill be used during training (effective maximum steps = %d) - \nParameters : \n(%s)',
-diff --git a/experiment/core/optim/optimizers.py b/experiment/core/optim/optimizers.py
-index 5afc4fd..f15686e 100644
---- a/experiment/core/optim/optimizers.py
-+++ b/experiment/core/optim/optimizers.py
-@@ -7,6 +7,7 @@ import torch.optim as optim
- from omegaconf import DictConfig, OmegaConf
- from torch.optim import adadelta, adagrad, adamax, rmsprop, rprop
- from torch.optim.optimizer import Optimizer
-+from torchtools.optim import Ranger
- 
- from core.config import OptimizerParams, get_optimizer_config, register_optimizer_params
- from core.optim.novograd import Novograd
-@@ -24,6 +25,7 @@ AVAILABLE_OPTIMIZERS = {
-     'rmsprop': rmsprop.RMSprop,
-     'rprop': rprop.Rprop,
-     'novograd': Novograd,
-+    'ranger': Ranger,
- }
- 
- 
-diff --git a/experiment/info.log b/experiment/info.log
-index b211d42..5024c4f 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,473 +1,2 @@
--[INFO] - shuffling train set
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4da7f8e0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.008413951416451957
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e67aa00>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          22.42      11.12      14.86       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                         38.62       7.46      12.50      19530
--- (label_id: 3)                                          6.60      15.75       9.30       1746
--. (label_id: 4)                                         33.33       0.01       0.01      17976
--: (label_id: 5)                                          0.79      19.15       1.52        376
--; (label_id: 6)                                          0.45       2.94       0.78        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.17      50.56       0.33         89
---------------------
--micro avg                                                5.14       5.14       5.14      48233
--macro avg                                               10.24      10.70       3.93      48233
--weighted avg                                            30.92       5.14       7.15      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          22.09      11.27      14.93       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                         35.39       6.74      11.32      19813
--- (label_id: 3)                                          6.03      16.28       8.79       1708
--. (label_id: 4)                                        100.00       0.01       0.02      18084
--: (label_id: 5)                                          0.78      20.69       1.51        348
--; (label_id: 6)                                          0.45       2.55       0.77        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.19      58.62       0.38         87
---------------------
--micro avg                                                4.89       4.89       4.89      48633
--macro avg                                               16.49      11.62       3.77      48633
--weighted avg                                            54.38       4.89       6.67      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.008413951416451957
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e74d910>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          19.61      14.42      16.62        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         38.84       6.82      11.60        689
--- (label_id: 3)                                          4.50      15.52       6.98         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.59      20.00       1.14         10
--; (label_id: 6)                                          2.56      16.67       4.44          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.25      50.00       0.51          4
---------------------
--micro avg                                                5.55       5.55       5.55       1641
--macro avg                                                6.64      12.34       4.13       1641
--weighted avg                                            18.97       5.55       7.25       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.63      98.91       7.00       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          3.96       1.76       2.44       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.63       3.63       3.63      48233
--macro avg                                                0.76      10.07       0.94      48233
--weighted avg                                             0.25       3.63       0.33      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e68b8b0>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          19.61      14.42      16.62        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         38.84       6.82      11.60        689
--- (label_id: 3)                                          4.50      15.52       6.98         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.59      20.00       1.14         10
--; (label_id: 6)                                          2.56      16.67       4.44          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.25      50.00       0.51          4
---------------------
--micro avg                                                5.55       5.55       5.55       1641
--macro avg                                                6.64      12.34       4.13       1641
--weighted avg                                            18.97       5.55       7.25       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                          0.00       0.00       0.00      19813
--- (label_id: 3)                                          3.51     100.00       6.79       1708
--. (label_id: 4)                                          0.00       0.00       0.00      18084
--: (label_id: 5)                                          0.00       0.00       0.00        348
--; (label_id: 6)                                          0.00       0.00       0.00        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.00       0.00       0.00         87
---------------------
--micro avg                                                3.51       3.51       3.51      48633
--macro avg                                                0.35      10.00       0.68      48633
--weighted avg                                             0.12       3.51       0.24      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75eaf0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                          0.00       0.00       0.00        689
--- (label_id: 3)                                          3.53     100.00       6.83         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.00       0.00       0.00         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                3.53       3.53       3.53       1641
--macro avg                                                0.35      10.00       0.68       1641
--weighted avg                                             0.12       3.53       0.24       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75e160>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                          0.00       0.00       0.00        689
--- (label_id: 3)                                          3.53     100.00       6.83         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.00       0.00       0.00         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                3.53       3.53       3.53       1641
--macro avg                                                0.35      10.00       0.68       1641
--weighted avg                                             0.12       3.53       0.24       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                          0.00       0.00       0.00      19813
--- (label_id: 3)                                          3.51     100.00       6.79       1708
--. (label_id: 4)                                          0.00       0.00       0.00      18084
--: (label_id: 5)                                          0.00       0.00       0.00        348
--; (label_id: 6)                                          0.00       0.00       0.00        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.00       0.00       0.00         87
---------------------
--micro avg                                                3.51       3.51       3.51      48633
--macro avg                                                0.35      10.00       0.68      48633
--weighted avg                                             0.12       3.51       0.24      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       7470
--! (label_id: 1)                                          0.00       0.00       0.00        148
--, (label_id: 2)                                          0.00       0.00       0.00      28513
--- (label_id: 3)                                          3.02     100.00       5.86       2074
--. (label_id: 4)                                          0.00       0.00       0.00      25120
--: (label_id: 5)                                          0.00       0.00       0.00        570
--; (label_id: 6)                                          0.00       0.00       0.00        534
--? (label_id: 7)                                          0.00       0.00       0.00       2085
--— (label_id: 8)                                          0.00       0.00       0.00       2073
--… (label_id: 9)                                          0.00       0.00       0.00        142
---------------------
--micro avg                                                3.02       3.02       3.02      68729
--macro avg                                                0.30      10.00       0.59      68729
--weighted avg                                             0.09       3.02       0.18      68729
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       3735
---------------------
--micro avg                                              100.00     100.00     100.00       3735
--macro avg                                              100.00     100.00     100.00       3735
--weighted avg                                           100.00     100.00     100.00       3735
--
--[INFO] - Internal process exited
-+[INFO] - GPU available: True, used: False
-+[INFO] - TPU available: None, using: 0 TPU cores
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index 782fbfa..464595d 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -374,6 +374,11 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             scheduler_config = None
- 
-+        if 'delay_epochs' in optim_config:
-+            delay_epochs = optim_config.pop('delay_epochs')
-+        else:
-+            delay_epochs = 0
-+
-         # Check if caller provided optimizer name, default to Adam otherwise
-         optimizer_cls = optim_config.get('_target_', None)
- 
-@@ -455,7 +460,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
- 
-         # Try to instantiate scheduler for optimizer
-         self._scheduler = prepare_lr_scheduler(
--            optimizer=self._optimizer, scheduler_config=scheduler_config,
-+            optimizer=self._optimizer, delay_epochs=delay_epochs,scheduler_config=scheduler_config,
-             train_dataloader=pp({'num_samples' : self.train_size*self.hparams.model.dataset.train_ds.batch_size, 
-             'batch_size': self.hparams.model.dataset.train_ds.batch_size,
-             'drop_last' : self.hparams.model.dataset.drop_last})
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/lightning_logs.txt
deleted file mode 100644
index 01e1664..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/lightning_logs.txt
+++ /dev/null
@@ -1,2 +0,0 @@
-GPU available: True, used: False
-TPU available: None, using: 0 TPU cores
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/nemo_error_log.txt
deleted file mode 100644
index 98e6fd2..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/nemo_error_log.txt
+++ /dev/null
@@ -1,4 +0,0 @@
-[NeMo W 2021-02-06 09:39:02 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 09:39:02 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 99e03a4..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,6 +0,0 @@
-[NeMo W 2021-02-06 09:39:02 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo I 2021-02-06 09:39:02 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02
-[NeMo I 2021-02-06 09:39:02 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-06 09:39:02 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/events.out.tfevents.1612577016.intern-instance.23889.0 b/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/events.out.tfevents.1612577016.intern-instance.23889.0
deleted file mode 100644
index 487e986..0000000
Binary files a/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/events.out.tfevents.1612577016.intern-instance.23889.0 and /dev/null differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/git-info.log
deleted file mode 100644
index 421b5c6..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/git-info.log
+++ /dev/null
@@ -1,726 +0,0 @@
-commit hash: cce540d4c95ad9ed32010552a7aa1fb2486658c8
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index 3d27dd7..a30ba33 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,43 +1,43 @@
- seed: 42
- trainer:
--    gpus: 1 # the number of gpus, 0 for CPU
--    num_nodes: 1
--    max_epochs: 2
--    max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 4 # accumulates grads every k batches
--    gradient_clip_val: 0
--    amp_level: O1 # O1/O2 for mixed precision
--    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    accelerator: ddp
--    checkpoint_callback: false  # Provided by exp_manager
--    logger: false #false  # Provided by exp_manager
--    log_every_n_steps: 1  # Interval of logging.
--    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    resume_from_checkpoint: null
--
--    # gpus: 0 # the number of gpus, 0 for CPU
-+    # gpus: 1 # the number of gpus, 0 for CPU
-     # num_nodes: 1
--    # max_epochs: 10
-+    # max_epochs: 2
-     # max_steps: null # precedence over max_epochs
-     # accumulate_grad_batches: 4 # accumulates grads every k batches
-     # gradient_clip_val: 0
--    # amp_level: O0 # O1/O2 for mixed precision
--    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # # accelerator: ddp
-+    # amp_level: O1 # O1/O2 for mixed precision
-+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-     # checkpoint_callback: false  # Provided by exp_manager
-     # logger: false #false  # Provided by exp_manager
-     # log_every_n_steps: 1  # Interval of logging.
-     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # reload_dataloaders_every_epoch: true
-     # resume_from_checkpoint: null
- 
-+    gpus: 0 # the number of gpus, 0 for CPU
-+    num_nodes: 1
-+    max_epochs: 8
-+    max_steps: null # precedence over max_epochs
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-+    gradient_clip_val: 0
-+    amp_level: O0 # O1/O2 for mixed precision
-+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-+    checkpoint_callback: false  # Provided by exp_manager
-+    logger: false #false  # Provided by exp_manager
-+    log_every_n_steps: 1  # Interval of logging.
-+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    reload_dataloaders_every_epoch: true
-+    resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /root/data # /home/nxingyu2/data # 
--tmp_path: /tmp # /home/nxingyu2/data/tmp # 
-+base_path: /home/nxingyu2/data # /root/data # 
-+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
-@@ -61,7 +61,7 @@ model:
-     punct_class_weights: true
-     
-     dataset:
--        data_dir: /root/data # /home/nxingyu2/data # 
-+        data_dir: /home/nxingyu2/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -125,27 +125,28 @@ model:
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 4
-+        alpha: 3
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: adamw
-+        name: ranger
-         lr: 1e-3
-         weight_decay: 0.00
- 
-+        delay_epochs: 6
-         sched:
--            name: WarmupAnnealing #CyclicLR
-+            name: CosineAnnealing #CyclicLR
-             # Scheduler params
--            warmup_steps: null
--            warmup_ratio: 0.1
-+            # warmup_steps: null
-+            # warmup_ratio: 0.1
-             last_epoch: -1
- 
-             # pytorch lightning args
-             monitor: val_loss
--            reduce_on_plateau: true
-+            reduce_on_plateau: false
- hydra:
-     run:
-         dir: .
-diff --git a/experiment/core/optim/lr_scheduler.py b/experiment/core/optim/lr_scheduler.py
-index a17ebf8..f87bb23 100644
---- a/experiment/core/optim/lr_scheduler.py
-+++ b/experiment/core/optim/lr_scheduler.py
-@@ -347,6 +347,40 @@ class PolynomialHoldDecayAnnealing(WarmupHoldPolicy):
-         return new_lrs
- 
- 
-+class DelayerScheduler(_LRScheduler):
-+    """ Starts with a flat lr schedule until it reaches N epochs the applies a scheduler 
-+    Args:
-+        optimizer (Optimizer): Wrapped optimizer.
-+        delay_epochs: number of epochs to keep the initial lr until starting aplying the scheduler
-+        after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)
-+    """
-+
-+    def __init__(self, optimizer, delay_epochs, after_scheduler):
-+        self.delay_epochs = delay_epochs
-+        self.after_scheduler = after_scheduler
-+        self.finished = False
-+        self.last_epoch = after_scheduler.last_epoch
-+        super().__init__(optimizer, self.last_epoch)
-+
-+    def get_lr(self):
-+        if self.last_epoch >= self.delay_epochs:
-+            if not self.finished:
-+                self.after_scheduler.base_lrs = self.base_lrs
-+                self.finished = True
-+            return self.after_scheduler._get_lr()
-+
-+        return self.base_lrs
-+
-+    def step(self, epoch=None):
-+        if self.finished:
-+            if epoch is None:
-+                self.after_scheduler.step(None)
-+            else:
-+                self.after_scheduler.step(epoch - self.delay_epochs)
-+        else:
-+            return super(DelayerScheduler, self).step(epoch)
-+
-+
- def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: SchedulerParams):
-     """
-     Checks if the scheduler name exists in the registry, and if it doesnt, adds it.
-@@ -365,7 +399,7 @@ def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: Sch
-     register_scheduler_params(name=sched_name, scheduler_params=scheduler_params)
- 
- 
--def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
-+def get_scheduler(name: str, optimizer, delay_epochs, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
-     """
-     Convenience method to obtain an _LRScheduler class and partially instantiate it with optimizer kwargs.
-     Args:
-@@ -380,12 +414,15 @@ def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler
-         )
- 
-     scheduler_cls = AVAILABLE_SCHEDULERS[name]
--    scheduler = partial(scheduler_cls, **kwargs)
-+    scheduler = scheduler_cls(optimizer=optimizer, **kwargs)
-+    if delay_epochs>0:
-+        scheduler = DelayerScheduler(optimizer=optimizer,delay_epochs=delay_epochs,after_scheduler=scheduler)
-     return scheduler
- 
- 
- def prepare_lr_scheduler(
-     optimizer: optim.Optimizer,
-+    delay_epochs: int,
-     scheduler_config: Union[Dict[str, Any], DictConfig],
-     train_dataloader: Optional[Dict[str,int]] = None,
- 
-@@ -584,10 +621,10 @@ def prepare_lr_scheduler(
-         scheduler_args['max_steps'] = max_steps
- 
-     # Get the scheduler class from the config
--    scheduler_cls = get_scheduler(scheduler_name, **scheduler_args)
-+    schedule = get_scheduler(scheduler_name, optimizer, delay_epochs, **scheduler_args)
- 
-     # Instantiate the LR schedule
--    schedule = scheduler_cls(optimizer, **scheduler_args)
-+    # schedule = scheduler_cls(optimizer, **scheduler_args)
- 
-     logging.info(
-         'Scheduler "%s" \nwill be used during training (effective maximum steps = %d) - \nParameters : \n(%s)',
-diff --git a/experiment/core/optim/optimizers.py b/experiment/core/optim/optimizers.py
-index 5afc4fd..f15686e 100644
---- a/experiment/core/optim/optimizers.py
-+++ b/experiment/core/optim/optimizers.py
-@@ -7,6 +7,7 @@ import torch.optim as optim
- from omegaconf import DictConfig, OmegaConf
- from torch.optim import adadelta, adagrad, adamax, rmsprop, rprop
- from torch.optim.optimizer import Optimizer
-+from torchtools.optim import Ranger
- 
- from core.config import OptimizerParams, get_optimizer_config, register_optimizer_params
- from core.optim.novograd import Novograd
-@@ -24,6 +25,7 @@ AVAILABLE_OPTIMIZERS = {
-     'rmsprop': rmsprop.RMSprop,
-     'rprop': rprop.Rprop,
-     'novograd': Novograd,
-+    'ranger': Ranger,
- }
- 
- 
-diff --git a/experiment/info.log b/experiment/info.log
-index b211d42..5024c4f 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,473 +1,2 @@
--[INFO] - shuffling train set
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4da7f8e0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.008413951416451957
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e67aa00>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          22.42      11.12      14.86       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                         38.62       7.46      12.50      19530
--- (label_id: 3)                                          6.60      15.75       9.30       1746
--. (label_id: 4)                                         33.33       0.01       0.01      17976
--: (label_id: 5)                                          0.79      19.15       1.52        376
--; (label_id: 6)                                          0.45       2.94       0.78        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.17      50.56       0.33         89
---------------------
--micro avg                                                5.14       5.14       5.14      48233
--macro avg                                               10.24      10.70       3.93      48233
--weighted avg                                            30.92       5.14       7.15      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          22.09      11.27      14.93       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                         35.39       6.74      11.32      19813
--- (label_id: 3)                                          6.03      16.28       8.79       1708
--. (label_id: 4)                                        100.00       0.01       0.02      18084
--: (label_id: 5)                                          0.78      20.69       1.51        348
--; (label_id: 6)                                          0.45       2.55       0.77        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.19      58.62       0.38         87
---------------------
--micro avg                                                4.89       4.89       4.89      48633
--macro avg                                               16.49      11.62       3.77      48633
--weighted avg                                            54.38       4.89       6.67      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.008413951416451957
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e74d910>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          19.61      14.42      16.62        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         38.84       6.82      11.60        689
--- (label_id: 3)                                          4.50      15.52       6.98         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.59      20.00       1.14         10
--; (label_id: 6)                                          2.56      16.67       4.44          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.25      50.00       0.51          4
---------------------
--micro avg                                                5.55       5.55       5.55       1641
--macro avg                                                6.64      12.34       4.13       1641
--weighted avg                                            18.97       5.55       7.25       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.63      98.91       7.00       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          3.96       1.76       2.44       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.63       3.63       3.63      48233
--macro avg                                                0.76      10.07       0.94      48233
--weighted avg                                             0.25       3.63       0.33      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e68b8b0>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          19.61      14.42      16.62        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         38.84       6.82      11.60        689
--- (label_id: 3)                                          4.50      15.52       6.98         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.59      20.00       1.14         10
--; (label_id: 6)                                          2.56      16.67       4.44          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.25      50.00       0.51          4
---------------------
--micro avg                                                5.55       5.55       5.55       1641
--macro avg                                                6.64      12.34       4.13       1641
--weighted avg                                            18.97       5.55       7.25       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                          0.00       0.00       0.00      19813
--- (label_id: 3)                                          3.51     100.00       6.79       1708
--. (label_id: 4)                                          0.00       0.00       0.00      18084
--: (label_id: 5)                                          0.00       0.00       0.00        348
--; (label_id: 6)                                          0.00       0.00       0.00        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.00       0.00       0.00         87
---------------------
--micro avg                                                3.51       3.51       3.51      48633
--macro avg                                                0.35      10.00       0.68      48633
--weighted avg                                             0.12       3.51       0.24      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75eaf0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                          0.00       0.00       0.00        689
--- (label_id: 3)                                          3.53     100.00       6.83         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.00       0.00       0.00         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                3.53       3.53       3.53       1641
--macro avg                                                0.35      10.00       0.68       1641
--weighted avg                                             0.12       3.53       0.24       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75e160>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                          0.00       0.00       0.00        689
--- (label_id: 3)                                          3.53     100.00       6.83         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.00       0.00       0.00         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                3.53       3.53       3.53       1641
--macro avg                                                0.35      10.00       0.68       1641
--weighted avg                                             0.12       3.53       0.24       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                          0.00       0.00       0.00      19813
--- (label_id: 3)                                          3.51     100.00       6.79       1708
--. (label_id: 4)                                          0.00       0.00       0.00      18084
--: (label_id: 5)                                          0.00       0.00       0.00        348
--; (label_id: 6)                                          0.00       0.00       0.00        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.00       0.00       0.00         87
---------------------
--micro avg                                                3.51       3.51       3.51      48633
--macro avg                                                0.35      10.00       0.68      48633
--weighted avg                                             0.12       3.51       0.24      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       7470
--! (label_id: 1)                                          0.00       0.00       0.00        148
--, (label_id: 2)                                          0.00       0.00       0.00      28513
--- (label_id: 3)                                          3.02     100.00       5.86       2074
--. (label_id: 4)                                          0.00       0.00       0.00      25120
--: (label_id: 5)                                          0.00       0.00       0.00        570
--; (label_id: 6)                                          0.00       0.00       0.00        534
--? (label_id: 7)                                          0.00       0.00       0.00       2085
--— (label_id: 8)                                          0.00       0.00       0.00       2073
--… (label_id: 9)                                          0.00       0.00       0.00        142
---------------------
--micro avg                                                3.02       3.02       3.02      68729
--macro avg                                                0.30      10.00       0.59      68729
--weighted avg                                             0.09       3.02       0.18      68729
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       3735
---------------------
--micro avg                                              100.00     100.00     100.00       3735
--macro avg                                              100.00     100.00     100.00       3735
--weighted avg                                           100.00     100.00     100.00       3735
--
--[INFO] - Internal process exited
-+[INFO] - GPU available: True, used: False
-+[INFO] - TPU available: None, using: 0 TPU cores
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index 782fbfa..464595d 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -374,6 +374,11 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             scheduler_config = None
- 
-+        if 'delay_epochs' in optim_config:
-+            delay_epochs = optim_config.pop('delay_epochs')
-+        else:
-+            delay_epochs = 0
-+
-         # Check if caller provided optimizer name, default to Adam otherwise
-         optimizer_cls = optim_config.get('_target_', None)
- 
-@@ -455,7 +460,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
- 
-         # Try to instantiate scheduler for optimizer
-         self._scheduler = prepare_lr_scheduler(
--            optimizer=self._optimizer, scheduler_config=scheduler_config,
-+            optimizer=self._optimizer, delay_epochs=delay_epochs,scheduler_config=scheduler_config,
-             train_dataloader=pp({'num_samples' : self.train_size*self.hparams.model.dataset.train_ds.batch_size, 
-             'batch_size': self.hparams.model.dataset.train_ds.batch_size,
-             'drop_last' : self.hparams.model.dataset.drop_last})
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/hparams.yaml
deleted file mode 100644
index 767cea3..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/hparams.yaml
+++ /dev/null
@@ -1,106 +0,0 @@
-seed: 42
-trainer:
-  gpus: 0
-  num_nodes: 1
-  max_epochs: 8
-  max_steps: null
-  accumulate_grad_batches: 4
-  gradient_clip_val: 0
-  amp_level: O0
-  precision: 32
-  checkpoint_callback: false
-  logger: false
-  log_every_n_steps: 1
-  val_check_interval: 1.0
-  reload_dataloaders_every_epoch: true
-  resume_from_checkpoint: null
-exp_manager:
-  exp_dir: /home/nxingyu2/project/
-  name: Punctuation_with_Domain_discriminator
-  create_tensorboard_logger: true
-  create_checkpoint_callback: true
-base_path: /home/nxingyu2/data
-tmp_path: /home/nxingyu2/data/tmp
-model:
-  nemo_path: null
-  transformer_path: google/electra-small-discriminator
-  unfrozen: 0
-  maximum_unfrozen: 2
-  unfreeze_step: 1
-  punct_label_ids:
-  - ''
-  - '!'
-  - ','
-  - '-'
-  - .
-  - ':'
-  - ;
-  - '?'
-  - —
-  - …
-  punct_class_weights: true
-  dataset:
-    data_dir: /home/nxingyu2/data
-    labelled:
-    - /home/nxingyu2/data/ted_talks_processed
-    unlabelled: null
-    max_seq_length: 128
-    pad_label: ''
-    ignore_extra_tokens: false
-    ignore_start_end: false
-    use_cache: false
-    num_workers: 0
-    pin_memory: true
-    drop_last: false
-    num_labels: 10
-    num_domains: 1
-    test_unlabelled: true
-    train_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 8
-    validation_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 8
-  tokenizer:
-    tokenizer_name: google/electra-small-discriminator
-    vocab_file: null
-    tokenizer_model: null
-    special_tokens: null
-  language_model:
-    pretrained_model_name: google/electra-small-discriminator
-    lm_checkpoint: null
-    config_file: null
-    config: null
-  punct_head:
-    punct_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: dice
-  domain_head:
-    domain_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: cel
-    gamma: 0.1
-  dice_loss:
-    epsilon: 0.01
-    alpha: 3
-    macro_average: true
-  focal_loss:
-    gamma: 5
-  optim:
-    name: ranger
-    lr: 0.001
-    weight_decay: 0.0
-    delay_epochs: 6
-    sched:
-      name: CosineAnnealing
-      last_epoch: -1
-      monitor: val_loss
-      reduce_on_plateau: false
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/lightning_logs.txt
deleted file mode 100644
index 368bc65..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/lightning_logs.txt
+++ /dev/null
@@ -1,33 +0,0 @@
-GPU available: True, used: False
-TPU available: None, using: 0 TPU cores
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-35.9 K    Trainable params
-13.4 M    Non-trainable params
-13.5 M    Total params
-Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/lr_find_temp_model.ckpt
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-35.9 K    Trainable params
-13.4 M    Non-trainable params
-13.5 M    Total params
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/nemo_error_log.txt
deleted file mode 100644
index 74b9905..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/nemo_error_log.txt
+++ /dev/null
@@ -1,19 +0,0 @@
-[NeMo W 2021-02-06 09:54:19 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 09:54:19 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-06 09:54:30 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 09:54:30 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 09:54:34 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 09:54:43 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtools/optim/radam.py:62: UserWarning: This overload of addcmul_ is deprecated:
-    	addcmul_(Number value, Tensor tensor1, Tensor tensor2)
-    Consider using one of the following signatures instead:
-    	addcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/torch/csrc/utils/python_arg_parser.cpp:882.)
-      exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 465eb8c..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,21 +0,0 @@
-[NeMo W 2021-02-06 09:54:19 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo I 2021-02-06 09:54:19 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19
-[NeMo I 2021-02-06 09:54:19 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-06 09:54:19 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-06 09:54:30 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 09:54:30 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 09:54:34 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 09:54:43 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtools/optim/radam.py:62: UserWarning: This overload of addcmul_ is deprecated:
-    	addcmul_(Number value, Tensor tensor1, Tensor tensor2)
-    Consider using one of the following signatures instead:
-    	addcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/torch/csrc/utils/python_arg_parser.cpp:882.)
-      exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/git-info.log
deleted file mode 100644
index a9491ab..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/git-info.log
+++ /dev/null
@@ -1,678 +0,0 @@
-commit hash: cce540d4c95ad9ed32010552a7aa1fb2486658c8
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index 3d27dd7..e314c45 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,43 +1,43 @@
- seed: 42
- trainer:
--    gpus: 1 # the number of gpus, 0 for CPU
--    num_nodes: 1
--    max_epochs: 2
--    max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 4 # accumulates grads every k batches
--    gradient_clip_val: 0
--    amp_level: O1 # O1/O2 for mixed precision
--    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    accelerator: ddp
--    checkpoint_callback: false  # Provided by exp_manager
--    logger: false #false  # Provided by exp_manager
--    log_every_n_steps: 1  # Interval of logging.
--    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    resume_from_checkpoint: null
--
--    # gpus: 0 # the number of gpus, 0 for CPU
-+    # gpus: 1 # the number of gpus, 0 for CPU
-     # num_nodes: 1
--    # max_epochs: 10
-+    # max_epochs: 2
-     # max_steps: null # precedence over max_epochs
-     # accumulate_grad_batches: 4 # accumulates grads every k batches
-     # gradient_clip_val: 0
--    # amp_level: O0 # O1/O2 for mixed precision
--    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # # accelerator: ddp
-+    # amp_level: O1 # O1/O2 for mixed precision
-+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-     # checkpoint_callback: false  # Provided by exp_manager
-     # logger: false #false  # Provided by exp_manager
-     # log_every_n_steps: 1  # Interval of logging.
-     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # reload_dataloaders_every_epoch: true
-     # resume_from_checkpoint: null
- 
-+    gpus: 0 # the number of gpus, 0 for CPU
-+    num_nodes: 1
-+    max_epochs: 8
-+    max_steps: null # precedence over max_epochs
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-+    gradient_clip_val: 0
-+    amp_level: O0 # O1/O2 for mixed precision
-+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-+    checkpoint_callback: false  # Provided by exp_manager
-+    logger: false #false  # Provided by exp_manager
-+    log_every_n_steps: 1  # Interval of logging.
-+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    reload_dataloaders_every_epoch: true
-+    resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /root/data # /home/nxingyu2/data # 
--tmp_path: /tmp # /home/nxingyu2/data/tmp # 
-+base_path: /home/nxingyu2/data # /root/data # 
-+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
-@@ -61,7 +61,7 @@ model:
-     punct_class_weights: true
-     
-     dataset:
--        data_dir: /root/data # /home/nxingyu2/data # 
-+        data_dir: /home/nxingyu2/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -125,27 +125,27 @@ model:
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 4
-+        alpha: 3
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: adamw
-+        name: ranger
-         lr: 1e-3
-         weight_decay: 0.00
--
-         sched:
--            name: WarmupAnnealing #CyclicLR
-+            name: CosineAnnealing #CyclicLR
-             # Scheduler params
--            warmup_steps: null
--            warmup_ratio: 0.1
-+            # warmup_steps: 6
-+            # warmup_ratio: 0.1
-+            hold_steps: 6
-             last_epoch: -1
- 
-             # pytorch lightning args
-             monitor: val_loss
--            reduce_on_plateau: true
-+            reduce_on_plateau: false
- hydra:
-     run:
-         dir: .
-diff --git a/experiment/core/optim/lr_scheduler.py b/experiment/core/optim/lr_scheduler.py
-index a17ebf8..4c5f4a7 100644
---- a/experiment/core/optim/lr_scheduler.py
-+++ b/experiment/core/optim/lr_scheduler.py
-@@ -347,6 +347,29 @@ class PolynomialHoldDecayAnnealing(WarmupHoldPolicy):
-         return new_lrs
- 
- 
-+class CosineHoldDecayAnnealing(WarmupHoldPolicy):
-+    def __init__(self, optimizer, *, max_steps, min_lr=0.0, last_epoch=-1, **kwargs):
-+        super().__init__(optimizer=optimizer, max_steps=max_steps, last_epoch=last_epoch, min_lr=min_lr, **kwargs)
-+
-+    def _get_lr(self, step):
-+        for initial_lr in self.base_lrs:
-+            if initial_lr < self.min_lr:
-+                raise ValueError(
-+                    f"{self} received an initial learning rate that was lower than the minimum learning rate."
-+                )
-+
-+        new_lrs = [
-+            _cosine_annealing(
-+                initial_lr=initial_lr,
-+                step=step - self.hold_steps,
-+                max_steps=self.max_steps - max(self.warmup_steps, self.hold_steps),
-+                min_lr=self.min_lr,
-+            )
-+            for initial_lr in self.base_lrs
-+        ]
-+        return new_lrs
-+
-+
- def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: SchedulerParams):
-     """
-     Checks if the scheduler name exists in the registry, and if it doesnt, adds it.
-@@ -365,7 +388,7 @@ def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: Sch
-     register_scheduler_params(name=sched_name, scheduler_params=scheduler_params)
- 
- 
--def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
-+def get_scheduler(name: str, optimizer, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
-     """
-     Convenience method to obtain an _LRScheduler class and partially instantiate it with optimizer kwargs.
-     Args:
-@@ -654,4 +677,5 @@ AVAILABLE_SCHEDULERS = {
-     'ExponentialLR': pt_scheduler.ExponentialLR,
-     'ReduceLROnPlateau': pt_scheduler.ReduceLROnPlateau,
-     'CyclicLR': pt_scheduler.CyclicLR,
-+    'CosineHoldDecayAnnealing': CosineHoldDecayAnnealing,
- }
-diff --git a/experiment/core/optim/optimizers.py b/experiment/core/optim/optimizers.py
-index 5afc4fd..f15686e 100644
---- a/experiment/core/optim/optimizers.py
-+++ b/experiment/core/optim/optimizers.py
-@@ -7,6 +7,7 @@ import torch.optim as optim
- from omegaconf import DictConfig, OmegaConf
- from torch.optim import adadelta, adagrad, adamax, rmsprop, rprop
- from torch.optim.optimizer import Optimizer
-+from torchtools.optim import Ranger
- 
- from core.config import OptimizerParams, get_optimizer_config, register_optimizer_params
- from core.optim.novograd import Novograd
-@@ -24,6 +25,7 @@ AVAILABLE_OPTIMIZERS = {
-     'rmsprop': rmsprop.RMSprop,
-     'rprop': rprop.Rprop,
-     'novograd': Novograd,
-+    'ranger': Ranger,
- }
- 
- 
-diff --git a/experiment/info.log b/experiment/info.log
-index b211d42..5024c4f 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,473 +1,2 @@
--[INFO] - shuffling train set
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4da7f8e0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.008413951416451957
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e67aa00>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          22.42      11.12      14.86       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                         38.62       7.46      12.50      19530
--- (label_id: 3)                                          6.60      15.75       9.30       1746
--. (label_id: 4)                                         33.33       0.01       0.01      17976
--: (label_id: 5)                                          0.79      19.15       1.52        376
--; (label_id: 6)                                          0.45       2.94       0.78        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.17      50.56       0.33         89
---------------------
--micro avg                                                5.14       5.14       5.14      48233
--macro avg                                               10.24      10.70       3.93      48233
--weighted avg                                            30.92       5.14       7.15      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          22.09      11.27      14.93       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                         35.39       6.74      11.32      19813
--- (label_id: 3)                                          6.03      16.28       8.79       1708
--. (label_id: 4)                                        100.00       0.01       0.02      18084
--: (label_id: 5)                                          0.78      20.69       1.51        348
--; (label_id: 6)                                          0.45       2.55       0.77        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.19      58.62       0.38         87
---------------------
--micro avg                                                4.89       4.89       4.89      48633
--macro avg                                               16.49      11.62       3.77      48633
--weighted avg                                            54.38       4.89       6.67      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.008413951416451957
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e74d910>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          19.61      14.42      16.62        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         38.84       6.82      11.60        689
--- (label_id: 3)                                          4.50      15.52       6.98         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.59      20.00       1.14         10
--; (label_id: 6)                                          2.56      16.67       4.44          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.25      50.00       0.51          4
---------------------
--micro avg                                                5.55       5.55       5.55       1641
--macro avg                                                6.64      12.34       4.13       1641
--weighted avg                                            18.97       5.55       7.25       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.63      98.91       7.00       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          3.96       1.76       2.44       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.63       3.63       3.63      48233
--macro avg                                                0.76      10.07       0.94      48233
--weighted avg                                             0.25       3.63       0.33      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e68b8b0>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          19.61      14.42      16.62        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         38.84       6.82      11.60        689
--- (label_id: 3)                                          4.50      15.52       6.98         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.59      20.00       1.14         10
--; (label_id: 6)                                          2.56      16.67       4.44          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.25      50.00       0.51          4
---------------------
--micro avg                                                5.55       5.55       5.55       1641
--macro avg                                                6.64      12.34       4.13       1641
--weighted avg                                            18.97       5.55       7.25       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                          0.00       0.00       0.00      19813
--- (label_id: 3)                                          3.51     100.00       6.79       1708
--. (label_id: 4)                                          0.00       0.00       0.00      18084
--: (label_id: 5)                                          0.00       0.00       0.00        348
--; (label_id: 6)                                          0.00       0.00       0.00        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.00       0.00       0.00         87
---------------------
--micro avg                                                3.51       3.51       3.51      48633
--macro avg                                                0.35      10.00       0.68      48633
--weighted avg                                             0.12       3.51       0.24      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75eaf0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                          0.00       0.00       0.00        689
--- (label_id: 3)                                          3.53     100.00       6.83         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.00       0.00       0.00         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                3.53       3.53       3.53       1641
--macro avg                                                0.35      10.00       0.68       1641
--weighted avg                                             0.12       3.53       0.24       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75e160>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                          0.00       0.00       0.00        689
--- (label_id: 3)                                          3.53     100.00       6.83         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.00       0.00       0.00         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                3.53       3.53       3.53       1641
--macro avg                                                0.35      10.00       0.68       1641
--weighted avg                                             0.12       3.53       0.24       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                          0.00       0.00       0.00      19813
--- (label_id: 3)                                          3.51     100.00       6.79       1708
--. (label_id: 4)                                          0.00       0.00       0.00      18084
--: (label_id: 5)                                          0.00       0.00       0.00        348
--; (label_id: 6)                                          0.00       0.00       0.00        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.00       0.00       0.00         87
---------------------
--micro avg                                                3.51       3.51       3.51      48633
--macro avg                                                0.35      10.00       0.68      48633
--weighted avg                                             0.12       3.51       0.24      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       7470
--! (label_id: 1)                                          0.00       0.00       0.00        148
--, (label_id: 2)                                          0.00       0.00       0.00      28513
--- (label_id: 3)                                          3.02     100.00       5.86       2074
--. (label_id: 4)                                          0.00       0.00       0.00      25120
--: (label_id: 5)                                          0.00       0.00       0.00        570
--; (label_id: 6)                                          0.00       0.00       0.00        534
--? (label_id: 7)                                          0.00       0.00       0.00       2085
--— (label_id: 8)                                          0.00       0.00       0.00       2073
--… (label_id: 9)                                          0.00       0.00       0.00        142
---------------------
--micro avg                                                3.02       3.02       3.02      68729
--macro avg                                                0.30      10.00       0.59      68729
--weighted avg                                             0.09       3.02       0.18      68729
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       3735
---------------------
--micro avg                                              100.00     100.00     100.00       3735
--macro avg                                              100.00     100.00     100.00       3735
--weighted avg                                           100.00     100.00     100.00       3735
--
--[INFO] - Internal process exited
-+[INFO] - GPU available: True, used: False
-+[INFO] - TPU available: None, using: 0 TPU cores
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index 782fbfa..fa37b4c 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -374,6 +374,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             scheduler_config = None
- 
-+
-         # Check if caller provided optimizer name, default to Adam otherwise
-         optimizer_cls = optim_config.get('_target_', None)
- 
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/lightning_logs.txt
deleted file mode 100644
index 01e1664..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/lightning_logs.txt
+++ /dev/null
@@ -1,2 +0,0 @@
-GPU available: True, used: False
-TPU available: None, using: 0 TPU cores
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/nemo_error_log.txt
deleted file mode 100644
index 2ed3a0d..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/nemo_error_log.txt
+++ /dev/null
@@ -1,4 +0,0 @@
-[NeMo W 2021-02-06 10:21:49 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 10:21:49 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index bba476b..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,6 +0,0 @@
-[NeMo W 2021-02-06 10:21:49 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo I 2021-02-06 10:21:49 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49
-[NeMo I 2021-02-06 10:21:49 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-06 10:21:49 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/git-info.log
deleted file mode 100644
index 7e71a38..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/git-info.log
+++ /dev/null
@@ -1,678 +0,0 @@
-commit hash: cce540d4c95ad9ed32010552a7aa1fb2486658c8
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index 3d27dd7..652b9dc 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,43 +1,43 @@
- seed: 42
- trainer:
--    gpus: 1 # the number of gpus, 0 for CPU
--    num_nodes: 1
--    max_epochs: 2
--    max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 4 # accumulates grads every k batches
--    gradient_clip_val: 0
--    amp_level: O1 # O1/O2 for mixed precision
--    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    accelerator: ddp
--    checkpoint_callback: false  # Provided by exp_manager
--    logger: false #false  # Provided by exp_manager
--    log_every_n_steps: 1  # Interval of logging.
--    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    resume_from_checkpoint: null
--
--    # gpus: 0 # the number of gpus, 0 for CPU
-+    # gpus: 1 # the number of gpus, 0 for CPU
-     # num_nodes: 1
--    # max_epochs: 10
-+    # max_epochs: 2
-     # max_steps: null # precedence over max_epochs
-     # accumulate_grad_batches: 4 # accumulates grads every k batches
-     # gradient_clip_val: 0
--    # amp_level: O0 # O1/O2 for mixed precision
--    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # # accelerator: ddp
-+    # amp_level: O1 # O1/O2 for mixed precision
-+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-     # checkpoint_callback: false  # Provided by exp_manager
-     # logger: false #false  # Provided by exp_manager
-     # log_every_n_steps: 1  # Interval of logging.
-     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # reload_dataloaders_every_epoch: true
-     # resume_from_checkpoint: null
- 
-+    gpus: 0 # the number of gpus, 0 for CPU
-+    num_nodes: 1
-+    max_epochs: 8
-+    max_steps: null # precedence over max_epochs
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-+    gradient_clip_val: 0
-+    amp_level: O0 # O1/O2 for mixed precision
-+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-+    checkpoint_callback: false  # Provided by exp_manager
-+    logger: false #false  # Provided by exp_manager
-+    log_every_n_steps: 1  # Interval of logging.
-+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    reload_dataloaders_every_epoch: true
-+    resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /root/data # /home/nxingyu2/data # 
--tmp_path: /tmp # /home/nxingyu2/data/tmp # 
-+base_path: /home/nxingyu2/data # /root/data # 
-+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
-@@ -61,7 +61,7 @@ model:
-     punct_class_weights: true
-     
-     dataset:
--        data_dir: /root/data # /home/nxingyu2/data # 
-+        data_dir: /home/nxingyu2/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -125,27 +125,27 @@ model:
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 4
-+        alpha: 3
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: adamw
-+        name: ranger
-         lr: 1e-3
-         weight_decay: 0.00
--
-         sched:
--            name: WarmupAnnealing #CyclicLR
-+            name: CosineHoldDecayAnnealing #CyclicLR
-             # Scheduler params
--            warmup_steps: null
--            warmup_ratio: 0.1
-+            # warmup_steps: 6
-+            # warmup_ratio: 0.1
-+            hold_steps: 6
-             last_epoch: -1
- 
-             # pytorch lightning args
-             monitor: val_loss
--            reduce_on_plateau: true
-+            reduce_on_plateau: false
- hydra:
-     run:
-         dir: .
-diff --git a/experiment/core/optim/lr_scheduler.py b/experiment/core/optim/lr_scheduler.py
-index a17ebf8..4c5f4a7 100644
---- a/experiment/core/optim/lr_scheduler.py
-+++ b/experiment/core/optim/lr_scheduler.py
-@@ -347,6 +347,29 @@ class PolynomialHoldDecayAnnealing(WarmupHoldPolicy):
-         return new_lrs
- 
- 
-+class CosineHoldDecayAnnealing(WarmupHoldPolicy):
-+    def __init__(self, optimizer, *, max_steps, min_lr=0.0, last_epoch=-1, **kwargs):
-+        super().__init__(optimizer=optimizer, max_steps=max_steps, last_epoch=last_epoch, min_lr=min_lr, **kwargs)
-+
-+    def _get_lr(self, step):
-+        for initial_lr in self.base_lrs:
-+            if initial_lr < self.min_lr:
-+                raise ValueError(
-+                    f"{self} received an initial learning rate that was lower than the minimum learning rate."
-+                )
-+
-+        new_lrs = [
-+            _cosine_annealing(
-+                initial_lr=initial_lr,
-+                step=step - self.hold_steps,
-+                max_steps=self.max_steps - max(self.warmup_steps, self.hold_steps),
-+                min_lr=self.min_lr,
-+            )
-+            for initial_lr in self.base_lrs
-+        ]
-+        return new_lrs
-+
-+
- def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: SchedulerParams):
-     """
-     Checks if the scheduler name exists in the registry, and if it doesnt, adds it.
-@@ -365,7 +388,7 @@ def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: Sch
-     register_scheduler_params(name=sched_name, scheduler_params=scheduler_params)
- 
- 
--def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
-+def get_scheduler(name: str, optimizer, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
-     """
-     Convenience method to obtain an _LRScheduler class and partially instantiate it with optimizer kwargs.
-     Args:
-@@ -654,4 +677,5 @@ AVAILABLE_SCHEDULERS = {
-     'ExponentialLR': pt_scheduler.ExponentialLR,
-     'ReduceLROnPlateau': pt_scheduler.ReduceLROnPlateau,
-     'CyclicLR': pt_scheduler.CyclicLR,
-+    'CosineHoldDecayAnnealing': CosineHoldDecayAnnealing,
- }
-diff --git a/experiment/core/optim/optimizers.py b/experiment/core/optim/optimizers.py
-index 5afc4fd..f15686e 100644
---- a/experiment/core/optim/optimizers.py
-+++ b/experiment/core/optim/optimizers.py
-@@ -7,6 +7,7 @@ import torch.optim as optim
- from omegaconf import DictConfig, OmegaConf
- from torch.optim import adadelta, adagrad, adamax, rmsprop, rprop
- from torch.optim.optimizer import Optimizer
-+from torchtools.optim import Ranger
- 
- from core.config import OptimizerParams, get_optimizer_config, register_optimizer_params
- from core.optim.novograd import Novograd
-@@ -24,6 +25,7 @@ AVAILABLE_OPTIMIZERS = {
-     'rmsprop': rmsprop.RMSprop,
-     'rprop': rprop.Rprop,
-     'novograd': Novograd,
-+    'ranger': Ranger,
- }
- 
- 
-diff --git a/experiment/info.log b/experiment/info.log
-index b211d42..5024c4f 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,473 +1,2 @@
--[INFO] - shuffling train set
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4da7f8e0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.008413951416451957
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e67aa00>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          22.42      11.12      14.86       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                         38.62       7.46      12.50      19530
--- (label_id: 3)                                          6.60      15.75       9.30       1746
--. (label_id: 4)                                         33.33       0.01       0.01      17976
--: (label_id: 5)                                          0.79      19.15       1.52        376
--; (label_id: 6)                                          0.45       2.94       0.78        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.17      50.56       0.33         89
---------------------
--micro avg                                                5.14       5.14       5.14      48233
--macro avg                                               10.24      10.70       3.93      48233
--weighted avg                                            30.92       5.14       7.15      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          22.09      11.27      14.93       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                         35.39       6.74      11.32      19813
--- (label_id: 3)                                          6.03      16.28       8.79       1708
--. (label_id: 4)                                        100.00       0.01       0.02      18084
--: (label_id: 5)                                          0.78      20.69       1.51        348
--; (label_id: 6)                                          0.45       2.55       0.77        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.19      58.62       0.38         87
---------------------
--micro avg                                                4.89       4.89       4.89      48633
--macro avg                                               16.49      11.62       3.77      48633
--weighted avg                                            54.38       4.89       6.67      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.008413951416451957
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e74d910>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          19.61      14.42      16.62        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         38.84       6.82      11.60        689
--- (label_id: 3)                                          4.50      15.52       6.98         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.59      20.00       1.14         10
--; (label_id: 6)                                          2.56      16.67       4.44          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.25      50.00       0.51          4
---------------------
--micro avg                                                5.55       5.55       5.55       1641
--macro avg                                                6.64      12.34       4.13       1641
--weighted avg                                            18.97       5.55       7.25       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.63      98.91       7.00       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          3.96       1.76       2.44       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.63       3.63       3.63      48233
--macro avg                                                0.76      10.07       0.94      48233
--weighted avg                                             0.25       3.63       0.33      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e68b8b0>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          19.61      14.42      16.62        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         38.84       6.82      11.60        689
--- (label_id: 3)                                          4.50      15.52       6.98         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.59      20.00       1.14         10
--; (label_id: 6)                                          2.56      16.67       4.44          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.25      50.00       0.51          4
---------------------
--micro avg                                                5.55       5.55       5.55       1641
--macro avg                                                6.64      12.34       4.13       1641
--weighted avg                                            18.97       5.55       7.25       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                          0.00       0.00       0.00      19813
--- (label_id: 3)                                          3.51     100.00       6.79       1708
--. (label_id: 4)                                          0.00       0.00       0.00      18084
--: (label_id: 5)                                          0.00       0.00       0.00        348
--; (label_id: 6)                                          0.00       0.00       0.00        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.00       0.00       0.00         87
---------------------
--micro avg                                                3.51       3.51       3.51      48633
--macro avg                                                0.35      10.00       0.68      48633
--weighted avg                                             0.12       3.51       0.24      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75eaf0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                          0.00       0.00       0.00        689
--- (label_id: 3)                                          3.53     100.00       6.83         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.00       0.00       0.00         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                3.53       3.53       3.53       1641
--macro avg                                                0.35      10.00       0.68       1641
--weighted avg                                             0.12       3.53       0.24       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75e160>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                          0.00       0.00       0.00        689
--- (label_id: 3)                                          3.53     100.00       6.83         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.00       0.00       0.00         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                3.53       3.53       3.53       1641
--macro avg                                                0.35      10.00       0.68       1641
--weighted avg                                             0.12       3.53       0.24       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                          0.00       0.00       0.00      19813
--- (label_id: 3)                                          3.51     100.00       6.79       1708
--. (label_id: 4)                                          0.00       0.00       0.00      18084
--: (label_id: 5)                                          0.00       0.00       0.00        348
--; (label_id: 6)                                          0.00       0.00       0.00        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.00       0.00       0.00         87
---------------------
--micro avg                                                3.51       3.51       3.51      48633
--macro avg                                                0.35      10.00       0.68      48633
--weighted avg                                             0.12       3.51       0.24      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       7470
--! (label_id: 1)                                          0.00       0.00       0.00        148
--, (label_id: 2)                                          0.00       0.00       0.00      28513
--- (label_id: 3)                                          3.02     100.00       5.86       2074
--. (label_id: 4)                                          0.00       0.00       0.00      25120
--: (label_id: 5)                                          0.00       0.00       0.00        570
--; (label_id: 6)                                          0.00       0.00       0.00        534
--? (label_id: 7)                                          0.00       0.00       0.00       2085
--— (label_id: 8)                                          0.00       0.00       0.00       2073
--… (label_id: 9)                                          0.00       0.00       0.00        142
---------------------
--micro avg                                                3.02       3.02       3.02      68729
--macro avg                                                0.30      10.00       0.59      68729
--weighted avg                                             0.09       3.02       0.18      68729
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       3735
---------------------
--micro avg                                              100.00     100.00     100.00       3735
--macro avg                                              100.00     100.00     100.00       3735
--weighted avg                                           100.00     100.00     100.00       3735
--
--[INFO] - Internal process exited
-+[INFO] - GPU available: True, used: False
-+[INFO] - TPU available: None, using: 0 TPU cores
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index 782fbfa..fa37b4c 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -374,6 +374,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             scheduler_config = None
- 
-+
-         # Check if caller provided optimizer name, default to Adam otherwise
-         optimizer_cls = optim_config.get('_target_', None)
- 
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/lightning_logs.txt
deleted file mode 100644
index 01e1664..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/lightning_logs.txt
+++ /dev/null
@@ -1,2 +0,0 @@
-GPU available: True, used: False
-TPU available: None, using: 0 TPU cores
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/nemo_error_log.txt
deleted file mode 100644
index b057ce1..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/nemo_error_log.txt
+++ /dev/null
@@ -1,4 +0,0 @@
-[NeMo W 2021-02-06 10:22:46 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 10:22:46 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 02f3d52..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,6 +0,0 @@
-[NeMo W 2021-02-06 10:22:46 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo I 2021-02-06 10:22:46 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46
-[NeMo I 2021-02-06 10:22:46 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-06 10:22:46 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/events.out.tfevents.1612579094.intern-instance.28215.0 b/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/events.out.tfevents.1612579094.intern-instance.28215.0
deleted file mode 100644
index 134df00..0000000
Binary files a/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/events.out.tfevents.1612579094.intern-instance.28215.0 and /dev/null differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/git-info.log
deleted file mode 100644
index 8b49757..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/git-info.log
+++ /dev/null
@@ -1,669 +0,0 @@
-commit hash: cce540d4c95ad9ed32010552a7aa1fb2486658c8
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index 3d27dd7..652b9dc 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,43 +1,43 @@
- seed: 42
- trainer:
--    gpus: 1 # the number of gpus, 0 for CPU
--    num_nodes: 1
--    max_epochs: 2
--    max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 4 # accumulates grads every k batches
--    gradient_clip_val: 0
--    amp_level: O1 # O1/O2 for mixed precision
--    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    accelerator: ddp
--    checkpoint_callback: false  # Provided by exp_manager
--    logger: false #false  # Provided by exp_manager
--    log_every_n_steps: 1  # Interval of logging.
--    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    resume_from_checkpoint: null
--
--    # gpus: 0 # the number of gpus, 0 for CPU
-+    # gpus: 1 # the number of gpus, 0 for CPU
-     # num_nodes: 1
--    # max_epochs: 10
-+    # max_epochs: 2
-     # max_steps: null # precedence over max_epochs
-     # accumulate_grad_batches: 4 # accumulates grads every k batches
-     # gradient_clip_val: 0
--    # amp_level: O0 # O1/O2 for mixed precision
--    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # # accelerator: ddp
-+    # amp_level: O1 # O1/O2 for mixed precision
-+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-     # checkpoint_callback: false  # Provided by exp_manager
-     # logger: false #false  # Provided by exp_manager
-     # log_every_n_steps: 1  # Interval of logging.
-     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # reload_dataloaders_every_epoch: true
-     # resume_from_checkpoint: null
- 
-+    gpus: 0 # the number of gpus, 0 for CPU
-+    num_nodes: 1
-+    max_epochs: 8
-+    max_steps: null # precedence over max_epochs
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-+    gradient_clip_val: 0
-+    amp_level: O0 # O1/O2 for mixed precision
-+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-+    checkpoint_callback: false  # Provided by exp_manager
-+    logger: false #false  # Provided by exp_manager
-+    log_every_n_steps: 1  # Interval of logging.
-+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    reload_dataloaders_every_epoch: true
-+    resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /root/data # /home/nxingyu2/data # 
--tmp_path: /tmp # /home/nxingyu2/data/tmp # 
-+base_path: /home/nxingyu2/data # /root/data # 
-+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
-@@ -61,7 +61,7 @@ model:
-     punct_class_weights: true
-     
-     dataset:
--        data_dir: /root/data # /home/nxingyu2/data # 
-+        data_dir: /home/nxingyu2/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -125,27 +125,27 @@ model:
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 4
-+        alpha: 3
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: adamw
-+        name: ranger
-         lr: 1e-3
-         weight_decay: 0.00
--
-         sched:
--            name: WarmupAnnealing #CyclicLR
-+            name: CosineHoldDecayAnnealing #CyclicLR
-             # Scheduler params
--            warmup_steps: null
--            warmup_ratio: 0.1
-+            # warmup_steps: 6
-+            # warmup_ratio: 0.1
-+            hold_steps: 6
-             last_epoch: -1
- 
-             # pytorch lightning args
-             monitor: val_loss
--            reduce_on_plateau: true
-+            reduce_on_plateau: false
- hydra:
-     run:
-         dir: .
-diff --git a/experiment/core/optim/lr_scheduler.py b/experiment/core/optim/lr_scheduler.py
-index a17ebf8..0c7d449 100644
---- a/experiment/core/optim/lr_scheduler.py
-+++ b/experiment/core/optim/lr_scheduler.py
-@@ -347,6 +347,29 @@ class PolynomialHoldDecayAnnealing(WarmupHoldPolicy):
-         return new_lrs
- 
- 
-+class CosineHoldDecayAnnealing(WarmupHoldPolicy):
-+    def __init__(self, optimizer, *, max_steps, min_lr=0.0, last_epoch=-1, **kwargs):
-+        super().__init__(optimizer=optimizer, max_steps=max_steps, last_epoch=last_epoch, min_lr=min_lr, **kwargs)
-+
-+    def _get_lr(self, step):
-+        for initial_lr in self.base_lrs:
-+            if initial_lr < self.min_lr:
-+                raise ValueError(
-+                    f"{self} received an initial learning rate that was lower than the minimum learning rate."
-+                )
-+
-+        new_lrs = [
-+            _cosine_annealing(
-+                initial_lr=initial_lr,
-+                step=step - self.hold_steps,
-+                max_steps=self.max_steps - max(self.warmup_steps, self.hold_steps),
-+                min_lr=self.min_lr,
-+            )
-+            for initial_lr in self.base_lrs
-+        ]
-+        return new_lrs
-+
-+
- def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: SchedulerParams):
-     """
-     Checks if the scheduler name exists in the registry, and if it doesnt, adds it.
-@@ -654,4 +677,5 @@ AVAILABLE_SCHEDULERS = {
-     'ExponentialLR': pt_scheduler.ExponentialLR,
-     'ReduceLROnPlateau': pt_scheduler.ReduceLROnPlateau,
-     'CyclicLR': pt_scheduler.CyclicLR,
-+    'CosineHoldDecayAnnealing': CosineHoldDecayAnnealing,
- }
-diff --git a/experiment/core/optim/optimizers.py b/experiment/core/optim/optimizers.py
-index 5afc4fd..f15686e 100644
---- a/experiment/core/optim/optimizers.py
-+++ b/experiment/core/optim/optimizers.py
-@@ -7,6 +7,7 @@ import torch.optim as optim
- from omegaconf import DictConfig, OmegaConf
- from torch.optim import adadelta, adagrad, adamax, rmsprop, rprop
- from torch.optim.optimizer import Optimizer
-+from torchtools.optim import Ranger
- 
- from core.config import OptimizerParams, get_optimizer_config, register_optimizer_params
- from core.optim.novograd import Novograd
-@@ -24,6 +25,7 @@ AVAILABLE_OPTIMIZERS = {
-     'rmsprop': rmsprop.RMSprop,
-     'rprop': rprop.Rprop,
-     'novograd': Novograd,
-+    'ranger': Ranger,
- }
- 
- 
-diff --git a/experiment/info.log b/experiment/info.log
-index b211d42..5024c4f 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,473 +1,2 @@
--[INFO] - shuffling train set
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4da7f8e0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.008413951416451957
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e67aa00>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          22.42      11.12      14.86       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                         38.62       7.46      12.50      19530
--- (label_id: 3)                                          6.60      15.75       9.30       1746
--. (label_id: 4)                                         33.33       0.01       0.01      17976
--: (label_id: 5)                                          0.79      19.15       1.52        376
--; (label_id: 6)                                          0.45       2.94       0.78        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.17      50.56       0.33         89
---------------------
--micro avg                                                5.14       5.14       5.14      48233
--macro avg                                               10.24      10.70       3.93      48233
--weighted avg                                            30.92       5.14       7.15      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          22.09      11.27      14.93       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                         35.39       6.74      11.32      19813
--- (label_id: 3)                                          6.03      16.28       8.79       1708
--. (label_id: 4)                                        100.00       0.01       0.02      18084
--: (label_id: 5)                                          0.78      20.69       1.51        348
--; (label_id: 6)                                          0.45       2.55       0.77        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.19      58.62       0.38         87
---------------------
--micro avg                                                4.89       4.89       4.89      48633
--macro avg                                               16.49      11.62       3.77      48633
--weighted avg                                            54.38       4.89       6.67      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.008413951416451957
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e74d910>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          19.61      14.42      16.62        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         38.84       6.82      11.60        689
--- (label_id: 3)                                          4.50      15.52       6.98         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.59      20.00       1.14         10
--; (label_id: 6)                                          2.56      16.67       4.44          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.25      50.00       0.51          4
---------------------
--micro avg                                                5.55       5.55       5.55       1641
--macro avg                                                6.64      12.34       4.13       1641
--weighted avg                                            18.97       5.55       7.25       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.63      98.91       7.00       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          3.96       1.76       2.44       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.63       3.63       3.63      48233
--macro avg                                                0.76      10.07       0.94      48233
--weighted avg                                             0.25       3.63       0.33      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e68b8b0>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          19.61      14.42      16.62        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         38.84       6.82      11.60        689
--- (label_id: 3)                                          4.50      15.52       6.98         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.59      20.00       1.14         10
--; (label_id: 6)                                          2.56      16.67       4.44          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.25      50.00       0.51          4
---------------------
--micro avg                                                5.55       5.55       5.55       1641
--macro avg                                                6.64      12.34       4.13       1641
--weighted avg                                            18.97       5.55       7.25       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                          0.00       0.00       0.00      19813
--- (label_id: 3)                                          3.51     100.00       6.79       1708
--. (label_id: 4)                                          0.00       0.00       0.00      18084
--: (label_id: 5)                                          0.00       0.00       0.00        348
--; (label_id: 6)                                          0.00       0.00       0.00        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.00       0.00       0.00         87
---------------------
--micro avg                                                3.51       3.51       3.51      48633
--macro avg                                                0.35      10.00       0.68      48633
--weighted avg                                             0.12       3.51       0.24      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75eaf0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                          0.00       0.00       0.00        689
--- (label_id: 3)                                          3.53     100.00       6.83         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.00       0.00       0.00         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                3.53       3.53       3.53       1641
--macro avg                                                0.35      10.00       0.68       1641
--weighted avg                                             0.12       3.53       0.24       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75e160>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                          0.00       0.00       0.00        689
--- (label_id: 3)                                          3.53     100.00       6.83         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.00       0.00       0.00         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                3.53       3.53       3.53       1641
--macro avg                                                0.35      10.00       0.68       1641
--weighted avg                                             0.12       3.53       0.24       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                          0.00       0.00       0.00      19813
--- (label_id: 3)                                          3.51     100.00       6.79       1708
--. (label_id: 4)                                          0.00       0.00       0.00      18084
--: (label_id: 5)                                          0.00       0.00       0.00        348
--; (label_id: 6)                                          0.00       0.00       0.00        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.00       0.00       0.00         87
---------------------
--micro avg                                                3.51       3.51       3.51      48633
--macro avg                                                0.35      10.00       0.68      48633
--weighted avg                                             0.12       3.51       0.24      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       7470
--! (label_id: 1)                                          0.00       0.00       0.00        148
--, (label_id: 2)                                          0.00       0.00       0.00      28513
--- (label_id: 3)                                          3.02     100.00       5.86       2074
--. (label_id: 4)                                          0.00       0.00       0.00      25120
--: (label_id: 5)                                          0.00       0.00       0.00        570
--; (label_id: 6)                                          0.00       0.00       0.00        534
--? (label_id: 7)                                          0.00       0.00       0.00       2085
--— (label_id: 8)                                          0.00       0.00       0.00       2073
--… (label_id: 9)                                          0.00       0.00       0.00        142
---------------------
--micro avg                                                3.02       3.02       3.02      68729
--macro avg                                                0.30      10.00       0.59      68729
--weighted avg                                             0.09       3.02       0.18      68729
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       3735
---------------------
--micro avg                                              100.00     100.00     100.00       3735
--macro avg                                              100.00     100.00     100.00       3735
--weighted avg                                           100.00     100.00     100.00       3735
--
--[INFO] - Internal process exited
-+[INFO] - GPU available: True, used: False
-+[INFO] - TPU available: None, using: 0 TPU cores
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index 782fbfa..fa37b4c 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -374,6 +374,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             scheduler_config = None
- 
-+
-         # Check if caller provided optimizer name, default to Adam otherwise
-         optimizer_cls = optim_config.get('_target_', None)
- 
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/hparams.yaml
deleted file mode 100644
index 3d76cb3..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/hparams.yaml
+++ /dev/null
@@ -1,106 +0,0 @@
-seed: 42
-trainer:
-  gpus: 0
-  num_nodes: 1
-  max_epochs: 8
-  max_steps: null
-  accumulate_grad_batches: 4
-  gradient_clip_val: 0
-  amp_level: O0
-  precision: 32
-  checkpoint_callback: false
-  logger: false
-  log_every_n_steps: 1
-  val_check_interval: 1.0
-  reload_dataloaders_every_epoch: true
-  resume_from_checkpoint: null
-exp_manager:
-  exp_dir: /home/nxingyu2/project/
-  name: Punctuation_with_Domain_discriminator
-  create_tensorboard_logger: true
-  create_checkpoint_callback: true
-base_path: /home/nxingyu2/data
-tmp_path: /home/nxingyu2/data/tmp
-model:
-  nemo_path: null
-  transformer_path: google/electra-small-discriminator
-  unfrozen: 0
-  maximum_unfrozen: 2
-  unfreeze_step: 1
-  punct_label_ids:
-  - ''
-  - '!'
-  - ','
-  - '-'
-  - .
-  - ':'
-  - ;
-  - '?'
-  - —
-  - …
-  punct_class_weights: true
-  dataset:
-    data_dir: /home/nxingyu2/data
-    labelled:
-    - /home/nxingyu2/data/ted_talks_processed
-    unlabelled: null
-    max_seq_length: 128
-    pad_label: ''
-    ignore_extra_tokens: false
-    ignore_start_end: false
-    use_cache: false
-    num_workers: 0
-    pin_memory: true
-    drop_last: false
-    num_labels: 10
-    num_domains: 1
-    test_unlabelled: true
-    train_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 8
-    validation_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 8
-  tokenizer:
-    tokenizer_name: google/electra-small-discriminator
-    vocab_file: null
-    tokenizer_model: null
-    special_tokens: null
-  language_model:
-    pretrained_model_name: google/electra-small-discriminator
-    lm_checkpoint: null
-    config_file: null
-    config: null
-  punct_head:
-    punct_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: dice
-  domain_head:
-    domain_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: cel
-    gamma: 0.1
-  dice_loss:
-    epsilon: 0.01
-    alpha: 3
-    macro_average: true
-  focal_loss:
-    gamma: 5
-  optim:
-    name: ranger
-    lr: 0.001
-    weight_decay: 0.0
-    sched:
-      name: CosineHoldDecayAnnealing
-      hold_steps: 6
-      last_epoch: -1
-      monitor: val_loss
-      reduce_on_plateau: false
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/lightning_logs.txt
deleted file mode 100644
index 54b8b13..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/lightning_logs.txt
+++ /dev/null
@@ -1,115 +0,0 @@
-GPU available: True, used: False
-TPU available: None, using: 0 TPU cores
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-35.9 K    Trainable params
-13.4 M    Non-trainable params
-13.5 M    Total params
-Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/lr_find_temp_model.ckpt
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-35.9 K    Trainable params
-13.4 M    Non-trainable params
-13.5 M    Total params
-Epoch 0, global step 100: val_loss reached 0.85947 (best 0.85947), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
-Epoch 1, global step 200: val_loss reached 0.85987 (best 0.85947), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=1.ckpt" as top 3
-Epoch 2, global step 300: val_loss reached 0.86472 (best 0.85947), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
-Epoch 3, step 400: val_loss was not in top 3
-Epoch 4, global step 500: val_loss reached 0.86165 (best 0.85947), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
-Epoch 5, step 600: val_loss was not in top 3
-Epoch 6, global step 700: val_loss reached 0.85772 (best 0.85772), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=6.ckpt" as top 3
-Epoch 7, step 800: val_loss was not in top 3
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-825 K     Trainable params
-12.7 M    Non-trainable params
-13.5 M    Total params
-LR finder stopped early due to diverging loss.
-Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/lr_find_temp_model.ckpt
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-825 K     Trainable params
-12.7 M    Non-trainable params
-13.5 M    Total params
-Epoch 0, global step 901: val_loss reached 0.32063 (best 0.32063), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.32-epoch=0.ckpt" as top 3
-Epoch 1, global step 1001: val_loss reached 0.32083 (best 0.32063), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.32-epoch=1.ckpt" as top 3
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-1.6 M     Trainable params
-11.9 M    Non-trainable params
-13.5 M    Total params
-LR finder stopped early due to diverging loss.
-Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/lr_find_temp_model.ckpt
-Failed to compute suggesting for `lr`. There might not be enough points.
-Traceback (most recent call last):
-  File "/home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 353, in suggestion
-    min_grad = np.gradient(loss).argmin()
-  File "<__array_function__ internals>", line 5, in gradient
-  File "/home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
-    raise ValueError(
-ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-1.6 M     Trainable params
-11.9 M    Non-trainable params
-13.5 M    Total params
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/nemo_error_log.txt
deleted file mode 100644
index e55ddc0..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/nemo_error_log.txt
+++ /dev/null
@@ -1,39 +0,0 @@
-[NeMo W 2021-02-06 10:29:26 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 10:29:26 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-06 10:29:37 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 10:29:37 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 10:29:41 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 10:29:50 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtools/optim/radam.py:62: UserWarning: This overload of addcmul_ is deprecated:
-    	addcmul_(Number value, Tensor tensor1, Tensor tensor2)
-    Consider using one of the following signatures instead:
-    	addcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/torch/csrc/utils/python_arg_parser.cpp:882.)
-      exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
-    
-[NeMo W 2021-02-06 10:49:14 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7efe5910e490> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-06 10:49:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7efe5910eeb0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-06 12:19:30 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 12:45:19 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-      warnings.warn(*args, **kwargs)
-    
-Failed to compute suggesting for `lr`. There might not be enough points.
-Traceback (most recent call last):
-  File "/home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 353, in suggestion
-    min_grad = np.gradient(loss).argmin()
-  File "<__array_function__ internals>", line 5, in gradient
-  File "/home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
-    raise ValueError(
-ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 21c5e73..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,33 +0,0 @@
-[NeMo W 2021-02-06 10:29:26 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo I 2021-02-06 10:29:26 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26
-[NeMo I 2021-02-06 10:29:26 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-06 10:29:26 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-06 10:29:37 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 10:29:37 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 10:29:41 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 10:29:50 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtools/optim/radam.py:62: UserWarning: This overload of addcmul_ is deprecated:
-    	addcmul_(Number value, Tensor tensor1, Tensor tensor2)
-    Consider using one of the following signatures instead:
-    	addcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/torch/csrc/utils/python_arg_parser.cpp:882.)
-      exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
-    
-[NeMo W 2021-02-06 10:49:14 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7efe5910e490> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-06 10:49:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7efe5910eeb0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-06 12:19:30 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 12:45:19 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/events.out.tfevents.1612587358.intern-instance.11054.0 b/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/events.out.tfevents.1612587358.intern-instance.11054.0
deleted file mode 100644
index 53c6ce5..0000000
Binary files a/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/events.out.tfevents.1612587358.intern-instance.11054.0 and /dev/null differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/git-info.log
deleted file mode 100644
index 11d90e2..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/git-info.log
+++ /dev/null
@@ -1,692 +0,0 @@
-commit hash: cce540d4c95ad9ed32010552a7aa1fb2486658c8
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index 3d27dd7..a7f721a 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,43 +1,43 @@
- seed: 42
- trainer:
--    gpus: 1 # the number of gpus, 0 for CPU
--    num_nodes: 1
--    max_epochs: 2
--    max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 4 # accumulates grads every k batches
--    gradient_clip_val: 0
--    amp_level: O1 # O1/O2 for mixed precision
--    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    accelerator: ddp
--    checkpoint_callback: false  # Provided by exp_manager
--    logger: false #false  # Provided by exp_manager
--    log_every_n_steps: 1  # Interval of logging.
--    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    resume_from_checkpoint: null
--
--    # gpus: 0 # the number of gpus, 0 for CPU
-+    # gpus: 1 # the number of gpus, 0 for CPU
-     # num_nodes: 1
--    # max_epochs: 10
-+    # max_epochs: 2
-     # max_steps: null # precedence over max_epochs
-     # accumulate_grad_batches: 4 # accumulates grads every k batches
-     # gradient_clip_val: 0
--    # amp_level: O0 # O1/O2 for mixed precision
--    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # # accelerator: ddp
-+    # amp_level: O1 # O1/O2 for mixed precision
-+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-     # checkpoint_callback: false  # Provided by exp_manager
-     # logger: false #false  # Provided by exp_manager
-     # log_every_n_steps: 1  # Interval of logging.
-     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # reload_dataloaders_every_epoch: true
-     # resume_from_checkpoint: null
- 
-+    gpus: 0 # the number of gpus, 0 for CPU
-+    num_nodes: 1
-+    max_epochs: 8
-+    max_steps: null # precedence over max_epochs
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-+    gradient_clip_val: 0
-+    amp_level: O0 # O1/O2 for mixed precision
-+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-+    checkpoint_callback: false  # Provided by exp_manager
-+    logger: false #false  # Provided by exp_manager
-+    log_every_n_steps: 1  # Interval of logging.
-+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    reload_dataloaders_every_epoch: true
-+    resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /root/data # /home/nxingyu2/data # 
--tmp_path: /tmp # /home/nxingyu2/data/tmp # 
-+base_path: /home/nxingyu2/data # /root/data # 
-+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
-@@ -61,7 +61,7 @@ model:
-     punct_class_weights: true
-     
-     dataset:
--        data_dir: /root/data # /home/nxingyu2/data # 
-+        data_dir: /home/nxingyu2/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -125,27 +125,27 @@ model:
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 4
-+        alpha: 3
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: adamw
-+        name: ranger
-         lr: 1e-3
-         weight_decay: 0.00
--
-         sched:
-             name: WarmupAnnealing #CyclicLR
-             # Scheduler params
-             warmup_steps: null
-             warmup_ratio: 0.1
-+            # hold_steps: 6
-             last_epoch: -1
- 
-             # pytorch lightning args
-             monitor: val_loss
--            reduce_on_plateau: true
-+            reduce_on_plateau: false
- hydra:
-     run:
-         dir: .
-diff --git a/experiment/core/optim/lr_scheduler.py b/experiment/core/optim/lr_scheduler.py
-index a17ebf8..0c7d449 100644
---- a/experiment/core/optim/lr_scheduler.py
-+++ b/experiment/core/optim/lr_scheduler.py
-@@ -347,6 +347,29 @@ class PolynomialHoldDecayAnnealing(WarmupHoldPolicy):
-         return new_lrs
- 
- 
-+class CosineHoldDecayAnnealing(WarmupHoldPolicy):
-+    def __init__(self, optimizer, *, max_steps, min_lr=0.0, last_epoch=-1, **kwargs):
-+        super().__init__(optimizer=optimizer, max_steps=max_steps, last_epoch=last_epoch, min_lr=min_lr, **kwargs)
-+
-+    def _get_lr(self, step):
-+        for initial_lr in self.base_lrs:
-+            if initial_lr < self.min_lr:
-+                raise ValueError(
-+                    f"{self} received an initial learning rate that was lower than the minimum learning rate."
-+                )
-+
-+        new_lrs = [
-+            _cosine_annealing(
-+                initial_lr=initial_lr,
-+                step=step - self.hold_steps,
-+                max_steps=self.max_steps - max(self.warmup_steps, self.hold_steps),
-+                min_lr=self.min_lr,
-+            )
-+            for initial_lr in self.base_lrs
-+        ]
-+        return new_lrs
-+
-+
- def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: SchedulerParams):
-     """
-     Checks if the scheduler name exists in the registry, and if it doesnt, adds it.
-@@ -654,4 +677,5 @@ AVAILABLE_SCHEDULERS = {
-     'ExponentialLR': pt_scheduler.ExponentialLR,
-     'ReduceLROnPlateau': pt_scheduler.ReduceLROnPlateau,
-     'CyclicLR': pt_scheduler.CyclicLR,
-+    'CosineHoldDecayAnnealing': CosineHoldDecayAnnealing,
- }
-diff --git a/experiment/core/optim/optimizers.py b/experiment/core/optim/optimizers.py
-index 5afc4fd..f15686e 100644
---- a/experiment/core/optim/optimizers.py
-+++ b/experiment/core/optim/optimizers.py
-@@ -7,6 +7,7 @@ import torch.optim as optim
- from omegaconf import DictConfig, OmegaConf
- from torch.optim import adadelta, adagrad, adamax, rmsprop, rprop
- from torch.optim.optimizer import Optimizer
-+from torchtools.optim import Ranger
- 
- from core.config import OptimizerParams, get_optimizer_config, register_optimizer_params
- from core.optim.novograd import Novograd
-@@ -24,6 +25,7 @@ AVAILABLE_OPTIMIZERS = {
-     'rmsprop': rmsprop.RMSprop,
-     'rprop': rprop.Rprop,
-     'novograd': Novograd,
-+    'ranger': Ranger,
- }
- 
- 
-diff --git a/experiment/info.log b/experiment/info.log
-index b211d42..5024c4f 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,473 +1,2 @@
--[INFO] - shuffling train set
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4da7f8e0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.008413951416451957
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e67aa00>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          22.42      11.12      14.86       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                         38.62       7.46      12.50      19530
--- (label_id: 3)                                          6.60      15.75       9.30       1746
--. (label_id: 4)                                         33.33       0.01       0.01      17976
--: (label_id: 5)                                          0.79      19.15       1.52        376
--; (label_id: 6)                                          0.45       2.94       0.78        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.17      50.56       0.33         89
---------------------
--micro avg                                                5.14       5.14       5.14      48233
--macro avg                                               10.24      10.70       3.93      48233
--weighted avg                                            30.92       5.14       7.15      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          22.09      11.27      14.93       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                         35.39       6.74      11.32      19813
--- (label_id: 3)                                          6.03      16.28       8.79       1708
--. (label_id: 4)                                        100.00       0.01       0.02      18084
--: (label_id: 5)                                          0.78      20.69       1.51        348
--; (label_id: 6)                                          0.45       2.55       0.77        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.19      58.62       0.38         87
---------------------
--micro avg                                                4.89       4.89       4.89      48633
--macro avg                                               16.49      11.62       3.77      48633
--weighted avg                                            54.38       4.89       6.67      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.008413951416451957
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e74d910>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          19.61      14.42      16.62        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         38.84       6.82      11.60        689
--- (label_id: 3)                                          4.50      15.52       6.98         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.59      20.00       1.14         10
--; (label_id: 6)                                          2.56      16.67       4.44          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.25      50.00       0.51          4
---------------------
--micro avg                                                5.55       5.55       5.55       1641
--macro avg                                                6.64      12.34       4.13       1641
--weighted avg                                            18.97       5.55       7.25       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.63      98.91       7.00       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          3.96       1.76       2.44       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.63       3.63       3.63      48233
--macro avg                                                0.76      10.07       0.94      48233
--weighted avg                                             0.25       3.63       0.33      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e68b8b0>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          19.61      14.42      16.62        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         38.84       6.82      11.60        689
--- (label_id: 3)                                          4.50      15.52       6.98         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.59      20.00       1.14         10
--; (label_id: 6)                                          2.56      16.67       4.44          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.25      50.00       0.51          4
---------------------
--micro avg                                                5.55       5.55       5.55       1641
--macro avg                                                6.64      12.34       4.13       1641
--weighted avg                                            18.97       5.55       7.25       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                          0.00       0.00       0.00      19813
--- (label_id: 3)                                          3.51     100.00       6.79       1708
--. (label_id: 4)                                          0.00       0.00       0.00      18084
--: (label_id: 5)                                          0.00       0.00       0.00        348
--; (label_id: 6)                                          0.00       0.00       0.00        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.00       0.00       0.00         87
---------------------
--micro avg                                                3.51       3.51       3.51      48633
--macro avg                                                0.35      10.00       0.68      48633
--weighted avg                                             0.12       3.51       0.24      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75eaf0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                          0.00       0.00       0.00        689
--- (label_id: 3)                                          3.53     100.00       6.83         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.00       0.00       0.00         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                3.53       3.53       3.53       1641
--macro avg                                                0.35      10.00       0.68       1641
--weighted avg                                             0.12       3.53       0.24       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75e160>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                          0.00       0.00       0.00        689
--- (label_id: 3)                                          3.53     100.00       6.83         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.00       0.00       0.00         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                3.53       3.53       3.53       1641
--macro avg                                                0.35      10.00       0.68       1641
--weighted avg                                             0.12       3.53       0.24       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                          0.00       0.00       0.00      19813
--- (label_id: 3)                                          3.51     100.00       6.79       1708
--. (label_id: 4)                                          0.00       0.00       0.00      18084
--: (label_id: 5)                                          0.00       0.00       0.00        348
--; (label_id: 6)                                          0.00       0.00       0.00        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.00       0.00       0.00         87
---------------------
--micro avg                                                3.51       3.51       3.51      48633
--macro avg                                                0.35      10.00       0.68      48633
--weighted avg                                             0.12       3.51       0.24      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       7470
--! (label_id: 1)                                          0.00       0.00       0.00        148
--, (label_id: 2)                                          0.00       0.00       0.00      28513
--- (label_id: 3)                                          3.02     100.00       5.86       2074
--. (label_id: 4)                                          0.00       0.00       0.00      25120
--: (label_id: 5)                                          0.00       0.00       0.00        570
--; (label_id: 6)                                          0.00       0.00       0.00        534
--? (label_id: 7)                                          0.00       0.00       0.00       2085
--— (label_id: 8)                                          0.00       0.00       0.00       2073
--… (label_id: 9)                                          0.00       0.00       0.00        142
---------------------
--micro avg                                                3.02       3.02       3.02      68729
--macro avg                                                0.30      10.00       0.59      68729
--weighted avg                                             0.09       3.02       0.18      68729
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       3735
---------------------
--micro avg                                              100.00     100.00     100.00       3735
--macro avg                                              100.00     100.00     100.00       3735
--weighted avg                                           100.00     100.00     100.00       3735
--
--[INFO] - Internal process exited
-+[INFO] - GPU available: True, used: False
-+[INFO] - TPU available: None, using: 0 TPU cores
-diff --git a/experiment/main.py b/experiment/main.py
-index cd4ca27..6f0a8ea 100644
---- a/experiment/main.py
-+++ b/experiment/main.py
-@@ -39,7 +39,7 @@ def main(cfg: DictConfig)->None:
-     
-     while(model.hparams.model.unfrozen<=cfg.model.maximum_unfrozen and model.hparams.model.unfrozen>=0):
-         trainer.current_epoch=0
--        lr_finder = trainer.tuner.lr_find(model,min_lr=1e-08, max_lr=1e-02, num_training=80, early_stop_threshold=None)
-+        lr_finder = trainer.tuner.lr_find(model,min_lr=1e-10, max_lr=1e-02, num_training=80, early_stop_threshold=None)
-         # Results can be found in
-         pp(lr_finder.results)
-         new_lr = lr_finder.suggestion()
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index 782fbfa..fa37b4c 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -374,6 +374,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             scheduler_config = None
- 
-+
-         # Check if caller provided optimizer name, default to Adam otherwise
-         optimizer_cls = optim_config.get('_target_', None)
- 
-diff --git a/linuxcommands.txt b/linuxcommands.txt
-index e72bad1..adc1fb0 100644
---- a/linuxcommands.txt
-+++ b/linuxcommands.txt
-@@ -48,7 +48,7 @@ git clone git@github.com:ngxingyu/dotfiles.git
- echo 'alias nv="~/nvim.appimage"' >> ~/.bashrc
- source ~/.profile
- 
--
-+ls -b | head -30 | xargs ls -d
- 
- 
- conda install script
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/hparams.yaml
deleted file mode 100644
index 75d484c..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/hparams.yaml
+++ /dev/null
@@ -1,107 +0,0 @@
-seed: 42
-trainer:
-  gpus: 0
-  num_nodes: 1
-  max_epochs: 8
-  max_steps: null
-  accumulate_grad_batches: 4
-  gradient_clip_val: 0
-  amp_level: O0
-  precision: 32
-  checkpoint_callback: false
-  logger: false
-  log_every_n_steps: 1
-  val_check_interval: 1.0
-  reload_dataloaders_every_epoch: true
-  resume_from_checkpoint: null
-exp_manager:
-  exp_dir: /home/nxingyu2/project/
-  name: Punctuation_with_Domain_discriminator
-  create_tensorboard_logger: true
-  create_checkpoint_callback: true
-base_path: /home/nxingyu2/data
-tmp_path: /home/nxingyu2/data/tmp
-model:
-  nemo_path: null
-  transformer_path: google/electra-small-discriminator
-  unfrozen: 0
-  maximum_unfrozen: 2
-  unfreeze_step: 1
-  punct_label_ids:
-  - ''
-  - '!'
-  - ','
-  - '-'
-  - .
-  - ':'
-  - ;
-  - '?'
-  - —
-  - …
-  punct_class_weights: true
-  dataset:
-    data_dir: /home/nxingyu2/data
-    labelled:
-    - /home/nxingyu2/data/ted_talks_processed
-    unlabelled: null
-    max_seq_length: 128
-    pad_label: ''
-    ignore_extra_tokens: false
-    ignore_start_end: false
-    use_cache: false
-    num_workers: 0
-    pin_memory: true
-    drop_last: false
-    num_labels: 10
-    num_domains: 1
-    test_unlabelled: true
-    train_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 8
-    validation_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 8
-  tokenizer:
-    tokenizer_name: google/electra-small-discriminator
-    vocab_file: null
-    tokenizer_model: null
-    special_tokens: null
-  language_model:
-    pretrained_model_name: google/electra-small-discriminator
-    lm_checkpoint: null
-    config_file: null
-    config: null
-  punct_head:
-    punct_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: dice
-  domain_head:
-    domain_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: cel
-    gamma: 0.1
-  dice_loss:
-    epsilon: 0.01
-    alpha: 3
-    macro_average: true
-  focal_loss:
-    gamma: 5
-  optim:
-    name: ranger
-    lr: 0.001
-    weight_decay: 0.0
-    sched:
-      name: WarmupAnnealing
-      warmup_steps: null
-      warmup_ratio: 0.1
-      last_epoch: -1
-      monitor: val_loss
-      reduce_on_plateau: false
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/lightning_logs.txt
deleted file mode 100644
index 9e41c86..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/lightning_logs.txt
+++ /dev/null
@@ -1,79 +0,0 @@
-GPU available: True, used: False
-TPU available: None, using: 0 TPU cores
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-35.9 K    Trainable params
-13.4 M    Non-trainable params
-13.5 M    Total params
-Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/lr_find_temp_model.ckpt
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-35.9 K    Trainable params
-13.4 M    Non-trainable params
-13.5 M    Total params
-Epoch 0, global step 100: val_loss reached 0.85947 (best 0.85947), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
-Epoch 1, global step 200: val_loss reached 0.85987 (best 0.85947), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=1.ckpt" as top 3
-Epoch 2, global step 300: val_loss reached 0.86473 (best 0.85947), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
-Epoch 3, step 400: val_loss was not in top 3
-Epoch 4, global step 500: val_loss reached 0.86166 (best 0.85947), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
-Epoch 5, step 600: val_loss was not in top 3
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-825 K     Trainable params
-12.7 M    Non-trainable params
-13.5 M    Total params
-LR finder stopped early due to diverging loss.
-Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/lr_find_temp_model.ckpt
-Failed to compute suggesting for `lr`. There might not be enough points.
-Traceback (most recent call last):
-  File "/home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 353, in suggestion
-    min_grad = np.gradient(loss).argmin()
-  File "<__array_function__ internals>", line 5, in gradient
-  File "/home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
-    raise ValueError(
-ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-825 K     Trainable params
-12.7 M    Non-trainable params
-13.5 M    Total params
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/nemo_error_log.txt
deleted file mode 100644
index dc28782..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/nemo_error_log.txt
+++ /dev/null
@@ -1,39 +0,0 @@
-[NeMo W 2021-02-06 12:47:20 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 12:47:20 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-06 12:47:30 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 12:47:30 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 12:47:35 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 12:47:43 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtools/optim/radam.py:62: UserWarning: This overload of addcmul_ is deprecated:
-    	addcmul_(Number value, Tensor tensor1, Tensor tensor2)
-    Consider using one of the following signatures instead:
-    	addcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/torch/csrc/utils/python_arg_parser.cpp:882.)
-      exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
-    
-[NeMo W 2021-02-06 13:06:22 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f82e4ea6af0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-06 13:06:50 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f82e4f64af0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-06 14:00:15 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 14:00:27 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
-      warnings.warn(*args, **kwargs)
-    
-Failed to compute suggesting for `lr`. There might not be enough points.
-Traceback (most recent call last):
-  File "/home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 353, in suggestion
-    min_grad = np.gradient(loss).argmin()
-  File "<__array_function__ internals>", line 5, in gradient
-  File "/home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
-    raise ValueError(
-ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index cdb87fd..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,33 +0,0 @@
-[NeMo W 2021-02-06 12:47:20 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo I 2021-02-06 12:47:20 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20
-[NeMo I 2021-02-06 12:47:20 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-06 12:47:20 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-06 12:47:30 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 12:47:30 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 12:47:35 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 12:47:43 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtools/optim/radam.py:62: UserWarning: This overload of addcmul_ is deprecated:
-    	addcmul_(Number value, Tensor tensor1, Tensor tensor2)
-    Consider using one of the following signatures instead:
-    	addcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/torch/csrc/utils/python_arg_parser.cpp:882.)
-      exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
-    
-[NeMo W 2021-02-06 13:06:22 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f82e4ea6af0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-06 13:06:50 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f82e4f64af0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-06 14:00:15 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 14:00:27 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/git-info.log
deleted file mode 100644
index c258a03..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/git-info.log
+++ /dev/null
@@ -1,697 +0,0 @@
-commit hash: cce540d4c95ad9ed32010552a7aa1fb2486658c8
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index 3d27dd7..7fe52d9 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,47 +1,47 @@
- seed: 42
- trainer:
--    gpus: 1 # the number of gpus, 0 for CPU
--    num_nodes: 1
--    max_epochs: 2
--    max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 4 # accumulates grads every k batches
--    gradient_clip_val: 0
--    amp_level: O1 # O1/O2 for mixed precision
--    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    accelerator: ddp
--    checkpoint_callback: false  # Provided by exp_manager
--    logger: false #false  # Provided by exp_manager
--    log_every_n_steps: 1  # Interval of logging.
--    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    resume_from_checkpoint: null
--
--    # gpus: 0 # the number of gpus, 0 for CPU
-+    # gpus: 1 # the number of gpus, 0 for CPU
-     # num_nodes: 1
--    # max_epochs: 10
-+    # max_epochs: 2
-     # max_steps: null # precedence over max_epochs
-     # accumulate_grad_batches: 4 # accumulates grads every k batches
-     # gradient_clip_val: 0
--    # amp_level: O0 # O1/O2 for mixed precision
--    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # # accelerator: ddp
-+    # amp_level: O1 # O1/O2 for mixed precision
-+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-     # checkpoint_callback: false  # Provided by exp_manager
-     # logger: false #false  # Provided by exp_manager
-     # log_every_n_steps: 1  # Interval of logging.
-     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # reload_dataloaders_every_epoch: true
-     # resume_from_checkpoint: null
- 
-+    gpus: 0 # the number of gpus, 0 for CPU
-+    num_nodes: 1
-+    max_epochs: 8
-+    max_steps: null # precedence over max_epochs
-+    accumulate_grad_batches: 8 # accumulates grads every k batches
-+    gradient_clip_val: 16
-+    amp_level: O0 # O1/O2 for mixed precision
-+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-+    checkpoint_callback: false  # Provided by exp_manager
-+    logger: false #false  # Provided by exp_manager
-+    log_every_n_steps: 1  # Interval of logging.
-+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    reload_dataloaders_every_epoch: true
-+    resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /root/data # /home/nxingyu2/data # 
--tmp_path: /tmp # /home/nxingyu2/data/tmp # 
-+base_path: /home/nxingyu2/data # /root/data # 
-+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
--    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-+    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-     unfrozen: 0
-     maximum_unfrozen: 2
-     unfreeze_step: 1
-@@ -61,7 +61,7 @@ model:
-     punct_class_weights: true
-     
-     dataset:
--        data_dir: /root/data # /home/nxingyu2/data # 
-+        data_dir: /home/nxingyu2/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -125,27 +125,27 @@ model:
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 4
-+        alpha: 3
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: adamw
-+        name: ranger
-         lr: 1e-3
-         weight_decay: 0.00
--
-         sched:
-             name: WarmupAnnealing #CyclicLR
-             # Scheduler params
-             warmup_steps: null
-             warmup_ratio: 0.1
-+            # hold_steps: 6
-             last_epoch: -1
- 
-             # pytorch lightning args
-             monitor: val_loss
--            reduce_on_plateau: true
-+            reduce_on_plateau: false
- hydra:
-     run:
-         dir: .
-diff --git a/experiment/core/optim/lr_scheduler.py b/experiment/core/optim/lr_scheduler.py
-index a17ebf8..0c7d449 100644
---- a/experiment/core/optim/lr_scheduler.py
-+++ b/experiment/core/optim/lr_scheduler.py
-@@ -347,6 +347,29 @@ class PolynomialHoldDecayAnnealing(WarmupHoldPolicy):
-         return new_lrs
- 
- 
-+class CosineHoldDecayAnnealing(WarmupHoldPolicy):
-+    def __init__(self, optimizer, *, max_steps, min_lr=0.0, last_epoch=-1, **kwargs):
-+        super().__init__(optimizer=optimizer, max_steps=max_steps, last_epoch=last_epoch, min_lr=min_lr, **kwargs)
-+
-+    def _get_lr(self, step):
-+        for initial_lr in self.base_lrs:
-+            if initial_lr < self.min_lr:
-+                raise ValueError(
-+                    f"{self} received an initial learning rate that was lower than the minimum learning rate."
-+                )
-+
-+        new_lrs = [
-+            _cosine_annealing(
-+                initial_lr=initial_lr,
-+                step=step - self.hold_steps,
-+                max_steps=self.max_steps - max(self.warmup_steps, self.hold_steps),
-+                min_lr=self.min_lr,
-+            )
-+            for initial_lr in self.base_lrs
-+        ]
-+        return new_lrs
-+
-+
- def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: SchedulerParams):
-     """
-     Checks if the scheduler name exists in the registry, and if it doesnt, adds it.
-@@ -654,4 +677,5 @@ AVAILABLE_SCHEDULERS = {
-     'ExponentialLR': pt_scheduler.ExponentialLR,
-     'ReduceLROnPlateau': pt_scheduler.ReduceLROnPlateau,
-     'CyclicLR': pt_scheduler.CyclicLR,
-+    'CosineHoldDecayAnnealing': CosineHoldDecayAnnealing,
- }
-diff --git a/experiment/core/optim/optimizers.py b/experiment/core/optim/optimizers.py
-index 5afc4fd..f15686e 100644
---- a/experiment/core/optim/optimizers.py
-+++ b/experiment/core/optim/optimizers.py
-@@ -7,6 +7,7 @@ import torch.optim as optim
- from omegaconf import DictConfig, OmegaConf
- from torch.optim import adadelta, adagrad, adamax, rmsprop, rprop
- from torch.optim.optimizer import Optimizer
-+from torchtools.optim import Ranger
- 
- from core.config import OptimizerParams, get_optimizer_config, register_optimizer_params
- from core.optim.novograd import Novograd
-@@ -24,6 +25,7 @@ AVAILABLE_OPTIMIZERS = {
-     'rmsprop': rmsprop.RMSprop,
-     'rprop': rprop.Rprop,
-     'novograd': Novograd,
-+    'ranger': Ranger,
- }
- 
- 
-diff --git a/experiment/info.log b/experiment/info.log
-index b211d42..5024c4f 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,473 +1,2 @@
--[INFO] - shuffling train set
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4da7f8e0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.008413951416451957
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e67aa00>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          22.42      11.12      14.86       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                         38.62       7.46      12.50      19530
--- (label_id: 3)                                          6.60      15.75       9.30       1746
--. (label_id: 4)                                         33.33       0.01       0.01      17976
--: (label_id: 5)                                          0.79      19.15       1.52        376
--; (label_id: 6)                                          0.45       2.94       0.78        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.17      50.56       0.33         89
---------------------
--micro avg                                                5.14       5.14       5.14      48233
--macro avg                                               10.24      10.70       3.93      48233
--weighted avg                                            30.92       5.14       7.15      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          22.09      11.27      14.93       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                         35.39       6.74      11.32      19813
--- (label_id: 3)                                          6.03      16.28       8.79       1708
--. (label_id: 4)                                        100.00       0.01       0.02      18084
--: (label_id: 5)                                          0.78      20.69       1.51        348
--; (label_id: 6)                                          0.45       2.55       0.77        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.19      58.62       0.38         87
---------------------
--micro avg                                                4.89       4.89       4.89      48633
--macro avg                                               16.49      11.62       3.77      48633
--weighted avg                                            54.38       4.89       6.67      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.008413951416451957
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e74d910>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          19.61      14.42      16.62        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         38.84       6.82      11.60        689
--- (label_id: 3)                                          4.50      15.52       6.98         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.59      20.00       1.14         10
--; (label_id: 6)                                          2.56      16.67       4.44          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.25      50.00       0.51          4
---------------------
--micro avg                                                5.55       5.55       5.55       1641
--macro avg                                                6.64      12.34       4.13       1641
--weighted avg                                            18.97       5.55       7.25       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.63      98.91       7.00       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          3.96       1.76       2.44       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.63       3.63       3.63      48233
--macro avg                                                0.76      10.07       0.94      48233
--weighted avg                                             0.25       3.63       0.33      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e68b8b0>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          19.61      14.42      16.62        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         38.84       6.82      11.60        689
--- (label_id: 3)                                          4.50      15.52       6.98         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.59      20.00       1.14         10
--; (label_id: 6)                                          2.56      16.67       4.44          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.25      50.00       0.51          4
---------------------
--micro avg                                                5.55       5.55       5.55       1641
--macro avg                                                6.64      12.34       4.13       1641
--weighted avg                                            18.97       5.55       7.25       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                          0.00       0.00       0.00      19813
--- (label_id: 3)                                          3.51     100.00       6.79       1708
--. (label_id: 4)                                          0.00       0.00       0.00      18084
--: (label_id: 5)                                          0.00       0.00       0.00        348
--; (label_id: 6)                                          0.00       0.00       0.00        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.00       0.00       0.00         87
---------------------
--micro avg                                                3.51       3.51       3.51      48633
--macro avg                                                0.35      10.00       0.68      48633
--weighted avg                                             0.12       3.51       0.24      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75eaf0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                          0.00       0.00       0.00        689
--- (label_id: 3)                                          3.53     100.00       6.83         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.00       0.00       0.00         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                3.53       3.53       3.53       1641
--macro avg                                                0.35      10.00       0.68       1641
--weighted avg                                             0.12       3.53       0.24       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75e160>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                          0.00       0.00       0.00        689
--- (label_id: 3)                                          3.53     100.00       6.83         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.00       0.00       0.00         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                3.53       3.53       3.53       1641
--macro avg                                                0.35      10.00       0.68       1641
--weighted avg                                             0.12       3.53       0.24       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                          0.00       0.00       0.00      19813
--- (label_id: 3)                                          3.51     100.00       6.79       1708
--. (label_id: 4)                                          0.00       0.00       0.00      18084
--: (label_id: 5)                                          0.00       0.00       0.00        348
--; (label_id: 6)                                          0.00       0.00       0.00        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.00       0.00       0.00         87
---------------------
--micro avg                                                3.51       3.51       3.51      48633
--macro avg                                                0.35      10.00       0.68      48633
--weighted avg                                             0.12       3.51       0.24      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       7470
--! (label_id: 1)                                          0.00       0.00       0.00        148
--, (label_id: 2)                                          0.00       0.00       0.00      28513
--- (label_id: 3)                                          3.02     100.00       5.86       2074
--. (label_id: 4)                                          0.00       0.00       0.00      25120
--: (label_id: 5)                                          0.00       0.00       0.00        570
--; (label_id: 6)                                          0.00       0.00       0.00        534
--? (label_id: 7)                                          0.00       0.00       0.00       2085
--— (label_id: 8)                                          0.00       0.00       0.00       2073
--… (label_id: 9)                                          0.00       0.00       0.00        142
---------------------
--micro avg                                                3.02       3.02       3.02      68729
--macro avg                                                0.30      10.00       0.59      68729
--weighted avg                                             0.09       3.02       0.18      68729
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       3735
---------------------
--micro avg                                              100.00     100.00     100.00       3735
--macro avg                                              100.00     100.00     100.00       3735
--weighted avg                                           100.00     100.00     100.00       3735
--
--[INFO] - Internal process exited
-+[INFO] - GPU available: True, used: False
-+[INFO] - TPU available: None, using: 0 TPU cores
-diff --git a/experiment/main.py b/experiment/main.py
-index cd4ca27..6f0a8ea 100644
---- a/experiment/main.py
-+++ b/experiment/main.py
-@@ -39,7 +39,7 @@ def main(cfg: DictConfig)->None:
-     
-     while(model.hparams.model.unfrozen<=cfg.model.maximum_unfrozen and model.hparams.model.unfrozen>=0):
-         trainer.current_epoch=0
--        lr_finder = trainer.tuner.lr_find(model,min_lr=1e-08, max_lr=1e-02, num_training=80, early_stop_threshold=None)
-+        lr_finder = trainer.tuner.lr_find(model,min_lr=1e-10, max_lr=1e-02, num_training=80, early_stop_threshold=None)
-         # Results can be found in
-         pp(lr_finder.results)
-         new_lr = lr_finder.suggestion()
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index 782fbfa..fa37b4c 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -374,6 +374,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             scheduler_config = None
- 
-+
-         # Check if caller provided optimizer name, default to Adam otherwise
-         optimizer_cls = optim_config.get('_target_', None)
- 
-diff --git a/linuxcommands.txt b/linuxcommands.txt
-index e72bad1..adc1fb0 100644
---- a/linuxcommands.txt
-+++ b/linuxcommands.txt
-@@ -48,7 +48,7 @@ git clone git@github.com:ngxingyu/dotfiles.git
- echo 'alias nv="~/nvim.appimage"' >> ~/.bashrc
- source ~/.profile
- 
--
-+ls -b | head -30 | xargs ls -d
- 
- 
- conda install script
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/lightning_logs.txt
deleted file mode 100644
index 01e1664..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/lightning_logs.txt
+++ /dev/null
@@ -1,2 +0,0 @@
-GPU available: True, used: False
-TPU available: None, using: 0 TPU cores
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/nemo_error_log.txt
deleted file mode 100644
index b0fa254..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/nemo_error_log.txt
+++ /dev/null
@@ -1,4 +0,0 @@
-[NeMo W 2021-02-06 14:03:12 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 14:03:12 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index b7a840f..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,6 +0,0 @@
-[NeMo W 2021-02-06 14:03:12 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo I 2021-02-06 14:03:12 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12
-[NeMo I 2021-02-06 14:03:12 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-06 14:03:12 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/events.out.tfevents.1612594376.intern-instance.19602.0 b/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/events.out.tfevents.1612594376.intern-instance.19602.0
deleted file mode 100644
index 94520e4..0000000
Binary files a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/events.out.tfevents.1612594376.intern-instance.19602.0 and /dev/null differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/git-info.log
deleted file mode 100644
index c258a03..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/git-info.log
+++ /dev/null
@@ -1,697 +0,0 @@
-commit hash: cce540d4c95ad9ed32010552a7aa1fb2486658c8
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index 3d27dd7..7fe52d9 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,47 +1,47 @@
- seed: 42
- trainer:
--    gpus: 1 # the number of gpus, 0 for CPU
--    num_nodes: 1
--    max_epochs: 2
--    max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 4 # accumulates grads every k batches
--    gradient_clip_val: 0
--    amp_level: O1 # O1/O2 for mixed precision
--    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    accelerator: ddp
--    checkpoint_callback: false  # Provided by exp_manager
--    logger: false #false  # Provided by exp_manager
--    log_every_n_steps: 1  # Interval of logging.
--    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    resume_from_checkpoint: null
--
--    # gpus: 0 # the number of gpus, 0 for CPU
-+    # gpus: 1 # the number of gpus, 0 for CPU
-     # num_nodes: 1
--    # max_epochs: 10
-+    # max_epochs: 2
-     # max_steps: null # precedence over max_epochs
-     # accumulate_grad_batches: 4 # accumulates grads every k batches
-     # gradient_clip_val: 0
--    # amp_level: O0 # O1/O2 for mixed precision
--    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # # accelerator: ddp
-+    # amp_level: O1 # O1/O2 for mixed precision
-+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-     # checkpoint_callback: false  # Provided by exp_manager
-     # logger: false #false  # Provided by exp_manager
-     # log_every_n_steps: 1  # Interval of logging.
-     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # reload_dataloaders_every_epoch: true
-     # resume_from_checkpoint: null
- 
-+    gpus: 0 # the number of gpus, 0 for CPU
-+    num_nodes: 1
-+    max_epochs: 8
-+    max_steps: null # precedence over max_epochs
-+    accumulate_grad_batches: 8 # accumulates grads every k batches
-+    gradient_clip_val: 16
-+    amp_level: O0 # O1/O2 for mixed precision
-+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-+    checkpoint_callback: false  # Provided by exp_manager
-+    logger: false #false  # Provided by exp_manager
-+    log_every_n_steps: 1  # Interval of logging.
-+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    reload_dataloaders_every_epoch: true
-+    resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /root/data # /home/nxingyu2/data # 
--tmp_path: /tmp # /home/nxingyu2/data/tmp # 
-+base_path: /home/nxingyu2/data # /root/data # 
-+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
--    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-+    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-     unfrozen: 0
-     maximum_unfrozen: 2
-     unfreeze_step: 1
-@@ -61,7 +61,7 @@ model:
-     punct_class_weights: true
-     
-     dataset:
--        data_dir: /root/data # /home/nxingyu2/data # 
-+        data_dir: /home/nxingyu2/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -125,27 +125,27 @@ model:
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 4
-+        alpha: 3
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: adamw
-+        name: ranger
-         lr: 1e-3
-         weight_decay: 0.00
--
-         sched:
-             name: WarmupAnnealing #CyclicLR
-             # Scheduler params
-             warmup_steps: null
-             warmup_ratio: 0.1
-+            # hold_steps: 6
-             last_epoch: -1
- 
-             # pytorch lightning args
-             monitor: val_loss
--            reduce_on_plateau: true
-+            reduce_on_plateau: false
- hydra:
-     run:
-         dir: .
-diff --git a/experiment/core/optim/lr_scheduler.py b/experiment/core/optim/lr_scheduler.py
-index a17ebf8..0c7d449 100644
---- a/experiment/core/optim/lr_scheduler.py
-+++ b/experiment/core/optim/lr_scheduler.py
-@@ -347,6 +347,29 @@ class PolynomialHoldDecayAnnealing(WarmupHoldPolicy):
-         return new_lrs
- 
- 
-+class CosineHoldDecayAnnealing(WarmupHoldPolicy):
-+    def __init__(self, optimizer, *, max_steps, min_lr=0.0, last_epoch=-1, **kwargs):
-+        super().__init__(optimizer=optimizer, max_steps=max_steps, last_epoch=last_epoch, min_lr=min_lr, **kwargs)
-+
-+    def _get_lr(self, step):
-+        for initial_lr in self.base_lrs:
-+            if initial_lr < self.min_lr:
-+                raise ValueError(
-+                    f"{self} received an initial learning rate that was lower than the minimum learning rate."
-+                )
-+
-+        new_lrs = [
-+            _cosine_annealing(
-+                initial_lr=initial_lr,
-+                step=step - self.hold_steps,
-+                max_steps=self.max_steps - max(self.warmup_steps, self.hold_steps),
-+                min_lr=self.min_lr,
-+            )
-+            for initial_lr in self.base_lrs
-+        ]
-+        return new_lrs
-+
-+
- def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: SchedulerParams):
-     """
-     Checks if the scheduler name exists in the registry, and if it doesnt, adds it.
-@@ -654,4 +677,5 @@ AVAILABLE_SCHEDULERS = {
-     'ExponentialLR': pt_scheduler.ExponentialLR,
-     'ReduceLROnPlateau': pt_scheduler.ReduceLROnPlateau,
-     'CyclicLR': pt_scheduler.CyclicLR,
-+    'CosineHoldDecayAnnealing': CosineHoldDecayAnnealing,
- }
-diff --git a/experiment/core/optim/optimizers.py b/experiment/core/optim/optimizers.py
-index 5afc4fd..f15686e 100644
---- a/experiment/core/optim/optimizers.py
-+++ b/experiment/core/optim/optimizers.py
-@@ -7,6 +7,7 @@ import torch.optim as optim
- from omegaconf import DictConfig, OmegaConf
- from torch.optim import adadelta, adagrad, adamax, rmsprop, rprop
- from torch.optim.optimizer import Optimizer
-+from torchtools.optim import Ranger
- 
- from core.config import OptimizerParams, get_optimizer_config, register_optimizer_params
- from core.optim.novograd import Novograd
-@@ -24,6 +25,7 @@ AVAILABLE_OPTIMIZERS = {
-     'rmsprop': rmsprop.RMSprop,
-     'rprop': rprop.Rprop,
-     'novograd': Novograd,
-+    'ranger': Ranger,
- }
- 
- 
-diff --git a/experiment/info.log b/experiment/info.log
-index b211d42..5024c4f 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,473 +1,2 @@
--[INFO] - shuffling train set
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4da7f8e0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.008413951416451957
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e67aa00>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          22.42      11.12      14.86       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                         38.62       7.46      12.50      19530
--- (label_id: 3)                                          6.60      15.75       9.30       1746
--. (label_id: 4)                                         33.33       0.01       0.01      17976
--: (label_id: 5)                                          0.79      19.15       1.52        376
--; (label_id: 6)                                          0.45       2.94       0.78        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.17      50.56       0.33         89
---------------------
--micro avg                                                5.14       5.14       5.14      48233
--macro avg                                               10.24      10.70       3.93      48233
--weighted avg                                            30.92       5.14       7.15      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          22.09      11.27      14.93       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                         35.39       6.74      11.32      19813
--- (label_id: 3)                                          6.03      16.28       8.79       1708
--. (label_id: 4)                                        100.00       0.01       0.02      18084
--: (label_id: 5)                                          0.78      20.69       1.51        348
--; (label_id: 6)                                          0.45       2.55       0.77        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.19      58.62       0.38         87
---------------------
--micro avg                                                4.89       4.89       4.89      48633
--macro avg                                               16.49      11.62       3.77      48633
--weighted avg                                            54.38       4.89       6.67      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.008413951416451957
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e74d910>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          19.61      14.42      16.62        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         38.84       6.82      11.60        689
--- (label_id: 3)                                          4.50      15.52       6.98         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.59      20.00       1.14         10
--; (label_id: 6)                                          2.56      16.67       4.44          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.25      50.00       0.51          4
---------------------
--micro avg                                                5.55       5.55       5.55       1641
--macro avg                                                6.64      12.34       4.13       1641
--weighted avg                                            18.97       5.55       7.25       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.63      98.91       7.00       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          3.96       1.76       2.44       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.63       3.63       3.63      48233
--macro avg                                                0.76      10.07       0.94      48233
--weighted avg                                             0.25       3.63       0.33      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e68b8b0>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          19.61      14.42      16.62        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         38.84       6.82      11.60        689
--- (label_id: 3)                                          4.50      15.52       6.98         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.59      20.00       1.14         10
--; (label_id: 6)                                          2.56      16.67       4.44          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.25      50.00       0.51          4
---------------------
--micro avg                                                5.55       5.55       5.55       1641
--macro avg                                                6.64      12.34       4.13       1641
--weighted avg                                            18.97       5.55       7.25       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                          0.00       0.00       0.00      19813
--- (label_id: 3)                                          3.51     100.00       6.79       1708
--. (label_id: 4)                                          0.00       0.00       0.00      18084
--: (label_id: 5)                                          0.00       0.00       0.00        348
--; (label_id: 6)                                          0.00       0.00       0.00        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.00       0.00       0.00         87
---------------------
--micro avg                                                3.51       3.51       3.51      48633
--macro avg                                                0.35      10.00       0.68      48633
--weighted avg                                             0.12       3.51       0.24      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75eaf0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                          0.00       0.00       0.00        689
--- (label_id: 3)                                          3.53     100.00       6.83         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.00       0.00       0.00         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                3.53       3.53       3.53       1641
--macro avg                                                0.35      10.00       0.68       1641
--weighted avg                                             0.12       3.53       0.24       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75e160>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                          0.00       0.00       0.00        689
--- (label_id: 3)                                          3.53     100.00       6.83         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.00       0.00       0.00         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                3.53       3.53       3.53       1641
--macro avg                                                0.35      10.00       0.68       1641
--weighted avg                                             0.12       3.53       0.24       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                          0.00       0.00       0.00      19813
--- (label_id: 3)                                          3.51     100.00       6.79       1708
--. (label_id: 4)                                          0.00       0.00       0.00      18084
--: (label_id: 5)                                          0.00       0.00       0.00        348
--; (label_id: 6)                                          0.00       0.00       0.00        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.00       0.00       0.00         87
---------------------
--micro avg                                                3.51       3.51       3.51      48633
--macro avg                                                0.35      10.00       0.68      48633
--weighted avg                                             0.12       3.51       0.24      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       7470
--! (label_id: 1)                                          0.00       0.00       0.00        148
--, (label_id: 2)                                          0.00       0.00       0.00      28513
--- (label_id: 3)                                          3.02     100.00       5.86       2074
--. (label_id: 4)                                          0.00       0.00       0.00      25120
--: (label_id: 5)                                          0.00       0.00       0.00        570
--; (label_id: 6)                                          0.00       0.00       0.00        534
--? (label_id: 7)                                          0.00       0.00       0.00       2085
--— (label_id: 8)                                          0.00       0.00       0.00       2073
--… (label_id: 9)                                          0.00       0.00       0.00        142
---------------------
--micro avg                                                3.02       3.02       3.02      68729
--macro avg                                                0.30      10.00       0.59      68729
--weighted avg                                             0.09       3.02       0.18      68729
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       3735
---------------------
--micro avg                                              100.00     100.00     100.00       3735
--macro avg                                              100.00     100.00     100.00       3735
--weighted avg                                           100.00     100.00     100.00       3735
--
--[INFO] - Internal process exited
-+[INFO] - GPU available: True, used: False
-+[INFO] - TPU available: None, using: 0 TPU cores
-diff --git a/experiment/main.py b/experiment/main.py
-index cd4ca27..6f0a8ea 100644
---- a/experiment/main.py
-+++ b/experiment/main.py
-@@ -39,7 +39,7 @@ def main(cfg: DictConfig)->None:
-     
-     while(model.hparams.model.unfrozen<=cfg.model.maximum_unfrozen and model.hparams.model.unfrozen>=0):
-         trainer.current_epoch=0
--        lr_finder = trainer.tuner.lr_find(model,min_lr=1e-08, max_lr=1e-02, num_training=80, early_stop_threshold=None)
-+        lr_finder = trainer.tuner.lr_find(model,min_lr=1e-10, max_lr=1e-02, num_training=80, early_stop_threshold=None)
-         # Results can be found in
-         pp(lr_finder.results)
-         new_lr = lr_finder.suggestion()
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index 782fbfa..fa37b4c 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -374,6 +374,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             scheduler_config = None
- 
-+
-         # Check if caller provided optimizer name, default to Adam otherwise
-         optimizer_cls = optim_config.get('_target_', None)
- 
-diff --git a/linuxcommands.txt b/linuxcommands.txt
-index e72bad1..adc1fb0 100644
---- a/linuxcommands.txt
-+++ b/linuxcommands.txt
-@@ -48,7 +48,7 @@ git clone git@github.com:ngxingyu/dotfiles.git
- echo 'alias nv="~/nvim.appimage"' >> ~/.bashrc
- source ~/.profile
- 
--
-+ls -b | head -30 | xargs ls -d
- 
- 
- conda install script
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/hparams.yaml
deleted file mode 100644
index 5cff5a5..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/hparams.yaml
+++ /dev/null
@@ -1,107 +0,0 @@
-seed: 42
-trainer:
-  gpus: 0
-  num_nodes: 1
-  max_epochs: 8
-  max_steps: null
-  accumulate_grad_batches: 8
-  gradient_clip_val: 16
-  amp_level: O0
-  precision: 32
-  checkpoint_callback: false
-  logger: false
-  log_every_n_steps: 1
-  val_check_interval: 1.0
-  reload_dataloaders_every_epoch: true
-  resume_from_checkpoint: null
-exp_manager:
-  exp_dir: /home/nxingyu2/project/
-  name: Punctuation_with_Domain_discriminator
-  create_tensorboard_logger: true
-  create_checkpoint_callback: true
-base_path: /home/nxingyu2/data
-tmp_path: /home/nxingyu2/data/tmp
-model:
-  nemo_path: null
-  transformer_path: google/electra-base-discriminator
-  unfrozen: 0
-  maximum_unfrozen: 2
-  unfreeze_step: 1
-  punct_label_ids:
-  - ''
-  - '!'
-  - ','
-  - '-'
-  - .
-  - ':'
-  - ;
-  - '?'
-  - —
-  - …
-  punct_class_weights: true
-  dataset:
-    data_dir: /home/nxingyu2/data
-    labelled:
-    - /home/nxingyu2/data/ted_talks_processed
-    unlabelled: null
-    max_seq_length: 128
-    pad_label: ''
-    ignore_extra_tokens: false
-    ignore_start_end: false
-    use_cache: false
-    num_workers: 0
-    pin_memory: true
-    drop_last: false
-    num_labels: 10
-    num_domains: 1
-    test_unlabelled: true
-    train_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 8
-    validation_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 8
-  tokenizer:
-    tokenizer_name: google/electra-base-discriminator
-    vocab_file: null
-    tokenizer_model: null
-    special_tokens: null
-  language_model:
-    pretrained_model_name: google/electra-base-discriminator
-    lm_checkpoint: null
-    config_file: null
-    config: null
-  punct_head:
-    punct_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: dice
-  domain_head:
-    domain_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: cel
-    gamma: 0.1
-  dice_loss:
-    epsilon: 0.01
-    alpha: 3
-    macro_average: true
-  focal_loss:
-    gamma: 5
-  optim:
-    name: ranger
-    lr: 0.001
-    weight_decay: 0.0
-    sched:
-      name: WarmupAnnealing
-      warmup_steps: null
-      warmup_ratio: 0.1
-      last_epoch: -1
-      monitor: val_loss
-      reduce_on_plateau: false
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/lightning_logs.txt
deleted file mode 100644
index 1999b86..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/lightning_logs.txt
+++ /dev/null
@@ -1,37 +0,0 @@
-GPU available: True, used: False
-TPU available: None, using: 0 TPU cores
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 108 M 
-1 | punct_classifier    | TokenClassifier      | 7.7 K 
-2 | domain_classifier   | SequenceClassifier   | 769   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-8.5 K     Trainable params
-108 M     Non-trainable params
-108 M     Total params
-Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/lr_find_temp_model.ckpt
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 108 M 
-1 | punct_classifier    | TokenClassifier      | 7.7 K 
-2 | domain_classifier   | SequenceClassifier   | 769   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-8.5 K     Trainable params
-108 M     Non-trainable params
-108 M     Total params
-Epoch 0, global step 50: val_loss reached 0.86867 (best 0.86867), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=0.ckpt" as top 3
-Epoch 1, global step 100: val_loss reached 0.86855 (best 0.86855), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
-Epoch 2, global step 150: val_loss reached 0.86816 (best 0.86816), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=2.ckpt" as top 3
-Epoch 3, global step 200: val_loss reached 0.86018 (best 0.86018), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/nemo_error_log.txt
deleted file mode 100644
index 5d0635b..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/nemo_error_log.txt
+++ /dev/null
@@ -1,28 +0,0 @@
-[NeMo W 2021-02-06 14:03:25 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 14:03:25 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-06 14:03:39 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 14:03:39 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 14:03:47 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 14:04:27 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtools/optim/radam.py:62: UserWarning: This overload of addcmul_ is deprecated:
-    	addcmul_(Number value, Tensor tensor1, Tensor tensor2)
-    Consider using one of the following signatures instead:
-    	addcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/torch/csrc/utils/python_arg_parser.cpp:882.)
-      exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
-    
-[NeMo W 2021-02-06 14:33:14 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7fdb2f2bd0a0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-06 14:35:16 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7fdb2f2bdf70> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-06 17:15:18 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 1df0208..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,30 +0,0 @@
-[NeMo W 2021-02-06 14:03:25 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo I 2021-02-06 14:03:25 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25
-[NeMo I 2021-02-06 14:03:25 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-06 14:03:25 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-06 14:03:39 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 14:03:39 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 14:03:47 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 14:04:27 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtools/optim/radam.py:62: UserWarning: This overload of addcmul_ is deprecated:
-    	addcmul_(Number value, Tensor tensor1, Tensor tensor2)
-    Consider using one of the following signatures instead:
-    	addcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/torch/csrc/utils/python_arg_parser.cpp:882.)
-      exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
-    
-[NeMo W 2021-02-06 14:33:14 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7fdb2f2bd0a0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-06 14:35:16 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7fdb2f2bdf70> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-06 17:15:18 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/events.out.tfevents.1612603306.intern-instance.26445.0 b/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/events.out.tfevents.1612603306.intern-instance.26445.0
deleted file mode 100644
index 00322d6..0000000
Binary files a/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/events.out.tfevents.1612603306.intern-instance.26445.0 and /dev/null differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/git-info.log
deleted file mode 100644
index adfc583..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/git-info.log
+++ /dev/null
@@ -1,697 +0,0 @@
-commit hash: cce540d4c95ad9ed32010552a7aa1fb2486658c8
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index 3d27dd7..fe58670 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,47 +1,47 @@
- seed: 42
- trainer:
--    gpus: 1 # the number of gpus, 0 for CPU
--    num_nodes: 1
--    max_epochs: 2
--    max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 4 # accumulates grads every k batches
--    gradient_clip_val: 0
--    amp_level: O1 # O1/O2 for mixed precision
--    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    accelerator: ddp
--    checkpoint_callback: false  # Provided by exp_manager
--    logger: false #false  # Provided by exp_manager
--    log_every_n_steps: 1  # Interval of logging.
--    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    resume_from_checkpoint: null
--
--    # gpus: 0 # the number of gpus, 0 for CPU
-+    # gpus: 1 # the number of gpus, 0 for CPU
-     # num_nodes: 1
--    # max_epochs: 10
-+    # max_epochs: 2
-     # max_steps: null # precedence over max_epochs
-     # accumulate_grad_batches: 4 # accumulates grads every k batches
-     # gradient_clip_val: 0
--    # amp_level: O0 # O1/O2 for mixed precision
--    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # # accelerator: ddp
-+    # amp_level: O1 # O1/O2 for mixed precision
-+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-     # checkpoint_callback: false  # Provided by exp_manager
-     # logger: false #false  # Provided by exp_manager
-     # log_every_n_steps: 1  # Interval of logging.
-     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # reload_dataloaders_every_epoch: true
-     # resume_from_checkpoint: null
- 
-+    gpus: 0 # the number of gpus, 0 for CPU
-+    num_nodes: 1
-+    max_epochs: 8
-+    max_steps: null # precedence over max_epochs
-+    accumulate_grad_batches: 1 # accumulates grads every k batches
-+    gradient_clip_val: 0
-+    amp_level: O0 # O1/O2 for mixed precision
-+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-+    checkpoint_callback: false  # Provided by exp_manager
-+    logger: false #false  # Provided by exp_manager
-+    log_every_n_steps: 1  # Interval of logging.
-+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    reload_dataloaders_every_epoch: true
-+    resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /root/data # /home/nxingyu2/data # 
--tmp_path: /tmp # /home/nxingyu2/data/tmp # 
-+base_path: /home/nxingyu2/data # /root/data # 
-+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
--    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-+    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-     unfrozen: 0
-     maximum_unfrozen: 2
-     unfreeze_step: 1
-@@ -61,7 +61,7 @@ model:
-     punct_class_weights: true
-     
-     dataset:
--        data_dir: /root/data # /home/nxingyu2/data # 
-+        data_dir: /home/nxingyu2/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -125,27 +125,27 @@ model:
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 4
-+        alpha: 3
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: adamw
-+        name: novograd
-         lr: 1e-3
-         weight_decay: 0.00
--
-         sched:
-             name: WarmupAnnealing #CyclicLR
-             # Scheduler params
-             warmup_steps: null
-             warmup_ratio: 0.1
-+            # hold_steps: 6
-             last_epoch: -1
- 
-             # pytorch lightning args
-             monitor: val_loss
--            reduce_on_plateau: true
-+            reduce_on_plateau: false
- hydra:
-     run:
-         dir: .
-diff --git a/experiment/core/optim/lr_scheduler.py b/experiment/core/optim/lr_scheduler.py
-index a17ebf8..0c7d449 100644
---- a/experiment/core/optim/lr_scheduler.py
-+++ b/experiment/core/optim/lr_scheduler.py
-@@ -347,6 +347,29 @@ class PolynomialHoldDecayAnnealing(WarmupHoldPolicy):
-         return new_lrs
- 
- 
-+class CosineHoldDecayAnnealing(WarmupHoldPolicy):
-+    def __init__(self, optimizer, *, max_steps, min_lr=0.0, last_epoch=-1, **kwargs):
-+        super().__init__(optimizer=optimizer, max_steps=max_steps, last_epoch=last_epoch, min_lr=min_lr, **kwargs)
-+
-+    def _get_lr(self, step):
-+        for initial_lr in self.base_lrs:
-+            if initial_lr < self.min_lr:
-+                raise ValueError(
-+                    f"{self} received an initial learning rate that was lower than the minimum learning rate."
-+                )
-+
-+        new_lrs = [
-+            _cosine_annealing(
-+                initial_lr=initial_lr,
-+                step=step - self.hold_steps,
-+                max_steps=self.max_steps - max(self.warmup_steps, self.hold_steps),
-+                min_lr=self.min_lr,
-+            )
-+            for initial_lr in self.base_lrs
-+        ]
-+        return new_lrs
-+
-+
- def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: SchedulerParams):
-     """
-     Checks if the scheduler name exists in the registry, and if it doesnt, adds it.
-@@ -654,4 +677,5 @@ AVAILABLE_SCHEDULERS = {
-     'ExponentialLR': pt_scheduler.ExponentialLR,
-     'ReduceLROnPlateau': pt_scheduler.ReduceLROnPlateau,
-     'CyclicLR': pt_scheduler.CyclicLR,
-+    'CosineHoldDecayAnnealing': CosineHoldDecayAnnealing,
- }
-diff --git a/experiment/core/optim/optimizers.py b/experiment/core/optim/optimizers.py
-index 5afc4fd..f15686e 100644
---- a/experiment/core/optim/optimizers.py
-+++ b/experiment/core/optim/optimizers.py
-@@ -7,6 +7,7 @@ import torch.optim as optim
- from omegaconf import DictConfig, OmegaConf
- from torch.optim import adadelta, adagrad, adamax, rmsprop, rprop
- from torch.optim.optimizer import Optimizer
-+from torchtools.optim import Ranger
- 
- from core.config import OptimizerParams, get_optimizer_config, register_optimizer_params
- from core.optim.novograd import Novograd
-@@ -24,6 +25,7 @@ AVAILABLE_OPTIMIZERS = {
-     'rmsprop': rmsprop.RMSprop,
-     'rprop': rprop.Rprop,
-     'novograd': Novograd,
-+    'ranger': Ranger,
- }
- 
- 
-diff --git a/experiment/info.log b/experiment/info.log
-index b211d42..5024c4f 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,473 +1,2 @@
--[INFO] - shuffling train set
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4da7f8e0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.008413951416451957
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e67aa00>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          24.14      10.10      14.24        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         34.00       2.47       4.60        689
--- (label_id: 3)                                          4.42       8.62       5.85         58
--. (label_id: 4)                                         50.00       1.57       3.05        572
--: (label_id: 5)                                          0.55      40.00       1.09         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          2.18      17.39       3.87         46
--— (label_id: 8)                                          1.42       5.71       2.27         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                4.02       4.02       4.02       1641
--macro avg                                               11.67       8.59       3.50       1641
--weighted avg                                            35.01       4.02       5.17       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          22.42      11.12      14.86       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                         38.62       7.46      12.50      19530
--- (label_id: 3)                                          6.60      15.75       9.30       1746
--. (label_id: 4)                                         33.33       0.01       0.01      17976
--: (label_id: 5)                                          0.79      19.15       1.52        376
--; (label_id: 6)                                          0.45       2.94       0.78        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.17      50.56       0.33         89
---------------------
--micro avg                                                5.14       5.14       5.14      48233
--macro avg                                               10.24      10.70       3.93      48233
--weighted avg                                            30.92       5.14       7.15      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          22.09      11.27      14.93       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                         35.39       6.74      11.32      19813
--- (label_id: 3)                                          6.03      16.28       8.79       1708
--. (label_id: 4)                                        100.00       0.01       0.02      18084
--: (label_id: 5)                                          0.78      20.69       1.51        348
--; (label_id: 6)                                          0.45       2.55       0.77        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.19      58.62       0.38         87
---------------------
--micro avg                                                4.89       4.89       4.89      48633
--macro avg                                               16.49      11.62       3.77      48633
--weighted avg                                            54.38       4.89       6.67      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.008413951416451957
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e74d910>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          19.61      14.42      16.62        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         38.84       6.82      11.60        689
--- (label_id: 3)                                          4.50      15.52       6.98         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.59      20.00       1.14         10
--; (label_id: 6)                                          2.56      16.67       4.44          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.25      50.00       0.51          4
---------------------
--micro avg                                                5.55       5.55       5.55       1641
--macro avg                                                6.64      12.34       4.13       1641
--weighted avg                                            18.97       5.55       7.25       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.63      98.91       7.00       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          3.96       1.76       2.44       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.63       3.63       3.63      48233
--macro avg                                                0.76      10.07       0.94      48233
--weighted avg                                             0.25       3.63       0.33      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e68b8b0>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          19.61      14.42      16.62        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                         38.84       6.82      11.60        689
--- (label_id: 3)                                          4.50      15.52       6.98         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.59      20.00       1.14         10
--; (label_id: 6)                                          2.56      16.67       4.44          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.25      50.00       0.51          4
---------------------
--micro avg                                                5.55       5.55       5.55       1641
--macro avg                                                6.64      12.34       4.13       1641
--weighted avg                                            18.97       5.55       7.25       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                          0.00       0.00       0.00      19813
--- (label_id: 3)                                          3.51     100.00       6.79       1708
--. (label_id: 4)                                          0.00       0.00       0.00      18084
--: (label_id: 5)                                          0.00       0.00       0.00        348
--; (label_id: 6)                                          0.00       0.00       0.00        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.00       0.00       0.00         87
---------------------
--micro avg                                                3.51       3.51       3.51      48633
--macro avg                                                0.35      10.00       0.68      48633
--weighted avg                                             0.12       3.51       0.24      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75eaf0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                          0.00       0.00       0.00        689
--- (label_id: 3)                                          3.53     100.00       6.83         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.00       0.00       0.00         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                3.53       3.53       3.53       1641
--macro avg                                                0.35      10.00       0.68       1641
--weighted avg                                             0.12       3.53       0.24       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.00031622776601683794
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75e160>" 
--will be used during training (effective maximum steps = 200) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 200
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        208
--! (label_id: 1)                                          0.00       0.00       0.00         13
--, (label_id: 2)                                          0.00       0.00       0.00        689
--- (label_id: 3)                                          3.53     100.00       6.83         58
--. (label_id: 4)                                          0.00       0.00       0.00        572
--: (label_id: 5)                                          0.00       0.00       0.00         10
--; (label_id: 6)                                          0.00       0.00       0.00          6
--? (label_id: 7)                                          0.00       0.00       0.00         46
--— (label_id: 8)                                          0.00       0.00       0.00         35
--… (label_id: 9)                                          0.00       0.00       0.00          4
---------------------
--micro avg                                                3.53       3.53       3.53       1641
--macro avg                                                0.35      10.00       0.68       1641
--weighted avg                                             0.12       3.53       0.24       1641
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        104
---------------------
--micro avg                                              100.00     100.00     100.00        104
--macro avg                                              100.00     100.00     100.00        104
--weighted avg                                           100.00     100.00     100.00        104
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5614
--! (label_id: 1)                                          0.00       0.00       0.00        108
--, (label_id: 2)                                          0.00       0.00       0.00      19530
--- (label_id: 3)                                          3.62     100.00       6.99       1746
--. (label_id: 4)                                          0.00       0.00       0.00      17976
--: (label_id: 5)                                          0.00       0.00       0.00        376
--; (label_id: 6)                                          0.00       0.00       0.00        170
--? (label_id: 7)                                          0.00       0.00       0.00       1418
--— (label_id: 8)                                          0.00       0.00       0.00       1206
--… (label_id: 9)                                          0.00       0.00       0.00         89
---------------------
--micro avg                                                3.62       3.62       3.62      48233
--macro avg                                                0.36      10.00       0.70      48233
--weighted avg                                             0.13       3.62       0.25      48233
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2807
---------------------
--micro avg                                              100.00     100.00     100.00       2807
--macro avg                                              100.00     100.00     100.00       2807
--weighted avg                                           100.00     100.00     100.00       2807
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       5634
--! (label_id: 1)                                          0.00       0.00       0.00        113
--, (label_id: 2)                                          0.00       0.00       0.00      19813
--- (label_id: 3)                                          3.51     100.00       6.79       1708
--. (label_id: 4)                                          0.00       0.00       0.00      18084
--: (label_id: 5)                                          0.00       0.00       0.00        348
--; (label_id: 6)                                          0.00       0.00       0.00        196
--? (label_id: 7)                                          0.00       0.00       0.00       1392
--— (label_id: 8)                                          0.00       0.00       0.00       1258
--… (label_id: 9)                                          0.00       0.00       0.00         87
---------------------
--micro avg                                                3.51       3.51       3.51      48633
--macro avg                                                0.35      10.00       0.68      48633
--weighted avg                                             0.12       3.51       0.24      48633
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2817
---------------------
--micro avg                                              100.00     100.00     100.00       2817
--macro avg                                              100.00     100.00     100.00       2817
--weighted avg                                           100.00     100.00     100.00       2817
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00       7470
--! (label_id: 1)                                          0.00       0.00       0.00        148
--, (label_id: 2)                                          0.00       0.00       0.00      28513
--- (label_id: 3)                                          3.02     100.00       5.86       2074
--. (label_id: 4)                                          0.00       0.00       0.00      25120
--: (label_id: 5)                                          0.00       0.00       0.00        570
--; (label_id: 6)                                          0.00       0.00       0.00        534
--? (label_id: 7)                                          0.00       0.00       0.00       2085
--— (label_id: 8)                                          0.00       0.00       0.00       2073
--… (label_id: 9)                                          0.00       0.00       0.00        142
---------------------
--micro avg                                                3.02       3.02       3.02      68729
--macro avg                                                0.30      10.00       0.59      68729
--weighted avg                                             0.09       3.02       0.18      68729
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       3735
---------------------
--micro avg                                              100.00     100.00     100.00       3735
--macro avg                                              100.00     100.00     100.00       3735
--weighted avg                                           100.00     100.00     100.00       3735
--
--[INFO] - Internal process exited
-+[INFO] - GPU available: True, used: False
-+[INFO] - TPU available: None, using: 0 TPU cores
-diff --git a/experiment/main.py b/experiment/main.py
-index cd4ca27..6f0a8ea 100644
---- a/experiment/main.py
-+++ b/experiment/main.py
-@@ -39,7 +39,7 @@ def main(cfg: DictConfig)->None:
-     
-     while(model.hparams.model.unfrozen<=cfg.model.maximum_unfrozen and model.hparams.model.unfrozen>=0):
-         trainer.current_epoch=0
--        lr_finder = trainer.tuner.lr_find(model,min_lr=1e-08, max_lr=1e-02, num_training=80, early_stop_threshold=None)
-+        lr_finder = trainer.tuner.lr_find(model,min_lr=1e-10, max_lr=1e-02, num_training=80, early_stop_threshold=None)
-         # Results can be found in
-         pp(lr_finder.results)
-         new_lr = lr_finder.suggestion()
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index 782fbfa..fa37b4c 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -374,6 +374,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             scheduler_config = None
- 
-+
-         # Check if caller provided optimizer name, default to Adam otherwise
-         optimizer_cls = optim_config.get('_target_', None)
- 
-diff --git a/linuxcommands.txt b/linuxcommands.txt
-index e72bad1..adc1fb0 100644
---- a/linuxcommands.txt
-+++ b/linuxcommands.txt
-@@ -48,7 +48,7 @@ git clone git@github.com:ngxingyu/dotfiles.git
- echo 'alias nv="~/nvim.appimage"' >> ~/.bashrc
- source ~/.profile
- 
--
-+ls -b | head -30 | xargs ls -d
- 
- 
- conda install script
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/hparams.yaml
deleted file mode 100644
index af74c43..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/hparams.yaml
+++ /dev/null
@@ -1,107 +0,0 @@
-seed: 42
-trainer:
-  gpus: 0
-  num_nodes: 1
-  max_epochs: 8
-  max_steps: null
-  accumulate_grad_batches: 1
-  gradient_clip_val: 0
-  amp_level: O0
-  precision: 32
-  checkpoint_callback: false
-  logger: false
-  log_every_n_steps: 1
-  val_check_interval: 1.0
-  reload_dataloaders_every_epoch: true
-  resume_from_checkpoint: null
-exp_manager:
-  exp_dir: /home/nxingyu2/project/
-  name: Punctuation_with_Domain_discriminator
-  create_tensorboard_logger: true
-  create_checkpoint_callback: true
-base_path: /home/nxingyu2/data
-tmp_path: /home/nxingyu2/data/tmp
-model:
-  nemo_path: null
-  transformer_path: google/electra-base-discriminator
-  unfrozen: 0
-  maximum_unfrozen: 2
-  unfreeze_step: 1
-  punct_label_ids:
-  - ''
-  - '!'
-  - ','
-  - '-'
-  - .
-  - ':'
-  - ;
-  - '?'
-  - —
-  - …
-  punct_class_weights: true
-  dataset:
-    data_dir: /home/nxingyu2/data
-    labelled:
-    - /home/nxingyu2/data/ted_talks_processed
-    unlabelled: null
-    max_seq_length: 128
-    pad_label: ''
-    ignore_extra_tokens: false
-    ignore_start_end: false
-    use_cache: false
-    num_workers: 0
-    pin_memory: true
-    drop_last: false
-    num_labels: 10
-    num_domains: 1
-    test_unlabelled: true
-    train_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 8
-    validation_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 8
-  tokenizer:
-    tokenizer_name: google/electra-base-discriminator
-    vocab_file: null
-    tokenizer_model: null
-    special_tokens: null
-  language_model:
-    pretrained_model_name: google/electra-base-discriminator
-    lm_checkpoint: null
-    config_file: null
-    config: null
-  punct_head:
-    punct_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: dice
-  domain_head:
-    domain_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: cel
-    gamma: 0.1
-  dice_loss:
-    epsilon: 0.01
-    alpha: 3
-    macro_average: true
-  focal_loss:
-    gamma: 5
-  optim:
-    name: novograd
-    lr: 0.001
-    weight_decay: 0.0
-    sched:
-      name: WarmupAnnealing
-      warmup_steps: null
-      warmup_ratio: 0.1
-      last_epoch: -1
-      monitor: val_loss
-      reduce_on_plateau: false
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lightning_logs.txt
deleted file mode 100644
index 851eabd..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lightning_logs.txt
+++ /dev/null
@@ -1,40 +0,0 @@
-GPU available: True, used: False
-TPU available: None, using: 0 TPU cores
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 108 M 
-1 | punct_classifier    | TokenClassifier      | 7.7 K 
-2 | domain_classifier   | SequenceClassifier   | 769   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-8.5 K     Trainable params
-108 M     Non-trainable params
-108 M     Total params
-Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 108 M 
-1 | punct_classifier    | TokenClassifier      | 7.7 K 
-2 | domain_classifier   | SequenceClassifier   | 769   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-8.5 K     Trainable params
-108 M     Non-trainable params
-108 M     Total params
-Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
-Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
-Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
-Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
-Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
-Epoch 5, step 2394: val_loss was not in top 3
-Epoch 6, step 2793: val_loss was not in top 3
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/nemo_error_log.txt
deleted file mode 100644
index 1adb293..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/nemo_error_log.txt
+++ /dev/null
@@ -1,19 +0,0 @@
-[NeMo W 2021-02-06 17:15:23 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 17:15:23 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-06 17:15:37 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 17:15:37 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 17:15:45 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 17:50:54 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f224f0c1d90> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-06 17:53:06 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f224f0c1a90> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 963f28e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,21 +0,0 @@
-[NeMo W 2021-02-06 17:15:23 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo I 2021-02-06 17:15:23 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23
-[NeMo I 2021-02-06 17:15:23 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-06 17:15:23 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-06 17:15:37 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 17:15:37 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 17:15:45 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-06 17:50:54 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f224f0c1d90> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-06 17:53:06 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f224f0c1a90> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/git-info.log
deleted file mode 100644
index 44fff95..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/git-info.log
+++ /dev/null
@@ -1,623 +0,0 @@
-commit hash: 939a671c8c117db6975316767ced5d95449e2b27
-diff --git a/README.md b/README.md
-index d52dba5..0b403d3 100644
---- a/README.md
-+++ b/README.md
-@@ -333,4 +333,71 @@ label                                                precision    recall       f
-  'punct_f1': 0.5858508944511414,
-  'punct_precision': 0.30176490545272827,
-  'punct_recall': 10.0,
-- 'test_loss': 0.8140875697135925}
-\ No newline at end of file
-+ 'test_loss': 0.8140875697135925}
-+
-+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
-+
-+ (label_id: 0)                                          62.15     100.00      76.66       5154
-+! (label_id: 1)                                          0.00       0.00       0.00        108
-+, (label_id: 2)                                          0.00       0.00       0.00      18022
-+- (label_id: 3)                                          0.00       0.00       0.00       1557
-+. (label_id: 4)                                         41.74      94.01      57.81      15164
-+: (label_id: 5)                                          0.00       0.00       0.00        319
-+; (label_id: 6)                                          0.00       0.00       0.00         88
-+? (label_id: 7)                                          0.00       0.00       0.00       1217
-+ (label_id: 8)                                          0.00       0.00       0.00        752
-+… (label_id: 9)                                          0.00       0.00       0.00         67
-+-------------------
-+micro avg                                               45.72      45.72      45.72      42448
-+macro avg                                               10.39      19.40      13.45      42448
-+weighted avg                                            22.46      45.72      29.96      42448
-+{ 'punct_f1': 13.446383476257324,
-+ 'punct_precision': 10.388500213623047,
-+ 'punct_recall': 19.400554656982422,
-+ 'test_loss': 0.44148480892181396}
-+
-+
-+ ## Log for 8/2/2021
-+
-+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
-+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
-+
-+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
-+End frozen 
-+micro avg                                               41.42      41.42      41.42      33406
-+macro avg                                               11.01      13.54      11.07      33406
-+weighted avg                                            34.88      41.42      34.20      33406
-+
-+1st layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               36.65      36.65      36.65      33463
-+macro avg                                               10.71       9.91       8.26      33463
-+weighted avg                                            34.32      36.65      31.08      33463
-+
-+2nd layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               35.72      35.72      35.72      42448
-+macro avg                                                3.57      10.00       5.26      42448
-+weighted avg                                            12.76      35.72      18.81      42448
-+
-+{'punct_f1': 5.264181137084961,
-+ 'punct_precision': 3.572371006011963,
-+ 'punct_recall': 10.0,
-+ 'test_loss': 18.49854850769043}
-+
-+
-+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
-+alpha from 3->4 seems to reduce convergence rate.
-+
-+micro avg                                               50.98      50.98      50.98      33463
-+macro avg                                               25.99      25.38      23.38      33463
-+weighted avg                                            50.31      50.98      48.27      33463
-+
-+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
-+micro avg                                               58.55      58.55      58.55      39340
-+macro avg                                               30.02      29.74      29.52      39340
-+weighted avg                                            57.91      58.55      57.51      39340
-+
-+still increasing?!
-+{'punct_f1': 29.523975372314453,
-+ 'punct_precision': 30.015613555908203,
-+ 'punct_recall': 29.738296508789062,
-+ 'test_loss': 0.3690211772918701}
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index fe58670..872a5ed 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,49 +1,49 @@
- seed: 42
- trainer:
--    # gpus: 1 # the number of gpus, 0 for CPU
--    # num_nodes: 1
--    # max_epochs: 2
--    # max_steps: null # precedence over max_epochs
--    # accumulate_grad_batches: 4 # accumulates grads every k batches
--    # gradient_clip_val: 0
--    # amp_level: O1 # O1/O2 for mixed precision
--    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
--    # checkpoint_callback: false  # Provided by exp_manager
--    # logger: false #false  # Provided by exp_manager
--    # log_every_n_steps: 1  # Interval of logging.
--    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # resume_from_checkpoint: null
--
--    gpus: 0 # the number of gpus, 0 for CPU
-+    gpus: 1 # the number of gpus, 0 for CPU
-     num_nodes: 1
--    max_epochs: 8
-+    max_epochs: 10
-     max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 1 # accumulates grads every k batches
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-     gradient_clip_val: 0
--    amp_level: O0 # O1/O2 for mixed precision
--    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
-+    amp_level: O1 # O1/O2 for mixed precision
-+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    accelerator: ddp
-     checkpoint_callback: false  # Provided by exp_manager
-     logger: false #false  # Provided by exp_manager
-     log_every_n_steps: 1  # Interval of logging.
-     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    reload_dataloaders_every_epoch: true
-     resume_from_checkpoint: null
- 
-+    # gpus: 0 # the number of gpus, 0 for CPU
-+    # num_nodes: 1
-+    # max_epochs: 8
-+    # max_steps: null # precedence over max_epochs
-+    # accumulate_grad_batches: 1 # accumulates grads every k batches
-+    # gradient_clip_val: 0
-+    # amp_level: O0 # O1/O2 for mixed precision
-+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # # accelerator: ddp
-+    # checkpoint_callback: false  # Provided by exp_manager
-+    # logger: false #false  # Provided by exp_manager
-+    # log_every_n_steps: 1  # Interval of logging.
-+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    # reload_dataloaders_every_epoch: true
-+    # resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /home/nxingyu2/data # /root/data # 
--tmp_path: /home/nxingyu2/data/tmp # /tmp # 
-+base_path: /home/nxingyu/data # /root/data # 
-+tmp_path: /home/nxingyu/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
--    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-     unfrozen: 0
--    maximum_unfrozen: 2
-+    maximum_unfrozen: 1
-     unfreeze_step: 1
-     # unfreeze_every: 3
-     punct_label_ids:
-@@ -58,10 +58,10 @@ model:
-         - "—"
-         - "…"
- 
--    punct_class_weights: true
-+    punct_class_weights: false
-     
-     dataset:
--        data_dir: /home/nxingyu2/data # /root/data # 
-+        data_dir: /home/nxingyu/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -106,6 +106,11 @@ model:
-         config: null
-         # unfrozen_layers: 1
- 
-+    mlp:
-+        num_fc_layers: 2
-+        fc_dropout: 0.1
-+        activation: 'relu'
-+        
-     punct_head:
-         punct_num_fc_layers: 1
-         fc_dropout: 0.1
-@@ -117,22 +122,24 @@ model:
-     domain_head:
-         domain_num_fc_layers: 1
-         fc_dropout: 0.1
--        activation: 'relu'
-+        activation: 'gelu'
-         log_softmax: false
-         use_transformer_init: true
-         loss: 'cel'
-         gamma: 0.1 # coefficient of gradient reversal
-+        pooling: 'token'
-+        idx_conditioned_on: 0
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 3
-+        alpha: 4
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: novograd
-+        name: adamw
-         lr: 1e-3
-         weight_decay: 0.00
-         sched:
-diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
-index d4ff927..b4b92b0 100644
---- a/experiment/core/layers/sequence_classifier.py
-+++ b/experiment/core/layers/sequence_classifier.py
-@@ -20,7 +20,8 @@ class SequenceClassifier(nn.Module):
-         log_softmax: bool = True,
-         dropout: float = 0.0,
-         use_transformer_init: bool = True,
--        idx_conditioned_on: int = 0,
-+        pooling: str = 'mean', # mean, max, mean_max, token
-+        idx_conditioned_on: int = None,
-     ):
-         """
-         Initializes the SequenceClassifier module.
-@@ -37,6 +38,7 @@ class SequenceClassifier(nn.Module):
-         super().__init__()
-         self.log_softmax = log_softmax
-         self._idx_conditioned_on = idx_conditioned_on
-+        self.pooling = pooling
-         self.mlp = MultiLayerPerceptron(
-             hidden_size=hidden_size,
-             num_classes=num_classes,
-@@ -48,7 +50,23 @@ class SequenceClassifier(nn.Module):
-         if use_transformer_init:
-             self.apply(lambda module: transformer_weights_init(module, xavier=False))
- 
--    def forward(self, hidden_states):
-+    def forward(self, hidden_states, subtoken_mask=None):
-         hidden_states = self.dropout(hidden_states)
--        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
-+        if self.pooling=='token':
-+            pooled = hidden_states[:, self._idx_conditioned_on]
-+        else:
-+            if subtoken_mask==None:
-+                ct=hidden_states.shape[1] # Seq len
-+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
-+            else:
-+                ct = torch.sum(subtoken_mask,axis=1)
-+            pooled_sum = torch.sum(hidden_states,axis=1)
-+            if self.pooling=='mean' or self.pooling == 'mean_max':
-+                pooled_mean = pooled_sum//ct
-+            if self.pooling=='max' or self.pooling=='mean_max':
-+                pooled_max = torch.max(hidden_states,axis=1)[0]
-+            pooled=pooled_mean if self.pooling=='mean' else \
-+                pooled_max if self.pooling=='max' else \
-+                    torch.cat([pooled_mean,pooled_max],axis=-1)
-+        logits = self.mlp(pooled)
-         return logits
-diff --git a/experiment/info.log b/experiment/info.log
-index 2471fe9..e69de29 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,300 +0,0 @@
--[INFO] - GPU available: True, used: False
--[INFO] - TPU available: None, using: 0 TPU cores
--[INFO] - shuffling train set
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        184
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.23       0.34       0.53        594
--- (label_id: 3)                                          3.06      25.42       5.46         59
--. (label_id: 4)                                         47.22      12.98      20.36        524
--: (label_id: 5)                                          0.00       0.00       0.00         18
--; (label_id: 6)                                          0.00       0.00       0.00         13
--? (label_id: 7)                                          8.45       6.32       7.23         95
--— (label_id: 8)                                          0.00       0.00       0.00         12
--… (label_id: 9)                                          0.00       0.00       0.00          0
---------------------
--micro avg                                                6.05       6.05       6.05       1503
--macro avg                                                6.66       5.01       3.73       1503
--weighted avg                                            17.61       6.05       7.98       1503
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00         92
---------------------
--micro avg                                              100.00     100.00     100.00         92
--macro avg                                              100.00     100.00     100.00         92
--weighted avg                                           100.00     100.00     100.00         92
--
--[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 1.5848931924611143e-08
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
--will be used during training (effective maximum steps = 3192) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 3192
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        202
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.62       0.45       0.70        669
--- (label_id: 3)                                          3.48      27.27       6.17         66
--. (label_id: 4)                                         45.06      13.01      20.19        561
--: (label_id: 5)                                          1.52       6.67       2.47         15
--; (label_id: 6)                                          0.00       0.00       0.00         15
--? (label_id: 7)                                          8.70       7.32       7.95         82
--— (label_id: 8)                                          0.00       0.00       0.00         13
--… (label_id: 9)                                          0.00       0.00       0.00          1
---------------------
--micro avg                                                6.20       6.20       6.20       1628
--macro avg                                                6.04       5.47       3.75       1628
--weighted avg                                            16.79       6.20       7.92       1628
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        101
---------------------
--micro avg                                              100.00     100.00     100.00        101
--macro avg                                              100.00     100.00     100.00        101
--weighted avg                                           100.00     100.00     100.00        101
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.34       0.73       0.46       4402
--! (label_id: 1)                                          0.42      13.95       0.82        129
--, (label_id: 2)                                          2.53       0.64       1.03      15243
--- (label_id: 3)                                          2.45      21.03       4.38       1322
--. (label_id: 4)                                         44.00      11.40      18.11      12542
--: (label_id: 5)                                          0.43       1.41       0.65        354
--; (label_id: 6)                                          0.00       0.00       0.00        163
--? (label_id: 7)                                          4.16       6.27       5.00       1117
--— (label_id: 8)                                          3.00       0.61       1.02        488
--… (label_id: 9)                                          0.97       6.17       1.68         81
---------------------
--micro avg                                                5.41       5.41       5.41      35841
--macro avg                                                5.83       6.22       3.32      35841
--weighted avg                                            16.78       5.41       7.18      35841
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2201
---------------------
--micro avg                                              100.00     100.00     100.00       2201
--macro avg                                              100.00     100.00     100.00       2201
--weighted avg                                           100.00     100.00     100.00       2201
--
--[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.20       0.43       0.27       4226
--! (label_id: 1)                                          0.44      14.17       0.86        127
--, (label_id: 2)                                          1.93       0.49       0.78      14611
--- (label_id: 3)                                          2.23      19.56       4.01       1237
--. (label_id: 4)                                         43.37      11.25      17.86      11977
--: (label_id: 5)                                          0.68       2.34       1.05        342
--; (label_id: 6)                                          0.00       0.00       0.00        129
--? (label_id: 7)                                          5.16       7.47       6.10       1058
--— (label_id: 8)                                          2.15       0.49       0.80        409
--… (label_id: 9)                                          0.69       4.23       1.19         71
---------------------
--micro avg                                                5.23       5.23       5.23      34187
--macro avg                                                5.68       6.04       3.29      34187
--weighted avg                                            16.32       5.23       6.98      34187
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2113
---------------------
--micro avg                                              100.00     100.00     100.00       2113
--macro avg                                              100.00     100.00     100.00       2113
--weighted avg                                           100.00     100.00     100.00       2113
--
--[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.61       0.39       4228
--! (label_id: 1)                                          0.30       8.28       0.58        145
--, (label_id: 2)                                          2.27       0.58       0.92      14495
--- (label_id: 3)                                          2.64      21.78       4.70       1327
--. (label_id: 4)                                         44.87      11.66      18.51      12193
--: (label_id: 5)                                          0.60       1.93       0.91        362
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.19       6.40       5.07       1078
--— (label_id: 8)                                          1.16       0.22       0.37        459
--… (label_id: 9)                                          0.85       4.17       1.41         96
---------------------
--micro avg                                                5.54       5.54       5.54      34547
--macro avg                                                5.72       5.56       3.29      34547
--weighted avg                                            17.08       5.54       7.33      34547
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2114
---------------------
--micro avg                                              100.00     100.00     100.00       2114
--macro avg                                              100.00     100.00     100.00       2114
--weighted avg                                           100.00     100.00     100.00       2114
--
--[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.29       0.63       0.40       4444
--! (label_id: 1)                                          0.38      10.67       0.74        150
--, (label_id: 2)                                          2.32       0.59       0.94      15290
--- (label_id: 3)                                          2.34      20.28       4.19       1292
--. (label_id: 4)                                         43.85      11.68      18.44      12599
--: (label_id: 5)                                          0.41       1.28       0.62        392
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.24       6.30       5.07       1111
--— (label_id: 8)                                          0.00       0.00       0.00        456
--… (label_id: 9)                                          0.38       2.41       0.65         83
---------------------
--micro avg                                                5.40       5.40       5.40      35981
--macro avg                                                5.42       5.38       3.11      35981
--weighted avg                                            16.59       5.40       7.22      35981
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2222
---------------------
--micro avg                                              100.00     100.00     100.00       2222
--macro avg                                              100.00     100.00     100.00       2222
--weighted avg                                           100.00     100.00     100.00       2222
--
--[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.35       0.73       0.48       3844
--! (label_id: 1)                                          0.54      14.62       1.04        130
--, (label_id: 2)                                          2.32       0.59       0.94      13056
--- (label_id: 3)                                          2.67      22.28       4.77       1194
--. (label_id: 4)                                         44.45      11.95      18.84      10791
--: (label_id: 5)                                          0.84       3.21       1.33        280
--; (label_id: 6)                                          0.00       0.00       0.00        140
--? (label_id: 7)                                          4.17       6.56       5.10        914
--— (label_id: 8)                                          0.00       0.00       0.00        401
--… (label_id: 9)                                          0.48       2.63       0.81         76
---------------------
--micro avg                                                5.68       5.68       5.68      30826
--macro avg                                                5.58       6.26       3.33      30826
--weighted avg                                            16.82       5.68       7.41      30826
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1922
---------------------
--micro avg                                              100.00     100.00     100.00       1922
--macro avg                                              100.00     100.00     100.00       1922
--weighted avg                                           100.00     100.00     100.00       1922
--
--[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.60       0.39       3970
--! (label_id: 1)                                          0.35      10.66       0.68        122
--, (label_id: 2)                                          2.09       0.53       0.85      13469
--- (label_id: 3)                                          2.29      19.32       4.10       1201
--. (label_id: 4)                                         43.43      11.24      17.86      11227
--: (label_id: 5)                                          0.63       2.30       0.99        304
--; (label_id: 6)                                          0.00       0.00       0.00        141
--? (label_id: 7)                                          4.52       6.86       5.45       1006
--— (label_id: 8)                                          1.15       0.23       0.38        444
--… (label_id: 9)                                          0.45       2.67       0.78         75
---------------------
--micro avg                                                5.26       5.26       5.26      31959
--macro avg                                                5.52       5.44       3.15      31959
--weighted avg                                            16.42       5.26       7.02      31959
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1985
---------------------
--micro avg                                              100.00     100.00     100.00       1985
--macro avg                                              100.00     100.00     100.00       1985
--weighted avg                                           100.00     100.00     100.00       1985
--
--[INFO] - Epoch 5, step 2394: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.23       0.48       0.31       4126
--! (label_id: 1)                                          0.29       9.40       0.56        117
--, (label_id: 2)                                          1.91       0.49       0.77      14019
--- (label_id: 3)                                          2.52      22.59       4.53       1164
--. (label_id: 4)                                         44.15      11.65      18.44      11789
--: (label_id: 5)                                          0.72       2.41       1.11        332
--; (label_id: 6)                                          0.56       0.61       0.58        165
--? (label_id: 7)                                          3.89       6.53       4.88        980
--— (label_id: 8)                                          2.30       0.47       0.77        430
--… (label_id: 9)                                          1.18       8.33       2.07         60
---------------------
--micro avg                                                5.47       5.47       5.47      33182
--macro avg                                                5.77       6.30       3.40      33182
--weighted avg                                            16.77       5.47       7.25      33182
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2063
---------------------
--micro avg                                              100.00     100.00     100.00       2063
--macro avg                                              100.00     100.00     100.00       2063
--weighted avg                                           100.00     100.00     100.00       2063
--
--[INFO] - Epoch 6, step 2793: val_loss was not in top 3
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index fa37b4c..fd77a7f 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             self.hparams.model.punct_class_weights=None
- 
-+        self.dropout=nn.Dropout(self.hparams.model.mlp.dropout)
-+        self.mlp = MultiLayerPerceptron(
-+            self.transformer.config.hidden_size,
-+            self.transformer.config.hidden_size,
-+            num_layers=self.hparams.model.mlp.num_fc_layers, 
-+            activation=self.hparams.model.mlp.activation, 
-+            log_softmax=self.hparams.model.mlp.log_softmax
-+        )
-+
-         self.punct_classifier = TokenClassifier(
-             hidden_size=self.transformer.config.hidden_size,
-             num_classes=len(self.labels_to_ids),
-@@ -80,6 +89,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-             log_softmax=self.hparams.model.domain_head.log_softmax,
-             dropout=self.hparams.model.domain_head.fc_dropout,
-             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
-+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
-         )
- 
-         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
-@@ -119,14 +129,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
-         self.freeze()
- 
--    def forward(self, input_ids, attention_mask, domain_ids=None):
-+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
-         hidden_states = self.transformer(
-             input_ids=input_ids, attention_mask=attention_mask
-         )[0]
-+        hidden_states = self.dropout(hidden_states)
-+        hidden_states = self.mlp(hidden_states)
-         punct_logits = self.punct_classifier(hidden_states=hidden_states)
-         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
-         domain_logits = self.domain_classifier(
--            hidden_states=reverse_grad_hidden_states)
-+            hidden_states=reverse_grad_hidden_states,
-+            subtoken_mask=subtoken_mask)
-         return punct_logits, domain_logits
- 
-     def _make_step(self, batch):
-@@ -136,7 +149,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         punct_labels = batch['labels']
-         domain_labels = batch['domain']
-         punct_logits, domain_logits = self(
--            input_ids=input_ids, attention_mask=attention_mask
-+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
-         )
-         punctuation_loss = self.punctuation_loss(
-             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/lightning_logs.txt
deleted file mode 100644
index 8942cad..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/lightning_logs.txt
+++ /dev/null
@@ -1,5 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-Using native 16bit precision.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/nemo_error_log.txt
deleted file mode 100644
index 7ce4d0c..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/nemo_error_log.txt
+++ /dev/null
@@ -1 +0,0 @@
-[NeMo W 2021-02-08 10:44:29 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index db09fa5..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,3 +0,0 @@
-[NeMo I 2021-02-08 10:44:29 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29
-[NeMo I 2021-02-08 10:44:29 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-08 10:44:29 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/git-info.log
deleted file mode 100644
index da9a71d..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/git-info.log
+++ /dev/null
@@ -1,623 +0,0 @@
-commit hash: 939a671c8c117db6975316767ced5d95449e2b27
-diff --git a/README.md b/README.md
-index d52dba5..0b403d3 100644
---- a/README.md
-+++ b/README.md
-@@ -333,4 +333,71 @@ label                                                precision    recall       f
-  'punct_f1': 0.5858508944511414,
-  'punct_precision': 0.30176490545272827,
-  'punct_recall': 10.0,
-- 'test_loss': 0.8140875697135925}
-\ No newline at end of file
-+ 'test_loss': 0.8140875697135925}
-+
-+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
-+
-+ (label_id: 0)                                          62.15     100.00      76.66       5154
-+! (label_id: 1)                                          0.00       0.00       0.00        108
-+, (label_id: 2)                                          0.00       0.00       0.00      18022
-+- (label_id: 3)                                          0.00       0.00       0.00       1557
-+. (label_id: 4)                                         41.74      94.01      57.81      15164
-+: (label_id: 5)                                          0.00       0.00       0.00        319
-+; (label_id: 6)                                          0.00       0.00       0.00         88
-+? (label_id: 7)                                          0.00       0.00       0.00       1217
-+ (label_id: 8)                                          0.00       0.00       0.00        752
-+… (label_id: 9)                                          0.00       0.00       0.00         67
-+-------------------
-+micro avg                                               45.72      45.72      45.72      42448
-+macro avg                                               10.39      19.40      13.45      42448
-+weighted avg                                            22.46      45.72      29.96      42448
-+{ 'punct_f1': 13.446383476257324,
-+ 'punct_precision': 10.388500213623047,
-+ 'punct_recall': 19.400554656982422,
-+ 'test_loss': 0.44148480892181396}
-+
-+
-+ ## Log for 8/2/2021
-+
-+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
-+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
-+
-+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
-+End frozen 
-+micro avg                                               41.42      41.42      41.42      33406
-+macro avg                                               11.01      13.54      11.07      33406
-+weighted avg                                            34.88      41.42      34.20      33406
-+
-+1st layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               36.65      36.65      36.65      33463
-+macro avg                                               10.71       9.91       8.26      33463
-+weighted avg                                            34.32      36.65      31.08      33463
-+
-+2nd layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               35.72      35.72      35.72      42448
-+macro avg                                                3.57      10.00       5.26      42448
-+weighted avg                                            12.76      35.72      18.81      42448
-+
-+{'punct_f1': 5.264181137084961,
-+ 'punct_precision': 3.572371006011963,
-+ 'punct_recall': 10.0,
-+ 'test_loss': 18.49854850769043}
-+
-+
-+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
-+alpha from 3->4 seems to reduce convergence rate.
-+
-+micro avg                                               50.98      50.98      50.98      33463
-+macro avg                                               25.99      25.38      23.38      33463
-+weighted avg                                            50.31      50.98      48.27      33463
-+
-+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
-+micro avg                                               58.55      58.55      58.55      39340
-+macro avg                                               30.02      29.74      29.52      39340
-+weighted avg                                            57.91      58.55      57.51      39340
-+
-+still increasing?!
-+{'punct_f1': 29.523975372314453,
-+ 'punct_precision': 30.015613555908203,
-+ 'punct_recall': 29.738296508789062,
-+ 'test_loss': 0.3690211772918701}
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index fe58670..872a5ed 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,49 +1,49 @@
- seed: 42
- trainer:
--    # gpus: 1 # the number of gpus, 0 for CPU
--    # num_nodes: 1
--    # max_epochs: 2
--    # max_steps: null # precedence over max_epochs
--    # accumulate_grad_batches: 4 # accumulates grads every k batches
--    # gradient_clip_val: 0
--    # amp_level: O1 # O1/O2 for mixed precision
--    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
--    # checkpoint_callback: false  # Provided by exp_manager
--    # logger: false #false  # Provided by exp_manager
--    # log_every_n_steps: 1  # Interval of logging.
--    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # resume_from_checkpoint: null
--
--    gpus: 0 # the number of gpus, 0 for CPU
-+    gpus: 1 # the number of gpus, 0 for CPU
-     num_nodes: 1
--    max_epochs: 8
-+    max_epochs: 10
-     max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 1 # accumulates grads every k batches
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-     gradient_clip_val: 0
--    amp_level: O0 # O1/O2 for mixed precision
--    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
-+    amp_level: O1 # O1/O2 for mixed precision
-+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    accelerator: ddp
-     checkpoint_callback: false  # Provided by exp_manager
-     logger: false #false  # Provided by exp_manager
-     log_every_n_steps: 1  # Interval of logging.
-     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    reload_dataloaders_every_epoch: true
-     resume_from_checkpoint: null
- 
-+    # gpus: 0 # the number of gpus, 0 for CPU
-+    # num_nodes: 1
-+    # max_epochs: 8
-+    # max_steps: null # precedence over max_epochs
-+    # accumulate_grad_batches: 1 # accumulates grads every k batches
-+    # gradient_clip_val: 0
-+    # amp_level: O0 # O1/O2 for mixed precision
-+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # # accelerator: ddp
-+    # checkpoint_callback: false  # Provided by exp_manager
-+    # logger: false #false  # Provided by exp_manager
-+    # log_every_n_steps: 1  # Interval of logging.
-+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    # reload_dataloaders_every_epoch: true
-+    # resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /home/nxingyu2/data # /root/data # 
--tmp_path: /home/nxingyu2/data/tmp # /tmp # 
-+base_path: /home/nxingyu/data # /root/data # 
-+tmp_path: /home/nxingyu/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
--    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-     unfrozen: 0
--    maximum_unfrozen: 2
-+    maximum_unfrozen: 1
-     unfreeze_step: 1
-     # unfreeze_every: 3
-     punct_label_ids:
-@@ -58,10 +58,10 @@ model:
-         - "—"
-         - "…"
- 
--    punct_class_weights: true
-+    punct_class_weights: false
-     
-     dataset:
--        data_dir: /home/nxingyu2/data # /root/data # 
-+        data_dir: /home/nxingyu/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -106,6 +106,11 @@ model:
-         config: null
-         # unfrozen_layers: 1
- 
-+    mlp:
-+        num_fc_layers: 2
-+        fc_dropout: 0.1
-+        activation: 'relu'
-+        
-     punct_head:
-         punct_num_fc_layers: 1
-         fc_dropout: 0.1
-@@ -117,22 +122,24 @@ model:
-     domain_head:
-         domain_num_fc_layers: 1
-         fc_dropout: 0.1
--        activation: 'relu'
-+        activation: 'gelu'
-         log_softmax: false
-         use_transformer_init: true
-         loss: 'cel'
-         gamma: 0.1 # coefficient of gradient reversal
-+        pooling: 'token'
-+        idx_conditioned_on: 0
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 3
-+        alpha: 4
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: novograd
-+        name: adamw
-         lr: 1e-3
-         weight_decay: 0.00
-         sched:
-diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
-index d4ff927..b4b92b0 100644
---- a/experiment/core/layers/sequence_classifier.py
-+++ b/experiment/core/layers/sequence_classifier.py
-@@ -20,7 +20,8 @@ class SequenceClassifier(nn.Module):
-         log_softmax: bool = True,
-         dropout: float = 0.0,
-         use_transformer_init: bool = True,
--        idx_conditioned_on: int = 0,
-+        pooling: str = 'mean', # mean, max, mean_max, token
-+        idx_conditioned_on: int = None,
-     ):
-         """
-         Initializes the SequenceClassifier module.
-@@ -37,6 +38,7 @@ class SequenceClassifier(nn.Module):
-         super().__init__()
-         self.log_softmax = log_softmax
-         self._idx_conditioned_on = idx_conditioned_on
-+        self.pooling = pooling
-         self.mlp = MultiLayerPerceptron(
-             hidden_size=hidden_size,
-             num_classes=num_classes,
-@@ -48,7 +50,23 @@ class SequenceClassifier(nn.Module):
-         if use_transformer_init:
-             self.apply(lambda module: transformer_weights_init(module, xavier=False))
- 
--    def forward(self, hidden_states):
-+    def forward(self, hidden_states, subtoken_mask=None):
-         hidden_states = self.dropout(hidden_states)
--        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
-+        if self.pooling=='token':
-+            pooled = hidden_states[:, self._idx_conditioned_on]
-+        else:
-+            if subtoken_mask==None:
-+                ct=hidden_states.shape[1] # Seq len
-+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
-+            else:
-+                ct = torch.sum(subtoken_mask,axis=1)
-+            pooled_sum = torch.sum(hidden_states,axis=1)
-+            if self.pooling=='mean' or self.pooling == 'mean_max':
-+                pooled_mean = pooled_sum//ct
-+            if self.pooling=='max' or self.pooling=='mean_max':
-+                pooled_max = torch.max(hidden_states,axis=1)[0]
-+            pooled=pooled_mean if self.pooling=='mean' else \
-+                pooled_max if self.pooling=='max' else \
-+                    torch.cat([pooled_mean,pooled_max],axis=-1)
-+        logits = self.mlp(pooled)
-         return logits
-diff --git a/experiment/info.log b/experiment/info.log
-index 2471fe9..e69de29 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,300 +0,0 @@
--[INFO] - GPU available: True, used: False
--[INFO] - TPU available: None, using: 0 TPU cores
--[INFO] - shuffling train set
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        184
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.23       0.34       0.53        594
--- (label_id: 3)                                          3.06      25.42       5.46         59
--. (label_id: 4)                                         47.22      12.98      20.36        524
--: (label_id: 5)                                          0.00       0.00       0.00         18
--; (label_id: 6)                                          0.00       0.00       0.00         13
--? (label_id: 7)                                          8.45       6.32       7.23         95
--— (label_id: 8)                                          0.00       0.00       0.00         12
--… (label_id: 9)                                          0.00       0.00       0.00          0
---------------------
--micro avg                                                6.05       6.05       6.05       1503
--macro avg                                                6.66       5.01       3.73       1503
--weighted avg                                            17.61       6.05       7.98       1503
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00         92
---------------------
--micro avg                                              100.00     100.00     100.00         92
--macro avg                                              100.00     100.00     100.00         92
--weighted avg                                           100.00     100.00     100.00         92
--
--[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 1.5848931924611143e-08
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
--will be used during training (effective maximum steps = 3192) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 3192
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        202
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.62       0.45       0.70        669
--- (label_id: 3)                                          3.48      27.27       6.17         66
--. (label_id: 4)                                         45.06      13.01      20.19        561
--: (label_id: 5)                                          1.52       6.67       2.47         15
--; (label_id: 6)                                          0.00       0.00       0.00         15
--? (label_id: 7)                                          8.70       7.32       7.95         82
--— (label_id: 8)                                          0.00       0.00       0.00         13
--… (label_id: 9)                                          0.00       0.00       0.00          1
---------------------
--micro avg                                                6.20       6.20       6.20       1628
--macro avg                                                6.04       5.47       3.75       1628
--weighted avg                                            16.79       6.20       7.92       1628
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        101
---------------------
--micro avg                                              100.00     100.00     100.00        101
--macro avg                                              100.00     100.00     100.00        101
--weighted avg                                           100.00     100.00     100.00        101
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.34       0.73       0.46       4402
--! (label_id: 1)                                          0.42      13.95       0.82        129
--, (label_id: 2)                                          2.53       0.64       1.03      15243
--- (label_id: 3)                                          2.45      21.03       4.38       1322
--. (label_id: 4)                                         44.00      11.40      18.11      12542
--: (label_id: 5)                                          0.43       1.41       0.65        354
--; (label_id: 6)                                          0.00       0.00       0.00        163
--? (label_id: 7)                                          4.16       6.27       5.00       1117
--— (label_id: 8)                                          3.00       0.61       1.02        488
--… (label_id: 9)                                          0.97       6.17       1.68         81
---------------------
--micro avg                                                5.41       5.41       5.41      35841
--macro avg                                                5.83       6.22       3.32      35841
--weighted avg                                            16.78       5.41       7.18      35841
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2201
---------------------
--micro avg                                              100.00     100.00     100.00       2201
--macro avg                                              100.00     100.00     100.00       2201
--weighted avg                                           100.00     100.00     100.00       2201
--
--[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.20       0.43       0.27       4226
--! (label_id: 1)                                          0.44      14.17       0.86        127
--, (label_id: 2)                                          1.93       0.49       0.78      14611
--- (label_id: 3)                                          2.23      19.56       4.01       1237
--. (label_id: 4)                                         43.37      11.25      17.86      11977
--: (label_id: 5)                                          0.68       2.34       1.05        342
--; (label_id: 6)                                          0.00       0.00       0.00        129
--? (label_id: 7)                                          5.16       7.47       6.10       1058
--— (label_id: 8)                                          2.15       0.49       0.80        409
--… (label_id: 9)                                          0.69       4.23       1.19         71
---------------------
--micro avg                                                5.23       5.23       5.23      34187
--macro avg                                                5.68       6.04       3.29      34187
--weighted avg                                            16.32       5.23       6.98      34187
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2113
---------------------
--micro avg                                              100.00     100.00     100.00       2113
--macro avg                                              100.00     100.00     100.00       2113
--weighted avg                                           100.00     100.00     100.00       2113
--
--[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.61       0.39       4228
--! (label_id: 1)                                          0.30       8.28       0.58        145
--, (label_id: 2)                                          2.27       0.58       0.92      14495
--- (label_id: 3)                                          2.64      21.78       4.70       1327
--. (label_id: 4)                                         44.87      11.66      18.51      12193
--: (label_id: 5)                                          0.60       1.93       0.91        362
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.19       6.40       5.07       1078
--— (label_id: 8)                                          1.16       0.22       0.37        459
--… (label_id: 9)                                          0.85       4.17       1.41         96
---------------------
--micro avg                                                5.54       5.54       5.54      34547
--macro avg                                                5.72       5.56       3.29      34547
--weighted avg                                            17.08       5.54       7.33      34547
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2114
---------------------
--micro avg                                              100.00     100.00     100.00       2114
--macro avg                                              100.00     100.00     100.00       2114
--weighted avg                                           100.00     100.00     100.00       2114
--
--[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.29       0.63       0.40       4444
--! (label_id: 1)                                          0.38      10.67       0.74        150
--, (label_id: 2)                                          2.32       0.59       0.94      15290
--- (label_id: 3)                                          2.34      20.28       4.19       1292
--. (label_id: 4)                                         43.85      11.68      18.44      12599
--: (label_id: 5)                                          0.41       1.28       0.62        392
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.24       6.30       5.07       1111
--— (label_id: 8)                                          0.00       0.00       0.00        456
--… (label_id: 9)                                          0.38       2.41       0.65         83
---------------------
--micro avg                                                5.40       5.40       5.40      35981
--macro avg                                                5.42       5.38       3.11      35981
--weighted avg                                            16.59       5.40       7.22      35981
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2222
---------------------
--micro avg                                              100.00     100.00     100.00       2222
--macro avg                                              100.00     100.00     100.00       2222
--weighted avg                                           100.00     100.00     100.00       2222
--
--[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.35       0.73       0.48       3844
--! (label_id: 1)                                          0.54      14.62       1.04        130
--, (label_id: 2)                                          2.32       0.59       0.94      13056
--- (label_id: 3)                                          2.67      22.28       4.77       1194
--. (label_id: 4)                                         44.45      11.95      18.84      10791
--: (label_id: 5)                                          0.84       3.21       1.33        280
--; (label_id: 6)                                          0.00       0.00       0.00        140
--? (label_id: 7)                                          4.17       6.56       5.10        914
--— (label_id: 8)                                          0.00       0.00       0.00        401
--… (label_id: 9)                                          0.48       2.63       0.81         76
---------------------
--micro avg                                                5.68       5.68       5.68      30826
--macro avg                                                5.58       6.26       3.33      30826
--weighted avg                                            16.82       5.68       7.41      30826
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1922
---------------------
--micro avg                                              100.00     100.00     100.00       1922
--macro avg                                              100.00     100.00     100.00       1922
--weighted avg                                           100.00     100.00     100.00       1922
--
--[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.60       0.39       3970
--! (label_id: 1)                                          0.35      10.66       0.68        122
--, (label_id: 2)                                          2.09       0.53       0.85      13469
--- (label_id: 3)                                          2.29      19.32       4.10       1201
--. (label_id: 4)                                         43.43      11.24      17.86      11227
--: (label_id: 5)                                          0.63       2.30       0.99        304
--; (label_id: 6)                                          0.00       0.00       0.00        141
--? (label_id: 7)                                          4.52       6.86       5.45       1006
--— (label_id: 8)                                          1.15       0.23       0.38        444
--… (label_id: 9)                                          0.45       2.67       0.78         75
---------------------
--micro avg                                                5.26       5.26       5.26      31959
--macro avg                                                5.52       5.44       3.15      31959
--weighted avg                                            16.42       5.26       7.02      31959
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1985
---------------------
--micro avg                                              100.00     100.00     100.00       1985
--macro avg                                              100.00     100.00     100.00       1985
--weighted avg                                           100.00     100.00     100.00       1985
--
--[INFO] - Epoch 5, step 2394: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.23       0.48       0.31       4126
--! (label_id: 1)                                          0.29       9.40       0.56        117
--, (label_id: 2)                                          1.91       0.49       0.77      14019
--- (label_id: 3)                                          2.52      22.59       4.53       1164
--. (label_id: 4)                                         44.15      11.65      18.44      11789
--: (label_id: 5)                                          0.72       2.41       1.11        332
--; (label_id: 6)                                          0.56       0.61       0.58        165
--? (label_id: 7)                                          3.89       6.53       4.88        980
--— (label_id: 8)                                          2.30       0.47       0.77        430
--… (label_id: 9)                                          1.18       8.33       2.07         60
---------------------
--micro avg                                                5.47       5.47       5.47      33182
--macro avg                                                5.77       6.30       3.40      33182
--weighted avg                                            16.77       5.47       7.25      33182
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2063
---------------------
--micro avg                                              100.00     100.00     100.00       2063
--macro avg                                              100.00     100.00     100.00       2063
--weighted avg                                           100.00     100.00     100.00       2063
--
--[INFO] - Epoch 6, step 2793: val_loss was not in top 3
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index fa37b4c..ae8f205 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             self.hparams.model.punct_class_weights=None
- 
-+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.dropout)
-+        self.mlp = MultiLayerPerceptron(
-+            self.transformer.config.hidden_size,
-+            self.transformer.config.hidden_size,
-+            num_layers=self.hparams.model.mlp.num_fc_layers, 
-+            activation=self.hparams.model.mlp.activation, 
-+            log_softmax=self.hparams.model.mlp.log_softmax
-+        )
-+
-         self.punct_classifier = TokenClassifier(
-             hidden_size=self.transformer.config.hidden_size,
-             num_classes=len(self.labels_to_ids),
-@@ -80,6 +89,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-             log_softmax=self.hparams.model.domain_head.log_softmax,
-             dropout=self.hparams.model.domain_head.fc_dropout,
-             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
-+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
-         )
- 
-         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
-@@ -119,14 +129,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
-         self.freeze()
- 
--    def forward(self, input_ids, attention_mask, domain_ids=None):
-+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
-         hidden_states = self.transformer(
-             input_ids=input_ids, attention_mask=attention_mask
-         )[0]
-+        hidden_states = self.dropout(hidden_states)
-+        hidden_states = self.mlp(hidden_states)
-         punct_logits = self.punct_classifier(hidden_states=hidden_states)
-         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
-         domain_logits = self.domain_classifier(
--            hidden_states=reverse_grad_hidden_states)
-+            hidden_states=reverse_grad_hidden_states,
-+            subtoken_mask=subtoken_mask)
-         return punct_logits, domain_logits
- 
-     def _make_step(self, batch):
-@@ -136,7 +149,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         punct_labels = batch['labels']
-         domain_labels = batch['domain']
-         punct_logits, domain_logits = self(
--            input_ids=input_ids, attention_mask=attention_mask
-+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
-         )
-         punctuation_loss = self.punctuation_loss(
-             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/lightning_logs.txt
deleted file mode 100644
index 8942cad..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/lightning_logs.txt
+++ /dev/null
@@ -1,5 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-Using native 16bit precision.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/nemo_error_log.txt
deleted file mode 100644
index 9358c62..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/nemo_error_log.txt
+++ /dev/null
@@ -1 +0,0 @@
-[NeMo W 2021-02-08 10:45:17 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 141e96e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,3 +0,0 @@
-[NeMo I 2021-02-08 10:45:16 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16
-[NeMo I 2021-02-08 10:45:16 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-08 10:45:17 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/git-info.log
deleted file mode 100644
index de2f700..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/git-info.log
+++ /dev/null
@@ -1,623 +0,0 @@
-commit hash: 939a671c8c117db6975316767ced5d95449e2b27
-diff --git a/README.md b/README.md
-index d52dba5..0b403d3 100644
---- a/README.md
-+++ b/README.md
-@@ -333,4 +333,71 @@ label                                                precision    recall       f
-  'punct_f1': 0.5858508944511414,
-  'punct_precision': 0.30176490545272827,
-  'punct_recall': 10.0,
-- 'test_loss': 0.8140875697135925}
-\ No newline at end of file
-+ 'test_loss': 0.8140875697135925}
-+
-+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
-+
-+ (label_id: 0)                                          62.15     100.00      76.66       5154
-+! (label_id: 1)                                          0.00       0.00       0.00        108
-+, (label_id: 2)                                          0.00       0.00       0.00      18022
-+- (label_id: 3)                                          0.00       0.00       0.00       1557
-+. (label_id: 4)                                         41.74      94.01      57.81      15164
-+: (label_id: 5)                                          0.00       0.00       0.00        319
-+; (label_id: 6)                                          0.00       0.00       0.00         88
-+? (label_id: 7)                                          0.00       0.00       0.00       1217
-+ (label_id: 8)                                          0.00       0.00       0.00        752
-+… (label_id: 9)                                          0.00       0.00       0.00         67
-+-------------------
-+micro avg                                               45.72      45.72      45.72      42448
-+macro avg                                               10.39      19.40      13.45      42448
-+weighted avg                                            22.46      45.72      29.96      42448
-+{ 'punct_f1': 13.446383476257324,
-+ 'punct_precision': 10.388500213623047,
-+ 'punct_recall': 19.400554656982422,
-+ 'test_loss': 0.44148480892181396}
-+
-+
-+ ## Log for 8/2/2021
-+
-+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
-+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
-+
-+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
-+End frozen 
-+micro avg                                               41.42      41.42      41.42      33406
-+macro avg                                               11.01      13.54      11.07      33406
-+weighted avg                                            34.88      41.42      34.20      33406
-+
-+1st layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               36.65      36.65      36.65      33463
-+macro avg                                               10.71       9.91       8.26      33463
-+weighted avg                                            34.32      36.65      31.08      33463
-+
-+2nd layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               35.72      35.72      35.72      42448
-+macro avg                                                3.57      10.00       5.26      42448
-+weighted avg                                            12.76      35.72      18.81      42448
-+
-+{'punct_f1': 5.264181137084961,
-+ 'punct_precision': 3.572371006011963,
-+ 'punct_recall': 10.0,
-+ 'test_loss': 18.49854850769043}
-+
-+
-+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
-+alpha from 3->4 seems to reduce convergence rate.
-+
-+micro avg                                               50.98      50.98      50.98      33463
-+macro avg                                               25.99      25.38      23.38      33463
-+weighted avg                                            50.31      50.98      48.27      33463
-+
-+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
-+micro avg                                               58.55      58.55      58.55      39340
-+macro avg                                               30.02      29.74      29.52      39340
-+weighted avg                                            57.91      58.55      57.51      39340
-+
-+still increasing?!
-+{'punct_f1': 29.523975372314453,
-+ 'punct_precision': 30.015613555908203,
-+ 'punct_recall': 29.738296508789062,
-+ 'test_loss': 0.3690211772918701}
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index fe58670..872a5ed 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,49 +1,49 @@
- seed: 42
- trainer:
--    # gpus: 1 # the number of gpus, 0 for CPU
--    # num_nodes: 1
--    # max_epochs: 2
--    # max_steps: null # precedence over max_epochs
--    # accumulate_grad_batches: 4 # accumulates grads every k batches
--    # gradient_clip_val: 0
--    # amp_level: O1 # O1/O2 for mixed precision
--    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
--    # checkpoint_callback: false  # Provided by exp_manager
--    # logger: false #false  # Provided by exp_manager
--    # log_every_n_steps: 1  # Interval of logging.
--    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # resume_from_checkpoint: null
--
--    gpus: 0 # the number of gpus, 0 for CPU
-+    gpus: 1 # the number of gpus, 0 for CPU
-     num_nodes: 1
--    max_epochs: 8
-+    max_epochs: 10
-     max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 1 # accumulates grads every k batches
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-     gradient_clip_val: 0
--    amp_level: O0 # O1/O2 for mixed precision
--    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
-+    amp_level: O1 # O1/O2 for mixed precision
-+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    accelerator: ddp
-     checkpoint_callback: false  # Provided by exp_manager
-     logger: false #false  # Provided by exp_manager
-     log_every_n_steps: 1  # Interval of logging.
-     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    reload_dataloaders_every_epoch: true
-     resume_from_checkpoint: null
- 
-+    # gpus: 0 # the number of gpus, 0 for CPU
-+    # num_nodes: 1
-+    # max_epochs: 8
-+    # max_steps: null # precedence over max_epochs
-+    # accumulate_grad_batches: 1 # accumulates grads every k batches
-+    # gradient_clip_val: 0
-+    # amp_level: O0 # O1/O2 for mixed precision
-+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # # accelerator: ddp
-+    # checkpoint_callback: false  # Provided by exp_manager
-+    # logger: false #false  # Provided by exp_manager
-+    # log_every_n_steps: 1  # Interval of logging.
-+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    # reload_dataloaders_every_epoch: true
-+    # resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /home/nxingyu2/data # /root/data # 
--tmp_path: /home/nxingyu2/data/tmp # /tmp # 
-+base_path: /home/nxingyu/data # /root/data # 
-+tmp_path: /home/nxingyu/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
--    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-     unfrozen: 0
--    maximum_unfrozen: 2
-+    maximum_unfrozen: 1
-     unfreeze_step: 1
-     # unfreeze_every: 3
-     punct_label_ids:
-@@ -58,10 +58,10 @@ model:
-         - "—"
-         - "…"
- 
--    punct_class_weights: true
-+    punct_class_weights: false
-     
-     dataset:
--        data_dir: /home/nxingyu2/data # /root/data # 
-+        data_dir: /home/nxingyu/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -106,6 +106,11 @@ model:
-         config: null
-         # unfrozen_layers: 1
- 
-+    mlp:
-+        num_fc_layers: 2
-+        fc_dropout: 0.1
-+        activation: 'relu'
-+        
-     punct_head:
-         punct_num_fc_layers: 1
-         fc_dropout: 0.1
-@@ -117,22 +122,24 @@ model:
-     domain_head:
-         domain_num_fc_layers: 1
-         fc_dropout: 0.1
--        activation: 'relu'
-+        activation: 'gelu'
-         log_softmax: false
-         use_transformer_init: true
-         loss: 'cel'
-         gamma: 0.1 # coefficient of gradient reversal
-+        pooling: 'token'
-+        idx_conditioned_on: 0
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 3
-+        alpha: 4
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: novograd
-+        name: adamw
-         lr: 1e-3
-         weight_decay: 0.00
-         sched:
-diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
-index d4ff927..b4b92b0 100644
---- a/experiment/core/layers/sequence_classifier.py
-+++ b/experiment/core/layers/sequence_classifier.py
-@@ -20,7 +20,8 @@ class SequenceClassifier(nn.Module):
-         log_softmax: bool = True,
-         dropout: float = 0.0,
-         use_transformer_init: bool = True,
--        idx_conditioned_on: int = 0,
-+        pooling: str = 'mean', # mean, max, mean_max, token
-+        idx_conditioned_on: int = None,
-     ):
-         """
-         Initializes the SequenceClassifier module.
-@@ -37,6 +38,7 @@ class SequenceClassifier(nn.Module):
-         super().__init__()
-         self.log_softmax = log_softmax
-         self._idx_conditioned_on = idx_conditioned_on
-+        self.pooling = pooling
-         self.mlp = MultiLayerPerceptron(
-             hidden_size=hidden_size,
-             num_classes=num_classes,
-@@ -48,7 +50,23 @@ class SequenceClassifier(nn.Module):
-         if use_transformer_init:
-             self.apply(lambda module: transformer_weights_init(module, xavier=False))
- 
--    def forward(self, hidden_states):
-+    def forward(self, hidden_states, subtoken_mask=None):
-         hidden_states = self.dropout(hidden_states)
--        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
-+        if self.pooling=='token':
-+            pooled = hidden_states[:, self._idx_conditioned_on]
-+        else:
-+            if subtoken_mask==None:
-+                ct=hidden_states.shape[1] # Seq len
-+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
-+            else:
-+                ct = torch.sum(subtoken_mask,axis=1)
-+            pooled_sum = torch.sum(hidden_states,axis=1)
-+            if self.pooling=='mean' or self.pooling == 'mean_max':
-+                pooled_mean = pooled_sum//ct
-+            if self.pooling=='max' or self.pooling=='mean_max':
-+                pooled_max = torch.max(hidden_states,axis=1)[0]
-+            pooled=pooled_mean if self.pooling=='mean' else \
-+                pooled_max if self.pooling=='max' else \
-+                    torch.cat([pooled_mean,pooled_max],axis=-1)
-+        logits = self.mlp(pooled)
-         return logits
-diff --git a/experiment/info.log b/experiment/info.log
-index 2471fe9..e69de29 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,300 +0,0 @@
--[INFO] - GPU available: True, used: False
--[INFO] - TPU available: None, using: 0 TPU cores
--[INFO] - shuffling train set
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        184
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.23       0.34       0.53        594
--- (label_id: 3)                                          3.06      25.42       5.46         59
--. (label_id: 4)                                         47.22      12.98      20.36        524
--: (label_id: 5)                                          0.00       0.00       0.00         18
--; (label_id: 6)                                          0.00       0.00       0.00         13
--? (label_id: 7)                                          8.45       6.32       7.23         95
--— (label_id: 8)                                          0.00       0.00       0.00         12
--… (label_id: 9)                                          0.00       0.00       0.00          0
---------------------
--micro avg                                                6.05       6.05       6.05       1503
--macro avg                                                6.66       5.01       3.73       1503
--weighted avg                                            17.61       6.05       7.98       1503
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00         92
---------------------
--micro avg                                              100.00     100.00     100.00         92
--macro avg                                              100.00     100.00     100.00         92
--weighted avg                                           100.00     100.00     100.00         92
--
--[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 1.5848931924611143e-08
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
--will be used during training (effective maximum steps = 3192) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 3192
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        202
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.62       0.45       0.70        669
--- (label_id: 3)                                          3.48      27.27       6.17         66
--. (label_id: 4)                                         45.06      13.01      20.19        561
--: (label_id: 5)                                          1.52       6.67       2.47         15
--; (label_id: 6)                                          0.00       0.00       0.00         15
--? (label_id: 7)                                          8.70       7.32       7.95         82
--— (label_id: 8)                                          0.00       0.00       0.00         13
--… (label_id: 9)                                          0.00       0.00       0.00          1
---------------------
--micro avg                                                6.20       6.20       6.20       1628
--macro avg                                                6.04       5.47       3.75       1628
--weighted avg                                            16.79       6.20       7.92       1628
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        101
---------------------
--micro avg                                              100.00     100.00     100.00        101
--macro avg                                              100.00     100.00     100.00        101
--weighted avg                                           100.00     100.00     100.00        101
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.34       0.73       0.46       4402
--! (label_id: 1)                                          0.42      13.95       0.82        129
--, (label_id: 2)                                          2.53       0.64       1.03      15243
--- (label_id: 3)                                          2.45      21.03       4.38       1322
--. (label_id: 4)                                         44.00      11.40      18.11      12542
--: (label_id: 5)                                          0.43       1.41       0.65        354
--; (label_id: 6)                                          0.00       0.00       0.00        163
--? (label_id: 7)                                          4.16       6.27       5.00       1117
--— (label_id: 8)                                          3.00       0.61       1.02        488
--… (label_id: 9)                                          0.97       6.17       1.68         81
---------------------
--micro avg                                                5.41       5.41       5.41      35841
--macro avg                                                5.83       6.22       3.32      35841
--weighted avg                                            16.78       5.41       7.18      35841
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2201
---------------------
--micro avg                                              100.00     100.00     100.00       2201
--macro avg                                              100.00     100.00     100.00       2201
--weighted avg                                           100.00     100.00     100.00       2201
--
--[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.20       0.43       0.27       4226
--! (label_id: 1)                                          0.44      14.17       0.86        127
--, (label_id: 2)                                          1.93       0.49       0.78      14611
--- (label_id: 3)                                          2.23      19.56       4.01       1237
--. (label_id: 4)                                         43.37      11.25      17.86      11977
--: (label_id: 5)                                          0.68       2.34       1.05        342
--; (label_id: 6)                                          0.00       0.00       0.00        129
--? (label_id: 7)                                          5.16       7.47       6.10       1058
--— (label_id: 8)                                          2.15       0.49       0.80        409
--… (label_id: 9)                                          0.69       4.23       1.19         71
---------------------
--micro avg                                                5.23       5.23       5.23      34187
--macro avg                                                5.68       6.04       3.29      34187
--weighted avg                                            16.32       5.23       6.98      34187
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2113
---------------------
--micro avg                                              100.00     100.00     100.00       2113
--macro avg                                              100.00     100.00     100.00       2113
--weighted avg                                           100.00     100.00     100.00       2113
--
--[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.61       0.39       4228
--! (label_id: 1)                                          0.30       8.28       0.58        145
--, (label_id: 2)                                          2.27       0.58       0.92      14495
--- (label_id: 3)                                          2.64      21.78       4.70       1327
--. (label_id: 4)                                         44.87      11.66      18.51      12193
--: (label_id: 5)                                          0.60       1.93       0.91        362
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.19       6.40       5.07       1078
--— (label_id: 8)                                          1.16       0.22       0.37        459
--… (label_id: 9)                                          0.85       4.17       1.41         96
---------------------
--micro avg                                                5.54       5.54       5.54      34547
--macro avg                                                5.72       5.56       3.29      34547
--weighted avg                                            17.08       5.54       7.33      34547
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2114
---------------------
--micro avg                                              100.00     100.00     100.00       2114
--macro avg                                              100.00     100.00     100.00       2114
--weighted avg                                           100.00     100.00     100.00       2114
--
--[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.29       0.63       0.40       4444
--! (label_id: 1)                                          0.38      10.67       0.74        150
--, (label_id: 2)                                          2.32       0.59       0.94      15290
--- (label_id: 3)                                          2.34      20.28       4.19       1292
--. (label_id: 4)                                         43.85      11.68      18.44      12599
--: (label_id: 5)                                          0.41       1.28       0.62        392
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.24       6.30       5.07       1111
--— (label_id: 8)                                          0.00       0.00       0.00        456
--… (label_id: 9)                                          0.38       2.41       0.65         83
---------------------
--micro avg                                                5.40       5.40       5.40      35981
--macro avg                                                5.42       5.38       3.11      35981
--weighted avg                                            16.59       5.40       7.22      35981
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2222
---------------------
--micro avg                                              100.00     100.00     100.00       2222
--macro avg                                              100.00     100.00     100.00       2222
--weighted avg                                           100.00     100.00     100.00       2222
--
--[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.35       0.73       0.48       3844
--! (label_id: 1)                                          0.54      14.62       1.04        130
--, (label_id: 2)                                          2.32       0.59       0.94      13056
--- (label_id: 3)                                          2.67      22.28       4.77       1194
--. (label_id: 4)                                         44.45      11.95      18.84      10791
--: (label_id: 5)                                          0.84       3.21       1.33        280
--; (label_id: 6)                                          0.00       0.00       0.00        140
--? (label_id: 7)                                          4.17       6.56       5.10        914
--— (label_id: 8)                                          0.00       0.00       0.00        401
--… (label_id: 9)                                          0.48       2.63       0.81         76
---------------------
--micro avg                                                5.68       5.68       5.68      30826
--macro avg                                                5.58       6.26       3.33      30826
--weighted avg                                            16.82       5.68       7.41      30826
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1922
---------------------
--micro avg                                              100.00     100.00     100.00       1922
--macro avg                                              100.00     100.00     100.00       1922
--weighted avg                                           100.00     100.00     100.00       1922
--
--[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.60       0.39       3970
--! (label_id: 1)                                          0.35      10.66       0.68        122
--, (label_id: 2)                                          2.09       0.53       0.85      13469
--- (label_id: 3)                                          2.29      19.32       4.10       1201
--. (label_id: 4)                                         43.43      11.24      17.86      11227
--: (label_id: 5)                                          0.63       2.30       0.99        304
--; (label_id: 6)                                          0.00       0.00       0.00        141
--? (label_id: 7)                                          4.52       6.86       5.45       1006
--— (label_id: 8)                                          1.15       0.23       0.38        444
--… (label_id: 9)                                          0.45       2.67       0.78         75
---------------------
--micro avg                                                5.26       5.26       5.26      31959
--macro avg                                                5.52       5.44       3.15      31959
--weighted avg                                            16.42       5.26       7.02      31959
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1985
---------------------
--micro avg                                              100.00     100.00     100.00       1985
--macro avg                                              100.00     100.00     100.00       1985
--weighted avg                                           100.00     100.00     100.00       1985
--
--[INFO] - Epoch 5, step 2394: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.23       0.48       0.31       4126
--! (label_id: 1)                                          0.29       9.40       0.56        117
--, (label_id: 2)                                          1.91       0.49       0.77      14019
--- (label_id: 3)                                          2.52      22.59       4.53       1164
--. (label_id: 4)                                         44.15      11.65      18.44      11789
--: (label_id: 5)                                          0.72       2.41       1.11        332
--; (label_id: 6)                                          0.56       0.61       0.58        165
--? (label_id: 7)                                          3.89       6.53       4.88        980
--— (label_id: 8)                                          2.30       0.47       0.77        430
--… (label_id: 9)                                          1.18       8.33       2.07         60
---------------------
--micro avg                                                5.47       5.47       5.47      33182
--macro avg                                                5.77       6.30       3.40      33182
--weighted avg                                            16.77       5.47       7.25      33182
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2063
---------------------
--micro avg                                              100.00     100.00     100.00       2063
--macro avg                                              100.00     100.00     100.00       2063
--weighted avg                                           100.00     100.00     100.00       2063
--
--[INFO] - Epoch 6, step 2793: val_loss was not in top 3
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index fa37b4c..6b5ff89 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             self.hparams.model.punct_class_weights=None
- 
-+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
-+        self.mlp = MultiLayerPerceptron(
-+            self.transformer.config.hidden_size,
-+            self.transformer.config.hidden_size,
-+            num_layers=self.hparams.model.mlp.num_fc_layers, 
-+            activation=self.hparams.model.mlp.activation, 
-+            log_softmax=self.hparams.model.mlp.log_softmax
-+        )
-+
-         self.punct_classifier = TokenClassifier(
-             hidden_size=self.transformer.config.hidden_size,
-             num_classes=len(self.labels_to_ids),
-@@ -80,6 +89,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-             log_softmax=self.hparams.model.domain_head.log_softmax,
-             dropout=self.hparams.model.domain_head.fc_dropout,
-             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
-+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
-         )
- 
-         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
-@@ -119,14 +129,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
-         self.freeze()
- 
--    def forward(self, input_ids, attention_mask, domain_ids=None):
-+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
-         hidden_states = self.transformer(
-             input_ids=input_ids, attention_mask=attention_mask
-         )[0]
-+        hidden_states = self.dropout(hidden_states)
-+        hidden_states = self.mlp(hidden_states)
-         punct_logits = self.punct_classifier(hidden_states=hidden_states)
-         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
-         domain_logits = self.domain_classifier(
--            hidden_states=reverse_grad_hidden_states)
-+            hidden_states=reverse_grad_hidden_states,
-+            subtoken_mask=subtoken_mask)
-         return punct_logits, domain_logits
- 
-     def _make_step(self, batch):
-@@ -136,7 +149,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         punct_labels = batch['labels']
-         domain_labels = batch['domain']
-         punct_logits, domain_logits = self(
--            input_ids=input_ids, attention_mask=attention_mask
-+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
-         )
-         punctuation_loss = self.punctuation_loss(
-             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/lightning_logs.txt
deleted file mode 100644
index 8942cad..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/lightning_logs.txt
+++ /dev/null
@@ -1,5 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-Using native 16bit precision.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/nemo_error_log.txt
deleted file mode 100644
index 135ccc7..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/nemo_error_log.txt
+++ /dev/null
@@ -1 +0,0 @@
-[NeMo W 2021-02-08 10:45:45 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 809269b..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,3 +0,0 @@
-[NeMo I 2021-02-08 10:45:45 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45
-[NeMo I 2021-02-08 10:45:45 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-08 10:45:45 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/git-info.log
deleted file mode 100644
index b80d6d5..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/git-info.log
+++ /dev/null
@@ -1,624 +0,0 @@
-commit hash: 939a671c8c117db6975316767ced5d95449e2b27
-diff --git a/README.md b/README.md
-index d52dba5..0b403d3 100644
---- a/README.md
-+++ b/README.md
-@@ -333,4 +333,71 @@ label                                                precision    recall       f
-  'punct_f1': 0.5858508944511414,
-  'punct_precision': 0.30176490545272827,
-  'punct_recall': 10.0,
-- 'test_loss': 0.8140875697135925}
-\ No newline at end of file
-+ 'test_loss': 0.8140875697135925}
-+
-+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
-+
-+ (label_id: 0)                                          62.15     100.00      76.66       5154
-+! (label_id: 1)                                          0.00       0.00       0.00        108
-+, (label_id: 2)                                          0.00       0.00       0.00      18022
-+- (label_id: 3)                                          0.00       0.00       0.00       1557
-+. (label_id: 4)                                         41.74      94.01      57.81      15164
-+: (label_id: 5)                                          0.00       0.00       0.00        319
-+; (label_id: 6)                                          0.00       0.00       0.00         88
-+? (label_id: 7)                                          0.00       0.00       0.00       1217
-+ (label_id: 8)                                          0.00       0.00       0.00        752
-+… (label_id: 9)                                          0.00       0.00       0.00         67
-+-------------------
-+micro avg                                               45.72      45.72      45.72      42448
-+macro avg                                               10.39      19.40      13.45      42448
-+weighted avg                                            22.46      45.72      29.96      42448
-+{ 'punct_f1': 13.446383476257324,
-+ 'punct_precision': 10.388500213623047,
-+ 'punct_recall': 19.400554656982422,
-+ 'test_loss': 0.44148480892181396}
-+
-+
-+ ## Log for 8/2/2021
-+
-+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
-+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
-+
-+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
-+End frozen 
-+micro avg                                               41.42      41.42      41.42      33406
-+macro avg                                               11.01      13.54      11.07      33406
-+weighted avg                                            34.88      41.42      34.20      33406
-+
-+1st layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               36.65      36.65      36.65      33463
-+macro avg                                               10.71       9.91       8.26      33463
-+weighted avg                                            34.32      36.65      31.08      33463
-+
-+2nd layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               35.72      35.72      35.72      42448
-+macro avg                                                3.57      10.00       5.26      42448
-+weighted avg                                            12.76      35.72      18.81      42448
-+
-+{'punct_f1': 5.264181137084961,
-+ 'punct_precision': 3.572371006011963,
-+ 'punct_recall': 10.0,
-+ 'test_loss': 18.49854850769043}
-+
-+
-+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
-+alpha from 3->4 seems to reduce convergence rate.
-+
-+micro avg                                               50.98      50.98      50.98      33463
-+macro avg                                               25.99      25.38      23.38      33463
-+weighted avg                                            50.31      50.98      48.27      33463
-+
-+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
-+micro avg                                               58.55      58.55      58.55      39340
-+macro avg                                               30.02      29.74      29.52      39340
-+weighted avg                                            57.91      58.55      57.51      39340
-+
-+still increasing?!
-+{'punct_f1': 29.523975372314453,
-+ 'punct_precision': 30.015613555908203,
-+ 'punct_recall': 29.738296508789062,
-+ 'test_loss': 0.3690211772918701}
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index fe58670..69c6f92 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,49 +1,49 @@
- seed: 42
- trainer:
--    # gpus: 1 # the number of gpus, 0 for CPU
--    # num_nodes: 1
--    # max_epochs: 2
--    # max_steps: null # precedence over max_epochs
--    # accumulate_grad_batches: 4 # accumulates grads every k batches
--    # gradient_clip_val: 0
--    # amp_level: O1 # O1/O2 for mixed precision
--    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
--    # checkpoint_callback: false  # Provided by exp_manager
--    # logger: false #false  # Provided by exp_manager
--    # log_every_n_steps: 1  # Interval of logging.
--    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # resume_from_checkpoint: null
--
--    gpus: 0 # the number of gpus, 0 for CPU
-+    gpus: 1 # the number of gpus, 0 for CPU
-     num_nodes: 1
--    max_epochs: 8
-+    max_epochs: 10
-     max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 1 # accumulates grads every k batches
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-     gradient_clip_val: 0
--    amp_level: O0 # O1/O2 for mixed precision
--    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
-+    amp_level: O1 # O1/O2 for mixed precision
-+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    accelerator: ddp
-     checkpoint_callback: false  # Provided by exp_manager
-     logger: false #false  # Provided by exp_manager
-     log_every_n_steps: 1  # Interval of logging.
-     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    reload_dataloaders_every_epoch: true
-     resume_from_checkpoint: null
- 
-+    # gpus: 0 # the number of gpus, 0 for CPU
-+    # num_nodes: 1
-+    # max_epochs: 8
-+    # max_steps: null # precedence over max_epochs
-+    # accumulate_grad_batches: 1 # accumulates grads every k batches
-+    # gradient_clip_val: 0
-+    # amp_level: O0 # O1/O2 for mixed precision
-+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # # accelerator: ddp
-+    # checkpoint_callback: false  # Provided by exp_manager
-+    # logger: false #false  # Provided by exp_manager
-+    # log_every_n_steps: 1  # Interval of logging.
-+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    # reload_dataloaders_every_epoch: true
-+    # resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /home/nxingyu2/data # /root/data # 
--tmp_path: /home/nxingyu2/data/tmp # /tmp # 
-+base_path: /home/nxingyu/data # /root/data # 
-+tmp_path: /home/nxingyu/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
--    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-     unfrozen: 0
--    maximum_unfrozen: 2
-+    maximum_unfrozen: 1
-     unfreeze_step: 1
-     # unfreeze_every: 3
-     punct_label_ids:
-@@ -58,10 +58,10 @@ model:
-         - "—"
-         - "…"
- 
--    punct_class_weights: true
-+    punct_class_weights: false
-     
-     dataset:
--        data_dir: /home/nxingyu2/data # /root/data # 
-+        data_dir: /home/nxingyu/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -106,6 +106,12 @@ model:
-         config: null
-         # unfrozen_layers: 1
- 
-+    mlp:
-+        num_fc_layers: 2
-+        fc_dropout: 0.1
-+        log_softmax: false
-+        activation: 'relu'
-+        
-     punct_head:
-         punct_num_fc_layers: 1
-         fc_dropout: 0.1
-@@ -117,22 +123,24 @@ model:
-     domain_head:
-         domain_num_fc_layers: 1
-         fc_dropout: 0.1
--        activation: 'relu'
-+        activation: 'gelu'
-         log_softmax: false
-         use_transformer_init: true
-         loss: 'cel'
-         gamma: 0.1 # coefficient of gradient reversal
-+        pooling: 'token'
-+        idx_conditioned_on: 0
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 3
-+        alpha: 4
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: novograd
-+        name: adamw
-         lr: 1e-3
-         weight_decay: 0.00
-         sched:
-diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
-index d4ff927..b4b92b0 100644
---- a/experiment/core/layers/sequence_classifier.py
-+++ b/experiment/core/layers/sequence_classifier.py
-@@ -20,7 +20,8 @@ class SequenceClassifier(nn.Module):
-         log_softmax: bool = True,
-         dropout: float = 0.0,
-         use_transformer_init: bool = True,
--        idx_conditioned_on: int = 0,
-+        pooling: str = 'mean', # mean, max, mean_max, token
-+        idx_conditioned_on: int = None,
-     ):
-         """
-         Initializes the SequenceClassifier module.
-@@ -37,6 +38,7 @@ class SequenceClassifier(nn.Module):
-         super().__init__()
-         self.log_softmax = log_softmax
-         self._idx_conditioned_on = idx_conditioned_on
-+        self.pooling = pooling
-         self.mlp = MultiLayerPerceptron(
-             hidden_size=hidden_size,
-             num_classes=num_classes,
-@@ -48,7 +50,23 @@ class SequenceClassifier(nn.Module):
-         if use_transformer_init:
-             self.apply(lambda module: transformer_weights_init(module, xavier=False))
- 
--    def forward(self, hidden_states):
-+    def forward(self, hidden_states, subtoken_mask=None):
-         hidden_states = self.dropout(hidden_states)
--        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
-+        if self.pooling=='token':
-+            pooled = hidden_states[:, self._idx_conditioned_on]
-+        else:
-+            if subtoken_mask==None:
-+                ct=hidden_states.shape[1] # Seq len
-+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
-+            else:
-+                ct = torch.sum(subtoken_mask,axis=1)
-+            pooled_sum = torch.sum(hidden_states,axis=1)
-+            if self.pooling=='mean' or self.pooling == 'mean_max':
-+                pooled_mean = pooled_sum//ct
-+            if self.pooling=='max' or self.pooling=='mean_max':
-+                pooled_max = torch.max(hidden_states,axis=1)[0]
-+            pooled=pooled_mean if self.pooling=='mean' else \
-+                pooled_max if self.pooling=='max' else \
-+                    torch.cat([pooled_mean,pooled_max],axis=-1)
-+        logits = self.mlp(pooled)
-         return logits
-diff --git a/experiment/info.log b/experiment/info.log
-index 2471fe9..e69de29 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,300 +0,0 @@
--[INFO] - GPU available: True, used: False
--[INFO] - TPU available: None, using: 0 TPU cores
--[INFO] - shuffling train set
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        184
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.23       0.34       0.53        594
--- (label_id: 3)                                          3.06      25.42       5.46         59
--. (label_id: 4)                                         47.22      12.98      20.36        524
--: (label_id: 5)                                          0.00       0.00       0.00         18
--; (label_id: 6)                                          0.00       0.00       0.00         13
--? (label_id: 7)                                          8.45       6.32       7.23         95
--— (label_id: 8)                                          0.00       0.00       0.00         12
--… (label_id: 9)                                          0.00       0.00       0.00          0
---------------------
--micro avg                                                6.05       6.05       6.05       1503
--macro avg                                                6.66       5.01       3.73       1503
--weighted avg                                            17.61       6.05       7.98       1503
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00         92
---------------------
--micro avg                                              100.00     100.00     100.00         92
--macro avg                                              100.00     100.00     100.00         92
--weighted avg                                           100.00     100.00     100.00         92
--
--[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 1.5848931924611143e-08
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
--will be used during training (effective maximum steps = 3192) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 3192
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        202
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.62       0.45       0.70        669
--- (label_id: 3)                                          3.48      27.27       6.17         66
--. (label_id: 4)                                         45.06      13.01      20.19        561
--: (label_id: 5)                                          1.52       6.67       2.47         15
--; (label_id: 6)                                          0.00       0.00       0.00         15
--? (label_id: 7)                                          8.70       7.32       7.95         82
--— (label_id: 8)                                          0.00       0.00       0.00         13
--… (label_id: 9)                                          0.00       0.00       0.00          1
---------------------
--micro avg                                                6.20       6.20       6.20       1628
--macro avg                                                6.04       5.47       3.75       1628
--weighted avg                                            16.79       6.20       7.92       1628
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        101
---------------------
--micro avg                                              100.00     100.00     100.00        101
--macro avg                                              100.00     100.00     100.00        101
--weighted avg                                           100.00     100.00     100.00        101
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.34       0.73       0.46       4402
--! (label_id: 1)                                          0.42      13.95       0.82        129
--, (label_id: 2)                                          2.53       0.64       1.03      15243
--- (label_id: 3)                                          2.45      21.03       4.38       1322
--. (label_id: 4)                                         44.00      11.40      18.11      12542
--: (label_id: 5)                                          0.43       1.41       0.65        354
--; (label_id: 6)                                          0.00       0.00       0.00        163
--? (label_id: 7)                                          4.16       6.27       5.00       1117
--— (label_id: 8)                                          3.00       0.61       1.02        488
--… (label_id: 9)                                          0.97       6.17       1.68         81
---------------------
--micro avg                                                5.41       5.41       5.41      35841
--macro avg                                                5.83       6.22       3.32      35841
--weighted avg                                            16.78       5.41       7.18      35841
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2201
---------------------
--micro avg                                              100.00     100.00     100.00       2201
--macro avg                                              100.00     100.00     100.00       2201
--weighted avg                                           100.00     100.00     100.00       2201
--
--[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.20       0.43       0.27       4226
--! (label_id: 1)                                          0.44      14.17       0.86        127
--, (label_id: 2)                                          1.93       0.49       0.78      14611
--- (label_id: 3)                                          2.23      19.56       4.01       1237
--. (label_id: 4)                                         43.37      11.25      17.86      11977
--: (label_id: 5)                                          0.68       2.34       1.05        342
--; (label_id: 6)                                          0.00       0.00       0.00        129
--? (label_id: 7)                                          5.16       7.47       6.10       1058
--— (label_id: 8)                                          2.15       0.49       0.80        409
--… (label_id: 9)                                          0.69       4.23       1.19         71
---------------------
--micro avg                                                5.23       5.23       5.23      34187
--macro avg                                                5.68       6.04       3.29      34187
--weighted avg                                            16.32       5.23       6.98      34187
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2113
---------------------
--micro avg                                              100.00     100.00     100.00       2113
--macro avg                                              100.00     100.00     100.00       2113
--weighted avg                                           100.00     100.00     100.00       2113
--
--[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.61       0.39       4228
--! (label_id: 1)                                          0.30       8.28       0.58        145
--, (label_id: 2)                                          2.27       0.58       0.92      14495
--- (label_id: 3)                                          2.64      21.78       4.70       1327
--. (label_id: 4)                                         44.87      11.66      18.51      12193
--: (label_id: 5)                                          0.60       1.93       0.91        362
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.19       6.40       5.07       1078
--— (label_id: 8)                                          1.16       0.22       0.37        459
--… (label_id: 9)                                          0.85       4.17       1.41         96
---------------------
--micro avg                                                5.54       5.54       5.54      34547
--macro avg                                                5.72       5.56       3.29      34547
--weighted avg                                            17.08       5.54       7.33      34547
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2114
---------------------
--micro avg                                              100.00     100.00     100.00       2114
--macro avg                                              100.00     100.00     100.00       2114
--weighted avg                                           100.00     100.00     100.00       2114
--
--[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.29       0.63       0.40       4444
--! (label_id: 1)                                          0.38      10.67       0.74        150
--, (label_id: 2)                                          2.32       0.59       0.94      15290
--- (label_id: 3)                                          2.34      20.28       4.19       1292
--. (label_id: 4)                                         43.85      11.68      18.44      12599
--: (label_id: 5)                                          0.41       1.28       0.62        392
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.24       6.30       5.07       1111
--— (label_id: 8)                                          0.00       0.00       0.00        456
--… (label_id: 9)                                          0.38       2.41       0.65         83
---------------------
--micro avg                                                5.40       5.40       5.40      35981
--macro avg                                                5.42       5.38       3.11      35981
--weighted avg                                            16.59       5.40       7.22      35981
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2222
---------------------
--micro avg                                              100.00     100.00     100.00       2222
--macro avg                                              100.00     100.00     100.00       2222
--weighted avg                                           100.00     100.00     100.00       2222
--
--[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.35       0.73       0.48       3844
--! (label_id: 1)                                          0.54      14.62       1.04        130
--, (label_id: 2)                                          2.32       0.59       0.94      13056
--- (label_id: 3)                                          2.67      22.28       4.77       1194
--. (label_id: 4)                                         44.45      11.95      18.84      10791
--: (label_id: 5)                                          0.84       3.21       1.33        280
--; (label_id: 6)                                          0.00       0.00       0.00        140
--? (label_id: 7)                                          4.17       6.56       5.10        914
--— (label_id: 8)                                          0.00       0.00       0.00        401
--… (label_id: 9)                                          0.48       2.63       0.81         76
---------------------
--micro avg                                                5.68       5.68       5.68      30826
--macro avg                                                5.58       6.26       3.33      30826
--weighted avg                                            16.82       5.68       7.41      30826
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1922
---------------------
--micro avg                                              100.00     100.00     100.00       1922
--macro avg                                              100.00     100.00     100.00       1922
--weighted avg                                           100.00     100.00     100.00       1922
--
--[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.60       0.39       3970
--! (label_id: 1)                                          0.35      10.66       0.68        122
--, (label_id: 2)                                          2.09       0.53       0.85      13469
--- (label_id: 3)                                          2.29      19.32       4.10       1201
--. (label_id: 4)                                         43.43      11.24      17.86      11227
--: (label_id: 5)                                          0.63       2.30       0.99        304
--; (label_id: 6)                                          0.00       0.00       0.00        141
--? (label_id: 7)                                          4.52       6.86       5.45       1006
--— (label_id: 8)                                          1.15       0.23       0.38        444
--… (label_id: 9)                                          0.45       2.67       0.78         75
---------------------
--micro avg                                                5.26       5.26       5.26      31959
--macro avg                                                5.52       5.44       3.15      31959
--weighted avg                                            16.42       5.26       7.02      31959
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1985
---------------------
--micro avg                                              100.00     100.00     100.00       1985
--macro avg                                              100.00     100.00     100.00       1985
--weighted avg                                           100.00     100.00     100.00       1985
--
--[INFO] - Epoch 5, step 2394: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.23       0.48       0.31       4126
--! (label_id: 1)                                          0.29       9.40       0.56        117
--, (label_id: 2)                                          1.91       0.49       0.77      14019
--- (label_id: 3)                                          2.52      22.59       4.53       1164
--. (label_id: 4)                                         44.15      11.65      18.44      11789
--: (label_id: 5)                                          0.72       2.41       1.11        332
--; (label_id: 6)                                          0.56       0.61       0.58        165
--? (label_id: 7)                                          3.89       6.53       4.88        980
--— (label_id: 8)                                          2.30       0.47       0.77        430
--… (label_id: 9)                                          1.18       8.33       2.07         60
---------------------
--micro avg                                                5.47       5.47       5.47      33182
--macro avg                                                5.77       6.30       3.40      33182
--weighted avg                                            16.77       5.47       7.25      33182
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2063
---------------------
--micro avg                                              100.00     100.00     100.00       2063
--macro avg                                              100.00     100.00     100.00       2063
--weighted avg                                           100.00     100.00     100.00       2063
--
--[INFO] - Epoch 6, step 2793: val_loss was not in top 3
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index fa37b4c..6b5ff89 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             self.hparams.model.punct_class_weights=None
- 
-+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
-+        self.mlp = MultiLayerPerceptron(
-+            self.transformer.config.hidden_size,
-+            self.transformer.config.hidden_size,
-+            num_layers=self.hparams.model.mlp.num_fc_layers, 
-+            activation=self.hparams.model.mlp.activation, 
-+            log_softmax=self.hparams.model.mlp.log_softmax
-+        )
-+
-         self.punct_classifier = TokenClassifier(
-             hidden_size=self.transformer.config.hidden_size,
-             num_classes=len(self.labels_to_ids),
-@@ -80,6 +89,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-             log_softmax=self.hparams.model.domain_head.log_softmax,
-             dropout=self.hparams.model.domain_head.fc_dropout,
-             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
-+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
-         )
- 
-         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
-@@ -119,14 +129,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
-         self.freeze()
- 
--    def forward(self, input_ids, attention_mask, domain_ids=None):
-+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
-         hidden_states = self.transformer(
-             input_ids=input_ids, attention_mask=attention_mask
-         )[0]
-+        hidden_states = self.dropout(hidden_states)
-+        hidden_states = self.mlp(hidden_states)
-         punct_logits = self.punct_classifier(hidden_states=hidden_states)
-         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
-         domain_logits = self.domain_classifier(
--            hidden_states=reverse_grad_hidden_states)
-+            hidden_states=reverse_grad_hidden_states,
-+            subtoken_mask=subtoken_mask)
-         return punct_logits, domain_logits
- 
-     def _make_step(self, batch):
-@@ -136,7 +149,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         punct_labels = batch['labels']
-         domain_labels = batch['domain']
-         punct_logits, domain_logits = self(
--            input_ids=input_ids, attention_mask=attention_mask
-+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
-         )
-         punctuation_loss = self.punctuation_loss(
-             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/lightning_logs.txt
deleted file mode 100644
index 9fa336d..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/lightning_logs.txt
+++ /dev/null
@@ -1,24 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-Using native 16bit precision.
-Global seed set to 42
-initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | dropout             | Dropout              | 0     
-2 | mlp                 | MultiLayerPerceptron | 131 K 
-3 | punct_classifier    | TokenClassifier      | 2.6 K 
-4 | domain_classifier   | SequenceClassifier   | 257   
-5 | punctuation_loss    | FocalDiceLoss        | 0     
-6 | domain_loss         | CrossEntropyLoss     | 0     
-7 | agg_loss            | AggregatorLoss       | 0     
-8 | punct_class_report  | ClassificationReport | 0     
-9 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-167 K     Trainable params
-13.4 M    Non-trainable params
-13.6 M    Total params
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/nemo_error_log.txt
deleted file mode 100644
index 97cb5eb..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/nemo_error_log.txt
+++ /dev/null
@@ -1,7 +0,0 @@
-[NeMo W 2021-02-08 10:46:07 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 10:46:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 10:46:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index d4331a1..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,9 +0,0 @@
-[NeMo I 2021-02-08 10:46:07 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07
-[NeMo I 2021-02-08 10:46:07 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-08 10:46:07 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 10:46:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 10:46:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/git-info.log
deleted file mode 100644
index 10d12b7..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/git-info.log
+++ /dev/null
@@ -1,623 +0,0 @@
-commit hash: 939a671c8c117db6975316767ced5d95449e2b27
-diff --git a/README.md b/README.md
-index d52dba5..0b403d3 100644
---- a/README.md
-+++ b/README.md
-@@ -333,4 +333,71 @@ label                                                precision    recall       f
-  'punct_f1': 0.5858508944511414,
-  'punct_precision': 0.30176490545272827,
-  'punct_recall': 10.0,
-- 'test_loss': 0.8140875697135925}
-\ No newline at end of file
-+ 'test_loss': 0.8140875697135925}
-+
-+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
-+
-+ (label_id: 0)                                          62.15     100.00      76.66       5154
-+! (label_id: 1)                                          0.00       0.00       0.00        108
-+, (label_id: 2)                                          0.00       0.00       0.00      18022
-+- (label_id: 3)                                          0.00       0.00       0.00       1557
-+. (label_id: 4)                                         41.74      94.01      57.81      15164
-+: (label_id: 5)                                          0.00       0.00       0.00        319
-+; (label_id: 6)                                          0.00       0.00       0.00         88
-+? (label_id: 7)                                          0.00       0.00       0.00       1217
-+ (label_id: 8)                                          0.00       0.00       0.00        752
-+… (label_id: 9)                                          0.00       0.00       0.00         67
-+-------------------
-+micro avg                                               45.72      45.72      45.72      42448
-+macro avg                                               10.39      19.40      13.45      42448
-+weighted avg                                            22.46      45.72      29.96      42448
-+{ 'punct_f1': 13.446383476257324,
-+ 'punct_precision': 10.388500213623047,
-+ 'punct_recall': 19.400554656982422,
-+ 'test_loss': 0.44148480892181396}
-+
-+
-+ ## Log for 8/2/2021
-+
-+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
-+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
-+
-+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
-+End frozen 
-+micro avg                                               41.42      41.42      41.42      33406
-+macro avg                                               11.01      13.54      11.07      33406
-+weighted avg                                            34.88      41.42      34.20      33406
-+
-+1st layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               36.65      36.65      36.65      33463
-+macro avg                                               10.71       9.91       8.26      33463
-+weighted avg                                            34.32      36.65      31.08      33463
-+
-+2nd layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               35.72      35.72      35.72      42448
-+macro avg                                                3.57      10.00       5.26      42448
-+weighted avg                                            12.76      35.72      18.81      42448
-+
-+{'punct_f1': 5.264181137084961,
-+ 'punct_precision': 3.572371006011963,
-+ 'punct_recall': 10.0,
-+ 'test_loss': 18.49854850769043}
-+
-+
-+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
-+alpha from 3->4 seems to reduce convergence rate.
-+
-+micro avg                                               50.98      50.98      50.98      33463
-+macro avg                                               25.99      25.38      23.38      33463
-+weighted avg                                            50.31      50.98      48.27      33463
-+
-+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
-+micro avg                                               58.55      58.55      58.55      39340
-+macro avg                                               30.02      29.74      29.52      39340
-+weighted avg                                            57.91      58.55      57.51      39340
-+
-+still increasing?!
-+{'punct_f1': 29.523975372314453,
-+ 'punct_precision': 30.015613555908203,
-+ 'punct_recall': 29.738296508789062,
-+ 'test_loss': 0.3690211772918701}
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index fe58670..716a889 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,49 +1,49 @@
- seed: 42
- trainer:
--    # gpus: 1 # the number of gpus, 0 for CPU
--    # num_nodes: 1
--    # max_epochs: 2
--    # max_steps: null # precedence over max_epochs
--    # accumulate_grad_batches: 4 # accumulates grads every k batches
--    # gradient_clip_val: 0
--    # amp_level: O1 # O1/O2 for mixed precision
--    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
--    # checkpoint_callback: false  # Provided by exp_manager
--    # logger: false #false  # Provided by exp_manager
--    # log_every_n_steps: 1  # Interval of logging.
--    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # resume_from_checkpoint: null
--
--    gpus: 0 # the number of gpus, 0 for CPU
-+    gpus: 1 # the number of gpus, 0 for CPU
-     num_nodes: 1
--    max_epochs: 8
-+    max_epochs: 10
-     max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 1 # accumulates grads every k batches
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-     gradient_clip_val: 0
--    amp_level: O0 # O1/O2 for mixed precision
--    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
-+    amp_level: O1 # O1/O2 for mixed precision
-+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    accelerator: ddp
-     checkpoint_callback: false  # Provided by exp_manager
-     logger: false #false  # Provided by exp_manager
-     log_every_n_steps: 1  # Interval of logging.
-     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    reload_dataloaders_every_epoch: true
-     resume_from_checkpoint: null
- 
-+    # gpus: 0 # the number of gpus, 0 for CPU
-+    # num_nodes: 1
-+    # max_epochs: 8
-+    # max_steps: null # precedence over max_epochs
-+    # accumulate_grad_batches: 1 # accumulates grads every k batches
-+    # gradient_clip_val: 0
-+    # amp_level: O0 # O1/O2 for mixed precision
-+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # # accelerator: ddp
-+    # checkpoint_callback: false  # Provided by exp_manager
-+    # logger: false #false  # Provided by exp_manager
-+    # log_every_n_steps: 1  # Interval of logging.
-+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    # reload_dataloaders_every_epoch: true
-+    # resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /home/nxingyu2/data # /root/data # 
--tmp_path: /home/nxingyu2/data/tmp # /tmp # 
-+base_path: /home/nxingyu/data # /root/data # 
-+tmp_path: /home/nxingyu/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
--    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-     unfrozen: 0
--    maximum_unfrozen: 2
-+    maximum_unfrozen: 1
-     unfreeze_step: 1
-     # unfreeze_every: 3
-     punct_label_ids:
-@@ -58,10 +58,10 @@ model:
-         - "—"
-         - "…"
- 
--    punct_class_weights: true
-+    punct_class_weights: false
-     
-     dataset:
--        data_dir: /home/nxingyu2/data # /root/data # 
-+        data_dir: /home/nxingyu/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -106,6 +106,12 @@ model:
-         config: null
-         # unfrozen_layers: 1
- 
-+    mlp:
-+        num_fc_layers: 2
-+        fc_dropout: 0.1
-+        log_softmax: false
-+        activation: 'gelu'
-+        
-     punct_head:
-         punct_num_fc_layers: 1
-         fc_dropout: 0.1
-@@ -122,17 +128,19 @@ model:
-         use_transformer_init: true
-         loss: 'cel'
-         gamma: 0.1 # coefficient of gradient reversal
-+        pooling: 'token'
-+        idx_conditioned_on: 0
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 3
-+        alpha: 4
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: novograd
-+        name: adamw
-         lr: 1e-3
-         weight_decay: 0.00
-         sched:
-diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
-index d4ff927..1a6fe93 100644
---- a/experiment/core/layers/sequence_classifier.py
-+++ b/experiment/core/layers/sequence_classifier.py
-@@ -1,3 +1,4 @@
-+import torch
- from torch import nn
- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
- from core.utils import transformer_weights_init
-@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
-         log_softmax: bool = True,
-         dropout: float = 0.0,
-         use_transformer_init: bool = True,
--        idx_conditioned_on: int = 0,
-+        pooling: str = 'mean', # mean, max, mean_max, token
-+        idx_conditioned_on: int = None,
-     ):
-         """
-         Initializes the SequenceClassifier module.
-@@ -37,6 +39,7 @@ class SequenceClassifier(nn.Module):
-         super().__init__()
-         self.log_softmax = log_softmax
-         self._idx_conditioned_on = idx_conditioned_on
-+        self.pooling = pooling
-         self.mlp = MultiLayerPerceptron(
-             hidden_size=hidden_size,
-             num_classes=num_classes,
-@@ -48,7 +51,23 @@ class SequenceClassifier(nn.Module):
-         if use_transformer_init:
-             self.apply(lambda module: transformer_weights_init(module, xavier=False))
- 
--    def forward(self, hidden_states):
-+    def forward(self, hidden_states, subtoken_mask=None):
-         hidden_states = self.dropout(hidden_states)
--        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
-+        if self.pooling=='token':
-+            pooled = hidden_states[:, self._idx_conditioned_on]
-+        else:
-+            if subtoken_mask==None:
-+                ct=hidden_states.shape[1] # Seq len
-+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
-+            else:
-+                ct = torch.sum(subtoken_mask,axis=1)
-+            pooled_sum = torch.sum(hidden_states,axis=1)
-+            if self.pooling=='mean' or self.pooling == 'mean_max':
-+                pooled_mean = pooled_sum//ct
-+            if self.pooling=='max' or self.pooling=='mean_max':
-+                pooled_max = torch.max(hidden_states,axis=1)[0]
-+            pooled=pooled_mean if self.pooling=='mean' else \
-+                pooled_max if self.pooling=='max' else \
-+                    torch.cat([pooled_mean,pooled_max],axis=-1)
-+        logits = self.mlp(pooled)
-         return logits
-diff --git a/experiment/info.log b/experiment/info.log
-index 2471fe9..e69de29 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,300 +0,0 @@
--[INFO] - GPU available: True, used: False
--[INFO] - TPU available: None, using: 0 TPU cores
--[INFO] - shuffling train set
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        184
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.23       0.34       0.53        594
--- (label_id: 3)                                          3.06      25.42       5.46         59
--. (label_id: 4)                                         47.22      12.98      20.36        524
--: (label_id: 5)                                          0.00       0.00       0.00         18
--; (label_id: 6)                                          0.00       0.00       0.00         13
--? (label_id: 7)                                          8.45       6.32       7.23         95
--— (label_id: 8)                                          0.00       0.00       0.00         12
--… (label_id: 9)                                          0.00       0.00       0.00          0
---------------------
--micro avg                                                6.05       6.05       6.05       1503
--macro avg                                                6.66       5.01       3.73       1503
--weighted avg                                            17.61       6.05       7.98       1503
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00         92
---------------------
--micro avg                                              100.00     100.00     100.00         92
--macro avg                                              100.00     100.00     100.00         92
--weighted avg                                           100.00     100.00     100.00         92
--
--[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 1.5848931924611143e-08
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
--will be used during training (effective maximum steps = 3192) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 3192
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        202
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.62       0.45       0.70        669
--- (label_id: 3)                                          3.48      27.27       6.17         66
--. (label_id: 4)                                         45.06      13.01      20.19        561
--: (label_id: 5)                                          1.52       6.67       2.47         15
--; (label_id: 6)                                          0.00       0.00       0.00         15
--? (label_id: 7)                                          8.70       7.32       7.95         82
--— (label_id: 8)                                          0.00       0.00       0.00         13
--… (label_id: 9)                                          0.00       0.00       0.00          1
---------------------
--micro avg                                                6.20       6.20       6.20       1628
--macro avg                                                6.04       5.47       3.75       1628
--weighted avg                                            16.79       6.20       7.92       1628
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        101
---------------------
--micro avg                                              100.00     100.00     100.00        101
--macro avg                                              100.00     100.00     100.00        101
--weighted avg                                           100.00     100.00     100.00        101
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.34       0.73       0.46       4402
--! (label_id: 1)                                          0.42      13.95       0.82        129
--, (label_id: 2)                                          2.53       0.64       1.03      15243
--- (label_id: 3)                                          2.45      21.03       4.38       1322
--. (label_id: 4)                                         44.00      11.40      18.11      12542
--: (label_id: 5)                                          0.43       1.41       0.65        354
--; (label_id: 6)                                          0.00       0.00       0.00        163
--? (label_id: 7)                                          4.16       6.27       5.00       1117
--— (label_id: 8)                                          3.00       0.61       1.02        488
--… (label_id: 9)                                          0.97       6.17       1.68         81
---------------------
--micro avg                                                5.41       5.41       5.41      35841
--macro avg                                                5.83       6.22       3.32      35841
--weighted avg                                            16.78       5.41       7.18      35841
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2201
---------------------
--micro avg                                              100.00     100.00     100.00       2201
--macro avg                                              100.00     100.00     100.00       2201
--weighted avg                                           100.00     100.00     100.00       2201
--
--[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.20       0.43       0.27       4226
--! (label_id: 1)                                          0.44      14.17       0.86        127
--, (label_id: 2)                                          1.93       0.49       0.78      14611
--- (label_id: 3)                                          2.23      19.56       4.01       1237
--. (label_id: 4)                                         43.37      11.25      17.86      11977
--: (label_id: 5)                                          0.68       2.34       1.05        342
--; (label_id: 6)                                          0.00       0.00       0.00        129
--? (label_id: 7)                                          5.16       7.47       6.10       1058
--— (label_id: 8)                                          2.15       0.49       0.80        409
--… (label_id: 9)                                          0.69       4.23       1.19         71
---------------------
--micro avg                                                5.23       5.23       5.23      34187
--macro avg                                                5.68       6.04       3.29      34187
--weighted avg                                            16.32       5.23       6.98      34187
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2113
---------------------
--micro avg                                              100.00     100.00     100.00       2113
--macro avg                                              100.00     100.00     100.00       2113
--weighted avg                                           100.00     100.00     100.00       2113
--
--[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.61       0.39       4228
--! (label_id: 1)                                          0.30       8.28       0.58        145
--, (label_id: 2)                                          2.27       0.58       0.92      14495
--- (label_id: 3)                                          2.64      21.78       4.70       1327
--. (label_id: 4)                                         44.87      11.66      18.51      12193
--: (label_id: 5)                                          0.60       1.93       0.91        362
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.19       6.40       5.07       1078
--— (label_id: 8)                                          1.16       0.22       0.37        459
--… (label_id: 9)                                          0.85       4.17       1.41         96
---------------------
--micro avg                                                5.54       5.54       5.54      34547
--macro avg                                                5.72       5.56       3.29      34547
--weighted avg                                            17.08       5.54       7.33      34547
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2114
---------------------
--micro avg                                              100.00     100.00     100.00       2114
--macro avg                                              100.00     100.00     100.00       2114
--weighted avg                                           100.00     100.00     100.00       2114
--
--[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.29       0.63       0.40       4444
--! (label_id: 1)                                          0.38      10.67       0.74        150
--, (label_id: 2)                                          2.32       0.59       0.94      15290
--- (label_id: 3)                                          2.34      20.28       4.19       1292
--. (label_id: 4)                                         43.85      11.68      18.44      12599
--: (label_id: 5)                                          0.41       1.28       0.62        392
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.24       6.30       5.07       1111
--— (label_id: 8)                                          0.00       0.00       0.00        456
--… (label_id: 9)                                          0.38       2.41       0.65         83
---------------------
--micro avg                                                5.40       5.40       5.40      35981
--macro avg                                                5.42       5.38       3.11      35981
--weighted avg                                            16.59       5.40       7.22      35981
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2222
---------------------
--micro avg                                              100.00     100.00     100.00       2222
--macro avg                                              100.00     100.00     100.00       2222
--weighted avg                                           100.00     100.00     100.00       2222
--
--[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.35       0.73       0.48       3844
--! (label_id: 1)                                          0.54      14.62       1.04        130
--, (label_id: 2)                                          2.32       0.59       0.94      13056
--- (label_id: 3)                                          2.67      22.28       4.77       1194
--. (label_id: 4)                                         44.45      11.95      18.84      10791
--: (label_id: 5)                                          0.84       3.21       1.33        280
--; (label_id: 6)                                          0.00       0.00       0.00        140
--? (label_id: 7)                                          4.17       6.56       5.10        914
--— (label_id: 8)                                          0.00       0.00       0.00        401
--… (label_id: 9)                                          0.48       2.63       0.81         76
---------------------
--micro avg                                                5.68       5.68       5.68      30826
--macro avg                                                5.58       6.26       3.33      30826
--weighted avg                                            16.82       5.68       7.41      30826
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1922
---------------------
--micro avg                                              100.00     100.00     100.00       1922
--macro avg                                              100.00     100.00     100.00       1922
--weighted avg                                           100.00     100.00     100.00       1922
--
--[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.60       0.39       3970
--! (label_id: 1)                                          0.35      10.66       0.68        122
--, (label_id: 2)                                          2.09       0.53       0.85      13469
--- (label_id: 3)                                          2.29      19.32       4.10       1201
--. (label_id: 4)                                         43.43      11.24      17.86      11227
--: (label_id: 5)                                          0.63       2.30       0.99        304
--; (label_id: 6)                                          0.00       0.00       0.00        141
--? (label_id: 7)                                          4.52       6.86       5.45       1006
--— (label_id: 8)                                          1.15       0.23       0.38        444
--… (label_id: 9)                                          0.45       2.67       0.78         75
---------------------
--micro avg                                                5.26       5.26       5.26      31959
--macro avg                                                5.52       5.44       3.15      31959
--weighted avg                                            16.42       5.26       7.02      31959
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1985
---------------------
--micro avg                                              100.00     100.00     100.00       1985
--macro avg                                              100.00     100.00     100.00       1985
--weighted avg                                           100.00     100.00     100.00       1985
--
--[INFO] - Epoch 5, step 2394: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.23       0.48       0.31       4126
--! (label_id: 1)                                          0.29       9.40       0.56        117
--, (label_id: 2)                                          1.91       0.49       0.77      14019
--- (label_id: 3)                                          2.52      22.59       4.53       1164
--. (label_id: 4)                                         44.15      11.65      18.44      11789
--: (label_id: 5)                                          0.72       2.41       1.11        332
--; (label_id: 6)                                          0.56       0.61       0.58        165
--? (label_id: 7)                                          3.89       6.53       4.88        980
--— (label_id: 8)                                          2.30       0.47       0.77        430
--… (label_id: 9)                                          1.18       8.33       2.07         60
---------------------
--micro avg                                                5.47       5.47       5.47      33182
--macro avg                                                5.77       6.30       3.40      33182
--weighted avg                                            16.77       5.47       7.25      33182
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2063
---------------------
--micro avg                                              100.00     100.00     100.00       2063
--macro avg                                              100.00     100.00     100.00       2063
--weighted avg                                           100.00     100.00     100.00       2063
--
--[INFO] - Epoch 6, step 2793: val_loss was not in top 3
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index fa37b4c..6b5ff89 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             self.hparams.model.punct_class_weights=None
- 
-+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
-+        self.mlp = MultiLayerPerceptron(
-+            self.transformer.config.hidden_size,
-+            self.transformer.config.hidden_size,
-+            num_layers=self.hparams.model.mlp.num_fc_layers, 
-+            activation=self.hparams.model.mlp.activation, 
-+            log_softmax=self.hparams.model.mlp.log_softmax
-+        )
-+
-         self.punct_classifier = TokenClassifier(
-             hidden_size=self.transformer.config.hidden_size,
-             num_classes=len(self.labels_to_ids),
-@@ -80,6 +89,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-             log_softmax=self.hparams.model.domain_head.log_softmax,
-             dropout=self.hparams.model.domain_head.fc_dropout,
-             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
-+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
-         )
- 
-         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
-@@ -119,14 +129,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
-         self.freeze()
- 
--    def forward(self, input_ids, attention_mask, domain_ids=None):
-+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
-         hidden_states = self.transformer(
-             input_ids=input_ids, attention_mask=attention_mask
-         )[0]
-+        hidden_states = self.dropout(hidden_states)
-+        hidden_states = self.mlp(hidden_states)
-         punct_logits = self.punct_classifier(hidden_states=hidden_states)
-         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
-         domain_logits = self.domain_classifier(
--            hidden_states=reverse_grad_hidden_states)
-+            hidden_states=reverse_grad_hidden_states,
-+            subtoken_mask=subtoken_mask)
-         return punct_logits, domain_logits
- 
-     def _make_step(self, batch):
-@@ -136,7 +149,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         punct_labels = batch['labels']
-         domain_labels = batch['domain']
-         punct_logits, domain_logits = self(
--            input_ids=input_ids, attention_mask=attention_mask
-+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
-         )
-         punctuation_loss = self.punctuation_loss(
-             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/lightning_logs.txt
deleted file mode 100644
index 8942cad..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/lightning_logs.txt
+++ /dev/null
@@ -1,5 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-Using native 16bit precision.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/nemo_error_log.txt
deleted file mode 100644
index a6f4130..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/nemo_error_log.txt
+++ /dev/null
@@ -1 +0,0 @@
-[NeMo W 2021-02-08 10:47:32 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 61a48c2..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,3 +0,0 @@
-[NeMo I 2021-02-08 10:47:32 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32
-[NeMo I 2021-02-08 10:47:32 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-08 10:47:32 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/git-info.log
deleted file mode 100644
index 4d1615a..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/git-info.log
+++ /dev/null
@@ -1,623 +0,0 @@
-commit hash: 939a671c8c117db6975316767ced5d95449e2b27
-diff --git a/README.md b/README.md
-index d52dba5..0b403d3 100644
---- a/README.md
-+++ b/README.md
-@@ -333,4 +333,71 @@ label                                                precision    recall       f
-  'punct_f1': 0.5858508944511414,
-  'punct_precision': 0.30176490545272827,
-  'punct_recall': 10.0,
-- 'test_loss': 0.8140875697135925}
-\ No newline at end of file
-+ 'test_loss': 0.8140875697135925}
-+
-+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
-+
-+ (label_id: 0)                                          62.15     100.00      76.66       5154
-+! (label_id: 1)                                          0.00       0.00       0.00        108
-+, (label_id: 2)                                          0.00       0.00       0.00      18022
-+- (label_id: 3)                                          0.00       0.00       0.00       1557
-+. (label_id: 4)                                         41.74      94.01      57.81      15164
-+: (label_id: 5)                                          0.00       0.00       0.00        319
-+; (label_id: 6)                                          0.00       0.00       0.00         88
-+? (label_id: 7)                                          0.00       0.00       0.00       1217
-+ (label_id: 8)                                          0.00       0.00       0.00        752
-+… (label_id: 9)                                          0.00       0.00       0.00         67
-+-------------------
-+micro avg                                               45.72      45.72      45.72      42448
-+macro avg                                               10.39      19.40      13.45      42448
-+weighted avg                                            22.46      45.72      29.96      42448
-+{ 'punct_f1': 13.446383476257324,
-+ 'punct_precision': 10.388500213623047,
-+ 'punct_recall': 19.400554656982422,
-+ 'test_loss': 0.44148480892181396}
-+
-+
-+ ## Log for 8/2/2021
-+
-+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
-+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
-+
-+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
-+End frozen 
-+micro avg                                               41.42      41.42      41.42      33406
-+macro avg                                               11.01      13.54      11.07      33406
-+weighted avg                                            34.88      41.42      34.20      33406
-+
-+1st layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               36.65      36.65      36.65      33463
-+macro avg                                               10.71       9.91       8.26      33463
-+weighted avg                                            34.32      36.65      31.08      33463
-+
-+2nd layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               35.72      35.72      35.72      42448
-+macro avg                                                3.57      10.00       5.26      42448
-+weighted avg                                            12.76      35.72      18.81      42448
-+
-+{'punct_f1': 5.264181137084961,
-+ 'punct_precision': 3.572371006011963,
-+ 'punct_recall': 10.0,
-+ 'test_loss': 18.49854850769043}
-+
-+
-+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
-+alpha from 3->4 seems to reduce convergence rate.
-+
-+micro avg                                               50.98      50.98      50.98      33463
-+macro avg                                               25.99      25.38      23.38      33463
-+weighted avg                                            50.31      50.98      48.27      33463
-+
-+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
-+micro avg                                               58.55      58.55      58.55      39340
-+macro avg                                               30.02      29.74      29.52      39340
-+weighted avg                                            57.91      58.55      57.51      39340
-+
-+still increasing?!
-+{'punct_f1': 29.523975372314453,
-+ 'punct_precision': 30.015613555908203,
-+ 'punct_recall': 29.738296508789062,
-+ 'test_loss': 0.3690211772918701}
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index fe58670..ab37d69 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,49 +1,49 @@
- seed: 42
- trainer:
--    # gpus: 1 # the number of gpus, 0 for CPU
--    # num_nodes: 1
--    # max_epochs: 2
--    # max_steps: null # precedence over max_epochs
--    # accumulate_grad_batches: 4 # accumulates grads every k batches
--    # gradient_clip_val: 0
--    # amp_level: O1 # O1/O2 for mixed precision
--    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
--    # checkpoint_callback: false  # Provided by exp_manager
--    # logger: false #false  # Provided by exp_manager
--    # log_every_n_steps: 1  # Interval of logging.
--    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # resume_from_checkpoint: null
--
--    gpus: 0 # the number of gpus, 0 for CPU
-+    gpus: 1 # the number of gpus, 0 for CPU
-     num_nodes: 1
--    max_epochs: 8
-+    max_epochs: 10
-     max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 1 # accumulates grads every k batches
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-     gradient_clip_val: 0
--    amp_level: O0 # O1/O2 for mixed precision
--    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
-+    amp_level: O1 # O1/O2 for mixed precision
-+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    accelerator: ddp
-     checkpoint_callback: false  # Provided by exp_manager
-     logger: false #false  # Provided by exp_manager
-     log_every_n_steps: 1  # Interval of logging.
-     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    reload_dataloaders_every_epoch: true
-     resume_from_checkpoint: null
- 
-+    # gpus: 0 # the number of gpus, 0 for CPU
-+    # num_nodes: 1
-+    # max_epochs: 8
-+    # max_steps: null # precedence over max_epochs
-+    # accumulate_grad_batches: 1 # accumulates grads every k batches
-+    # gradient_clip_val: 0
-+    # amp_level: O0 # O1/O2 for mixed precision
-+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # # accelerator: ddp
-+    # checkpoint_callback: false  # Provided by exp_manager
-+    # logger: false #false  # Provided by exp_manager
-+    # log_every_n_steps: 1  # Interval of logging.
-+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    # reload_dataloaders_every_epoch: true
-+    # resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /home/nxingyu2/data # /root/data # 
--tmp_path: /home/nxingyu2/data/tmp # /tmp # 
-+base_path: /home/nxingyu/data # /root/data # 
-+tmp_path: /home/nxingyu/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
--    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-     unfrozen: 0
--    maximum_unfrozen: 2
-+    maximum_unfrozen: 1
-     unfreeze_step: 1
-     # unfreeze_every: 3
-     punct_label_ids:
-@@ -58,10 +58,10 @@ model:
-         - "—"
-         - "…"
- 
--    punct_class_weights: true
-+    punct_class_weights: false
-     
-     dataset:
--        data_dir: /home/nxingyu2/data # /root/data # 
-+        data_dir: /home/nxingyu/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -106,6 +106,12 @@ model:
-         config: null
-         # unfrozen_layers: 1
- 
-+    mlp:
-+        num_fc_layers: 2
-+        fc_dropout: 0.1
-+        log_softmax: false
-+        activation: 'GELU'
-+        
-     punct_head:
-         punct_num_fc_layers: 1
-         fc_dropout: 0.1
-@@ -122,17 +128,19 @@ model:
-         use_transformer_init: true
-         loss: 'cel'
-         gamma: 0.1 # coefficient of gradient reversal
-+        pooling: 'token'
-+        idx_conditioned_on: 0
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 3
-+        alpha: 4
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: novograd
-+        name: adamw
-         lr: 1e-3
-         weight_decay: 0.00
-         sched:
-diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
-index d4ff927..1a6fe93 100644
---- a/experiment/core/layers/sequence_classifier.py
-+++ b/experiment/core/layers/sequence_classifier.py
-@@ -1,3 +1,4 @@
-+import torch
- from torch import nn
- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
- from core.utils import transformer_weights_init
-@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
-         log_softmax: bool = True,
-         dropout: float = 0.0,
-         use_transformer_init: bool = True,
--        idx_conditioned_on: int = 0,
-+        pooling: str = 'mean', # mean, max, mean_max, token
-+        idx_conditioned_on: int = None,
-     ):
-         """
-         Initializes the SequenceClassifier module.
-@@ -37,6 +39,7 @@ class SequenceClassifier(nn.Module):
-         super().__init__()
-         self.log_softmax = log_softmax
-         self._idx_conditioned_on = idx_conditioned_on
-+        self.pooling = pooling
-         self.mlp = MultiLayerPerceptron(
-             hidden_size=hidden_size,
-             num_classes=num_classes,
-@@ -48,7 +51,23 @@ class SequenceClassifier(nn.Module):
-         if use_transformer_init:
-             self.apply(lambda module: transformer_weights_init(module, xavier=False))
- 
--    def forward(self, hidden_states):
-+    def forward(self, hidden_states, subtoken_mask=None):
-         hidden_states = self.dropout(hidden_states)
--        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
-+        if self.pooling=='token':
-+            pooled = hidden_states[:, self._idx_conditioned_on]
-+        else:
-+            if subtoken_mask==None:
-+                ct=hidden_states.shape[1] # Seq len
-+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
-+            else:
-+                ct = torch.sum(subtoken_mask,axis=1)
-+            pooled_sum = torch.sum(hidden_states,axis=1)
-+            if self.pooling=='mean' or self.pooling == 'mean_max':
-+                pooled_mean = pooled_sum//ct
-+            if self.pooling=='max' or self.pooling=='mean_max':
-+                pooled_max = torch.max(hidden_states,axis=1)[0]
-+            pooled=pooled_mean if self.pooling=='mean' else \
-+                pooled_max if self.pooling=='max' else \
-+                    torch.cat([pooled_mean,pooled_max],axis=-1)
-+        logits = self.mlp(pooled)
-         return logits
-diff --git a/experiment/info.log b/experiment/info.log
-index 2471fe9..e69de29 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,300 +0,0 @@
--[INFO] - GPU available: True, used: False
--[INFO] - TPU available: None, using: 0 TPU cores
--[INFO] - shuffling train set
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        184
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.23       0.34       0.53        594
--- (label_id: 3)                                          3.06      25.42       5.46         59
--. (label_id: 4)                                         47.22      12.98      20.36        524
--: (label_id: 5)                                          0.00       0.00       0.00         18
--; (label_id: 6)                                          0.00       0.00       0.00         13
--? (label_id: 7)                                          8.45       6.32       7.23         95
--— (label_id: 8)                                          0.00       0.00       0.00         12
--… (label_id: 9)                                          0.00       0.00       0.00          0
---------------------
--micro avg                                                6.05       6.05       6.05       1503
--macro avg                                                6.66       5.01       3.73       1503
--weighted avg                                            17.61       6.05       7.98       1503
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00         92
---------------------
--micro avg                                              100.00     100.00     100.00         92
--macro avg                                              100.00     100.00     100.00         92
--weighted avg                                           100.00     100.00     100.00         92
--
--[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 1.5848931924611143e-08
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
--will be used during training (effective maximum steps = 3192) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 3192
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        202
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.62       0.45       0.70        669
--- (label_id: 3)                                          3.48      27.27       6.17         66
--. (label_id: 4)                                         45.06      13.01      20.19        561
--: (label_id: 5)                                          1.52       6.67       2.47         15
--; (label_id: 6)                                          0.00       0.00       0.00         15
--? (label_id: 7)                                          8.70       7.32       7.95         82
--— (label_id: 8)                                          0.00       0.00       0.00         13
--… (label_id: 9)                                          0.00       0.00       0.00          1
---------------------
--micro avg                                                6.20       6.20       6.20       1628
--macro avg                                                6.04       5.47       3.75       1628
--weighted avg                                            16.79       6.20       7.92       1628
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        101
---------------------
--micro avg                                              100.00     100.00     100.00        101
--macro avg                                              100.00     100.00     100.00        101
--weighted avg                                           100.00     100.00     100.00        101
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.34       0.73       0.46       4402
--! (label_id: 1)                                          0.42      13.95       0.82        129
--, (label_id: 2)                                          2.53       0.64       1.03      15243
--- (label_id: 3)                                          2.45      21.03       4.38       1322
--. (label_id: 4)                                         44.00      11.40      18.11      12542
--: (label_id: 5)                                          0.43       1.41       0.65        354
--; (label_id: 6)                                          0.00       0.00       0.00        163
--? (label_id: 7)                                          4.16       6.27       5.00       1117
--— (label_id: 8)                                          3.00       0.61       1.02        488
--… (label_id: 9)                                          0.97       6.17       1.68         81
---------------------
--micro avg                                                5.41       5.41       5.41      35841
--macro avg                                                5.83       6.22       3.32      35841
--weighted avg                                            16.78       5.41       7.18      35841
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2201
---------------------
--micro avg                                              100.00     100.00     100.00       2201
--macro avg                                              100.00     100.00     100.00       2201
--weighted avg                                           100.00     100.00     100.00       2201
--
--[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.20       0.43       0.27       4226
--! (label_id: 1)                                          0.44      14.17       0.86        127
--, (label_id: 2)                                          1.93       0.49       0.78      14611
--- (label_id: 3)                                          2.23      19.56       4.01       1237
--. (label_id: 4)                                         43.37      11.25      17.86      11977
--: (label_id: 5)                                          0.68       2.34       1.05        342
--; (label_id: 6)                                          0.00       0.00       0.00        129
--? (label_id: 7)                                          5.16       7.47       6.10       1058
--— (label_id: 8)                                          2.15       0.49       0.80        409
--… (label_id: 9)                                          0.69       4.23       1.19         71
---------------------
--micro avg                                                5.23       5.23       5.23      34187
--macro avg                                                5.68       6.04       3.29      34187
--weighted avg                                            16.32       5.23       6.98      34187
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2113
---------------------
--micro avg                                              100.00     100.00     100.00       2113
--macro avg                                              100.00     100.00     100.00       2113
--weighted avg                                           100.00     100.00     100.00       2113
--
--[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.61       0.39       4228
--! (label_id: 1)                                          0.30       8.28       0.58        145
--, (label_id: 2)                                          2.27       0.58       0.92      14495
--- (label_id: 3)                                          2.64      21.78       4.70       1327
--. (label_id: 4)                                         44.87      11.66      18.51      12193
--: (label_id: 5)                                          0.60       1.93       0.91        362
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.19       6.40       5.07       1078
--— (label_id: 8)                                          1.16       0.22       0.37        459
--… (label_id: 9)                                          0.85       4.17       1.41         96
---------------------
--micro avg                                                5.54       5.54       5.54      34547
--macro avg                                                5.72       5.56       3.29      34547
--weighted avg                                            17.08       5.54       7.33      34547
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2114
---------------------
--micro avg                                              100.00     100.00     100.00       2114
--macro avg                                              100.00     100.00     100.00       2114
--weighted avg                                           100.00     100.00     100.00       2114
--
--[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.29       0.63       0.40       4444
--! (label_id: 1)                                          0.38      10.67       0.74        150
--, (label_id: 2)                                          2.32       0.59       0.94      15290
--- (label_id: 3)                                          2.34      20.28       4.19       1292
--. (label_id: 4)                                         43.85      11.68      18.44      12599
--: (label_id: 5)                                          0.41       1.28       0.62        392
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.24       6.30       5.07       1111
--— (label_id: 8)                                          0.00       0.00       0.00        456
--… (label_id: 9)                                          0.38       2.41       0.65         83
---------------------
--micro avg                                                5.40       5.40       5.40      35981
--macro avg                                                5.42       5.38       3.11      35981
--weighted avg                                            16.59       5.40       7.22      35981
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2222
---------------------
--micro avg                                              100.00     100.00     100.00       2222
--macro avg                                              100.00     100.00     100.00       2222
--weighted avg                                           100.00     100.00     100.00       2222
--
--[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.35       0.73       0.48       3844
--! (label_id: 1)                                          0.54      14.62       1.04        130
--, (label_id: 2)                                          2.32       0.59       0.94      13056
--- (label_id: 3)                                          2.67      22.28       4.77       1194
--. (label_id: 4)                                         44.45      11.95      18.84      10791
--: (label_id: 5)                                          0.84       3.21       1.33        280
--; (label_id: 6)                                          0.00       0.00       0.00        140
--? (label_id: 7)                                          4.17       6.56       5.10        914
--— (label_id: 8)                                          0.00       0.00       0.00        401
--… (label_id: 9)                                          0.48       2.63       0.81         76
---------------------
--micro avg                                                5.68       5.68       5.68      30826
--macro avg                                                5.58       6.26       3.33      30826
--weighted avg                                            16.82       5.68       7.41      30826
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1922
---------------------
--micro avg                                              100.00     100.00     100.00       1922
--macro avg                                              100.00     100.00     100.00       1922
--weighted avg                                           100.00     100.00     100.00       1922
--
--[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.60       0.39       3970
--! (label_id: 1)                                          0.35      10.66       0.68        122
--, (label_id: 2)                                          2.09       0.53       0.85      13469
--- (label_id: 3)                                          2.29      19.32       4.10       1201
--. (label_id: 4)                                         43.43      11.24      17.86      11227
--: (label_id: 5)                                          0.63       2.30       0.99        304
--; (label_id: 6)                                          0.00       0.00       0.00        141
--? (label_id: 7)                                          4.52       6.86       5.45       1006
--— (label_id: 8)                                          1.15       0.23       0.38        444
--… (label_id: 9)                                          0.45       2.67       0.78         75
---------------------
--micro avg                                                5.26       5.26       5.26      31959
--macro avg                                                5.52       5.44       3.15      31959
--weighted avg                                            16.42       5.26       7.02      31959
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1985
---------------------
--micro avg                                              100.00     100.00     100.00       1985
--macro avg                                              100.00     100.00     100.00       1985
--weighted avg                                           100.00     100.00     100.00       1985
--
--[INFO] - Epoch 5, step 2394: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.23       0.48       0.31       4126
--! (label_id: 1)                                          0.29       9.40       0.56        117
--, (label_id: 2)                                          1.91       0.49       0.77      14019
--- (label_id: 3)                                          2.52      22.59       4.53       1164
--. (label_id: 4)                                         44.15      11.65      18.44      11789
--: (label_id: 5)                                          0.72       2.41       1.11        332
--; (label_id: 6)                                          0.56       0.61       0.58        165
--? (label_id: 7)                                          3.89       6.53       4.88        980
--— (label_id: 8)                                          2.30       0.47       0.77        430
--… (label_id: 9)                                          1.18       8.33       2.07         60
---------------------
--micro avg                                                5.47       5.47       5.47      33182
--macro avg                                                5.77       6.30       3.40      33182
--weighted avg                                            16.77       5.47       7.25      33182
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2063
---------------------
--micro avg                                              100.00     100.00     100.00       2063
--macro avg                                              100.00     100.00     100.00       2063
--weighted avg                                           100.00     100.00     100.00       2063
--
--[INFO] - Epoch 6, step 2793: val_loss was not in top 3
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index fa37b4c..6b5ff89 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             self.hparams.model.punct_class_weights=None
- 
-+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
-+        self.mlp = MultiLayerPerceptron(
-+            self.transformer.config.hidden_size,
-+            self.transformer.config.hidden_size,
-+            num_layers=self.hparams.model.mlp.num_fc_layers, 
-+            activation=self.hparams.model.mlp.activation, 
-+            log_softmax=self.hparams.model.mlp.log_softmax
-+        )
-+
-         self.punct_classifier = TokenClassifier(
-             hidden_size=self.transformer.config.hidden_size,
-             num_classes=len(self.labels_to_ids),
-@@ -80,6 +89,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-             log_softmax=self.hparams.model.domain_head.log_softmax,
-             dropout=self.hparams.model.domain_head.fc_dropout,
-             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
-+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
-         )
- 
-         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
-@@ -119,14 +129,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
-         self.freeze()
- 
--    def forward(self, input_ids, attention_mask, domain_ids=None):
-+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
-         hidden_states = self.transformer(
-             input_ids=input_ids, attention_mask=attention_mask
-         )[0]
-+        hidden_states = self.dropout(hidden_states)
-+        hidden_states = self.mlp(hidden_states)
-         punct_logits = self.punct_classifier(hidden_states=hidden_states)
-         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
-         domain_logits = self.domain_classifier(
--            hidden_states=reverse_grad_hidden_states)
-+            hidden_states=reverse_grad_hidden_states,
-+            subtoken_mask=subtoken_mask)
-         return punct_logits, domain_logits
- 
-     def _make_step(self, batch):
-@@ -136,7 +149,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         punct_labels = batch['labels']
-         domain_labels = batch['domain']
-         punct_logits, domain_logits = self(
--            input_ids=input_ids, attention_mask=attention_mask
-+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
-         )
-         punctuation_loss = self.punctuation_loss(
-             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/lightning_logs.txt
deleted file mode 100644
index 8942cad..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/lightning_logs.txt
+++ /dev/null
@@ -1,5 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-Using native 16bit precision.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/nemo_error_log.txt
deleted file mode 100644
index c2984dc..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/nemo_error_log.txt
+++ /dev/null
@@ -1 +0,0 @@
-[NeMo W 2021-02-08 10:48:11 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 0131ae4..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,3 +0,0 @@
-[NeMo I 2021-02-08 10:48:11 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11
-[NeMo I 2021-02-08 10:48:11 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-08 10:48:11 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/git-info.log
deleted file mode 100644
index 4d1615a..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/git-info.log
+++ /dev/null
@@ -1,623 +0,0 @@
-commit hash: 939a671c8c117db6975316767ced5d95449e2b27
-diff --git a/README.md b/README.md
-index d52dba5..0b403d3 100644
---- a/README.md
-+++ b/README.md
-@@ -333,4 +333,71 @@ label                                                precision    recall       f
-  'punct_f1': 0.5858508944511414,
-  'punct_precision': 0.30176490545272827,
-  'punct_recall': 10.0,
-- 'test_loss': 0.8140875697135925}
-\ No newline at end of file
-+ 'test_loss': 0.8140875697135925}
-+
-+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
-+
-+ (label_id: 0)                                          62.15     100.00      76.66       5154
-+! (label_id: 1)                                          0.00       0.00       0.00        108
-+, (label_id: 2)                                          0.00       0.00       0.00      18022
-+- (label_id: 3)                                          0.00       0.00       0.00       1557
-+. (label_id: 4)                                         41.74      94.01      57.81      15164
-+: (label_id: 5)                                          0.00       0.00       0.00        319
-+; (label_id: 6)                                          0.00       0.00       0.00         88
-+? (label_id: 7)                                          0.00       0.00       0.00       1217
-+ (label_id: 8)                                          0.00       0.00       0.00        752
-+… (label_id: 9)                                          0.00       0.00       0.00         67
-+-------------------
-+micro avg                                               45.72      45.72      45.72      42448
-+macro avg                                               10.39      19.40      13.45      42448
-+weighted avg                                            22.46      45.72      29.96      42448
-+{ 'punct_f1': 13.446383476257324,
-+ 'punct_precision': 10.388500213623047,
-+ 'punct_recall': 19.400554656982422,
-+ 'test_loss': 0.44148480892181396}
-+
-+
-+ ## Log for 8/2/2021
-+
-+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
-+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
-+
-+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
-+End frozen 
-+micro avg                                               41.42      41.42      41.42      33406
-+macro avg                                               11.01      13.54      11.07      33406
-+weighted avg                                            34.88      41.42      34.20      33406
-+
-+1st layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               36.65      36.65      36.65      33463
-+macro avg                                               10.71       9.91       8.26      33463
-+weighted avg                                            34.32      36.65      31.08      33463
-+
-+2nd layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               35.72      35.72      35.72      42448
-+macro avg                                                3.57      10.00       5.26      42448
-+weighted avg                                            12.76      35.72      18.81      42448
-+
-+{'punct_f1': 5.264181137084961,
-+ 'punct_precision': 3.572371006011963,
-+ 'punct_recall': 10.0,
-+ 'test_loss': 18.49854850769043}
-+
-+
-+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
-+alpha from 3->4 seems to reduce convergence rate.
-+
-+micro avg                                               50.98      50.98      50.98      33463
-+macro avg                                               25.99      25.38      23.38      33463
-+weighted avg                                            50.31      50.98      48.27      33463
-+
-+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
-+micro avg                                               58.55      58.55      58.55      39340
-+macro avg                                               30.02      29.74      29.52      39340
-+weighted avg                                            57.91      58.55      57.51      39340
-+
-+still increasing?!
-+{'punct_f1': 29.523975372314453,
-+ 'punct_precision': 30.015613555908203,
-+ 'punct_recall': 29.738296508789062,
-+ 'test_loss': 0.3690211772918701}
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index fe58670..ab37d69 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,49 +1,49 @@
- seed: 42
- trainer:
--    # gpus: 1 # the number of gpus, 0 for CPU
--    # num_nodes: 1
--    # max_epochs: 2
--    # max_steps: null # precedence over max_epochs
--    # accumulate_grad_batches: 4 # accumulates grads every k batches
--    # gradient_clip_val: 0
--    # amp_level: O1 # O1/O2 for mixed precision
--    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
--    # checkpoint_callback: false  # Provided by exp_manager
--    # logger: false #false  # Provided by exp_manager
--    # log_every_n_steps: 1  # Interval of logging.
--    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # resume_from_checkpoint: null
--
--    gpus: 0 # the number of gpus, 0 for CPU
-+    gpus: 1 # the number of gpus, 0 for CPU
-     num_nodes: 1
--    max_epochs: 8
-+    max_epochs: 10
-     max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 1 # accumulates grads every k batches
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-     gradient_clip_val: 0
--    amp_level: O0 # O1/O2 for mixed precision
--    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
-+    amp_level: O1 # O1/O2 for mixed precision
-+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    accelerator: ddp
-     checkpoint_callback: false  # Provided by exp_manager
-     logger: false #false  # Provided by exp_manager
-     log_every_n_steps: 1  # Interval of logging.
-     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    reload_dataloaders_every_epoch: true
-     resume_from_checkpoint: null
- 
-+    # gpus: 0 # the number of gpus, 0 for CPU
-+    # num_nodes: 1
-+    # max_epochs: 8
-+    # max_steps: null # precedence over max_epochs
-+    # accumulate_grad_batches: 1 # accumulates grads every k batches
-+    # gradient_clip_val: 0
-+    # amp_level: O0 # O1/O2 for mixed precision
-+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # # accelerator: ddp
-+    # checkpoint_callback: false  # Provided by exp_manager
-+    # logger: false #false  # Provided by exp_manager
-+    # log_every_n_steps: 1  # Interval of logging.
-+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    # reload_dataloaders_every_epoch: true
-+    # resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /home/nxingyu2/data # /root/data # 
--tmp_path: /home/nxingyu2/data/tmp # /tmp # 
-+base_path: /home/nxingyu/data # /root/data # 
-+tmp_path: /home/nxingyu/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
--    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-     unfrozen: 0
--    maximum_unfrozen: 2
-+    maximum_unfrozen: 1
-     unfreeze_step: 1
-     # unfreeze_every: 3
-     punct_label_ids:
-@@ -58,10 +58,10 @@ model:
-         - "—"
-         - "…"
- 
--    punct_class_weights: true
-+    punct_class_weights: false
-     
-     dataset:
--        data_dir: /home/nxingyu2/data # /root/data # 
-+        data_dir: /home/nxingyu/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -106,6 +106,12 @@ model:
-         config: null
-         # unfrozen_layers: 1
- 
-+    mlp:
-+        num_fc_layers: 2
-+        fc_dropout: 0.1
-+        log_softmax: false
-+        activation: 'GELU'
-+        
-     punct_head:
-         punct_num_fc_layers: 1
-         fc_dropout: 0.1
-@@ -122,17 +128,19 @@ model:
-         use_transformer_init: true
-         loss: 'cel'
-         gamma: 0.1 # coefficient of gradient reversal
-+        pooling: 'token'
-+        idx_conditioned_on: 0
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 3
-+        alpha: 4
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: novograd
-+        name: adamw
-         lr: 1e-3
-         weight_decay: 0.00
-         sched:
-diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
-index d4ff927..1a6fe93 100644
---- a/experiment/core/layers/sequence_classifier.py
-+++ b/experiment/core/layers/sequence_classifier.py
-@@ -1,3 +1,4 @@
-+import torch
- from torch import nn
- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
- from core.utils import transformer_weights_init
-@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
-         log_softmax: bool = True,
-         dropout: float = 0.0,
-         use_transformer_init: bool = True,
--        idx_conditioned_on: int = 0,
-+        pooling: str = 'mean', # mean, max, mean_max, token
-+        idx_conditioned_on: int = None,
-     ):
-         """
-         Initializes the SequenceClassifier module.
-@@ -37,6 +39,7 @@ class SequenceClassifier(nn.Module):
-         super().__init__()
-         self.log_softmax = log_softmax
-         self._idx_conditioned_on = idx_conditioned_on
-+        self.pooling = pooling
-         self.mlp = MultiLayerPerceptron(
-             hidden_size=hidden_size,
-             num_classes=num_classes,
-@@ -48,7 +51,23 @@ class SequenceClassifier(nn.Module):
-         if use_transformer_init:
-             self.apply(lambda module: transformer_weights_init(module, xavier=False))
- 
--    def forward(self, hidden_states):
-+    def forward(self, hidden_states, subtoken_mask=None):
-         hidden_states = self.dropout(hidden_states)
--        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
-+        if self.pooling=='token':
-+            pooled = hidden_states[:, self._idx_conditioned_on]
-+        else:
-+            if subtoken_mask==None:
-+                ct=hidden_states.shape[1] # Seq len
-+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
-+            else:
-+                ct = torch.sum(subtoken_mask,axis=1)
-+            pooled_sum = torch.sum(hidden_states,axis=1)
-+            if self.pooling=='mean' or self.pooling == 'mean_max':
-+                pooled_mean = pooled_sum//ct
-+            if self.pooling=='max' or self.pooling=='mean_max':
-+                pooled_max = torch.max(hidden_states,axis=1)[0]
-+            pooled=pooled_mean if self.pooling=='mean' else \
-+                pooled_max if self.pooling=='max' else \
-+                    torch.cat([pooled_mean,pooled_max],axis=-1)
-+        logits = self.mlp(pooled)
-         return logits
-diff --git a/experiment/info.log b/experiment/info.log
-index 2471fe9..e69de29 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,300 +0,0 @@
--[INFO] - GPU available: True, used: False
--[INFO] - TPU available: None, using: 0 TPU cores
--[INFO] - shuffling train set
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        184
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.23       0.34       0.53        594
--- (label_id: 3)                                          3.06      25.42       5.46         59
--. (label_id: 4)                                         47.22      12.98      20.36        524
--: (label_id: 5)                                          0.00       0.00       0.00         18
--; (label_id: 6)                                          0.00       0.00       0.00         13
--? (label_id: 7)                                          8.45       6.32       7.23         95
--— (label_id: 8)                                          0.00       0.00       0.00         12
--… (label_id: 9)                                          0.00       0.00       0.00          0
---------------------
--micro avg                                                6.05       6.05       6.05       1503
--macro avg                                                6.66       5.01       3.73       1503
--weighted avg                                            17.61       6.05       7.98       1503
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00         92
---------------------
--micro avg                                              100.00     100.00     100.00         92
--macro avg                                              100.00     100.00     100.00         92
--weighted avg                                           100.00     100.00     100.00         92
--
--[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 1.5848931924611143e-08
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
--will be used during training (effective maximum steps = 3192) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 3192
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        202
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.62       0.45       0.70        669
--- (label_id: 3)                                          3.48      27.27       6.17         66
--. (label_id: 4)                                         45.06      13.01      20.19        561
--: (label_id: 5)                                          1.52       6.67       2.47         15
--; (label_id: 6)                                          0.00       0.00       0.00         15
--? (label_id: 7)                                          8.70       7.32       7.95         82
--— (label_id: 8)                                          0.00       0.00       0.00         13
--… (label_id: 9)                                          0.00       0.00       0.00          1
---------------------
--micro avg                                                6.20       6.20       6.20       1628
--macro avg                                                6.04       5.47       3.75       1628
--weighted avg                                            16.79       6.20       7.92       1628
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        101
---------------------
--micro avg                                              100.00     100.00     100.00        101
--macro avg                                              100.00     100.00     100.00        101
--weighted avg                                           100.00     100.00     100.00        101
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.34       0.73       0.46       4402
--! (label_id: 1)                                          0.42      13.95       0.82        129
--, (label_id: 2)                                          2.53       0.64       1.03      15243
--- (label_id: 3)                                          2.45      21.03       4.38       1322
--. (label_id: 4)                                         44.00      11.40      18.11      12542
--: (label_id: 5)                                          0.43       1.41       0.65        354
--; (label_id: 6)                                          0.00       0.00       0.00        163
--? (label_id: 7)                                          4.16       6.27       5.00       1117
--— (label_id: 8)                                          3.00       0.61       1.02        488
--… (label_id: 9)                                          0.97       6.17       1.68         81
---------------------
--micro avg                                                5.41       5.41       5.41      35841
--macro avg                                                5.83       6.22       3.32      35841
--weighted avg                                            16.78       5.41       7.18      35841
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2201
---------------------
--micro avg                                              100.00     100.00     100.00       2201
--macro avg                                              100.00     100.00     100.00       2201
--weighted avg                                           100.00     100.00     100.00       2201
--
--[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.20       0.43       0.27       4226
--! (label_id: 1)                                          0.44      14.17       0.86        127
--, (label_id: 2)                                          1.93       0.49       0.78      14611
--- (label_id: 3)                                          2.23      19.56       4.01       1237
--. (label_id: 4)                                         43.37      11.25      17.86      11977
--: (label_id: 5)                                          0.68       2.34       1.05        342
--; (label_id: 6)                                          0.00       0.00       0.00        129
--? (label_id: 7)                                          5.16       7.47       6.10       1058
--— (label_id: 8)                                          2.15       0.49       0.80        409
--… (label_id: 9)                                          0.69       4.23       1.19         71
---------------------
--micro avg                                                5.23       5.23       5.23      34187
--macro avg                                                5.68       6.04       3.29      34187
--weighted avg                                            16.32       5.23       6.98      34187
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2113
---------------------
--micro avg                                              100.00     100.00     100.00       2113
--macro avg                                              100.00     100.00     100.00       2113
--weighted avg                                           100.00     100.00     100.00       2113
--
--[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.61       0.39       4228
--! (label_id: 1)                                          0.30       8.28       0.58        145
--, (label_id: 2)                                          2.27       0.58       0.92      14495
--- (label_id: 3)                                          2.64      21.78       4.70       1327
--. (label_id: 4)                                         44.87      11.66      18.51      12193
--: (label_id: 5)                                          0.60       1.93       0.91        362
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.19       6.40       5.07       1078
--— (label_id: 8)                                          1.16       0.22       0.37        459
--… (label_id: 9)                                          0.85       4.17       1.41         96
---------------------
--micro avg                                                5.54       5.54       5.54      34547
--macro avg                                                5.72       5.56       3.29      34547
--weighted avg                                            17.08       5.54       7.33      34547
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2114
---------------------
--micro avg                                              100.00     100.00     100.00       2114
--macro avg                                              100.00     100.00     100.00       2114
--weighted avg                                           100.00     100.00     100.00       2114
--
--[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.29       0.63       0.40       4444
--! (label_id: 1)                                          0.38      10.67       0.74        150
--, (label_id: 2)                                          2.32       0.59       0.94      15290
--- (label_id: 3)                                          2.34      20.28       4.19       1292
--. (label_id: 4)                                         43.85      11.68      18.44      12599
--: (label_id: 5)                                          0.41       1.28       0.62        392
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.24       6.30       5.07       1111
--— (label_id: 8)                                          0.00       0.00       0.00        456
--… (label_id: 9)                                          0.38       2.41       0.65         83
---------------------
--micro avg                                                5.40       5.40       5.40      35981
--macro avg                                                5.42       5.38       3.11      35981
--weighted avg                                            16.59       5.40       7.22      35981
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2222
---------------------
--micro avg                                              100.00     100.00     100.00       2222
--macro avg                                              100.00     100.00     100.00       2222
--weighted avg                                           100.00     100.00     100.00       2222
--
--[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.35       0.73       0.48       3844
--! (label_id: 1)                                          0.54      14.62       1.04        130
--, (label_id: 2)                                          2.32       0.59       0.94      13056
--- (label_id: 3)                                          2.67      22.28       4.77       1194
--. (label_id: 4)                                         44.45      11.95      18.84      10791
--: (label_id: 5)                                          0.84       3.21       1.33        280
--; (label_id: 6)                                          0.00       0.00       0.00        140
--? (label_id: 7)                                          4.17       6.56       5.10        914
--— (label_id: 8)                                          0.00       0.00       0.00        401
--… (label_id: 9)                                          0.48       2.63       0.81         76
---------------------
--micro avg                                                5.68       5.68       5.68      30826
--macro avg                                                5.58       6.26       3.33      30826
--weighted avg                                            16.82       5.68       7.41      30826
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1922
---------------------
--micro avg                                              100.00     100.00     100.00       1922
--macro avg                                              100.00     100.00     100.00       1922
--weighted avg                                           100.00     100.00     100.00       1922
--
--[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.60       0.39       3970
--! (label_id: 1)                                          0.35      10.66       0.68        122
--, (label_id: 2)                                          2.09       0.53       0.85      13469
--- (label_id: 3)                                          2.29      19.32       4.10       1201
--. (label_id: 4)                                         43.43      11.24      17.86      11227
--: (label_id: 5)                                          0.63       2.30       0.99        304
--; (label_id: 6)                                          0.00       0.00       0.00        141
--? (label_id: 7)                                          4.52       6.86       5.45       1006
--— (label_id: 8)                                          1.15       0.23       0.38        444
--… (label_id: 9)                                          0.45       2.67       0.78         75
---------------------
--micro avg                                                5.26       5.26       5.26      31959
--macro avg                                                5.52       5.44       3.15      31959
--weighted avg                                            16.42       5.26       7.02      31959
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1985
---------------------
--micro avg                                              100.00     100.00     100.00       1985
--macro avg                                              100.00     100.00     100.00       1985
--weighted avg                                           100.00     100.00     100.00       1985
--
--[INFO] - Epoch 5, step 2394: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.23       0.48       0.31       4126
--! (label_id: 1)                                          0.29       9.40       0.56        117
--, (label_id: 2)                                          1.91       0.49       0.77      14019
--- (label_id: 3)                                          2.52      22.59       4.53       1164
--. (label_id: 4)                                         44.15      11.65      18.44      11789
--: (label_id: 5)                                          0.72       2.41       1.11        332
--; (label_id: 6)                                          0.56       0.61       0.58        165
--? (label_id: 7)                                          3.89       6.53       4.88        980
--— (label_id: 8)                                          2.30       0.47       0.77        430
--… (label_id: 9)                                          1.18       8.33       2.07         60
---------------------
--micro avg                                                5.47       5.47       5.47      33182
--macro avg                                                5.77       6.30       3.40      33182
--weighted avg                                            16.77       5.47       7.25      33182
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2063
---------------------
--micro avg                                              100.00     100.00     100.00       2063
--macro avg                                              100.00     100.00     100.00       2063
--weighted avg                                           100.00     100.00     100.00       2063
--
--[INFO] - Epoch 6, step 2793: val_loss was not in top 3
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index fa37b4c..6b5ff89 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             self.hparams.model.punct_class_weights=None
- 
-+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
-+        self.mlp = MultiLayerPerceptron(
-+            self.transformer.config.hidden_size,
-+            self.transformer.config.hidden_size,
-+            num_layers=self.hparams.model.mlp.num_fc_layers, 
-+            activation=self.hparams.model.mlp.activation, 
-+            log_softmax=self.hparams.model.mlp.log_softmax
-+        )
-+
-         self.punct_classifier = TokenClassifier(
-             hidden_size=self.transformer.config.hidden_size,
-             num_classes=len(self.labels_to_ids),
-@@ -80,6 +89,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-             log_softmax=self.hparams.model.domain_head.log_softmax,
-             dropout=self.hparams.model.domain_head.fc_dropout,
-             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
-+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
-         )
- 
-         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
-@@ -119,14 +129,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
-         self.freeze()
- 
--    def forward(self, input_ids, attention_mask, domain_ids=None):
-+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
-         hidden_states = self.transformer(
-             input_ids=input_ids, attention_mask=attention_mask
-         )[0]
-+        hidden_states = self.dropout(hidden_states)
-+        hidden_states = self.mlp(hidden_states)
-         punct_logits = self.punct_classifier(hidden_states=hidden_states)
-         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
-         domain_logits = self.domain_classifier(
--            hidden_states=reverse_grad_hidden_states)
-+            hidden_states=reverse_grad_hidden_states,
-+            subtoken_mask=subtoken_mask)
-         return punct_logits, domain_logits
- 
-     def _make_step(self, batch):
-@@ -136,7 +149,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         punct_labels = batch['labels']
-         domain_labels = batch['domain']
-         punct_logits, domain_logits = self(
--            input_ids=input_ids, attention_mask=attention_mask
-+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
-         )
-         punctuation_loss = self.punctuation_loss(
-             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/lightning_logs.txt
deleted file mode 100644
index 8942cad..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/lightning_logs.txt
+++ /dev/null
@@ -1,5 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-Using native 16bit precision.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/nemo_error_log.txt
deleted file mode 100644
index f7767e2..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/nemo_error_log.txt
+++ /dev/null
@@ -1 +0,0 @@
-[NeMo W 2021-02-08 10:50:33 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 03a045a..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,3 +0,0 @@
-[NeMo I 2021-02-08 10:50:33 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33
-[NeMo I 2021-02-08 10:50:33 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-08 10:50:33 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/git-info.log
deleted file mode 100644
index 10d12b7..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/git-info.log
+++ /dev/null
@@ -1,623 +0,0 @@
-commit hash: 939a671c8c117db6975316767ced5d95449e2b27
-diff --git a/README.md b/README.md
-index d52dba5..0b403d3 100644
---- a/README.md
-+++ b/README.md
-@@ -333,4 +333,71 @@ label                                                precision    recall       f
-  'punct_f1': 0.5858508944511414,
-  'punct_precision': 0.30176490545272827,
-  'punct_recall': 10.0,
-- 'test_loss': 0.8140875697135925}
-\ No newline at end of file
-+ 'test_loss': 0.8140875697135925}
-+
-+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
-+
-+ (label_id: 0)                                          62.15     100.00      76.66       5154
-+! (label_id: 1)                                          0.00       0.00       0.00        108
-+, (label_id: 2)                                          0.00       0.00       0.00      18022
-+- (label_id: 3)                                          0.00       0.00       0.00       1557
-+. (label_id: 4)                                         41.74      94.01      57.81      15164
-+: (label_id: 5)                                          0.00       0.00       0.00        319
-+; (label_id: 6)                                          0.00       0.00       0.00         88
-+? (label_id: 7)                                          0.00       0.00       0.00       1217
-+ (label_id: 8)                                          0.00       0.00       0.00        752
-+… (label_id: 9)                                          0.00       0.00       0.00         67
-+-------------------
-+micro avg                                               45.72      45.72      45.72      42448
-+macro avg                                               10.39      19.40      13.45      42448
-+weighted avg                                            22.46      45.72      29.96      42448
-+{ 'punct_f1': 13.446383476257324,
-+ 'punct_precision': 10.388500213623047,
-+ 'punct_recall': 19.400554656982422,
-+ 'test_loss': 0.44148480892181396}
-+
-+
-+ ## Log for 8/2/2021
-+
-+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
-+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
-+
-+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
-+End frozen 
-+micro avg                                               41.42      41.42      41.42      33406
-+macro avg                                               11.01      13.54      11.07      33406
-+weighted avg                                            34.88      41.42      34.20      33406
-+
-+1st layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               36.65      36.65      36.65      33463
-+macro avg                                               10.71       9.91       8.26      33463
-+weighted avg                                            34.32      36.65      31.08      33463
-+
-+2nd layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               35.72      35.72      35.72      42448
-+macro avg                                                3.57      10.00       5.26      42448
-+weighted avg                                            12.76      35.72      18.81      42448
-+
-+{'punct_f1': 5.264181137084961,
-+ 'punct_precision': 3.572371006011963,
-+ 'punct_recall': 10.0,
-+ 'test_loss': 18.49854850769043}
-+
-+
-+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
-+alpha from 3->4 seems to reduce convergence rate.
-+
-+micro avg                                               50.98      50.98      50.98      33463
-+macro avg                                               25.99      25.38      23.38      33463
-+weighted avg                                            50.31      50.98      48.27      33463
-+
-+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
-+micro avg                                               58.55      58.55      58.55      39340
-+macro avg                                               30.02      29.74      29.52      39340
-+weighted avg                                            57.91      58.55      57.51      39340
-+
-+still increasing?!
-+{'punct_f1': 29.523975372314453,
-+ 'punct_precision': 30.015613555908203,
-+ 'punct_recall': 29.738296508789062,
-+ 'test_loss': 0.3690211772918701}
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index fe58670..716a889 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,49 +1,49 @@
- seed: 42
- trainer:
--    # gpus: 1 # the number of gpus, 0 for CPU
--    # num_nodes: 1
--    # max_epochs: 2
--    # max_steps: null # precedence over max_epochs
--    # accumulate_grad_batches: 4 # accumulates grads every k batches
--    # gradient_clip_val: 0
--    # amp_level: O1 # O1/O2 for mixed precision
--    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
--    # checkpoint_callback: false  # Provided by exp_manager
--    # logger: false #false  # Provided by exp_manager
--    # log_every_n_steps: 1  # Interval of logging.
--    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # resume_from_checkpoint: null
--
--    gpus: 0 # the number of gpus, 0 for CPU
-+    gpus: 1 # the number of gpus, 0 for CPU
-     num_nodes: 1
--    max_epochs: 8
-+    max_epochs: 10
-     max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 1 # accumulates grads every k batches
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-     gradient_clip_val: 0
--    amp_level: O0 # O1/O2 for mixed precision
--    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
-+    amp_level: O1 # O1/O2 for mixed precision
-+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    accelerator: ddp
-     checkpoint_callback: false  # Provided by exp_manager
-     logger: false #false  # Provided by exp_manager
-     log_every_n_steps: 1  # Interval of logging.
-     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    reload_dataloaders_every_epoch: true
-     resume_from_checkpoint: null
- 
-+    # gpus: 0 # the number of gpus, 0 for CPU
-+    # num_nodes: 1
-+    # max_epochs: 8
-+    # max_steps: null # precedence over max_epochs
-+    # accumulate_grad_batches: 1 # accumulates grads every k batches
-+    # gradient_clip_val: 0
-+    # amp_level: O0 # O1/O2 for mixed precision
-+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # # accelerator: ddp
-+    # checkpoint_callback: false  # Provided by exp_manager
-+    # logger: false #false  # Provided by exp_manager
-+    # log_every_n_steps: 1  # Interval of logging.
-+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    # reload_dataloaders_every_epoch: true
-+    # resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /home/nxingyu2/data # /root/data # 
--tmp_path: /home/nxingyu2/data/tmp # /tmp # 
-+base_path: /home/nxingyu/data # /root/data # 
-+tmp_path: /home/nxingyu/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
--    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-     unfrozen: 0
--    maximum_unfrozen: 2
-+    maximum_unfrozen: 1
-     unfreeze_step: 1
-     # unfreeze_every: 3
-     punct_label_ids:
-@@ -58,10 +58,10 @@ model:
-         - "—"
-         - "…"
- 
--    punct_class_weights: true
-+    punct_class_weights: false
-     
-     dataset:
--        data_dir: /home/nxingyu2/data # /root/data # 
-+        data_dir: /home/nxingyu/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -106,6 +106,12 @@ model:
-         config: null
-         # unfrozen_layers: 1
- 
-+    mlp:
-+        num_fc_layers: 2
-+        fc_dropout: 0.1
-+        log_softmax: false
-+        activation: 'gelu'
-+        
-     punct_head:
-         punct_num_fc_layers: 1
-         fc_dropout: 0.1
-@@ -122,17 +128,19 @@ model:
-         use_transformer_init: true
-         loss: 'cel'
-         gamma: 0.1 # coefficient of gradient reversal
-+        pooling: 'token'
-+        idx_conditioned_on: 0
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 3
-+        alpha: 4
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: novograd
-+        name: adamw
-         lr: 1e-3
-         weight_decay: 0.00
-         sched:
-diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
-index d4ff927..1a6fe93 100644
---- a/experiment/core/layers/sequence_classifier.py
-+++ b/experiment/core/layers/sequence_classifier.py
-@@ -1,3 +1,4 @@
-+import torch
- from torch import nn
- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
- from core.utils import transformer_weights_init
-@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
-         log_softmax: bool = True,
-         dropout: float = 0.0,
-         use_transformer_init: bool = True,
--        idx_conditioned_on: int = 0,
-+        pooling: str = 'mean', # mean, max, mean_max, token
-+        idx_conditioned_on: int = None,
-     ):
-         """
-         Initializes the SequenceClassifier module.
-@@ -37,6 +39,7 @@ class SequenceClassifier(nn.Module):
-         super().__init__()
-         self.log_softmax = log_softmax
-         self._idx_conditioned_on = idx_conditioned_on
-+        self.pooling = pooling
-         self.mlp = MultiLayerPerceptron(
-             hidden_size=hidden_size,
-             num_classes=num_classes,
-@@ -48,7 +51,23 @@ class SequenceClassifier(nn.Module):
-         if use_transformer_init:
-             self.apply(lambda module: transformer_weights_init(module, xavier=False))
- 
--    def forward(self, hidden_states):
-+    def forward(self, hidden_states, subtoken_mask=None):
-         hidden_states = self.dropout(hidden_states)
--        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
-+        if self.pooling=='token':
-+            pooled = hidden_states[:, self._idx_conditioned_on]
-+        else:
-+            if subtoken_mask==None:
-+                ct=hidden_states.shape[1] # Seq len
-+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
-+            else:
-+                ct = torch.sum(subtoken_mask,axis=1)
-+            pooled_sum = torch.sum(hidden_states,axis=1)
-+            if self.pooling=='mean' or self.pooling == 'mean_max':
-+                pooled_mean = pooled_sum//ct
-+            if self.pooling=='max' or self.pooling=='mean_max':
-+                pooled_max = torch.max(hidden_states,axis=1)[0]
-+            pooled=pooled_mean if self.pooling=='mean' else \
-+                pooled_max if self.pooling=='max' else \
-+                    torch.cat([pooled_mean,pooled_max],axis=-1)
-+        logits = self.mlp(pooled)
-         return logits
-diff --git a/experiment/info.log b/experiment/info.log
-index 2471fe9..e69de29 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,300 +0,0 @@
--[INFO] - GPU available: True, used: False
--[INFO] - TPU available: None, using: 0 TPU cores
--[INFO] - shuffling train set
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        184
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.23       0.34       0.53        594
--- (label_id: 3)                                          3.06      25.42       5.46         59
--. (label_id: 4)                                         47.22      12.98      20.36        524
--: (label_id: 5)                                          0.00       0.00       0.00         18
--; (label_id: 6)                                          0.00       0.00       0.00         13
--? (label_id: 7)                                          8.45       6.32       7.23         95
--— (label_id: 8)                                          0.00       0.00       0.00         12
--… (label_id: 9)                                          0.00       0.00       0.00          0
---------------------
--micro avg                                                6.05       6.05       6.05       1503
--macro avg                                                6.66       5.01       3.73       1503
--weighted avg                                            17.61       6.05       7.98       1503
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00         92
---------------------
--micro avg                                              100.00     100.00     100.00         92
--macro avg                                              100.00     100.00     100.00         92
--weighted avg                                           100.00     100.00     100.00         92
--
--[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 1.5848931924611143e-08
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
--will be used during training (effective maximum steps = 3192) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 3192
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        202
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.62       0.45       0.70        669
--- (label_id: 3)                                          3.48      27.27       6.17         66
--. (label_id: 4)                                         45.06      13.01      20.19        561
--: (label_id: 5)                                          1.52       6.67       2.47         15
--; (label_id: 6)                                          0.00       0.00       0.00         15
--? (label_id: 7)                                          8.70       7.32       7.95         82
--— (label_id: 8)                                          0.00       0.00       0.00         13
--… (label_id: 9)                                          0.00       0.00       0.00          1
---------------------
--micro avg                                                6.20       6.20       6.20       1628
--macro avg                                                6.04       5.47       3.75       1628
--weighted avg                                            16.79       6.20       7.92       1628
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        101
---------------------
--micro avg                                              100.00     100.00     100.00        101
--macro avg                                              100.00     100.00     100.00        101
--weighted avg                                           100.00     100.00     100.00        101
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.34       0.73       0.46       4402
--! (label_id: 1)                                          0.42      13.95       0.82        129
--, (label_id: 2)                                          2.53       0.64       1.03      15243
--- (label_id: 3)                                          2.45      21.03       4.38       1322
--. (label_id: 4)                                         44.00      11.40      18.11      12542
--: (label_id: 5)                                          0.43       1.41       0.65        354
--; (label_id: 6)                                          0.00       0.00       0.00        163
--? (label_id: 7)                                          4.16       6.27       5.00       1117
--— (label_id: 8)                                          3.00       0.61       1.02        488
--… (label_id: 9)                                          0.97       6.17       1.68         81
---------------------
--micro avg                                                5.41       5.41       5.41      35841
--macro avg                                                5.83       6.22       3.32      35841
--weighted avg                                            16.78       5.41       7.18      35841
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2201
---------------------
--micro avg                                              100.00     100.00     100.00       2201
--macro avg                                              100.00     100.00     100.00       2201
--weighted avg                                           100.00     100.00     100.00       2201
--
--[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.20       0.43       0.27       4226
--! (label_id: 1)                                          0.44      14.17       0.86        127
--, (label_id: 2)                                          1.93       0.49       0.78      14611
--- (label_id: 3)                                          2.23      19.56       4.01       1237
--. (label_id: 4)                                         43.37      11.25      17.86      11977
--: (label_id: 5)                                          0.68       2.34       1.05        342
--; (label_id: 6)                                          0.00       0.00       0.00        129
--? (label_id: 7)                                          5.16       7.47       6.10       1058
--— (label_id: 8)                                          2.15       0.49       0.80        409
--… (label_id: 9)                                          0.69       4.23       1.19         71
---------------------
--micro avg                                                5.23       5.23       5.23      34187
--macro avg                                                5.68       6.04       3.29      34187
--weighted avg                                            16.32       5.23       6.98      34187
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2113
---------------------
--micro avg                                              100.00     100.00     100.00       2113
--macro avg                                              100.00     100.00     100.00       2113
--weighted avg                                           100.00     100.00     100.00       2113
--
--[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.61       0.39       4228
--! (label_id: 1)                                          0.30       8.28       0.58        145
--, (label_id: 2)                                          2.27       0.58       0.92      14495
--- (label_id: 3)                                          2.64      21.78       4.70       1327
--. (label_id: 4)                                         44.87      11.66      18.51      12193
--: (label_id: 5)                                          0.60       1.93       0.91        362
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.19       6.40       5.07       1078
--— (label_id: 8)                                          1.16       0.22       0.37        459
--… (label_id: 9)                                          0.85       4.17       1.41         96
---------------------
--micro avg                                                5.54       5.54       5.54      34547
--macro avg                                                5.72       5.56       3.29      34547
--weighted avg                                            17.08       5.54       7.33      34547
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2114
---------------------
--micro avg                                              100.00     100.00     100.00       2114
--macro avg                                              100.00     100.00     100.00       2114
--weighted avg                                           100.00     100.00     100.00       2114
--
--[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.29       0.63       0.40       4444
--! (label_id: 1)                                          0.38      10.67       0.74        150
--, (label_id: 2)                                          2.32       0.59       0.94      15290
--- (label_id: 3)                                          2.34      20.28       4.19       1292
--. (label_id: 4)                                         43.85      11.68      18.44      12599
--: (label_id: 5)                                          0.41       1.28       0.62        392
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.24       6.30       5.07       1111
--— (label_id: 8)                                          0.00       0.00       0.00        456
--… (label_id: 9)                                          0.38       2.41       0.65         83
---------------------
--micro avg                                                5.40       5.40       5.40      35981
--macro avg                                                5.42       5.38       3.11      35981
--weighted avg                                            16.59       5.40       7.22      35981
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2222
---------------------
--micro avg                                              100.00     100.00     100.00       2222
--macro avg                                              100.00     100.00     100.00       2222
--weighted avg                                           100.00     100.00     100.00       2222
--
--[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.35       0.73       0.48       3844
--! (label_id: 1)                                          0.54      14.62       1.04        130
--, (label_id: 2)                                          2.32       0.59       0.94      13056
--- (label_id: 3)                                          2.67      22.28       4.77       1194
--. (label_id: 4)                                         44.45      11.95      18.84      10791
--: (label_id: 5)                                          0.84       3.21       1.33        280
--; (label_id: 6)                                          0.00       0.00       0.00        140
--? (label_id: 7)                                          4.17       6.56       5.10        914
--— (label_id: 8)                                          0.00       0.00       0.00        401
--… (label_id: 9)                                          0.48       2.63       0.81         76
---------------------
--micro avg                                                5.68       5.68       5.68      30826
--macro avg                                                5.58       6.26       3.33      30826
--weighted avg                                            16.82       5.68       7.41      30826
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1922
---------------------
--micro avg                                              100.00     100.00     100.00       1922
--macro avg                                              100.00     100.00     100.00       1922
--weighted avg                                           100.00     100.00     100.00       1922
--
--[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.60       0.39       3970
--! (label_id: 1)                                          0.35      10.66       0.68        122
--, (label_id: 2)                                          2.09       0.53       0.85      13469
--- (label_id: 3)                                          2.29      19.32       4.10       1201
--. (label_id: 4)                                         43.43      11.24      17.86      11227
--: (label_id: 5)                                          0.63       2.30       0.99        304
--; (label_id: 6)                                          0.00       0.00       0.00        141
--? (label_id: 7)                                          4.52       6.86       5.45       1006
--— (label_id: 8)                                          1.15       0.23       0.38        444
--… (label_id: 9)                                          0.45       2.67       0.78         75
---------------------
--micro avg                                                5.26       5.26       5.26      31959
--macro avg                                                5.52       5.44       3.15      31959
--weighted avg                                            16.42       5.26       7.02      31959
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1985
---------------------
--micro avg                                              100.00     100.00     100.00       1985
--macro avg                                              100.00     100.00     100.00       1985
--weighted avg                                           100.00     100.00     100.00       1985
--
--[INFO] - Epoch 5, step 2394: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.23       0.48       0.31       4126
--! (label_id: 1)                                          0.29       9.40       0.56        117
--, (label_id: 2)                                          1.91       0.49       0.77      14019
--- (label_id: 3)                                          2.52      22.59       4.53       1164
--. (label_id: 4)                                         44.15      11.65      18.44      11789
--: (label_id: 5)                                          0.72       2.41       1.11        332
--; (label_id: 6)                                          0.56       0.61       0.58        165
--? (label_id: 7)                                          3.89       6.53       4.88        980
--— (label_id: 8)                                          2.30       0.47       0.77        430
--… (label_id: 9)                                          1.18       8.33       2.07         60
---------------------
--micro avg                                                5.47       5.47       5.47      33182
--macro avg                                                5.77       6.30       3.40      33182
--weighted avg                                            16.77       5.47       7.25      33182
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2063
---------------------
--micro avg                                              100.00     100.00     100.00       2063
--macro avg                                              100.00     100.00     100.00       2063
--weighted avg                                           100.00     100.00     100.00       2063
--
--[INFO] - Epoch 6, step 2793: val_loss was not in top 3
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index fa37b4c..6b5ff89 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             self.hparams.model.punct_class_weights=None
- 
-+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
-+        self.mlp = MultiLayerPerceptron(
-+            self.transformer.config.hidden_size,
-+            self.transformer.config.hidden_size,
-+            num_layers=self.hparams.model.mlp.num_fc_layers, 
-+            activation=self.hparams.model.mlp.activation, 
-+            log_softmax=self.hparams.model.mlp.log_softmax
-+        )
-+
-         self.punct_classifier = TokenClassifier(
-             hidden_size=self.transformer.config.hidden_size,
-             num_classes=len(self.labels_to_ids),
-@@ -80,6 +89,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-             log_softmax=self.hparams.model.domain_head.log_softmax,
-             dropout=self.hparams.model.domain_head.fc_dropout,
-             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
-+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
-         )
- 
-         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
-@@ -119,14 +129,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
-         self.freeze()
- 
--    def forward(self, input_ids, attention_mask, domain_ids=None):
-+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
-         hidden_states = self.transformer(
-             input_ids=input_ids, attention_mask=attention_mask
-         )[0]
-+        hidden_states = self.dropout(hidden_states)
-+        hidden_states = self.mlp(hidden_states)
-         punct_logits = self.punct_classifier(hidden_states=hidden_states)
-         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
-         domain_logits = self.domain_classifier(
--            hidden_states=reverse_grad_hidden_states)
-+            hidden_states=reverse_grad_hidden_states,
-+            subtoken_mask=subtoken_mask)
-         return punct_logits, domain_logits
- 
-     def _make_step(self, batch):
-@@ -136,7 +149,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         punct_labels = batch['labels']
-         domain_labels = batch['domain']
-         punct_logits, domain_logits = self(
--            input_ids=input_ids, attention_mask=attention_mask
-+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
-         )
-         punctuation_loss = self.punctuation_loss(
-             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/lightning_logs.txt
deleted file mode 100644
index 8942cad..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/lightning_logs.txt
+++ /dev/null
@@ -1,5 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-Using native 16bit precision.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/nemo_error_log.txt
deleted file mode 100644
index 96631f6..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/nemo_error_log.txt
+++ /dev/null
@@ -1 +0,0 @@
-[NeMo W 2021-02-08 10:51:23 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 8d946e1..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,3 +0,0 @@
-[NeMo I 2021-02-08 10:51:23 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23
-[NeMo I 2021-02-08 10:51:23 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-08 10:51:23 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/git-info.log
deleted file mode 100644
index 3daac52..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/git-info.log
+++ /dev/null
@@ -1,623 +0,0 @@
-commit hash: 939a671c8c117db6975316767ced5d95449e2b27
-diff --git a/README.md b/README.md
-index d52dba5..0b403d3 100644
---- a/README.md
-+++ b/README.md
-@@ -333,4 +333,71 @@ label                                                precision    recall       f
-  'punct_f1': 0.5858508944511414,
-  'punct_precision': 0.30176490545272827,
-  'punct_recall': 10.0,
-- 'test_loss': 0.8140875697135925}
-\ No newline at end of file
-+ 'test_loss': 0.8140875697135925}
-+
-+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
-+
-+ (label_id: 0)                                          62.15     100.00      76.66       5154
-+! (label_id: 1)                                          0.00       0.00       0.00        108
-+, (label_id: 2)                                          0.00       0.00       0.00      18022
-+- (label_id: 3)                                          0.00       0.00       0.00       1557
-+. (label_id: 4)                                         41.74      94.01      57.81      15164
-+: (label_id: 5)                                          0.00       0.00       0.00        319
-+; (label_id: 6)                                          0.00       0.00       0.00         88
-+? (label_id: 7)                                          0.00       0.00       0.00       1217
-+ (label_id: 8)                                          0.00       0.00       0.00        752
-+… (label_id: 9)                                          0.00       0.00       0.00         67
-+-------------------
-+micro avg                                               45.72      45.72      45.72      42448
-+macro avg                                               10.39      19.40      13.45      42448
-+weighted avg                                            22.46      45.72      29.96      42448
-+{ 'punct_f1': 13.446383476257324,
-+ 'punct_precision': 10.388500213623047,
-+ 'punct_recall': 19.400554656982422,
-+ 'test_loss': 0.44148480892181396}
-+
-+
-+ ## Log for 8/2/2021
-+
-+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
-+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
-+
-+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
-+End frozen 
-+micro avg                                               41.42      41.42      41.42      33406
-+macro avg                                               11.01      13.54      11.07      33406
-+weighted avg                                            34.88      41.42      34.20      33406
-+
-+1st layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               36.65      36.65      36.65      33463
-+macro avg                                               10.71       9.91       8.26      33463
-+weighted avg                                            34.32      36.65      31.08      33463
-+
-+2nd layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               35.72      35.72      35.72      42448
-+macro avg                                                3.57      10.00       5.26      42448
-+weighted avg                                            12.76      35.72      18.81      42448
-+
-+{'punct_f1': 5.264181137084961,
-+ 'punct_precision': 3.572371006011963,
-+ 'punct_recall': 10.0,
-+ 'test_loss': 18.49854850769043}
-+
-+
-+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
-+alpha from 3->4 seems to reduce convergence rate.
-+
-+micro avg                                               50.98      50.98      50.98      33463
-+macro avg                                               25.99      25.38      23.38      33463
-+weighted avg                                            50.31      50.98      48.27      33463
-+
-+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
-+micro avg                                               58.55      58.55      58.55      39340
-+macro avg                                               30.02      29.74      29.52      39340
-+weighted avg                                            57.91      58.55      57.51      39340
-+
-+still increasing?!
-+{'punct_f1': 29.523975372314453,
-+ 'punct_precision': 30.015613555908203,
-+ 'punct_recall': 29.738296508789062,
-+ 'test_loss': 0.3690211772918701}
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index fe58670..26bf21c 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,49 +1,49 @@
- seed: 42
- trainer:
--    # gpus: 1 # the number of gpus, 0 for CPU
--    # num_nodes: 1
--    # max_epochs: 2
--    # max_steps: null # precedence over max_epochs
--    # accumulate_grad_batches: 4 # accumulates grads every k batches
--    # gradient_clip_val: 0
--    # amp_level: O1 # O1/O2 for mixed precision
--    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
--    # checkpoint_callback: false  # Provided by exp_manager
--    # logger: false #false  # Provided by exp_manager
--    # log_every_n_steps: 1  # Interval of logging.
--    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # resume_from_checkpoint: null
--
--    gpus: 0 # the number of gpus, 0 for CPU
-+    gpus: 1 # the number of gpus, 0 for CPU
-     num_nodes: 1
--    max_epochs: 8
-+    max_epochs: 10
-     max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 1 # accumulates grads every k batches
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-     gradient_clip_val: 0
--    amp_level: O0 # O1/O2 for mixed precision
--    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
-+    amp_level: O1 # O1/O2 for mixed precision
-+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    accelerator: ddp
-     checkpoint_callback: false  # Provided by exp_manager
-     logger: false #false  # Provided by exp_manager
-     log_every_n_steps: 1  # Interval of logging.
-     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    reload_dataloaders_every_epoch: true
-     resume_from_checkpoint: null
- 
-+    # gpus: 0 # the number of gpus, 0 for CPU
-+    # num_nodes: 1
-+    # max_epochs: 8
-+    # max_steps: null # precedence over max_epochs
-+    # accumulate_grad_batches: 1 # accumulates grads every k batches
-+    # gradient_clip_val: 0
-+    # amp_level: O0 # O1/O2 for mixed precision
-+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # # accelerator: ddp
-+    # checkpoint_callback: false  # Provided by exp_manager
-+    # logger: false #false  # Provided by exp_manager
-+    # log_every_n_steps: 1  # Interval of logging.
-+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    # reload_dataloaders_every_epoch: true
-+    # resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /home/nxingyu2/data # /root/data # 
--tmp_path: /home/nxingyu2/data/tmp # /tmp # 
-+base_path: /home/nxingyu/data # /root/data # 
-+tmp_path: /home/nxingyu/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
--    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-     unfrozen: 0
--    maximum_unfrozen: 2
-+    maximum_unfrozen: 1
-     unfreeze_step: 1
-     # unfreeze_every: 3
-     punct_label_ids:
-@@ -58,10 +58,10 @@ model:
-         - "—"
-         - "…"
- 
--    punct_class_weights: true
-+    punct_class_weights: false
-     
-     dataset:
--        data_dir: /home/nxingyu2/data # /root/data # 
-+        data_dir: /home/nxingyu/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -106,6 +106,12 @@ model:
-         config: null
-         # unfrozen_layers: 1
- 
-+    mlp:
-+        num_fc_layers: 2
-+        fc_dropout: 0.1
-+        log_softmax: false
-+        activation: 'relu'
-+        
-     punct_head:
-         punct_num_fc_layers: 1
-         fc_dropout: 0.1
-@@ -122,17 +128,19 @@ model:
-         use_transformer_init: true
-         loss: 'cel'
-         gamma: 0.1 # coefficient of gradient reversal
-+        pooling: 'token'
-+        idx_conditioned_on: 0
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 3
-+        alpha: 4
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: novograd
-+        name: adamw
-         lr: 1e-3
-         weight_decay: 0.00
-         sched:
-diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
-index d4ff927..1a6fe93 100644
---- a/experiment/core/layers/sequence_classifier.py
-+++ b/experiment/core/layers/sequence_classifier.py
-@@ -1,3 +1,4 @@
-+import torch
- from torch import nn
- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
- from core.utils import transformer_weights_init
-@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
-         log_softmax: bool = True,
-         dropout: float = 0.0,
-         use_transformer_init: bool = True,
--        idx_conditioned_on: int = 0,
-+        pooling: str = 'mean', # mean, max, mean_max, token
-+        idx_conditioned_on: int = None,
-     ):
-         """
-         Initializes the SequenceClassifier module.
-@@ -37,6 +39,7 @@ class SequenceClassifier(nn.Module):
-         super().__init__()
-         self.log_softmax = log_softmax
-         self._idx_conditioned_on = idx_conditioned_on
-+        self.pooling = pooling
-         self.mlp = MultiLayerPerceptron(
-             hidden_size=hidden_size,
-             num_classes=num_classes,
-@@ -48,7 +51,23 @@ class SequenceClassifier(nn.Module):
-         if use_transformer_init:
-             self.apply(lambda module: transformer_weights_init(module, xavier=False))
- 
--    def forward(self, hidden_states):
-+    def forward(self, hidden_states, subtoken_mask=None):
-         hidden_states = self.dropout(hidden_states)
--        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
-+        if self.pooling=='token':
-+            pooled = hidden_states[:, self._idx_conditioned_on]
-+        else:
-+            if subtoken_mask==None:
-+                ct=hidden_states.shape[1] # Seq len
-+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
-+            else:
-+                ct = torch.sum(subtoken_mask,axis=1)
-+            pooled_sum = torch.sum(hidden_states,axis=1)
-+            if self.pooling=='mean' or self.pooling == 'mean_max':
-+                pooled_mean = pooled_sum//ct
-+            if self.pooling=='max' or self.pooling=='mean_max':
-+                pooled_max = torch.max(hidden_states,axis=1)[0]
-+            pooled=pooled_mean if self.pooling=='mean' else \
-+                pooled_max if self.pooling=='max' else \
-+                    torch.cat([pooled_mean,pooled_max],axis=-1)
-+        logits = self.mlp(pooled)
-         return logits
-diff --git a/experiment/info.log b/experiment/info.log
-index 2471fe9..e69de29 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,300 +0,0 @@
--[INFO] - GPU available: True, used: False
--[INFO] - TPU available: None, using: 0 TPU cores
--[INFO] - shuffling train set
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        184
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.23       0.34       0.53        594
--- (label_id: 3)                                          3.06      25.42       5.46         59
--. (label_id: 4)                                         47.22      12.98      20.36        524
--: (label_id: 5)                                          0.00       0.00       0.00         18
--; (label_id: 6)                                          0.00       0.00       0.00         13
--? (label_id: 7)                                          8.45       6.32       7.23         95
--— (label_id: 8)                                          0.00       0.00       0.00         12
--… (label_id: 9)                                          0.00       0.00       0.00          0
---------------------
--micro avg                                                6.05       6.05       6.05       1503
--macro avg                                                6.66       5.01       3.73       1503
--weighted avg                                            17.61       6.05       7.98       1503
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00         92
---------------------
--micro avg                                              100.00     100.00     100.00         92
--macro avg                                              100.00     100.00     100.00         92
--weighted avg                                           100.00     100.00     100.00         92
--
--[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 1.5848931924611143e-08
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
--will be used during training (effective maximum steps = 3192) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 3192
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        202
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.62       0.45       0.70        669
--- (label_id: 3)                                          3.48      27.27       6.17         66
--. (label_id: 4)                                         45.06      13.01      20.19        561
--: (label_id: 5)                                          1.52       6.67       2.47         15
--; (label_id: 6)                                          0.00       0.00       0.00         15
--? (label_id: 7)                                          8.70       7.32       7.95         82
--— (label_id: 8)                                          0.00       0.00       0.00         13
--… (label_id: 9)                                          0.00       0.00       0.00          1
---------------------
--micro avg                                                6.20       6.20       6.20       1628
--macro avg                                                6.04       5.47       3.75       1628
--weighted avg                                            16.79       6.20       7.92       1628
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        101
---------------------
--micro avg                                              100.00     100.00     100.00        101
--macro avg                                              100.00     100.00     100.00        101
--weighted avg                                           100.00     100.00     100.00        101
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.34       0.73       0.46       4402
--! (label_id: 1)                                          0.42      13.95       0.82        129
--, (label_id: 2)                                          2.53       0.64       1.03      15243
--- (label_id: 3)                                          2.45      21.03       4.38       1322
--. (label_id: 4)                                         44.00      11.40      18.11      12542
--: (label_id: 5)                                          0.43       1.41       0.65        354
--; (label_id: 6)                                          0.00       0.00       0.00        163
--? (label_id: 7)                                          4.16       6.27       5.00       1117
--— (label_id: 8)                                          3.00       0.61       1.02        488
--… (label_id: 9)                                          0.97       6.17       1.68         81
---------------------
--micro avg                                                5.41       5.41       5.41      35841
--macro avg                                                5.83       6.22       3.32      35841
--weighted avg                                            16.78       5.41       7.18      35841
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2201
---------------------
--micro avg                                              100.00     100.00     100.00       2201
--macro avg                                              100.00     100.00     100.00       2201
--weighted avg                                           100.00     100.00     100.00       2201
--
--[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.20       0.43       0.27       4226
--! (label_id: 1)                                          0.44      14.17       0.86        127
--, (label_id: 2)                                          1.93       0.49       0.78      14611
--- (label_id: 3)                                          2.23      19.56       4.01       1237
--. (label_id: 4)                                         43.37      11.25      17.86      11977
--: (label_id: 5)                                          0.68       2.34       1.05        342
--; (label_id: 6)                                          0.00       0.00       0.00        129
--? (label_id: 7)                                          5.16       7.47       6.10       1058
--— (label_id: 8)                                          2.15       0.49       0.80        409
--… (label_id: 9)                                          0.69       4.23       1.19         71
---------------------
--micro avg                                                5.23       5.23       5.23      34187
--macro avg                                                5.68       6.04       3.29      34187
--weighted avg                                            16.32       5.23       6.98      34187
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2113
---------------------
--micro avg                                              100.00     100.00     100.00       2113
--macro avg                                              100.00     100.00     100.00       2113
--weighted avg                                           100.00     100.00     100.00       2113
--
--[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.61       0.39       4228
--! (label_id: 1)                                          0.30       8.28       0.58        145
--, (label_id: 2)                                          2.27       0.58       0.92      14495
--- (label_id: 3)                                          2.64      21.78       4.70       1327
--. (label_id: 4)                                         44.87      11.66      18.51      12193
--: (label_id: 5)                                          0.60       1.93       0.91        362
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.19       6.40       5.07       1078
--— (label_id: 8)                                          1.16       0.22       0.37        459
--… (label_id: 9)                                          0.85       4.17       1.41         96
---------------------
--micro avg                                                5.54       5.54       5.54      34547
--macro avg                                                5.72       5.56       3.29      34547
--weighted avg                                            17.08       5.54       7.33      34547
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2114
---------------------
--micro avg                                              100.00     100.00     100.00       2114
--macro avg                                              100.00     100.00     100.00       2114
--weighted avg                                           100.00     100.00     100.00       2114
--
--[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.29       0.63       0.40       4444
--! (label_id: 1)                                          0.38      10.67       0.74        150
--, (label_id: 2)                                          2.32       0.59       0.94      15290
--- (label_id: 3)                                          2.34      20.28       4.19       1292
--. (label_id: 4)                                         43.85      11.68      18.44      12599
--: (label_id: 5)                                          0.41       1.28       0.62        392
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.24       6.30       5.07       1111
--— (label_id: 8)                                          0.00       0.00       0.00        456
--… (label_id: 9)                                          0.38       2.41       0.65         83
---------------------
--micro avg                                                5.40       5.40       5.40      35981
--macro avg                                                5.42       5.38       3.11      35981
--weighted avg                                            16.59       5.40       7.22      35981
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2222
---------------------
--micro avg                                              100.00     100.00     100.00       2222
--macro avg                                              100.00     100.00     100.00       2222
--weighted avg                                           100.00     100.00     100.00       2222
--
--[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.35       0.73       0.48       3844
--! (label_id: 1)                                          0.54      14.62       1.04        130
--, (label_id: 2)                                          2.32       0.59       0.94      13056
--- (label_id: 3)                                          2.67      22.28       4.77       1194
--. (label_id: 4)                                         44.45      11.95      18.84      10791
--: (label_id: 5)                                          0.84       3.21       1.33        280
--; (label_id: 6)                                          0.00       0.00       0.00        140
--? (label_id: 7)                                          4.17       6.56       5.10        914
--— (label_id: 8)                                          0.00       0.00       0.00        401
--… (label_id: 9)                                          0.48       2.63       0.81         76
---------------------
--micro avg                                                5.68       5.68       5.68      30826
--macro avg                                                5.58       6.26       3.33      30826
--weighted avg                                            16.82       5.68       7.41      30826
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1922
---------------------
--micro avg                                              100.00     100.00     100.00       1922
--macro avg                                              100.00     100.00     100.00       1922
--weighted avg                                           100.00     100.00     100.00       1922
--
--[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.60       0.39       3970
--! (label_id: 1)                                          0.35      10.66       0.68        122
--, (label_id: 2)                                          2.09       0.53       0.85      13469
--- (label_id: 3)                                          2.29      19.32       4.10       1201
--. (label_id: 4)                                         43.43      11.24      17.86      11227
--: (label_id: 5)                                          0.63       2.30       0.99        304
--; (label_id: 6)                                          0.00       0.00       0.00        141
--? (label_id: 7)                                          4.52       6.86       5.45       1006
--— (label_id: 8)                                          1.15       0.23       0.38        444
--… (label_id: 9)                                          0.45       2.67       0.78         75
---------------------
--micro avg                                                5.26       5.26       5.26      31959
--macro avg                                                5.52       5.44       3.15      31959
--weighted avg                                            16.42       5.26       7.02      31959
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1985
---------------------
--micro avg                                              100.00     100.00     100.00       1985
--macro avg                                              100.00     100.00     100.00       1985
--weighted avg                                           100.00     100.00     100.00       1985
--
--[INFO] - Epoch 5, step 2394: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.23       0.48       0.31       4126
--! (label_id: 1)                                          0.29       9.40       0.56        117
--, (label_id: 2)                                          1.91       0.49       0.77      14019
--- (label_id: 3)                                          2.52      22.59       4.53       1164
--. (label_id: 4)                                         44.15      11.65      18.44      11789
--: (label_id: 5)                                          0.72       2.41       1.11        332
--; (label_id: 6)                                          0.56       0.61       0.58        165
--? (label_id: 7)                                          3.89       6.53       4.88        980
--— (label_id: 8)                                          2.30       0.47       0.77        430
--… (label_id: 9)                                          1.18       8.33       2.07         60
---------------------
--micro avg                                                5.47       5.47       5.47      33182
--macro avg                                                5.77       6.30       3.40      33182
--weighted avg                                            16.77       5.47       7.25      33182
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2063
---------------------
--micro avg                                              100.00     100.00     100.00       2063
--macro avg                                              100.00     100.00     100.00       2063
--weighted avg                                           100.00     100.00     100.00       2063
--
--[INFO] - Epoch 6, step 2793: val_loss was not in top 3
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index fa37b4c..6b5ff89 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             self.hparams.model.punct_class_weights=None
- 
-+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
-+        self.mlp = MultiLayerPerceptron(
-+            self.transformer.config.hidden_size,
-+            self.transformer.config.hidden_size,
-+            num_layers=self.hparams.model.mlp.num_fc_layers, 
-+            activation=self.hparams.model.mlp.activation, 
-+            log_softmax=self.hparams.model.mlp.log_softmax
-+        )
-+
-         self.punct_classifier = TokenClassifier(
-             hidden_size=self.transformer.config.hidden_size,
-             num_classes=len(self.labels_to_ids),
-@@ -80,6 +89,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-             log_softmax=self.hparams.model.domain_head.log_softmax,
-             dropout=self.hparams.model.domain_head.fc_dropout,
-             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
-+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
-         )
- 
-         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
-@@ -119,14 +129,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
-         self.freeze()
- 
--    def forward(self, input_ids, attention_mask, domain_ids=None):
-+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
-         hidden_states = self.transformer(
-             input_ids=input_ids, attention_mask=attention_mask
-         )[0]
-+        hidden_states = self.dropout(hidden_states)
-+        hidden_states = self.mlp(hidden_states)
-         punct_logits = self.punct_classifier(hidden_states=hidden_states)
-         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
-         domain_logits = self.domain_classifier(
--            hidden_states=reverse_grad_hidden_states)
-+            hidden_states=reverse_grad_hidden_states,
-+            subtoken_mask=subtoken_mask)
-         return punct_logits, domain_logits
- 
-     def _make_step(self, batch):
-@@ -136,7 +149,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         punct_labels = batch['labels']
-         domain_labels = batch['domain']
-         punct_logits, domain_logits = self(
--            input_ids=input_ids, attention_mask=attention_mask
-+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
-         )
-         punctuation_loss = self.punctuation_loss(
-             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/lightning_logs.txt
deleted file mode 100644
index 9fa336d..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/lightning_logs.txt
+++ /dev/null
@@ -1,24 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-Using native 16bit precision.
-Global seed set to 42
-initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | dropout             | Dropout              | 0     
-2 | mlp                 | MultiLayerPerceptron | 131 K 
-3 | punct_classifier    | TokenClassifier      | 2.6 K 
-4 | domain_classifier   | SequenceClassifier   | 257   
-5 | punctuation_loss    | FocalDiceLoss        | 0     
-6 | domain_loss         | CrossEntropyLoss     | 0     
-7 | agg_loss            | AggregatorLoss       | 0     
-8 | punct_class_report  | ClassificationReport | 0     
-9 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-167 K     Trainable params
-13.4 M    Non-trainable params
-13.6 M    Total params
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/nemo_error_log.txt
deleted file mode 100644
index 315a43b..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/nemo_error_log.txt
+++ /dev/null
@@ -1,7 +0,0 @@
-[NeMo W 2021-02-08 10:51:41 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 10:51:58 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 10:51:58 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 179aed7..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,9 +0,0 @@
-[NeMo I 2021-02-08 10:51:41 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41
-[NeMo I 2021-02-08 10:51:41 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-08 10:51:41 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 10:51:58 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 10:51:58 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/git-info.log
deleted file mode 100644
index a2ded57..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/git-info.log
+++ /dev/null
@@ -1,624 +0,0 @@
-commit hash: 939a671c8c117db6975316767ced5d95449e2b27
-diff --git a/README.md b/README.md
-index d52dba5..0b403d3 100644
---- a/README.md
-+++ b/README.md
-@@ -333,4 +333,71 @@ label                                                precision    recall       f
-  'punct_f1': 0.5858508944511414,
-  'punct_precision': 0.30176490545272827,
-  'punct_recall': 10.0,
-- 'test_loss': 0.8140875697135925}
-\ No newline at end of file
-+ 'test_loss': 0.8140875697135925}
-+
-+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
-+
-+ (label_id: 0)                                          62.15     100.00      76.66       5154
-+! (label_id: 1)                                          0.00       0.00       0.00        108
-+, (label_id: 2)                                          0.00       0.00       0.00      18022
-+- (label_id: 3)                                          0.00       0.00       0.00       1557
-+. (label_id: 4)                                         41.74      94.01      57.81      15164
-+: (label_id: 5)                                          0.00       0.00       0.00        319
-+; (label_id: 6)                                          0.00       0.00       0.00         88
-+? (label_id: 7)                                          0.00       0.00       0.00       1217
-+ (label_id: 8)                                          0.00       0.00       0.00        752
-+… (label_id: 9)                                          0.00       0.00       0.00         67
-+-------------------
-+micro avg                                               45.72      45.72      45.72      42448
-+macro avg                                               10.39      19.40      13.45      42448
-+weighted avg                                            22.46      45.72      29.96      42448
-+{ 'punct_f1': 13.446383476257324,
-+ 'punct_precision': 10.388500213623047,
-+ 'punct_recall': 19.400554656982422,
-+ 'test_loss': 0.44148480892181396}
-+
-+
-+ ## Log for 8/2/2021
-+
-+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
-+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
-+
-+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
-+End frozen 
-+micro avg                                               41.42      41.42      41.42      33406
-+macro avg                                               11.01      13.54      11.07      33406
-+weighted avg                                            34.88      41.42      34.20      33406
-+
-+1st layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               36.65      36.65      36.65      33463
-+macro avg                                               10.71       9.91       8.26      33463
-+weighted avg                                            34.32      36.65      31.08      33463
-+
-+2nd layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               35.72      35.72      35.72      42448
-+macro avg                                                3.57      10.00       5.26      42448
-+weighted avg                                            12.76      35.72      18.81      42448
-+
-+{'punct_f1': 5.264181137084961,
-+ 'punct_precision': 3.572371006011963,
-+ 'punct_recall': 10.0,
-+ 'test_loss': 18.49854850769043}
-+
-+
-+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
-+alpha from 3->4 seems to reduce convergence rate.
-+
-+micro avg                                               50.98      50.98      50.98      33463
-+macro avg                                               25.99      25.38      23.38      33463
-+weighted avg                                            50.31      50.98      48.27      33463
-+
-+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
-+micro avg                                               58.55      58.55      58.55      39340
-+macro avg                                               30.02      29.74      29.52      39340
-+weighted avg                                            57.91      58.55      57.51      39340
-+
-+still increasing?!
-+{'punct_f1': 29.523975372314453,
-+ 'punct_precision': 30.015613555908203,
-+ 'punct_recall': 29.738296508789062,
-+ 'test_loss': 0.3690211772918701}
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index fe58670..26bf21c 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,49 +1,49 @@
- seed: 42
- trainer:
--    # gpus: 1 # the number of gpus, 0 for CPU
--    # num_nodes: 1
--    # max_epochs: 2
--    # max_steps: null # precedence over max_epochs
--    # accumulate_grad_batches: 4 # accumulates grads every k batches
--    # gradient_clip_val: 0
--    # amp_level: O1 # O1/O2 for mixed precision
--    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
--    # checkpoint_callback: false  # Provided by exp_manager
--    # logger: false #false  # Provided by exp_manager
--    # log_every_n_steps: 1  # Interval of logging.
--    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # resume_from_checkpoint: null
--
--    gpus: 0 # the number of gpus, 0 for CPU
-+    gpus: 1 # the number of gpus, 0 for CPU
-     num_nodes: 1
--    max_epochs: 8
-+    max_epochs: 10
-     max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 1 # accumulates grads every k batches
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-     gradient_clip_val: 0
--    amp_level: O0 # O1/O2 for mixed precision
--    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
-+    amp_level: O1 # O1/O2 for mixed precision
-+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    accelerator: ddp
-     checkpoint_callback: false  # Provided by exp_manager
-     logger: false #false  # Provided by exp_manager
-     log_every_n_steps: 1  # Interval of logging.
-     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    reload_dataloaders_every_epoch: true
-     resume_from_checkpoint: null
- 
-+    # gpus: 0 # the number of gpus, 0 for CPU
-+    # num_nodes: 1
-+    # max_epochs: 8
-+    # max_steps: null # precedence over max_epochs
-+    # accumulate_grad_batches: 1 # accumulates grads every k batches
-+    # gradient_clip_val: 0
-+    # amp_level: O0 # O1/O2 for mixed precision
-+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # # accelerator: ddp
-+    # checkpoint_callback: false  # Provided by exp_manager
-+    # logger: false #false  # Provided by exp_manager
-+    # log_every_n_steps: 1  # Interval of logging.
-+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    # reload_dataloaders_every_epoch: true
-+    # resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /home/nxingyu2/data # /root/data # 
--tmp_path: /home/nxingyu2/data/tmp # /tmp # 
-+base_path: /home/nxingyu/data # /root/data # 
-+tmp_path: /home/nxingyu/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
--    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-     unfrozen: 0
--    maximum_unfrozen: 2
-+    maximum_unfrozen: 1
-     unfreeze_step: 1
-     # unfreeze_every: 3
-     punct_label_ids:
-@@ -58,10 +58,10 @@ model:
-         - "—"
-         - "…"
- 
--    punct_class_weights: true
-+    punct_class_weights: false
-     
-     dataset:
--        data_dir: /home/nxingyu2/data # /root/data # 
-+        data_dir: /home/nxingyu/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -106,6 +106,12 @@ model:
-         config: null
-         # unfrozen_layers: 1
- 
-+    mlp:
-+        num_fc_layers: 2
-+        fc_dropout: 0.1
-+        log_softmax: false
-+        activation: 'relu'
-+        
-     punct_head:
-         punct_num_fc_layers: 1
-         fc_dropout: 0.1
-@@ -122,17 +128,19 @@ model:
-         use_transformer_init: true
-         loss: 'cel'
-         gamma: 0.1 # coefficient of gradient reversal
-+        pooling: 'token'
-+        idx_conditioned_on: 0
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 3
-+        alpha: 4
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: novograd
-+        name: adamw
-         lr: 1e-3
-         weight_decay: 0.00
-         sched:
-diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
-index d4ff927..cafc57d 100644
---- a/experiment/core/layers/sequence_classifier.py
-+++ b/experiment/core/layers/sequence_classifier.py
-@@ -1,3 +1,4 @@
-+import torch
- from torch import nn
- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
- from core.utils import transformer_weights_init
-@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
-         log_softmax: bool = True,
-         dropout: float = 0.0,
-         use_transformer_init: bool = True,
--        idx_conditioned_on: int = 0,
-+        pooling: str = 'mean', # mean, max, mean_max, token
-+        idx_conditioned_on: int = None,
-     ):
-         """
-         Initializes the SequenceClassifier module.
-@@ -37,6 +39,7 @@ class SequenceClassifier(nn.Module):
-         super().__init__()
-         self.log_softmax = log_softmax
-         self._idx_conditioned_on = idx_conditioned_on
-+        self.pooling = pooling
-         self.mlp = MultiLayerPerceptron(
-             hidden_size=hidden_size,
-             num_classes=num_classes,
-@@ -48,7 +51,24 @@ class SequenceClassifier(nn.Module):
-         if use_transformer_init:
-             self.apply(lambda module: transformer_weights_init(module, xavier=False))
- 
--    def forward(self, hidden_states):
-+    def forward(self, hidden_states, subtoken_mask=None):
-         hidden_states = self.dropout(hidden_states)
--        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
-+        pp(self.pooling)
-+        if self.pooling=='token':
-+            pooled = hidden_states[:, self._idx_conditioned_on]
-+        else:
-+            if subtoken_mask==None:
-+                ct=hidden_states.shape[1] # Seq len
-+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
-+            else:
-+                ct = torch.sum(subtoken_mask,axis=1)
-+            pooled_sum = torch.sum(hidden_states,axis=1)
-+            if self.pooling=='mean' or self.pooling == 'mean_max':
-+                pooled_mean = pooled_sum//ct
-+            if self.pooling=='max' or self.pooling=='mean_max':
-+                pooled_max = torch.max(hidden_states,axis=1)[0]
-+            pooled=pooled_mean if self.pooling=='mean' else \
-+                pooled_max if self.pooling=='max' else \
-+                    torch.cat([pooled_mean,pooled_max],axis=-1)
-+        logits = self.mlp(pooled)
-         return logits
-diff --git a/experiment/info.log b/experiment/info.log
-index 2471fe9..e69de29 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,300 +0,0 @@
--[INFO] - GPU available: True, used: False
--[INFO] - TPU available: None, using: 0 TPU cores
--[INFO] - shuffling train set
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        184
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.23       0.34       0.53        594
--- (label_id: 3)                                          3.06      25.42       5.46         59
--. (label_id: 4)                                         47.22      12.98      20.36        524
--: (label_id: 5)                                          0.00       0.00       0.00         18
--; (label_id: 6)                                          0.00       0.00       0.00         13
--? (label_id: 7)                                          8.45       6.32       7.23         95
--— (label_id: 8)                                          0.00       0.00       0.00         12
--… (label_id: 9)                                          0.00       0.00       0.00          0
---------------------
--micro avg                                                6.05       6.05       6.05       1503
--macro avg                                                6.66       5.01       3.73       1503
--weighted avg                                            17.61       6.05       7.98       1503
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00         92
---------------------
--micro avg                                              100.00     100.00     100.00         92
--macro avg                                              100.00     100.00     100.00         92
--weighted avg                                           100.00     100.00     100.00         92
--
--[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 1.5848931924611143e-08
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
--will be used during training (effective maximum steps = 3192) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 3192
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        202
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.62       0.45       0.70        669
--- (label_id: 3)                                          3.48      27.27       6.17         66
--. (label_id: 4)                                         45.06      13.01      20.19        561
--: (label_id: 5)                                          1.52       6.67       2.47         15
--; (label_id: 6)                                          0.00       0.00       0.00         15
--? (label_id: 7)                                          8.70       7.32       7.95         82
--— (label_id: 8)                                          0.00       0.00       0.00         13
--… (label_id: 9)                                          0.00       0.00       0.00          1
---------------------
--micro avg                                                6.20       6.20       6.20       1628
--macro avg                                                6.04       5.47       3.75       1628
--weighted avg                                            16.79       6.20       7.92       1628
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        101
---------------------
--micro avg                                              100.00     100.00     100.00        101
--macro avg                                              100.00     100.00     100.00        101
--weighted avg                                           100.00     100.00     100.00        101
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.34       0.73       0.46       4402
--! (label_id: 1)                                          0.42      13.95       0.82        129
--, (label_id: 2)                                          2.53       0.64       1.03      15243
--- (label_id: 3)                                          2.45      21.03       4.38       1322
--. (label_id: 4)                                         44.00      11.40      18.11      12542
--: (label_id: 5)                                          0.43       1.41       0.65        354
--; (label_id: 6)                                          0.00       0.00       0.00        163
--? (label_id: 7)                                          4.16       6.27       5.00       1117
--— (label_id: 8)                                          3.00       0.61       1.02        488
--… (label_id: 9)                                          0.97       6.17       1.68         81
---------------------
--micro avg                                                5.41       5.41       5.41      35841
--macro avg                                                5.83       6.22       3.32      35841
--weighted avg                                            16.78       5.41       7.18      35841
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2201
---------------------
--micro avg                                              100.00     100.00     100.00       2201
--macro avg                                              100.00     100.00     100.00       2201
--weighted avg                                           100.00     100.00     100.00       2201
--
--[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.20       0.43       0.27       4226
--! (label_id: 1)                                          0.44      14.17       0.86        127
--, (label_id: 2)                                          1.93       0.49       0.78      14611
--- (label_id: 3)                                          2.23      19.56       4.01       1237
--. (label_id: 4)                                         43.37      11.25      17.86      11977
--: (label_id: 5)                                          0.68       2.34       1.05        342
--; (label_id: 6)                                          0.00       0.00       0.00        129
--? (label_id: 7)                                          5.16       7.47       6.10       1058
--— (label_id: 8)                                          2.15       0.49       0.80        409
--… (label_id: 9)                                          0.69       4.23       1.19         71
---------------------
--micro avg                                                5.23       5.23       5.23      34187
--macro avg                                                5.68       6.04       3.29      34187
--weighted avg                                            16.32       5.23       6.98      34187
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2113
---------------------
--micro avg                                              100.00     100.00     100.00       2113
--macro avg                                              100.00     100.00     100.00       2113
--weighted avg                                           100.00     100.00     100.00       2113
--
--[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.61       0.39       4228
--! (label_id: 1)                                          0.30       8.28       0.58        145
--, (label_id: 2)                                          2.27       0.58       0.92      14495
--- (label_id: 3)                                          2.64      21.78       4.70       1327
--. (label_id: 4)                                         44.87      11.66      18.51      12193
--: (label_id: 5)                                          0.60       1.93       0.91        362
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.19       6.40       5.07       1078
--— (label_id: 8)                                          1.16       0.22       0.37        459
--… (label_id: 9)                                          0.85       4.17       1.41         96
---------------------
--micro avg                                                5.54       5.54       5.54      34547
--macro avg                                                5.72       5.56       3.29      34547
--weighted avg                                            17.08       5.54       7.33      34547
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2114
---------------------
--micro avg                                              100.00     100.00     100.00       2114
--macro avg                                              100.00     100.00     100.00       2114
--weighted avg                                           100.00     100.00     100.00       2114
--
--[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.29       0.63       0.40       4444
--! (label_id: 1)                                          0.38      10.67       0.74        150
--, (label_id: 2)                                          2.32       0.59       0.94      15290
--- (label_id: 3)                                          2.34      20.28       4.19       1292
--. (label_id: 4)                                         43.85      11.68      18.44      12599
--: (label_id: 5)                                          0.41       1.28       0.62        392
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.24       6.30       5.07       1111
--— (label_id: 8)                                          0.00       0.00       0.00        456
--… (label_id: 9)                                          0.38       2.41       0.65         83
---------------------
--micro avg                                                5.40       5.40       5.40      35981
--macro avg                                                5.42       5.38       3.11      35981
--weighted avg                                            16.59       5.40       7.22      35981
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2222
---------------------
--micro avg                                              100.00     100.00     100.00       2222
--macro avg                                              100.00     100.00     100.00       2222
--weighted avg                                           100.00     100.00     100.00       2222
--
--[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.35       0.73       0.48       3844
--! (label_id: 1)                                          0.54      14.62       1.04        130
--, (label_id: 2)                                          2.32       0.59       0.94      13056
--- (label_id: 3)                                          2.67      22.28       4.77       1194
--. (label_id: 4)                                         44.45      11.95      18.84      10791
--: (label_id: 5)                                          0.84       3.21       1.33        280
--; (label_id: 6)                                          0.00       0.00       0.00        140
--? (label_id: 7)                                          4.17       6.56       5.10        914
--— (label_id: 8)                                          0.00       0.00       0.00        401
--… (label_id: 9)                                          0.48       2.63       0.81         76
---------------------
--micro avg                                                5.68       5.68       5.68      30826
--macro avg                                                5.58       6.26       3.33      30826
--weighted avg                                            16.82       5.68       7.41      30826
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1922
---------------------
--micro avg                                              100.00     100.00     100.00       1922
--macro avg                                              100.00     100.00     100.00       1922
--weighted avg                                           100.00     100.00     100.00       1922
--
--[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.60       0.39       3970
--! (label_id: 1)                                          0.35      10.66       0.68        122
--, (label_id: 2)                                          2.09       0.53       0.85      13469
--- (label_id: 3)                                          2.29      19.32       4.10       1201
--. (label_id: 4)                                         43.43      11.24      17.86      11227
--: (label_id: 5)                                          0.63       2.30       0.99        304
--; (label_id: 6)                                          0.00       0.00       0.00        141
--? (label_id: 7)                                          4.52       6.86       5.45       1006
--— (label_id: 8)                                          1.15       0.23       0.38        444
--… (label_id: 9)                                          0.45       2.67       0.78         75
---------------------
--micro avg                                                5.26       5.26       5.26      31959
--macro avg                                                5.52       5.44       3.15      31959
--weighted avg                                            16.42       5.26       7.02      31959
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1985
---------------------
--micro avg                                              100.00     100.00     100.00       1985
--macro avg                                              100.00     100.00     100.00       1985
--weighted avg                                           100.00     100.00     100.00       1985
--
--[INFO] - Epoch 5, step 2394: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.23       0.48       0.31       4126
--! (label_id: 1)                                          0.29       9.40       0.56        117
--, (label_id: 2)                                          1.91       0.49       0.77      14019
--- (label_id: 3)                                          2.52      22.59       4.53       1164
--. (label_id: 4)                                         44.15      11.65      18.44      11789
--: (label_id: 5)                                          0.72       2.41       1.11        332
--; (label_id: 6)                                          0.56       0.61       0.58        165
--? (label_id: 7)                                          3.89       6.53       4.88        980
--— (label_id: 8)                                          2.30       0.47       0.77        430
--… (label_id: 9)                                          1.18       8.33       2.07         60
---------------------
--micro avg                                                5.47       5.47       5.47      33182
--macro avg                                                5.77       6.30       3.40      33182
--weighted avg                                            16.77       5.47       7.25      33182
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2063
---------------------
--micro avg                                              100.00     100.00     100.00       2063
--macro avg                                              100.00     100.00     100.00       2063
--weighted avg                                           100.00     100.00     100.00       2063
--
--[INFO] - Epoch 6, step 2793: val_loss was not in top 3
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index fa37b4c..6b5ff89 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             self.hparams.model.punct_class_weights=None
- 
-+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
-+        self.mlp = MultiLayerPerceptron(
-+            self.transformer.config.hidden_size,
-+            self.transformer.config.hidden_size,
-+            num_layers=self.hparams.model.mlp.num_fc_layers, 
-+            activation=self.hparams.model.mlp.activation, 
-+            log_softmax=self.hparams.model.mlp.log_softmax
-+        )
-+
-         self.punct_classifier = TokenClassifier(
-             hidden_size=self.transformer.config.hidden_size,
-             num_classes=len(self.labels_to_ids),
-@@ -80,6 +89,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-             log_softmax=self.hparams.model.domain_head.log_softmax,
-             dropout=self.hparams.model.domain_head.fc_dropout,
-             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
-+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
-         )
- 
-         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
-@@ -119,14 +129,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
-         self.freeze()
- 
--    def forward(self, input_ids, attention_mask, domain_ids=None):
-+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
-         hidden_states = self.transformer(
-             input_ids=input_ids, attention_mask=attention_mask
-         )[0]
-+        hidden_states = self.dropout(hidden_states)
-+        hidden_states = self.mlp(hidden_states)
-         punct_logits = self.punct_classifier(hidden_states=hidden_states)
-         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
-         domain_logits = self.domain_classifier(
--            hidden_states=reverse_grad_hidden_states)
-+            hidden_states=reverse_grad_hidden_states,
-+            subtoken_mask=subtoken_mask)
-         return punct_logits, domain_logits
- 
-     def _make_step(self, batch):
-@@ -136,7 +149,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         punct_labels = batch['labels']
-         domain_labels = batch['domain']
-         punct_logits, domain_logits = self(
--            input_ids=input_ids, attention_mask=attention_mask
-+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
-         )
-         punctuation_loss = self.punctuation_loss(
-             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/lightning_logs.txt
deleted file mode 100644
index 9fa336d..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/lightning_logs.txt
+++ /dev/null
@@ -1,24 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-Using native 16bit precision.
-Global seed set to 42
-initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | dropout             | Dropout              | 0     
-2 | mlp                 | MultiLayerPerceptron | 131 K 
-3 | punct_classifier    | TokenClassifier      | 2.6 K 
-4 | domain_classifier   | SequenceClassifier   | 257   
-5 | punctuation_loss    | FocalDiceLoss        | 0     
-6 | domain_loss         | CrossEntropyLoss     | 0     
-7 | agg_loss            | AggregatorLoss       | 0     
-8 | punct_class_report  | ClassificationReport | 0     
-9 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-167 K     Trainable params
-13.4 M    Non-trainable params
-13.6 M    Total params
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/nemo_error_log.txt
deleted file mode 100644
index 0e115e9..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/nemo_error_log.txt
+++ /dev/null
@@ -1,7 +0,0 @@
-[NeMo W 2021-02-08 10:53:14 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 10:53:23 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 10:53:23 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index b137c51..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,9 +0,0 @@
-[NeMo I 2021-02-08 10:53:14 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14
-[NeMo I 2021-02-08 10:53:14 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-08 10:53:14 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 10:53:23 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 10:53:23 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/events.out.tfevents.1612753145.Titan.17415.0 b/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/events.out.tfevents.1612753145.Titan.17415.0
deleted file mode 100644
index 388ea94..0000000
Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/events.out.tfevents.1612753145.Titan.17415.0 and /dev/null differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/git-info.log
deleted file mode 100644
index a7bf865..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/git-info.log
+++ /dev/null
@@ -1,625 +0,0 @@
-commit hash: 939a671c8c117db6975316767ced5d95449e2b27
-diff --git a/README.md b/README.md
-index d52dba5..0b403d3 100644
---- a/README.md
-+++ b/README.md
-@@ -333,4 +333,71 @@ label                                                precision    recall       f
-  'punct_f1': 0.5858508944511414,
-  'punct_precision': 0.30176490545272827,
-  'punct_recall': 10.0,
-- 'test_loss': 0.8140875697135925}
-\ No newline at end of file
-+ 'test_loss': 0.8140875697135925}
-+
-+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
-+
-+ (label_id: 0)                                          62.15     100.00      76.66       5154
-+! (label_id: 1)                                          0.00       0.00       0.00        108
-+, (label_id: 2)                                          0.00       0.00       0.00      18022
-+- (label_id: 3)                                          0.00       0.00       0.00       1557
-+. (label_id: 4)                                         41.74      94.01      57.81      15164
-+: (label_id: 5)                                          0.00       0.00       0.00        319
-+; (label_id: 6)                                          0.00       0.00       0.00         88
-+? (label_id: 7)                                          0.00       0.00       0.00       1217
-+ (label_id: 8)                                          0.00       0.00       0.00        752
-+… (label_id: 9)                                          0.00       0.00       0.00         67
-+-------------------
-+micro avg                                               45.72      45.72      45.72      42448
-+macro avg                                               10.39      19.40      13.45      42448
-+weighted avg                                            22.46      45.72      29.96      42448
-+{ 'punct_f1': 13.446383476257324,
-+ 'punct_precision': 10.388500213623047,
-+ 'punct_recall': 19.400554656982422,
-+ 'test_loss': 0.44148480892181396}
-+
-+
-+ ## Log for 8/2/2021
-+
-+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
-+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
-+
-+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
-+End frozen 
-+micro avg                                               41.42      41.42      41.42      33406
-+macro avg                                               11.01      13.54      11.07      33406
-+weighted avg                                            34.88      41.42      34.20      33406
-+
-+1st layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               36.65      36.65      36.65      33463
-+macro avg                                               10.71       9.91       8.26      33463
-+weighted avg                                            34.32      36.65      31.08      33463
-+
-+2nd layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               35.72      35.72      35.72      42448
-+macro avg                                                3.57      10.00       5.26      42448
-+weighted avg                                            12.76      35.72      18.81      42448
-+
-+{'punct_f1': 5.264181137084961,
-+ 'punct_precision': 3.572371006011963,
-+ 'punct_recall': 10.0,
-+ 'test_loss': 18.49854850769043}
-+
-+
-+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
-+alpha from 3->4 seems to reduce convergence rate.
-+
-+micro avg                                               50.98      50.98      50.98      33463
-+macro avg                                               25.99      25.38      23.38      33463
-+weighted avg                                            50.31      50.98      48.27      33463
-+
-+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
-+micro avg                                               58.55      58.55      58.55      39340
-+macro avg                                               30.02      29.74      29.52      39340
-+weighted avg                                            57.91      58.55      57.51      39340
-+
-+still increasing?!
-+{'punct_f1': 29.523975372314453,
-+ 'punct_precision': 30.015613555908203,
-+ 'punct_recall': 29.738296508789062,
-+ 'test_loss': 0.3690211772918701}
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index fe58670..26bf21c 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,49 +1,49 @@
- seed: 42
- trainer:
--    # gpus: 1 # the number of gpus, 0 for CPU
--    # num_nodes: 1
--    # max_epochs: 2
--    # max_steps: null # precedence over max_epochs
--    # accumulate_grad_batches: 4 # accumulates grads every k batches
--    # gradient_clip_val: 0
--    # amp_level: O1 # O1/O2 for mixed precision
--    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
--    # checkpoint_callback: false  # Provided by exp_manager
--    # logger: false #false  # Provided by exp_manager
--    # log_every_n_steps: 1  # Interval of logging.
--    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # resume_from_checkpoint: null
--
--    gpus: 0 # the number of gpus, 0 for CPU
-+    gpus: 1 # the number of gpus, 0 for CPU
-     num_nodes: 1
--    max_epochs: 8
-+    max_epochs: 10
-     max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 1 # accumulates grads every k batches
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-     gradient_clip_val: 0
--    amp_level: O0 # O1/O2 for mixed precision
--    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
-+    amp_level: O1 # O1/O2 for mixed precision
-+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    accelerator: ddp
-     checkpoint_callback: false  # Provided by exp_manager
-     logger: false #false  # Provided by exp_manager
-     log_every_n_steps: 1  # Interval of logging.
-     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    reload_dataloaders_every_epoch: true
-     resume_from_checkpoint: null
- 
-+    # gpus: 0 # the number of gpus, 0 for CPU
-+    # num_nodes: 1
-+    # max_epochs: 8
-+    # max_steps: null # precedence over max_epochs
-+    # accumulate_grad_batches: 1 # accumulates grads every k batches
-+    # gradient_clip_val: 0
-+    # amp_level: O0 # O1/O2 for mixed precision
-+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # # accelerator: ddp
-+    # checkpoint_callback: false  # Provided by exp_manager
-+    # logger: false #false  # Provided by exp_manager
-+    # log_every_n_steps: 1  # Interval of logging.
-+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    # reload_dataloaders_every_epoch: true
-+    # resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /home/nxingyu2/data # /root/data # 
--tmp_path: /home/nxingyu2/data/tmp # /tmp # 
-+base_path: /home/nxingyu/data # /root/data # 
-+tmp_path: /home/nxingyu/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
--    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-     unfrozen: 0
--    maximum_unfrozen: 2
-+    maximum_unfrozen: 1
-     unfreeze_step: 1
-     # unfreeze_every: 3
-     punct_label_ids:
-@@ -58,10 +58,10 @@ model:
-         - "—"
-         - "…"
- 
--    punct_class_weights: true
-+    punct_class_weights: false
-     
-     dataset:
--        data_dir: /home/nxingyu2/data # /root/data # 
-+        data_dir: /home/nxingyu/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -106,6 +106,12 @@ model:
-         config: null
-         # unfrozen_layers: 1
- 
-+    mlp:
-+        num_fc_layers: 2
-+        fc_dropout: 0.1
-+        log_softmax: false
-+        activation: 'relu'
-+        
-     punct_head:
-         punct_num_fc_layers: 1
-         fc_dropout: 0.1
-@@ -122,17 +128,19 @@ model:
-         use_transformer_init: true
-         loss: 'cel'
-         gamma: 0.1 # coefficient of gradient reversal
-+        pooling: 'token'
-+        idx_conditioned_on: 0
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 3
-+        alpha: 4
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: novograd
-+        name: adamw
-         lr: 1e-3
-         weight_decay: 0.00
-         sched:
-diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
-index d4ff927..cafc57d 100644
---- a/experiment/core/layers/sequence_classifier.py
-+++ b/experiment/core/layers/sequence_classifier.py
-@@ -1,3 +1,4 @@
-+import torch
- from torch import nn
- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
- from core.utils import transformer_weights_init
-@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
-         log_softmax: bool = True,
-         dropout: float = 0.0,
-         use_transformer_init: bool = True,
--        idx_conditioned_on: int = 0,
-+        pooling: str = 'mean', # mean, max, mean_max, token
-+        idx_conditioned_on: int = None,
-     ):
-         """
-         Initializes the SequenceClassifier module.
-@@ -37,6 +39,7 @@ class SequenceClassifier(nn.Module):
-         super().__init__()
-         self.log_softmax = log_softmax
-         self._idx_conditioned_on = idx_conditioned_on
-+        self.pooling = pooling
-         self.mlp = MultiLayerPerceptron(
-             hidden_size=hidden_size,
-             num_classes=num_classes,
-@@ -48,7 +51,24 @@ class SequenceClassifier(nn.Module):
-         if use_transformer_init:
-             self.apply(lambda module: transformer_weights_init(module, xavier=False))
- 
--    def forward(self, hidden_states):
-+    def forward(self, hidden_states, subtoken_mask=None):
-         hidden_states = self.dropout(hidden_states)
--        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
-+        pp(self.pooling)
-+        if self.pooling=='token':
-+            pooled = hidden_states[:, self._idx_conditioned_on]
-+        else:
-+            if subtoken_mask==None:
-+                ct=hidden_states.shape[1] # Seq len
-+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
-+            else:
-+                ct = torch.sum(subtoken_mask,axis=1)
-+            pooled_sum = torch.sum(hidden_states,axis=1)
-+            if self.pooling=='mean' or self.pooling == 'mean_max':
-+                pooled_mean = pooled_sum//ct
-+            if self.pooling=='max' or self.pooling=='mean_max':
-+                pooled_max = torch.max(hidden_states,axis=1)[0]
-+            pooled=pooled_mean if self.pooling=='mean' else \
-+                pooled_max if self.pooling=='max' else \
-+                    torch.cat([pooled_mean,pooled_max],axis=-1)
-+        logits = self.mlp(pooled)
-         return logits
-diff --git a/experiment/info.log b/experiment/info.log
-index 2471fe9..e69de29 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,300 +0,0 @@
--[INFO] - GPU available: True, used: False
--[INFO] - TPU available: None, using: 0 TPU cores
--[INFO] - shuffling train set
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        184
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.23       0.34       0.53        594
--- (label_id: 3)                                          3.06      25.42       5.46         59
--. (label_id: 4)                                         47.22      12.98      20.36        524
--: (label_id: 5)                                          0.00       0.00       0.00         18
--; (label_id: 6)                                          0.00       0.00       0.00         13
--? (label_id: 7)                                          8.45       6.32       7.23         95
--— (label_id: 8)                                          0.00       0.00       0.00         12
--… (label_id: 9)                                          0.00       0.00       0.00          0
---------------------
--micro avg                                                6.05       6.05       6.05       1503
--macro avg                                                6.66       5.01       3.73       1503
--weighted avg                                            17.61       6.05       7.98       1503
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00         92
---------------------
--micro avg                                              100.00     100.00     100.00         92
--macro avg                                              100.00     100.00     100.00         92
--weighted avg                                           100.00     100.00     100.00         92
--
--[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 1.5848931924611143e-08
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
--will be used during training (effective maximum steps = 3192) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 3192
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        202
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.62       0.45       0.70        669
--- (label_id: 3)                                          3.48      27.27       6.17         66
--. (label_id: 4)                                         45.06      13.01      20.19        561
--: (label_id: 5)                                          1.52       6.67       2.47         15
--; (label_id: 6)                                          0.00       0.00       0.00         15
--? (label_id: 7)                                          8.70       7.32       7.95         82
--— (label_id: 8)                                          0.00       0.00       0.00         13
--… (label_id: 9)                                          0.00       0.00       0.00          1
---------------------
--micro avg                                                6.20       6.20       6.20       1628
--macro avg                                                6.04       5.47       3.75       1628
--weighted avg                                            16.79       6.20       7.92       1628
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        101
---------------------
--micro avg                                              100.00     100.00     100.00        101
--macro avg                                              100.00     100.00     100.00        101
--weighted avg                                           100.00     100.00     100.00        101
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.34       0.73       0.46       4402
--! (label_id: 1)                                          0.42      13.95       0.82        129
--, (label_id: 2)                                          2.53       0.64       1.03      15243
--- (label_id: 3)                                          2.45      21.03       4.38       1322
--. (label_id: 4)                                         44.00      11.40      18.11      12542
--: (label_id: 5)                                          0.43       1.41       0.65        354
--; (label_id: 6)                                          0.00       0.00       0.00        163
--? (label_id: 7)                                          4.16       6.27       5.00       1117
--— (label_id: 8)                                          3.00       0.61       1.02        488
--… (label_id: 9)                                          0.97       6.17       1.68         81
---------------------
--micro avg                                                5.41       5.41       5.41      35841
--macro avg                                                5.83       6.22       3.32      35841
--weighted avg                                            16.78       5.41       7.18      35841
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2201
---------------------
--micro avg                                              100.00     100.00     100.00       2201
--macro avg                                              100.00     100.00     100.00       2201
--weighted avg                                           100.00     100.00     100.00       2201
--
--[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.20       0.43       0.27       4226
--! (label_id: 1)                                          0.44      14.17       0.86        127
--, (label_id: 2)                                          1.93       0.49       0.78      14611
--- (label_id: 3)                                          2.23      19.56       4.01       1237
--. (label_id: 4)                                         43.37      11.25      17.86      11977
--: (label_id: 5)                                          0.68       2.34       1.05        342
--; (label_id: 6)                                          0.00       0.00       0.00        129
--? (label_id: 7)                                          5.16       7.47       6.10       1058
--— (label_id: 8)                                          2.15       0.49       0.80        409
--… (label_id: 9)                                          0.69       4.23       1.19         71
---------------------
--micro avg                                                5.23       5.23       5.23      34187
--macro avg                                                5.68       6.04       3.29      34187
--weighted avg                                            16.32       5.23       6.98      34187
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2113
---------------------
--micro avg                                              100.00     100.00     100.00       2113
--macro avg                                              100.00     100.00     100.00       2113
--weighted avg                                           100.00     100.00     100.00       2113
--
--[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.61       0.39       4228
--! (label_id: 1)                                          0.30       8.28       0.58        145
--, (label_id: 2)                                          2.27       0.58       0.92      14495
--- (label_id: 3)                                          2.64      21.78       4.70       1327
--. (label_id: 4)                                         44.87      11.66      18.51      12193
--: (label_id: 5)                                          0.60       1.93       0.91        362
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.19       6.40       5.07       1078
--— (label_id: 8)                                          1.16       0.22       0.37        459
--… (label_id: 9)                                          0.85       4.17       1.41         96
---------------------
--micro avg                                                5.54       5.54       5.54      34547
--macro avg                                                5.72       5.56       3.29      34547
--weighted avg                                            17.08       5.54       7.33      34547
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2114
---------------------
--micro avg                                              100.00     100.00     100.00       2114
--macro avg                                              100.00     100.00     100.00       2114
--weighted avg                                           100.00     100.00     100.00       2114
--
--[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.29       0.63       0.40       4444
--! (label_id: 1)                                          0.38      10.67       0.74        150
--, (label_id: 2)                                          2.32       0.59       0.94      15290
--- (label_id: 3)                                          2.34      20.28       4.19       1292
--. (label_id: 4)                                         43.85      11.68      18.44      12599
--: (label_id: 5)                                          0.41       1.28       0.62        392
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.24       6.30       5.07       1111
--— (label_id: 8)                                          0.00       0.00       0.00        456
--… (label_id: 9)                                          0.38       2.41       0.65         83
---------------------
--micro avg                                                5.40       5.40       5.40      35981
--macro avg                                                5.42       5.38       3.11      35981
--weighted avg                                            16.59       5.40       7.22      35981
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2222
---------------------
--micro avg                                              100.00     100.00     100.00       2222
--macro avg                                              100.00     100.00     100.00       2222
--weighted avg                                           100.00     100.00     100.00       2222
--
--[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.35       0.73       0.48       3844
--! (label_id: 1)                                          0.54      14.62       1.04        130
--, (label_id: 2)                                          2.32       0.59       0.94      13056
--- (label_id: 3)                                          2.67      22.28       4.77       1194
--. (label_id: 4)                                         44.45      11.95      18.84      10791
--: (label_id: 5)                                          0.84       3.21       1.33        280
--; (label_id: 6)                                          0.00       0.00       0.00        140
--? (label_id: 7)                                          4.17       6.56       5.10        914
--— (label_id: 8)                                          0.00       0.00       0.00        401
--… (label_id: 9)                                          0.48       2.63       0.81         76
---------------------
--micro avg                                                5.68       5.68       5.68      30826
--macro avg                                                5.58       6.26       3.33      30826
--weighted avg                                            16.82       5.68       7.41      30826
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1922
---------------------
--micro avg                                              100.00     100.00     100.00       1922
--macro avg                                              100.00     100.00     100.00       1922
--weighted avg                                           100.00     100.00     100.00       1922
--
--[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.60       0.39       3970
--! (label_id: 1)                                          0.35      10.66       0.68        122
--, (label_id: 2)                                          2.09       0.53       0.85      13469
--- (label_id: 3)                                          2.29      19.32       4.10       1201
--. (label_id: 4)                                         43.43      11.24      17.86      11227
--: (label_id: 5)                                          0.63       2.30       0.99        304
--; (label_id: 6)                                          0.00       0.00       0.00        141
--? (label_id: 7)                                          4.52       6.86       5.45       1006
--— (label_id: 8)                                          1.15       0.23       0.38        444
--… (label_id: 9)                                          0.45       2.67       0.78         75
---------------------
--micro avg                                                5.26       5.26       5.26      31959
--macro avg                                                5.52       5.44       3.15      31959
--weighted avg                                            16.42       5.26       7.02      31959
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1985
---------------------
--micro avg                                              100.00     100.00     100.00       1985
--macro avg                                              100.00     100.00     100.00       1985
--weighted avg                                           100.00     100.00     100.00       1985
--
--[INFO] - Epoch 5, step 2394: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.23       0.48       0.31       4126
--! (label_id: 1)                                          0.29       9.40       0.56        117
--, (label_id: 2)                                          1.91       0.49       0.77      14019
--- (label_id: 3)                                          2.52      22.59       4.53       1164
--. (label_id: 4)                                         44.15      11.65      18.44      11789
--: (label_id: 5)                                          0.72       2.41       1.11        332
--; (label_id: 6)                                          0.56       0.61       0.58        165
--? (label_id: 7)                                          3.89       6.53       4.88        980
--— (label_id: 8)                                          2.30       0.47       0.77        430
--… (label_id: 9)                                          1.18       8.33       2.07         60
---------------------
--micro avg                                                5.47       5.47       5.47      33182
--macro avg                                                5.77       6.30       3.40      33182
--weighted avg                                            16.77       5.47       7.25      33182
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2063
---------------------
--micro avg                                              100.00     100.00     100.00       2063
--macro avg                                              100.00     100.00     100.00       2063
--weighted avg                                           100.00     100.00     100.00       2063
--
--[INFO] - Epoch 6, step 2793: val_loss was not in top 3
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index fa37b4c..43fc93d 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             self.hparams.model.punct_class_weights=None
- 
-+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
-+        self.mlp = MultiLayerPerceptron(
-+            self.transformer.config.hidden_size,
-+            self.transformer.config.hidden_size,
-+            num_layers=self.hparams.model.mlp.num_fc_layers, 
-+            activation=self.hparams.model.mlp.activation, 
-+            log_softmax=self.hparams.model.mlp.log_softmax
-+        )
-+
-         self.punct_classifier = TokenClassifier(
-             hidden_size=self.transformer.config.hidden_size,
-             num_classes=len(self.labels_to_ids),
-@@ -80,6 +89,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-             log_softmax=self.hparams.model.domain_head.log_softmax,
-             dropout=self.hparams.model.domain_head.fc_dropout,
-             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
-+            pooling=self.hparams.model.domain_head.pooling,
-+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
-         )
- 
-         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
-@@ -119,14 +130,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
-         self.freeze()
- 
--    def forward(self, input_ids, attention_mask, domain_ids=None):
-+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
-         hidden_states = self.transformer(
-             input_ids=input_ids, attention_mask=attention_mask
-         )[0]
-+        hidden_states = self.dropout(hidden_states)
-+        hidden_states = self.mlp(hidden_states)
-         punct_logits = self.punct_classifier(hidden_states=hidden_states)
-         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
-         domain_logits = self.domain_classifier(
--            hidden_states=reverse_grad_hidden_states)
-+            hidden_states=reverse_grad_hidden_states,
-+            subtoken_mask=subtoken_mask)
-         return punct_logits, domain_logits
- 
-     def _make_step(self, batch):
-@@ -136,7 +150,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         punct_labels = batch['labels']
-         domain_labels = batch['domain']
-         punct_logits, domain_logits = self(
--            input_ids=input_ids, attention_mask=attention_mask
-+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
-         )
-         punctuation_loss = self.punctuation_loss(
-             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/hparams.yaml
deleted file mode 100644
index 37788ae..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/hparams.yaml
+++ /dev/null
@@ -1,114 +0,0 @@
-seed: 42
-trainer:
-  gpus: 1
-  num_nodes: 1
-  max_epochs: 10
-  max_steps: null
-  accumulate_grad_batches: 4
-  gradient_clip_val: 0
-  amp_level: O1
-  precision: 16
-  accelerator: ddp
-  checkpoint_callback: false
-  logger: false
-  log_every_n_steps: 1
-  val_check_interval: 1.0
-  resume_from_checkpoint: null
-exp_manager:
-  exp_dir: /home/nxingyu/project/
-  name: Punctuation_with_Domain_discriminator
-  create_tensorboard_logger: true
-  create_checkpoint_callback: true
-base_path: /home/nxingyu/data
-tmp_path: /home/nxingyu/data/tmp
-model:
-  nemo_path: null
-  transformer_path: google/electra-small-discriminator
-  unfrozen: 0
-  maximum_unfrozen: 1
-  unfreeze_step: 1
-  punct_label_ids:
-  - ''
-  - '!'
-  - ','
-  - '-'
-  - .
-  - ':'
-  - ;
-  - '?'
-  - —
-  - …
-  punct_class_weights: false
-  dataset:
-    data_dir: /home/nxingyu/data
-    labelled:
-    - /home/nxingyu/data/ted_talks_processed
-    unlabelled: null
-    max_seq_length: 128
-    pad_label: ''
-    ignore_extra_tokens: false
-    ignore_start_end: false
-    use_cache: false
-    num_workers: 0
-    pin_memory: true
-    drop_last: false
-    num_labels: 10
-    num_domains: 1
-    test_unlabelled: true
-    train_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 8
-    validation_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 8
-  tokenizer:
-    tokenizer_name: google/electra-small-discriminator
-    vocab_file: null
-    tokenizer_model: null
-    special_tokens: null
-  language_model:
-    pretrained_model_name: google/electra-small-discriminator
-    lm_checkpoint: null
-    config_file: null
-    config: null
-  mlp:
-    num_fc_layers: 2
-    fc_dropout: 0.1
-    log_softmax: false
-    activation: relu
-  punct_head:
-    punct_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: dice
-  domain_head:
-    domain_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: cel
-    gamma: 0.1
-    pooling: token
-    idx_conditioned_on: 0
-  dice_loss:
-    epsilon: 0.01
-    alpha: 4
-    macro_average: true
-  focal_loss:
-    gamma: 5
-  optim:
-    name: adamw
-    lr: 0.001
-    weight_decay: 0.0
-    sched:
-      name: WarmupAnnealing
-      warmup_steps: null
-      warmup_ratio: 0.1
-      last_epoch: -1
-      monitor: val_loss
-      reduce_on_plateau: false
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/lightning_logs.txt
deleted file mode 100644
index 30fbbb4..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/lightning_logs.txt
+++ /dev/null
@@ -1,52 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-Using native 16bit precision.
-Global seed set to 42
-initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | dropout             | Dropout              | 0     
-2 | mlp                 | MultiLayerPerceptron | 131 K 
-3 | punct_classifier    | TokenClassifier      | 2.6 K 
-4 | domain_classifier   | SequenceClassifier   | 257   
-5 | punctuation_loss    | FocalDiceLoss        | 0     
-6 | domain_loss         | CrossEntropyLoss     | 0     
-7 | agg_loss            | AggregatorLoss       | 0     
-8 | punct_class_report  | ClassificationReport | 0     
-9 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-167 K     Trainable params
-13.4 M    Non-trainable params
-13.6 M    Total params
-LR finder stopped early due to diverging loss.
-Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/lr_find_temp_model.ckpt
-Failed to compute suggesting for `lr`. There might not be enough points.
-Traceback (most recent call last):
-  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 355, in suggestion
-    min_grad = np.gradient(loss).argmin()
-  File "<__array_function__ internals>", line 5, in gradient
-  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
-    raise ValueError(
-ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
-Global seed set to 42
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | dropout             | Dropout              | 0     
-2 | mlp                 | MultiLayerPerceptron | 131 K 
-3 | punct_classifier    | TokenClassifier      | 2.6 K 
-4 | domain_classifier   | SequenceClassifier   | 257   
-5 | punctuation_loss    | FocalDiceLoss        | 0     
-6 | domain_loss         | CrossEntropyLoss     | 0     
-7 | agg_loss            | AggregatorLoss       | 0     
-8 | punct_class_report  | ClassificationReport | 0     
-9 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-167 K     Trainable params
-13.4 M    Non-trainable params
-13.6 M    Total params
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/nemo_error_log.txt
deleted file mode 100644
index 7e0fd53..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/nemo_error_log.txt
+++ /dev/null
@@ -1,27 +0,0 @@
-[NeMo W 2021-02-08 10:58:45 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 10:58:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 10:58:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 10:58:57 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 10:59:03 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-      warnings.warn(*args, **kwargs)
-    
-Failed to compute suggesting for `lr`. There might not be enough points.
-Traceback (most recent call last):
-  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 355, in suggestion
-    min_grad = np.gradient(loss).argmin()
-  File "<__array_function__ internals>", line 5, in gradient
-  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
-    raise ValueError(
-ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
-[NeMo W 2021-02-08 10:59:05 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 10:59:05 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index b17022d..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,21 +0,0 @@
-[NeMo I 2021-02-08 10:58:45 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45
-[NeMo I 2021-02-08 10:58:45 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-08 10:58:45 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 10:58:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 10:58:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 10:58:57 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 10:59:03 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 10:59:05 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 10:59:05 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/git-info.log
deleted file mode 100644
index 9310747..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/git-info.log
+++ /dev/null
@@ -1,625 +0,0 @@
-commit hash: 939a671c8c117db6975316767ced5d95449e2b27
-diff --git a/README.md b/README.md
-index d52dba5..0b403d3 100644
---- a/README.md
-+++ b/README.md
-@@ -333,4 +333,71 @@ label                                                precision    recall       f
-  'punct_f1': 0.5858508944511414,
-  'punct_precision': 0.30176490545272827,
-  'punct_recall': 10.0,
-- 'test_loss': 0.8140875697135925}
-\ No newline at end of file
-+ 'test_loss': 0.8140875697135925}
-+
-+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
-+
-+ (label_id: 0)                                          62.15     100.00      76.66       5154
-+! (label_id: 1)                                          0.00       0.00       0.00        108
-+, (label_id: 2)                                          0.00       0.00       0.00      18022
-+- (label_id: 3)                                          0.00       0.00       0.00       1557
-+. (label_id: 4)                                         41.74      94.01      57.81      15164
-+: (label_id: 5)                                          0.00       0.00       0.00        319
-+; (label_id: 6)                                          0.00       0.00       0.00         88
-+? (label_id: 7)                                          0.00       0.00       0.00       1217
-+ (label_id: 8)                                          0.00       0.00       0.00        752
-+… (label_id: 9)                                          0.00       0.00       0.00         67
-+-------------------
-+micro avg                                               45.72      45.72      45.72      42448
-+macro avg                                               10.39      19.40      13.45      42448
-+weighted avg                                            22.46      45.72      29.96      42448
-+{ 'punct_f1': 13.446383476257324,
-+ 'punct_precision': 10.388500213623047,
-+ 'punct_recall': 19.400554656982422,
-+ 'test_loss': 0.44148480892181396}
-+
-+
-+ ## Log for 8/2/2021
-+
-+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
-+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
-+
-+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
-+End frozen 
-+micro avg                                               41.42      41.42      41.42      33406
-+macro avg                                               11.01      13.54      11.07      33406
-+weighted avg                                            34.88      41.42      34.20      33406
-+
-+1st layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               36.65      36.65      36.65      33463
-+macro avg                                               10.71       9.91       8.26      33463
-+weighted avg                                            34.32      36.65      31.08      33463
-+
-+2nd layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               35.72      35.72      35.72      42448
-+macro avg                                                3.57      10.00       5.26      42448
-+weighted avg                                            12.76      35.72      18.81      42448
-+
-+{'punct_f1': 5.264181137084961,
-+ 'punct_precision': 3.572371006011963,
-+ 'punct_recall': 10.0,
-+ 'test_loss': 18.49854850769043}
-+
-+
-+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
-+alpha from 3->4 seems to reduce convergence rate.
-+
-+micro avg                                               50.98      50.98      50.98      33463
-+macro avg                                               25.99      25.38      23.38      33463
-+weighted avg                                            50.31      50.98      48.27      33463
-+
-+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
-+micro avg                                               58.55      58.55      58.55      39340
-+macro avg                                               30.02      29.74      29.52      39340
-+weighted avg                                            57.91      58.55      57.51      39340
-+
-+still increasing?!
-+{'punct_f1': 29.523975372314453,
-+ 'punct_precision': 30.015613555908203,
-+ 'punct_recall': 29.738296508789062,
-+ 'test_loss': 0.3690211772918701}
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index fe58670..a4a012a 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,49 +1,49 @@
- seed: 42
- trainer:
--    # gpus: 1 # the number of gpus, 0 for CPU
--    # num_nodes: 1
--    # max_epochs: 2
--    # max_steps: null # precedence over max_epochs
--    # accumulate_grad_batches: 4 # accumulates grads every k batches
--    # gradient_clip_val: 0
--    # amp_level: O1 # O1/O2 for mixed precision
--    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
--    # checkpoint_callback: false  # Provided by exp_manager
--    # logger: false #false  # Provided by exp_manager
--    # log_every_n_steps: 1  # Interval of logging.
--    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # resume_from_checkpoint: null
--
--    gpus: 0 # the number of gpus, 0 for CPU
-+    gpus: 1 # the number of gpus, 0 for CPU
-     num_nodes: 1
--    max_epochs: 8
-+    max_epochs: 10
-     max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 1 # accumulates grads every k batches
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-     gradient_clip_val: 0
--    amp_level: O0 # O1/O2 for mixed precision
--    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
-+    amp_level: O1 # O1/O2 for mixed precision
-+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    accelerator: ddp
-     checkpoint_callback: false  # Provided by exp_manager
-     logger: false #false  # Provided by exp_manager
-     log_every_n_steps: 1  # Interval of logging.
-     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    reload_dataloaders_every_epoch: true
-     resume_from_checkpoint: null
- 
-+    # gpus: 0 # the number of gpus, 0 for CPU
-+    # num_nodes: 1
-+    # max_epochs: 8
-+    # max_steps: null # precedence over max_epochs
-+    # accumulate_grad_batches: 1 # accumulates grads every k batches
-+    # gradient_clip_val: 0
-+    # amp_level: O0 # O1/O2 for mixed precision
-+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # # accelerator: ddp
-+    # checkpoint_callback: false  # Provided by exp_manager
-+    # logger: false #false  # Provided by exp_manager
-+    # log_every_n_steps: 1  # Interval of logging.
-+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    # reload_dataloaders_every_epoch: true
-+    # resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /home/nxingyu2/data # /root/data # 
--tmp_path: /home/nxingyu2/data/tmp # /tmp # 
-+base_path: /home/nxingyu/data # /root/data # 
-+tmp_path: /home/nxingyu/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
--    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-     unfrozen: 0
--    maximum_unfrozen: 2
-+    maximum_unfrozen: 1
-     unfreeze_step: 1
-     # unfreeze_every: 3
-     punct_label_ids:
-@@ -58,10 +58,10 @@ model:
-         - "—"
-         - "…"
- 
--    punct_class_weights: true
-+    punct_class_weights: false
-     
-     dataset:
--        data_dir: /home/nxingyu2/data # /root/data # 
-+        data_dir: /home/nxingyu/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -106,6 +106,12 @@ model:
-         config: null
-         # unfrozen_layers: 1
- 
-+    mlp:
-+        num_fc_layers: 2
-+        fc_dropout: 0.1
-+        log_softmax: false
-+        activation: 'relu'
-+        
-     punct_head:
-         punct_num_fc_layers: 1
-         fc_dropout: 0.1
-@@ -122,17 +128,19 @@ model:
-         use_transformer_init: true
-         loss: 'cel'
-         gamma: 0.1 # coefficient of gradient reversal
-+        pooling: 'mean'
-+        idx_conditioned_on: 0
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 3
-+        alpha: 4
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: novograd
-+        name: adamw
-         lr: 1e-3
-         weight_decay: 0.00
-         sched:
-diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
-index d4ff927..cafc57d 100644
---- a/experiment/core/layers/sequence_classifier.py
-+++ b/experiment/core/layers/sequence_classifier.py
-@@ -1,3 +1,4 @@
-+import torch
- from torch import nn
- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
- from core.utils import transformer_weights_init
-@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
-         log_softmax: bool = True,
-         dropout: float = 0.0,
-         use_transformer_init: bool = True,
--        idx_conditioned_on: int = 0,
-+        pooling: str = 'mean', # mean, max, mean_max, token
-+        idx_conditioned_on: int = None,
-     ):
-         """
-         Initializes the SequenceClassifier module.
-@@ -37,6 +39,7 @@ class SequenceClassifier(nn.Module):
-         super().__init__()
-         self.log_softmax = log_softmax
-         self._idx_conditioned_on = idx_conditioned_on
-+        self.pooling = pooling
-         self.mlp = MultiLayerPerceptron(
-             hidden_size=hidden_size,
-             num_classes=num_classes,
-@@ -48,7 +51,24 @@ class SequenceClassifier(nn.Module):
-         if use_transformer_init:
-             self.apply(lambda module: transformer_weights_init(module, xavier=False))
- 
--    def forward(self, hidden_states):
-+    def forward(self, hidden_states, subtoken_mask=None):
-         hidden_states = self.dropout(hidden_states)
--        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
-+        pp(self.pooling)
-+        if self.pooling=='token':
-+            pooled = hidden_states[:, self._idx_conditioned_on]
-+        else:
-+            if subtoken_mask==None:
-+                ct=hidden_states.shape[1] # Seq len
-+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
-+            else:
-+                ct = torch.sum(subtoken_mask,axis=1)
-+            pooled_sum = torch.sum(hidden_states,axis=1)
-+            if self.pooling=='mean' or self.pooling == 'mean_max':
-+                pooled_mean = pooled_sum//ct
-+            if self.pooling=='max' or self.pooling=='mean_max':
-+                pooled_max = torch.max(hidden_states,axis=1)[0]
-+            pooled=pooled_mean if self.pooling=='mean' else \
-+                pooled_max if self.pooling=='max' else \
-+                    torch.cat([pooled_mean,pooled_max],axis=-1)
-+        logits = self.mlp(pooled)
-         return logits
-diff --git a/experiment/info.log b/experiment/info.log
-index 2471fe9..e69de29 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,300 +0,0 @@
--[INFO] - GPU available: True, used: False
--[INFO] - TPU available: None, using: 0 TPU cores
--[INFO] - shuffling train set
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        184
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.23       0.34       0.53        594
--- (label_id: 3)                                          3.06      25.42       5.46         59
--. (label_id: 4)                                         47.22      12.98      20.36        524
--: (label_id: 5)                                          0.00       0.00       0.00         18
--; (label_id: 6)                                          0.00       0.00       0.00         13
--? (label_id: 7)                                          8.45       6.32       7.23         95
--— (label_id: 8)                                          0.00       0.00       0.00         12
--… (label_id: 9)                                          0.00       0.00       0.00          0
---------------------
--micro avg                                                6.05       6.05       6.05       1503
--macro avg                                                6.66       5.01       3.73       1503
--weighted avg                                            17.61       6.05       7.98       1503
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00         92
---------------------
--micro avg                                              100.00     100.00     100.00         92
--macro avg                                              100.00     100.00     100.00         92
--weighted avg                                           100.00     100.00     100.00         92
--
--[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 1.5848931924611143e-08
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
--will be used during training (effective maximum steps = 3192) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 3192
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        202
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.62       0.45       0.70        669
--- (label_id: 3)                                          3.48      27.27       6.17         66
--. (label_id: 4)                                         45.06      13.01      20.19        561
--: (label_id: 5)                                          1.52       6.67       2.47         15
--; (label_id: 6)                                          0.00       0.00       0.00         15
--? (label_id: 7)                                          8.70       7.32       7.95         82
--— (label_id: 8)                                          0.00       0.00       0.00         13
--… (label_id: 9)                                          0.00       0.00       0.00          1
---------------------
--micro avg                                                6.20       6.20       6.20       1628
--macro avg                                                6.04       5.47       3.75       1628
--weighted avg                                            16.79       6.20       7.92       1628
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        101
---------------------
--micro avg                                              100.00     100.00     100.00        101
--macro avg                                              100.00     100.00     100.00        101
--weighted avg                                           100.00     100.00     100.00        101
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.34       0.73       0.46       4402
--! (label_id: 1)                                          0.42      13.95       0.82        129
--, (label_id: 2)                                          2.53       0.64       1.03      15243
--- (label_id: 3)                                          2.45      21.03       4.38       1322
--. (label_id: 4)                                         44.00      11.40      18.11      12542
--: (label_id: 5)                                          0.43       1.41       0.65        354
--; (label_id: 6)                                          0.00       0.00       0.00        163
--? (label_id: 7)                                          4.16       6.27       5.00       1117
--— (label_id: 8)                                          3.00       0.61       1.02        488
--… (label_id: 9)                                          0.97       6.17       1.68         81
---------------------
--micro avg                                                5.41       5.41       5.41      35841
--macro avg                                                5.83       6.22       3.32      35841
--weighted avg                                            16.78       5.41       7.18      35841
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2201
---------------------
--micro avg                                              100.00     100.00     100.00       2201
--macro avg                                              100.00     100.00     100.00       2201
--weighted avg                                           100.00     100.00     100.00       2201
--
--[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.20       0.43       0.27       4226
--! (label_id: 1)                                          0.44      14.17       0.86        127
--, (label_id: 2)                                          1.93       0.49       0.78      14611
--- (label_id: 3)                                          2.23      19.56       4.01       1237
--. (label_id: 4)                                         43.37      11.25      17.86      11977
--: (label_id: 5)                                          0.68       2.34       1.05        342
--; (label_id: 6)                                          0.00       0.00       0.00        129
--? (label_id: 7)                                          5.16       7.47       6.10       1058
--— (label_id: 8)                                          2.15       0.49       0.80        409
--… (label_id: 9)                                          0.69       4.23       1.19         71
---------------------
--micro avg                                                5.23       5.23       5.23      34187
--macro avg                                                5.68       6.04       3.29      34187
--weighted avg                                            16.32       5.23       6.98      34187
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2113
---------------------
--micro avg                                              100.00     100.00     100.00       2113
--macro avg                                              100.00     100.00     100.00       2113
--weighted avg                                           100.00     100.00     100.00       2113
--
--[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.61       0.39       4228
--! (label_id: 1)                                          0.30       8.28       0.58        145
--, (label_id: 2)                                          2.27       0.58       0.92      14495
--- (label_id: 3)                                          2.64      21.78       4.70       1327
--. (label_id: 4)                                         44.87      11.66      18.51      12193
--: (label_id: 5)                                          0.60       1.93       0.91        362
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.19       6.40       5.07       1078
--— (label_id: 8)                                          1.16       0.22       0.37        459
--… (label_id: 9)                                          0.85       4.17       1.41         96
---------------------
--micro avg                                                5.54       5.54       5.54      34547
--macro avg                                                5.72       5.56       3.29      34547
--weighted avg                                            17.08       5.54       7.33      34547
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2114
---------------------
--micro avg                                              100.00     100.00     100.00       2114
--macro avg                                              100.00     100.00     100.00       2114
--weighted avg                                           100.00     100.00     100.00       2114
--
--[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.29       0.63       0.40       4444
--! (label_id: 1)                                          0.38      10.67       0.74        150
--, (label_id: 2)                                          2.32       0.59       0.94      15290
--- (label_id: 3)                                          2.34      20.28       4.19       1292
--. (label_id: 4)                                         43.85      11.68      18.44      12599
--: (label_id: 5)                                          0.41       1.28       0.62        392
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.24       6.30       5.07       1111
--— (label_id: 8)                                          0.00       0.00       0.00        456
--… (label_id: 9)                                          0.38       2.41       0.65         83
---------------------
--micro avg                                                5.40       5.40       5.40      35981
--macro avg                                                5.42       5.38       3.11      35981
--weighted avg                                            16.59       5.40       7.22      35981
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2222
---------------------
--micro avg                                              100.00     100.00     100.00       2222
--macro avg                                              100.00     100.00     100.00       2222
--weighted avg                                           100.00     100.00     100.00       2222
--
--[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.35       0.73       0.48       3844
--! (label_id: 1)                                          0.54      14.62       1.04        130
--, (label_id: 2)                                          2.32       0.59       0.94      13056
--- (label_id: 3)                                          2.67      22.28       4.77       1194
--. (label_id: 4)                                         44.45      11.95      18.84      10791
--: (label_id: 5)                                          0.84       3.21       1.33        280
--; (label_id: 6)                                          0.00       0.00       0.00        140
--? (label_id: 7)                                          4.17       6.56       5.10        914
--— (label_id: 8)                                          0.00       0.00       0.00        401
--… (label_id: 9)                                          0.48       2.63       0.81         76
---------------------
--micro avg                                                5.68       5.68       5.68      30826
--macro avg                                                5.58       6.26       3.33      30826
--weighted avg                                            16.82       5.68       7.41      30826
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1922
---------------------
--micro avg                                              100.00     100.00     100.00       1922
--macro avg                                              100.00     100.00     100.00       1922
--weighted avg                                           100.00     100.00     100.00       1922
--
--[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.60       0.39       3970
--! (label_id: 1)                                          0.35      10.66       0.68        122
--, (label_id: 2)                                          2.09       0.53       0.85      13469
--- (label_id: 3)                                          2.29      19.32       4.10       1201
--. (label_id: 4)                                         43.43      11.24      17.86      11227
--: (label_id: 5)                                          0.63       2.30       0.99        304
--; (label_id: 6)                                          0.00       0.00       0.00        141
--? (label_id: 7)                                          4.52       6.86       5.45       1006
--— (label_id: 8)                                          1.15       0.23       0.38        444
--… (label_id: 9)                                          0.45       2.67       0.78         75
---------------------
--micro avg                                                5.26       5.26       5.26      31959
--macro avg                                                5.52       5.44       3.15      31959
--weighted avg                                            16.42       5.26       7.02      31959
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1985
---------------------
--micro avg                                              100.00     100.00     100.00       1985
--macro avg                                              100.00     100.00     100.00       1985
--weighted avg                                           100.00     100.00     100.00       1985
--
--[INFO] - Epoch 5, step 2394: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.23       0.48       0.31       4126
--! (label_id: 1)                                          0.29       9.40       0.56        117
--, (label_id: 2)                                          1.91       0.49       0.77      14019
--- (label_id: 3)                                          2.52      22.59       4.53       1164
--. (label_id: 4)                                         44.15      11.65      18.44      11789
--: (label_id: 5)                                          0.72       2.41       1.11        332
--; (label_id: 6)                                          0.56       0.61       0.58        165
--? (label_id: 7)                                          3.89       6.53       4.88        980
--— (label_id: 8)                                          2.30       0.47       0.77        430
--… (label_id: 9)                                          1.18       8.33       2.07         60
---------------------
--micro avg                                                5.47       5.47       5.47      33182
--macro avg                                                5.77       6.30       3.40      33182
--weighted avg                                            16.77       5.47       7.25      33182
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2063
---------------------
--micro avg                                              100.00     100.00     100.00       2063
--macro avg                                              100.00     100.00     100.00       2063
--weighted avg                                           100.00     100.00     100.00       2063
--
--[INFO] - Epoch 6, step 2793: val_loss was not in top 3
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index fa37b4c..43fc93d 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             self.hparams.model.punct_class_weights=None
- 
-+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
-+        self.mlp = MultiLayerPerceptron(
-+            self.transformer.config.hidden_size,
-+            self.transformer.config.hidden_size,
-+            num_layers=self.hparams.model.mlp.num_fc_layers, 
-+            activation=self.hparams.model.mlp.activation, 
-+            log_softmax=self.hparams.model.mlp.log_softmax
-+        )
-+
-         self.punct_classifier = TokenClassifier(
-             hidden_size=self.transformer.config.hidden_size,
-             num_classes=len(self.labels_to_ids),
-@@ -80,6 +89,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-             log_softmax=self.hparams.model.domain_head.log_softmax,
-             dropout=self.hparams.model.domain_head.fc_dropout,
-             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
-+            pooling=self.hparams.model.domain_head.pooling,
-+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
-         )
- 
-         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
-@@ -119,14 +130,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
-         self.freeze()
- 
--    def forward(self, input_ids, attention_mask, domain_ids=None):
-+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
-         hidden_states = self.transformer(
-             input_ids=input_ids, attention_mask=attention_mask
-         )[0]
-+        hidden_states = self.dropout(hidden_states)
-+        hidden_states = self.mlp(hidden_states)
-         punct_logits = self.punct_classifier(hidden_states=hidden_states)
-         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
-         domain_logits = self.domain_classifier(
--            hidden_states=reverse_grad_hidden_states)
-+            hidden_states=reverse_grad_hidden_states,
-+            subtoken_mask=subtoken_mask)
-         return punct_logits, domain_logits
- 
-     def _make_step(self, batch):
-@@ -136,7 +150,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         punct_labels = batch['labels']
-         domain_labels = batch['domain']
-         punct_logits, domain_logits = self(
--            input_ids=input_ids, attention_mask=attention_mask
-+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
-         )
-         punctuation_loss = self.punctuation_loss(
-             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/lightning_logs.txt
deleted file mode 100644
index 9fa336d..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/lightning_logs.txt
+++ /dev/null
@@ -1,24 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-Using native 16bit precision.
-Global seed set to 42
-initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | dropout             | Dropout              | 0     
-2 | mlp                 | MultiLayerPerceptron | 131 K 
-3 | punct_classifier    | TokenClassifier      | 2.6 K 
-4 | domain_classifier   | SequenceClassifier   | 257   
-5 | punctuation_loss    | FocalDiceLoss        | 0     
-6 | domain_loss         | CrossEntropyLoss     | 0     
-7 | agg_loss            | AggregatorLoss       | 0     
-8 | punct_class_report  | ClassificationReport | 0     
-9 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-167 K     Trainable params
-13.4 M    Non-trainable params
-13.6 M    Total params
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/nemo_error_log.txt
deleted file mode 100644
index 1ad0ccb..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/nemo_error_log.txt
+++ /dev/null
@@ -1,7 +0,0 @@
-[NeMo W 2021-02-08 10:59:31 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 10:59:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 10:59:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 3b439bb..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,9 +0,0 @@
-[NeMo I 2021-02-08 10:59:31 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31
-[NeMo I 2021-02-08 10:59:31 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-08 10:59:31 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 10:59:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 10:59:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/git-info.log
deleted file mode 100644
index 4bb1d67..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/git-info.log
+++ /dev/null
@@ -1,625 +0,0 @@
-commit hash: 939a671c8c117db6975316767ced5d95449e2b27
-diff --git a/README.md b/README.md
-index d52dba5..0b403d3 100644
---- a/README.md
-+++ b/README.md
-@@ -333,4 +333,71 @@ label                                                precision    recall       f
-  'punct_f1': 0.5858508944511414,
-  'punct_precision': 0.30176490545272827,
-  'punct_recall': 10.0,
-- 'test_loss': 0.8140875697135925}
-\ No newline at end of file
-+ 'test_loss': 0.8140875697135925}
-+
-+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
-+
-+ (label_id: 0)                                          62.15     100.00      76.66       5154
-+! (label_id: 1)                                          0.00       0.00       0.00        108
-+, (label_id: 2)                                          0.00       0.00       0.00      18022
-+- (label_id: 3)                                          0.00       0.00       0.00       1557
-+. (label_id: 4)                                         41.74      94.01      57.81      15164
-+: (label_id: 5)                                          0.00       0.00       0.00        319
-+; (label_id: 6)                                          0.00       0.00       0.00         88
-+? (label_id: 7)                                          0.00       0.00       0.00       1217
-+ (label_id: 8)                                          0.00       0.00       0.00        752
-+… (label_id: 9)                                          0.00       0.00       0.00         67
-+-------------------
-+micro avg                                               45.72      45.72      45.72      42448
-+macro avg                                               10.39      19.40      13.45      42448
-+weighted avg                                            22.46      45.72      29.96      42448
-+{ 'punct_f1': 13.446383476257324,
-+ 'punct_precision': 10.388500213623047,
-+ 'punct_recall': 19.400554656982422,
-+ 'test_loss': 0.44148480892181396}
-+
-+
-+ ## Log for 8/2/2021
-+
-+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
-+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
-+
-+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
-+End frozen 
-+micro avg                                               41.42      41.42      41.42      33406
-+macro avg                                               11.01      13.54      11.07      33406
-+weighted avg                                            34.88      41.42      34.20      33406
-+
-+1st layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               36.65      36.65      36.65      33463
-+macro avg                                               10.71       9.91       8.26      33463
-+weighted avg                                            34.32      36.65      31.08      33463
-+
-+2nd layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               35.72      35.72      35.72      42448
-+macro avg                                                3.57      10.00       5.26      42448
-+weighted avg                                            12.76      35.72      18.81      42448
-+
-+{'punct_f1': 5.264181137084961,
-+ 'punct_precision': 3.572371006011963,
-+ 'punct_recall': 10.0,
-+ 'test_loss': 18.49854850769043}
-+
-+
-+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
-+alpha from 3->4 seems to reduce convergence rate.
-+
-+micro avg                                               50.98      50.98      50.98      33463
-+macro avg                                               25.99      25.38      23.38      33463
-+weighted avg                                            50.31      50.98      48.27      33463
-+
-+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
-+micro avg                                               58.55      58.55      58.55      39340
-+macro avg                                               30.02      29.74      29.52      39340
-+weighted avg                                            57.91      58.55      57.51      39340
-+
-+still increasing?!
-+{'punct_f1': 29.523975372314453,
-+ 'punct_precision': 30.015613555908203,
-+ 'punct_recall': 29.738296508789062,
-+ 'test_loss': 0.3690211772918701}
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index fe58670..499ec58 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,49 +1,49 @@
- seed: 42
- trainer:
--    # gpus: 1 # the number of gpus, 0 for CPU
--    # num_nodes: 1
--    # max_epochs: 2
--    # max_steps: null # precedence over max_epochs
--    # accumulate_grad_batches: 4 # accumulates grads every k batches
--    # gradient_clip_val: 0
--    # amp_level: O1 # O1/O2 for mixed precision
--    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
--    # checkpoint_callback: false  # Provided by exp_manager
--    # logger: false #false  # Provided by exp_manager
--    # log_every_n_steps: 1  # Interval of logging.
--    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # resume_from_checkpoint: null
--
--    gpus: 0 # the number of gpus, 0 for CPU
-+    gpus: 1 # the number of gpus, 0 for CPU
-     num_nodes: 1
--    max_epochs: 8
-+    max_epochs: 10
-     max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 1 # accumulates grads every k batches
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-     gradient_clip_val: 0
--    amp_level: O0 # O1/O2 for mixed precision
--    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
-+    amp_level: O1 # O1/O2 for mixed precision
-+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    accelerator: ddp
-     checkpoint_callback: false  # Provided by exp_manager
-     logger: false #false  # Provided by exp_manager
-     log_every_n_steps: 1  # Interval of logging.
-     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    reload_dataloaders_every_epoch: true
-     resume_from_checkpoint: null
- 
-+    # gpus: 0 # the number of gpus, 0 for CPU
-+    # num_nodes: 1
-+    # max_epochs: 8
-+    # max_steps: null # precedence over max_epochs
-+    # accumulate_grad_batches: 1 # accumulates grads every k batches
-+    # gradient_clip_val: 0
-+    # amp_level: O0 # O1/O2 for mixed precision
-+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # # accelerator: ddp
-+    # checkpoint_callback: false  # Provided by exp_manager
-+    # logger: false #false  # Provided by exp_manager
-+    # log_every_n_steps: 1  # Interval of logging.
-+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    # reload_dataloaders_every_epoch: true
-+    # resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /home/nxingyu2/data # /root/data # 
--tmp_path: /home/nxingyu2/data/tmp # /tmp # 
-+base_path: /home/nxingyu/data # /root/data # 
-+tmp_path: /home/nxingyu/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
--    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-     unfrozen: 0
--    maximum_unfrozen: 2
-+    maximum_unfrozen: 1
-     unfreeze_step: 1
-     # unfreeze_every: 3
-     punct_label_ids:
-@@ -58,10 +58,10 @@ model:
-         - "—"
-         - "…"
- 
--    punct_class_weights: true
-+    punct_class_weights: false
-     
-     dataset:
--        data_dir: /home/nxingyu2/data # /root/data # 
-+        data_dir: /home/nxingyu/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -106,6 +106,12 @@ model:
-         config: null
-         # unfrozen_layers: 1
- 
-+    mlp:
-+        num_fc_layers: 2
-+        fc_dropout: 0.1
-+        log_softmax: false
-+        activation: 'relu'
-+        
-     punct_head:
-         punct_num_fc_layers: 1
-         fc_dropout: 0.1
-@@ -122,17 +128,19 @@ model:
-         use_transformer_init: true
-         loss: 'cel'
-         gamma: 0.1 # coefficient of gradient reversal
-+        pooling: 'max'
-+        idx_conditioned_on: 0
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 3
-+        alpha: 4
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: novograd
-+        name: adamw
-         lr: 1e-3
-         weight_decay: 0.00
-         sched:
-diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
-index d4ff927..cafc57d 100644
---- a/experiment/core/layers/sequence_classifier.py
-+++ b/experiment/core/layers/sequence_classifier.py
-@@ -1,3 +1,4 @@
-+import torch
- from torch import nn
- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
- from core.utils import transformer_weights_init
-@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
-         log_softmax: bool = True,
-         dropout: float = 0.0,
-         use_transformer_init: bool = True,
--        idx_conditioned_on: int = 0,
-+        pooling: str = 'mean', # mean, max, mean_max, token
-+        idx_conditioned_on: int = None,
-     ):
-         """
-         Initializes the SequenceClassifier module.
-@@ -37,6 +39,7 @@ class SequenceClassifier(nn.Module):
-         super().__init__()
-         self.log_softmax = log_softmax
-         self._idx_conditioned_on = idx_conditioned_on
-+        self.pooling = pooling
-         self.mlp = MultiLayerPerceptron(
-             hidden_size=hidden_size,
-             num_classes=num_classes,
-@@ -48,7 +51,24 @@ class SequenceClassifier(nn.Module):
-         if use_transformer_init:
-             self.apply(lambda module: transformer_weights_init(module, xavier=False))
- 
--    def forward(self, hidden_states):
-+    def forward(self, hidden_states, subtoken_mask=None):
-         hidden_states = self.dropout(hidden_states)
--        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
-+        pp(self.pooling)
-+        if self.pooling=='token':
-+            pooled = hidden_states[:, self._idx_conditioned_on]
-+        else:
-+            if subtoken_mask==None:
-+                ct=hidden_states.shape[1] # Seq len
-+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
-+            else:
-+                ct = torch.sum(subtoken_mask,axis=1)
-+            pooled_sum = torch.sum(hidden_states,axis=1)
-+            if self.pooling=='mean' or self.pooling == 'mean_max':
-+                pooled_mean = pooled_sum//ct
-+            if self.pooling=='max' or self.pooling=='mean_max':
-+                pooled_max = torch.max(hidden_states,axis=1)[0]
-+            pooled=pooled_mean if self.pooling=='mean' else \
-+                pooled_max if self.pooling=='max' else \
-+                    torch.cat([pooled_mean,pooled_max],axis=-1)
-+        logits = self.mlp(pooled)
-         return logits
-diff --git a/experiment/info.log b/experiment/info.log
-index 2471fe9..e69de29 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,300 +0,0 @@
--[INFO] - GPU available: True, used: False
--[INFO] - TPU available: None, using: 0 TPU cores
--[INFO] - shuffling train set
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        184
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.23       0.34       0.53        594
--- (label_id: 3)                                          3.06      25.42       5.46         59
--. (label_id: 4)                                         47.22      12.98      20.36        524
--: (label_id: 5)                                          0.00       0.00       0.00         18
--; (label_id: 6)                                          0.00       0.00       0.00         13
--? (label_id: 7)                                          8.45       6.32       7.23         95
--— (label_id: 8)                                          0.00       0.00       0.00         12
--… (label_id: 9)                                          0.00       0.00       0.00          0
---------------------
--micro avg                                                6.05       6.05       6.05       1503
--macro avg                                                6.66       5.01       3.73       1503
--weighted avg                                            17.61       6.05       7.98       1503
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00         92
---------------------
--micro avg                                              100.00     100.00     100.00         92
--macro avg                                              100.00     100.00     100.00         92
--weighted avg                                           100.00     100.00     100.00         92
--
--[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 1.5848931924611143e-08
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
--will be used during training (effective maximum steps = 3192) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 3192
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        202
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.62       0.45       0.70        669
--- (label_id: 3)                                          3.48      27.27       6.17         66
--. (label_id: 4)                                         45.06      13.01      20.19        561
--: (label_id: 5)                                          1.52       6.67       2.47         15
--; (label_id: 6)                                          0.00       0.00       0.00         15
--? (label_id: 7)                                          8.70       7.32       7.95         82
--— (label_id: 8)                                          0.00       0.00       0.00         13
--… (label_id: 9)                                          0.00       0.00       0.00          1
---------------------
--micro avg                                                6.20       6.20       6.20       1628
--macro avg                                                6.04       5.47       3.75       1628
--weighted avg                                            16.79       6.20       7.92       1628
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        101
---------------------
--micro avg                                              100.00     100.00     100.00        101
--macro avg                                              100.00     100.00     100.00        101
--weighted avg                                           100.00     100.00     100.00        101
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.34       0.73       0.46       4402
--! (label_id: 1)                                          0.42      13.95       0.82        129
--, (label_id: 2)                                          2.53       0.64       1.03      15243
--- (label_id: 3)                                          2.45      21.03       4.38       1322
--. (label_id: 4)                                         44.00      11.40      18.11      12542
--: (label_id: 5)                                          0.43       1.41       0.65        354
--; (label_id: 6)                                          0.00       0.00       0.00        163
--? (label_id: 7)                                          4.16       6.27       5.00       1117
--— (label_id: 8)                                          3.00       0.61       1.02        488
--… (label_id: 9)                                          0.97       6.17       1.68         81
---------------------
--micro avg                                                5.41       5.41       5.41      35841
--macro avg                                                5.83       6.22       3.32      35841
--weighted avg                                            16.78       5.41       7.18      35841
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2201
---------------------
--micro avg                                              100.00     100.00     100.00       2201
--macro avg                                              100.00     100.00     100.00       2201
--weighted avg                                           100.00     100.00     100.00       2201
--
--[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.20       0.43       0.27       4226
--! (label_id: 1)                                          0.44      14.17       0.86        127
--, (label_id: 2)                                          1.93       0.49       0.78      14611
--- (label_id: 3)                                          2.23      19.56       4.01       1237
--. (label_id: 4)                                         43.37      11.25      17.86      11977
--: (label_id: 5)                                          0.68       2.34       1.05        342
--; (label_id: 6)                                          0.00       0.00       0.00        129
--? (label_id: 7)                                          5.16       7.47       6.10       1058
--— (label_id: 8)                                          2.15       0.49       0.80        409
--… (label_id: 9)                                          0.69       4.23       1.19         71
---------------------
--micro avg                                                5.23       5.23       5.23      34187
--macro avg                                                5.68       6.04       3.29      34187
--weighted avg                                            16.32       5.23       6.98      34187
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2113
---------------------
--micro avg                                              100.00     100.00     100.00       2113
--macro avg                                              100.00     100.00     100.00       2113
--weighted avg                                           100.00     100.00     100.00       2113
--
--[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.61       0.39       4228
--! (label_id: 1)                                          0.30       8.28       0.58        145
--, (label_id: 2)                                          2.27       0.58       0.92      14495
--- (label_id: 3)                                          2.64      21.78       4.70       1327
--. (label_id: 4)                                         44.87      11.66      18.51      12193
--: (label_id: 5)                                          0.60       1.93       0.91        362
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.19       6.40       5.07       1078
--— (label_id: 8)                                          1.16       0.22       0.37        459
--… (label_id: 9)                                          0.85       4.17       1.41         96
---------------------
--micro avg                                                5.54       5.54       5.54      34547
--macro avg                                                5.72       5.56       3.29      34547
--weighted avg                                            17.08       5.54       7.33      34547
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2114
---------------------
--micro avg                                              100.00     100.00     100.00       2114
--macro avg                                              100.00     100.00     100.00       2114
--weighted avg                                           100.00     100.00     100.00       2114
--
--[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.29       0.63       0.40       4444
--! (label_id: 1)                                          0.38      10.67       0.74        150
--, (label_id: 2)                                          2.32       0.59       0.94      15290
--- (label_id: 3)                                          2.34      20.28       4.19       1292
--. (label_id: 4)                                         43.85      11.68      18.44      12599
--: (label_id: 5)                                          0.41       1.28       0.62        392
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.24       6.30       5.07       1111
--— (label_id: 8)                                          0.00       0.00       0.00        456
--… (label_id: 9)                                          0.38       2.41       0.65         83
---------------------
--micro avg                                                5.40       5.40       5.40      35981
--macro avg                                                5.42       5.38       3.11      35981
--weighted avg                                            16.59       5.40       7.22      35981
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2222
---------------------
--micro avg                                              100.00     100.00     100.00       2222
--macro avg                                              100.00     100.00     100.00       2222
--weighted avg                                           100.00     100.00     100.00       2222
--
--[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.35       0.73       0.48       3844
--! (label_id: 1)                                          0.54      14.62       1.04        130
--, (label_id: 2)                                          2.32       0.59       0.94      13056
--- (label_id: 3)                                          2.67      22.28       4.77       1194
--. (label_id: 4)                                         44.45      11.95      18.84      10791
--: (label_id: 5)                                          0.84       3.21       1.33        280
--; (label_id: 6)                                          0.00       0.00       0.00        140
--? (label_id: 7)                                          4.17       6.56       5.10        914
--— (label_id: 8)                                          0.00       0.00       0.00        401
--… (label_id: 9)                                          0.48       2.63       0.81         76
---------------------
--micro avg                                                5.68       5.68       5.68      30826
--macro avg                                                5.58       6.26       3.33      30826
--weighted avg                                            16.82       5.68       7.41      30826
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1922
---------------------
--micro avg                                              100.00     100.00     100.00       1922
--macro avg                                              100.00     100.00     100.00       1922
--weighted avg                                           100.00     100.00     100.00       1922
--
--[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.60       0.39       3970
--! (label_id: 1)                                          0.35      10.66       0.68        122
--, (label_id: 2)                                          2.09       0.53       0.85      13469
--- (label_id: 3)                                          2.29      19.32       4.10       1201
--. (label_id: 4)                                         43.43      11.24      17.86      11227
--: (label_id: 5)                                          0.63       2.30       0.99        304
--; (label_id: 6)                                          0.00       0.00       0.00        141
--? (label_id: 7)                                          4.52       6.86       5.45       1006
--— (label_id: 8)                                          1.15       0.23       0.38        444
--… (label_id: 9)                                          0.45       2.67       0.78         75
---------------------
--micro avg                                                5.26       5.26       5.26      31959
--macro avg                                                5.52       5.44       3.15      31959
--weighted avg                                            16.42       5.26       7.02      31959
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1985
---------------------
--micro avg                                              100.00     100.00     100.00       1985
--macro avg                                              100.00     100.00     100.00       1985
--weighted avg                                           100.00     100.00     100.00       1985
--
--[INFO] - Epoch 5, step 2394: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.23       0.48       0.31       4126
--! (label_id: 1)                                          0.29       9.40       0.56        117
--, (label_id: 2)                                          1.91       0.49       0.77      14019
--- (label_id: 3)                                          2.52      22.59       4.53       1164
--. (label_id: 4)                                         44.15      11.65      18.44      11789
--: (label_id: 5)                                          0.72       2.41       1.11        332
--; (label_id: 6)                                          0.56       0.61       0.58        165
--? (label_id: 7)                                          3.89       6.53       4.88        980
--— (label_id: 8)                                          2.30       0.47       0.77        430
--… (label_id: 9)                                          1.18       8.33       2.07         60
---------------------
--micro avg                                                5.47       5.47       5.47      33182
--macro avg                                                5.77       6.30       3.40      33182
--weighted avg                                            16.77       5.47       7.25      33182
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2063
---------------------
--micro avg                                              100.00     100.00     100.00       2063
--macro avg                                              100.00     100.00     100.00       2063
--weighted avg                                           100.00     100.00     100.00       2063
--
--[INFO] - Epoch 6, step 2793: val_loss was not in top 3
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index fa37b4c..43fc93d 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             self.hparams.model.punct_class_weights=None
- 
-+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
-+        self.mlp = MultiLayerPerceptron(
-+            self.transformer.config.hidden_size,
-+            self.transformer.config.hidden_size,
-+            num_layers=self.hparams.model.mlp.num_fc_layers, 
-+            activation=self.hparams.model.mlp.activation, 
-+            log_softmax=self.hparams.model.mlp.log_softmax
-+        )
-+
-         self.punct_classifier = TokenClassifier(
-             hidden_size=self.transformer.config.hidden_size,
-             num_classes=len(self.labels_to_ids),
-@@ -80,6 +89,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-             log_softmax=self.hparams.model.domain_head.log_softmax,
-             dropout=self.hparams.model.domain_head.fc_dropout,
-             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
-+            pooling=self.hparams.model.domain_head.pooling,
-+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
-         )
- 
-         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
-@@ -119,14 +130,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
-         self.freeze()
- 
--    def forward(self, input_ids, attention_mask, domain_ids=None):
-+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
-         hidden_states = self.transformer(
-             input_ids=input_ids, attention_mask=attention_mask
-         )[0]
-+        hidden_states = self.dropout(hidden_states)
-+        hidden_states = self.mlp(hidden_states)
-         punct_logits = self.punct_classifier(hidden_states=hidden_states)
-         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
-         domain_logits = self.domain_classifier(
--            hidden_states=reverse_grad_hidden_states)
-+            hidden_states=reverse_grad_hidden_states,
-+            subtoken_mask=subtoken_mask)
-         return punct_logits, domain_logits
- 
-     def _make_step(self, batch):
-@@ -136,7 +150,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         punct_labels = batch['labels']
-         domain_labels = batch['domain']
-         punct_logits, domain_logits = self(
--            input_ids=input_ids, attention_mask=attention_mask
-+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
-         )
-         punctuation_loss = self.punctuation_loss(
-             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/lightning_logs.txt
deleted file mode 100644
index fe58044..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/lightning_logs.txt
+++ /dev/null
@@ -1,35 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-Using native 16bit precision.
-Global seed set to 42
-initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | dropout             | Dropout              | 0     
-2 | mlp                 | MultiLayerPerceptron | 131 K 
-3 | punct_classifier    | TokenClassifier      | 2.6 K 
-4 | domain_classifier   | SequenceClassifier   | 257   
-5 | punctuation_loss    | FocalDiceLoss        | 0     
-6 | domain_loss         | CrossEntropyLoss     | 0     
-7 | agg_loss            | AggregatorLoss       | 0     
-8 | punct_class_report  | ClassificationReport | 0     
-9 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-167 K     Trainable params
-13.4 M    Non-trainable params
-13.6 M    Total params
-LR finder stopped early due to diverging loss.
-Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/lr_find_temp_model.ckpt
-Failed to compute suggesting for `lr`. There might not be enough points.
-Traceback (most recent call last):
-  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 355, in suggestion
-    min_grad = np.gradient(loss).argmin()
-  File "<__array_function__ internals>", line 5, in gradient
-  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
-    raise ValueError(
-ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
-Global seed set to 42
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/nemo_error_log.txt
deleted file mode 100644
index 620260e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/nemo_error_log.txt
+++ /dev/null
@@ -1,21 +0,0 @@
-[NeMo W 2021-02-08 10:59:58 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 11:00:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:00:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:00:09 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:00:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-      warnings.warn(*args, **kwargs)
-    
-Failed to compute suggesting for `lr`. There might not be enough points.
-Traceback (most recent call last):
-  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 355, in suggestion
-    min_grad = np.gradient(loss).argmin()
-  File "<__array_function__ internals>", line 5, in gradient
-  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
-    raise ValueError(
-ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index bb424d8..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,15 +0,0 @@
-[NeMo I 2021-02-08 10:59:58 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58
-[NeMo I 2021-02-08 10:59:58 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-08 10:59:58 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 11:00:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:00:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:00:09 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:00:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/git-info.log
deleted file mode 100644
index a12885f..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/git-info.log
+++ /dev/null
@@ -1,625 +0,0 @@
-commit hash: 939a671c8c117db6975316767ced5d95449e2b27
-diff --git a/README.md b/README.md
-index d52dba5..0b403d3 100644
---- a/README.md
-+++ b/README.md
-@@ -333,4 +333,71 @@ label                                                precision    recall       f
-  'punct_f1': 0.5858508944511414,
-  'punct_precision': 0.30176490545272827,
-  'punct_recall': 10.0,
-- 'test_loss': 0.8140875697135925}
-\ No newline at end of file
-+ 'test_loss': 0.8140875697135925}
-+
-+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
-+
-+ (label_id: 0)                                          62.15     100.00      76.66       5154
-+! (label_id: 1)                                          0.00       0.00       0.00        108
-+, (label_id: 2)                                          0.00       0.00       0.00      18022
-+- (label_id: 3)                                          0.00       0.00       0.00       1557
-+. (label_id: 4)                                         41.74      94.01      57.81      15164
-+: (label_id: 5)                                          0.00       0.00       0.00        319
-+; (label_id: 6)                                          0.00       0.00       0.00         88
-+? (label_id: 7)                                          0.00       0.00       0.00       1217
-+ (label_id: 8)                                          0.00       0.00       0.00        752
-+… (label_id: 9)                                          0.00       0.00       0.00         67
-+-------------------
-+micro avg                                               45.72      45.72      45.72      42448
-+macro avg                                               10.39      19.40      13.45      42448
-+weighted avg                                            22.46      45.72      29.96      42448
-+{ 'punct_f1': 13.446383476257324,
-+ 'punct_precision': 10.388500213623047,
-+ 'punct_recall': 19.400554656982422,
-+ 'test_loss': 0.44148480892181396}
-+
-+
-+ ## Log for 8/2/2021
-+
-+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
-+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
-+
-+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
-+End frozen 
-+micro avg                                               41.42      41.42      41.42      33406
-+macro avg                                               11.01      13.54      11.07      33406
-+weighted avg                                            34.88      41.42      34.20      33406
-+
-+1st layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               36.65      36.65      36.65      33463
-+macro avg                                               10.71       9.91       8.26      33463
-+weighted avg                                            34.32      36.65      31.08      33463
-+
-+2nd layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               35.72      35.72      35.72      42448
-+macro avg                                                3.57      10.00       5.26      42448
-+weighted avg                                            12.76      35.72      18.81      42448
-+
-+{'punct_f1': 5.264181137084961,
-+ 'punct_precision': 3.572371006011963,
-+ 'punct_recall': 10.0,
-+ 'test_loss': 18.49854850769043}
-+
-+
-+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
-+alpha from 3->4 seems to reduce convergence rate.
-+
-+micro avg                                               50.98      50.98      50.98      33463
-+macro avg                                               25.99      25.38      23.38      33463
-+weighted avg                                            50.31      50.98      48.27      33463
-+
-+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
-+micro avg                                               58.55      58.55      58.55      39340
-+macro avg                                               30.02      29.74      29.52      39340
-+weighted avg                                            57.91      58.55      57.51      39340
-+
-+still increasing?!
-+{'punct_f1': 29.523975372314453,
-+ 'punct_precision': 30.015613555908203,
-+ 'punct_recall': 29.738296508789062,
-+ 'test_loss': 0.3690211772918701}
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index fe58670..a4a012a 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,49 +1,49 @@
- seed: 42
- trainer:
--    # gpus: 1 # the number of gpus, 0 for CPU
--    # num_nodes: 1
--    # max_epochs: 2
--    # max_steps: null # precedence over max_epochs
--    # accumulate_grad_batches: 4 # accumulates grads every k batches
--    # gradient_clip_val: 0
--    # amp_level: O1 # O1/O2 for mixed precision
--    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
--    # checkpoint_callback: false  # Provided by exp_manager
--    # logger: false #false  # Provided by exp_manager
--    # log_every_n_steps: 1  # Interval of logging.
--    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # resume_from_checkpoint: null
--
--    gpus: 0 # the number of gpus, 0 for CPU
-+    gpus: 1 # the number of gpus, 0 for CPU
-     num_nodes: 1
--    max_epochs: 8
-+    max_epochs: 10
-     max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 1 # accumulates grads every k batches
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-     gradient_clip_val: 0
--    amp_level: O0 # O1/O2 for mixed precision
--    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
-+    amp_level: O1 # O1/O2 for mixed precision
-+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    accelerator: ddp
-     checkpoint_callback: false  # Provided by exp_manager
-     logger: false #false  # Provided by exp_manager
-     log_every_n_steps: 1  # Interval of logging.
-     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    reload_dataloaders_every_epoch: true
-     resume_from_checkpoint: null
- 
-+    # gpus: 0 # the number of gpus, 0 for CPU
-+    # num_nodes: 1
-+    # max_epochs: 8
-+    # max_steps: null # precedence over max_epochs
-+    # accumulate_grad_batches: 1 # accumulates grads every k batches
-+    # gradient_clip_val: 0
-+    # amp_level: O0 # O1/O2 for mixed precision
-+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # # accelerator: ddp
-+    # checkpoint_callback: false  # Provided by exp_manager
-+    # logger: false #false  # Provided by exp_manager
-+    # log_every_n_steps: 1  # Interval of logging.
-+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    # reload_dataloaders_every_epoch: true
-+    # resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /home/nxingyu2/data # /root/data # 
--tmp_path: /home/nxingyu2/data/tmp # /tmp # 
-+base_path: /home/nxingyu/data # /root/data # 
-+tmp_path: /home/nxingyu/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
--    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-     unfrozen: 0
--    maximum_unfrozen: 2
-+    maximum_unfrozen: 1
-     unfreeze_step: 1
-     # unfreeze_every: 3
-     punct_label_ids:
-@@ -58,10 +58,10 @@ model:
-         - "—"
-         - "…"
- 
--    punct_class_weights: true
-+    punct_class_weights: false
-     
-     dataset:
--        data_dir: /home/nxingyu2/data # /root/data # 
-+        data_dir: /home/nxingyu/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -106,6 +106,12 @@ model:
-         config: null
-         # unfrozen_layers: 1
- 
-+    mlp:
-+        num_fc_layers: 2
-+        fc_dropout: 0.1
-+        log_softmax: false
-+        activation: 'relu'
-+        
-     punct_head:
-         punct_num_fc_layers: 1
-         fc_dropout: 0.1
-@@ -122,17 +128,19 @@ model:
-         use_transformer_init: true
-         loss: 'cel'
-         gamma: 0.1 # coefficient of gradient reversal
-+        pooling: 'mean'
-+        idx_conditioned_on: 0
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 3
-+        alpha: 4
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: novograd
-+        name: adamw
-         lr: 1e-3
-         weight_decay: 0.00
-         sched:
-diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
-index d4ff927..3dfd622 100644
---- a/experiment/core/layers/sequence_classifier.py
-+++ b/experiment/core/layers/sequence_classifier.py
-@@ -1,3 +1,4 @@
-+import torch
- from torch import nn
- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
- from core.utils import transformer_weights_init
-@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
-         log_softmax: bool = True,
-         dropout: float = 0.0,
-         use_transformer_init: bool = True,
--        idx_conditioned_on: int = 0,
-+        pooling: str = 'mean', # mean, max, mean_max, token
-+        idx_conditioned_on: int = None,
-     ):
-         """
-         Initializes the SequenceClassifier module.
-@@ -37,6 +39,7 @@ class SequenceClassifier(nn.Module):
-         super().__init__()
-         self.log_softmax = log_softmax
-         self._idx_conditioned_on = idx_conditioned_on
-+        self.pooling = pooling
-         self.mlp = MultiLayerPerceptron(
-             hidden_size=hidden_size,
-             num_classes=num_classes,
-@@ -48,7 +51,24 @@ class SequenceClassifier(nn.Module):
-         if use_transformer_init:
-             self.apply(lambda module: transformer_weights_init(module, xavier=False))
- 
--    def forward(self, hidden_states):
-+    def forward(self, hidden_states, subtoken_mask=None):
-         hidden_states = self.dropout(hidden_states)
--        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
-+        if self.pooling=='token':
-+            pooled = hidden_states[:, self._idx_conditioned_on]
-+        else:
-+            if subtoken_mask==None:
-+                ct=hidden_states.shape[1] # Seq len
-+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
-+            else:
-+                ct = torch.sum(subtoken_mask,axis=1)
-+            pooled_sum = torch.sum(hidden_states,axis=1)
-+            pp(ct,pooled_sum)
-+            if self.pooling=='mean' or self.pooling == 'mean_max':
-+                pooled_mean = pooled_sum//ct
-+            if self.pooling=='max' or self.pooling=='mean_max':
-+                pooled_max = torch.max(hidden_states,axis=1)[0]
-+            pooled=pooled_mean if self.pooling=='mean' else \
-+                pooled_max if self.pooling=='max' else \
-+                    torch.cat([pooled_mean,pooled_max],axis=-1)
-+        logits = self.mlp(pooled)
-         return logits
-diff --git a/experiment/info.log b/experiment/info.log
-index 2471fe9..e69de29 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,300 +0,0 @@
--[INFO] - GPU available: True, used: False
--[INFO] - TPU available: None, using: 0 TPU cores
--[INFO] - shuffling train set
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        184
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.23       0.34       0.53        594
--- (label_id: 3)                                          3.06      25.42       5.46         59
--. (label_id: 4)                                         47.22      12.98      20.36        524
--: (label_id: 5)                                          0.00       0.00       0.00         18
--; (label_id: 6)                                          0.00       0.00       0.00         13
--? (label_id: 7)                                          8.45       6.32       7.23         95
--— (label_id: 8)                                          0.00       0.00       0.00         12
--… (label_id: 9)                                          0.00       0.00       0.00          0
---------------------
--micro avg                                                6.05       6.05       6.05       1503
--macro avg                                                6.66       5.01       3.73       1503
--weighted avg                                            17.61       6.05       7.98       1503
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00         92
---------------------
--micro avg                                              100.00     100.00     100.00         92
--macro avg                                              100.00     100.00     100.00         92
--weighted avg                                           100.00     100.00     100.00         92
--
--[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 1.5848931924611143e-08
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
--will be used during training (effective maximum steps = 3192) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 3192
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        202
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.62       0.45       0.70        669
--- (label_id: 3)                                          3.48      27.27       6.17         66
--. (label_id: 4)                                         45.06      13.01      20.19        561
--: (label_id: 5)                                          1.52       6.67       2.47         15
--; (label_id: 6)                                          0.00       0.00       0.00         15
--? (label_id: 7)                                          8.70       7.32       7.95         82
--— (label_id: 8)                                          0.00       0.00       0.00         13
--… (label_id: 9)                                          0.00       0.00       0.00          1
---------------------
--micro avg                                                6.20       6.20       6.20       1628
--macro avg                                                6.04       5.47       3.75       1628
--weighted avg                                            16.79       6.20       7.92       1628
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        101
---------------------
--micro avg                                              100.00     100.00     100.00        101
--macro avg                                              100.00     100.00     100.00        101
--weighted avg                                           100.00     100.00     100.00        101
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.34       0.73       0.46       4402
--! (label_id: 1)                                          0.42      13.95       0.82        129
--, (label_id: 2)                                          2.53       0.64       1.03      15243
--- (label_id: 3)                                          2.45      21.03       4.38       1322
--. (label_id: 4)                                         44.00      11.40      18.11      12542
--: (label_id: 5)                                          0.43       1.41       0.65        354
--; (label_id: 6)                                          0.00       0.00       0.00        163
--? (label_id: 7)                                          4.16       6.27       5.00       1117
--— (label_id: 8)                                          3.00       0.61       1.02        488
--… (label_id: 9)                                          0.97       6.17       1.68         81
---------------------
--micro avg                                                5.41       5.41       5.41      35841
--macro avg                                                5.83       6.22       3.32      35841
--weighted avg                                            16.78       5.41       7.18      35841
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2201
---------------------
--micro avg                                              100.00     100.00     100.00       2201
--macro avg                                              100.00     100.00     100.00       2201
--weighted avg                                           100.00     100.00     100.00       2201
--
--[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.20       0.43       0.27       4226
--! (label_id: 1)                                          0.44      14.17       0.86        127
--, (label_id: 2)                                          1.93       0.49       0.78      14611
--- (label_id: 3)                                          2.23      19.56       4.01       1237
--. (label_id: 4)                                         43.37      11.25      17.86      11977
--: (label_id: 5)                                          0.68       2.34       1.05        342
--; (label_id: 6)                                          0.00       0.00       0.00        129
--? (label_id: 7)                                          5.16       7.47       6.10       1058
--— (label_id: 8)                                          2.15       0.49       0.80        409
--… (label_id: 9)                                          0.69       4.23       1.19         71
---------------------
--micro avg                                                5.23       5.23       5.23      34187
--macro avg                                                5.68       6.04       3.29      34187
--weighted avg                                            16.32       5.23       6.98      34187
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2113
---------------------
--micro avg                                              100.00     100.00     100.00       2113
--macro avg                                              100.00     100.00     100.00       2113
--weighted avg                                           100.00     100.00     100.00       2113
--
--[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.61       0.39       4228
--! (label_id: 1)                                          0.30       8.28       0.58        145
--, (label_id: 2)                                          2.27       0.58       0.92      14495
--- (label_id: 3)                                          2.64      21.78       4.70       1327
--. (label_id: 4)                                         44.87      11.66      18.51      12193
--: (label_id: 5)                                          0.60       1.93       0.91        362
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.19       6.40       5.07       1078
--— (label_id: 8)                                          1.16       0.22       0.37        459
--… (label_id: 9)                                          0.85       4.17       1.41         96
---------------------
--micro avg                                                5.54       5.54       5.54      34547
--macro avg                                                5.72       5.56       3.29      34547
--weighted avg                                            17.08       5.54       7.33      34547
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2114
---------------------
--micro avg                                              100.00     100.00     100.00       2114
--macro avg                                              100.00     100.00     100.00       2114
--weighted avg                                           100.00     100.00     100.00       2114
--
--[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.29       0.63       0.40       4444
--! (label_id: 1)                                          0.38      10.67       0.74        150
--, (label_id: 2)                                          2.32       0.59       0.94      15290
--- (label_id: 3)                                          2.34      20.28       4.19       1292
--. (label_id: 4)                                         43.85      11.68      18.44      12599
--: (label_id: 5)                                          0.41       1.28       0.62        392
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.24       6.30       5.07       1111
--— (label_id: 8)                                          0.00       0.00       0.00        456
--… (label_id: 9)                                          0.38       2.41       0.65         83
---------------------
--micro avg                                                5.40       5.40       5.40      35981
--macro avg                                                5.42       5.38       3.11      35981
--weighted avg                                            16.59       5.40       7.22      35981
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2222
---------------------
--micro avg                                              100.00     100.00     100.00       2222
--macro avg                                              100.00     100.00     100.00       2222
--weighted avg                                           100.00     100.00     100.00       2222
--
--[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.35       0.73       0.48       3844
--! (label_id: 1)                                          0.54      14.62       1.04        130
--, (label_id: 2)                                          2.32       0.59       0.94      13056
--- (label_id: 3)                                          2.67      22.28       4.77       1194
--. (label_id: 4)                                         44.45      11.95      18.84      10791
--: (label_id: 5)                                          0.84       3.21       1.33        280
--; (label_id: 6)                                          0.00       0.00       0.00        140
--? (label_id: 7)                                          4.17       6.56       5.10        914
--— (label_id: 8)                                          0.00       0.00       0.00        401
--… (label_id: 9)                                          0.48       2.63       0.81         76
---------------------
--micro avg                                                5.68       5.68       5.68      30826
--macro avg                                                5.58       6.26       3.33      30826
--weighted avg                                            16.82       5.68       7.41      30826
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1922
---------------------
--micro avg                                              100.00     100.00     100.00       1922
--macro avg                                              100.00     100.00     100.00       1922
--weighted avg                                           100.00     100.00     100.00       1922
--
--[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.60       0.39       3970
--! (label_id: 1)                                          0.35      10.66       0.68        122
--, (label_id: 2)                                          2.09       0.53       0.85      13469
--- (label_id: 3)                                          2.29      19.32       4.10       1201
--. (label_id: 4)                                         43.43      11.24      17.86      11227
--: (label_id: 5)                                          0.63       2.30       0.99        304
--; (label_id: 6)                                          0.00       0.00       0.00        141
--? (label_id: 7)                                          4.52       6.86       5.45       1006
--— (label_id: 8)                                          1.15       0.23       0.38        444
--… (label_id: 9)                                          0.45       2.67       0.78         75
---------------------
--micro avg                                                5.26       5.26       5.26      31959
--macro avg                                                5.52       5.44       3.15      31959
--weighted avg                                            16.42       5.26       7.02      31959
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1985
---------------------
--micro avg                                              100.00     100.00     100.00       1985
--macro avg                                              100.00     100.00     100.00       1985
--weighted avg                                           100.00     100.00     100.00       1985
--
--[INFO] - Epoch 5, step 2394: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.23       0.48       0.31       4126
--! (label_id: 1)                                          0.29       9.40       0.56        117
--, (label_id: 2)                                          1.91       0.49       0.77      14019
--- (label_id: 3)                                          2.52      22.59       4.53       1164
--. (label_id: 4)                                         44.15      11.65      18.44      11789
--: (label_id: 5)                                          0.72       2.41       1.11        332
--; (label_id: 6)                                          0.56       0.61       0.58        165
--? (label_id: 7)                                          3.89       6.53       4.88        980
--— (label_id: 8)                                          2.30       0.47       0.77        430
--… (label_id: 9)                                          1.18       8.33       2.07         60
---------------------
--micro avg                                                5.47       5.47       5.47      33182
--macro avg                                                5.77       6.30       3.40      33182
--weighted avg                                            16.77       5.47       7.25      33182
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2063
---------------------
--micro avg                                              100.00     100.00     100.00       2063
--macro avg                                              100.00     100.00     100.00       2063
--weighted avg                                           100.00     100.00     100.00       2063
--
--[INFO] - Epoch 6, step 2793: val_loss was not in top 3
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index fa37b4c..43fc93d 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             self.hparams.model.punct_class_weights=None
- 
-+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
-+        self.mlp = MultiLayerPerceptron(
-+            self.transformer.config.hidden_size,
-+            self.transformer.config.hidden_size,
-+            num_layers=self.hparams.model.mlp.num_fc_layers, 
-+            activation=self.hparams.model.mlp.activation, 
-+            log_softmax=self.hparams.model.mlp.log_softmax
-+        )
-+
-         self.punct_classifier = TokenClassifier(
-             hidden_size=self.transformer.config.hidden_size,
-             num_classes=len(self.labels_to_ids),
-@@ -80,6 +89,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-             log_softmax=self.hparams.model.domain_head.log_softmax,
-             dropout=self.hparams.model.domain_head.fc_dropout,
-             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
-+            pooling=self.hparams.model.domain_head.pooling,
-+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
-         )
- 
-         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
-@@ -119,14 +130,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
-         self.freeze()
- 
--    def forward(self, input_ids, attention_mask, domain_ids=None):
-+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
-         hidden_states = self.transformer(
-             input_ids=input_ids, attention_mask=attention_mask
-         )[0]
-+        hidden_states = self.dropout(hidden_states)
-+        hidden_states = self.mlp(hidden_states)
-         punct_logits = self.punct_classifier(hidden_states=hidden_states)
-         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
-         domain_logits = self.domain_classifier(
--            hidden_states=reverse_grad_hidden_states)
-+            hidden_states=reverse_grad_hidden_states,
-+            subtoken_mask=subtoken_mask)
-         return punct_logits, domain_logits
- 
-     def _make_step(self, batch):
-@@ -136,7 +150,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         punct_labels = batch['labels']
-         domain_labels = batch['domain']
-         punct_logits, domain_logits = self(
--            input_ids=input_ids, attention_mask=attention_mask
-+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
-         )
-         punctuation_loss = self.punctuation_loss(
-             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/lightning_logs.txt
deleted file mode 100644
index 9fa336d..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/lightning_logs.txt
+++ /dev/null
@@ -1,24 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-Using native 16bit precision.
-Global seed set to 42
-initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | dropout             | Dropout              | 0     
-2 | mlp                 | MultiLayerPerceptron | 131 K 
-3 | punct_classifier    | TokenClassifier      | 2.6 K 
-4 | domain_classifier   | SequenceClassifier   | 257   
-5 | punctuation_loss    | FocalDiceLoss        | 0     
-6 | domain_loss         | CrossEntropyLoss     | 0     
-7 | agg_loss            | AggregatorLoss       | 0     
-8 | punct_class_report  | ClassificationReport | 0     
-9 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-167 K     Trainable params
-13.4 M    Non-trainable params
-13.6 M    Total params
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/nemo_error_log.txt
deleted file mode 100644
index 6028b56..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/nemo_error_log.txt
+++ /dev/null
@@ -1,7 +0,0 @@
-[NeMo W 2021-02-08 11:00:55 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 11:01:04 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:01:04 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 703b70c..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,9 +0,0 @@
-[NeMo I 2021-02-08 11:00:55 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55
-[NeMo I 2021-02-08 11:00:55 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-08 11:00:55 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 11:01:04 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:01:04 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/git-info.log
deleted file mode 100644
index c7188a9..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/git-info.log
+++ /dev/null
@@ -1,625 +0,0 @@
-commit hash: 939a671c8c117db6975316767ced5d95449e2b27
-diff --git a/README.md b/README.md
-index d52dba5..0b403d3 100644
---- a/README.md
-+++ b/README.md
-@@ -333,4 +333,71 @@ label                                                precision    recall       f
-  'punct_f1': 0.5858508944511414,
-  'punct_precision': 0.30176490545272827,
-  'punct_recall': 10.0,
-- 'test_loss': 0.8140875697135925}
-\ No newline at end of file
-+ 'test_loss': 0.8140875697135925}
-+
-+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
-+
-+ (label_id: 0)                                          62.15     100.00      76.66       5154
-+! (label_id: 1)                                          0.00       0.00       0.00        108
-+, (label_id: 2)                                          0.00       0.00       0.00      18022
-+- (label_id: 3)                                          0.00       0.00       0.00       1557
-+. (label_id: 4)                                         41.74      94.01      57.81      15164
-+: (label_id: 5)                                          0.00       0.00       0.00        319
-+; (label_id: 6)                                          0.00       0.00       0.00         88
-+? (label_id: 7)                                          0.00       0.00       0.00       1217
-+ (label_id: 8)                                          0.00       0.00       0.00        752
-+… (label_id: 9)                                          0.00       0.00       0.00         67
-+-------------------
-+micro avg                                               45.72      45.72      45.72      42448
-+macro avg                                               10.39      19.40      13.45      42448
-+weighted avg                                            22.46      45.72      29.96      42448
-+{ 'punct_f1': 13.446383476257324,
-+ 'punct_precision': 10.388500213623047,
-+ 'punct_recall': 19.400554656982422,
-+ 'test_loss': 0.44148480892181396}
-+
-+
-+ ## Log for 8/2/2021
-+
-+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
-+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
-+
-+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
-+End frozen 
-+micro avg                                               41.42      41.42      41.42      33406
-+macro avg                                               11.01      13.54      11.07      33406
-+weighted avg                                            34.88      41.42      34.20      33406
-+
-+1st layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               36.65      36.65      36.65      33463
-+macro avg                                               10.71       9.91       8.26      33463
-+weighted avg                                            34.32      36.65      31.08      33463
-+
-+2nd layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               35.72      35.72      35.72      42448
-+macro avg                                                3.57      10.00       5.26      42448
-+weighted avg                                            12.76      35.72      18.81      42448
-+
-+{'punct_f1': 5.264181137084961,
-+ 'punct_precision': 3.572371006011963,
-+ 'punct_recall': 10.0,
-+ 'test_loss': 18.49854850769043}
-+
-+
-+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
-+alpha from 3->4 seems to reduce convergence rate.
-+
-+micro avg                                               50.98      50.98      50.98      33463
-+macro avg                                               25.99      25.38      23.38      33463
-+weighted avg                                            50.31      50.98      48.27      33463
-+
-+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
-+micro avg                                               58.55      58.55      58.55      39340
-+macro avg                                               30.02      29.74      29.52      39340
-+weighted avg                                            57.91      58.55      57.51      39340
-+
-+still increasing?!
-+{'punct_f1': 29.523975372314453,
-+ 'punct_precision': 30.015613555908203,
-+ 'punct_recall': 29.738296508789062,
-+ 'test_loss': 0.3690211772918701}
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index fe58670..a4a012a 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,49 +1,49 @@
- seed: 42
- trainer:
--    # gpus: 1 # the number of gpus, 0 for CPU
--    # num_nodes: 1
--    # max_epochs: 2
--    # max_steps: null # precedence over max_epochs
--    # accumulate_grad_batches: 4 # accumulates grads every k batches
--    # gradient_clip_val: 0
--    # amp_level: O1 # O1/O2 for mixed precision
--    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
--    # checkpoint_callback: false  # Provided by exp_manager
--    # logger: false #false  # Provided by exp_manager
--    # log_every_n_steps: 1  # Interval of logging.
--    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # resume_from_checkpoint: null
--
--    gpus: 0 # the number of gpus, 0 for CPU
-+    gpus: 1 # the number of gpus, 0 for CPU
-     num_nodes: 1
--    max_epochs: 8
-+    max_epochs: 10
-     max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 1 # accumulates grads every k batches
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-     gradient_clip_val: 0
--    amp_level: O0 # O1/O2 for mixed precision
--    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
-+    amp_level: O1 # O1/O2 for mixed precision
-+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    accelerator: ddp
-     checkpoint_callback: false  # Provided by exp_manager
-     logger: false #false  # Provided by exp_manager
-     log_every_n_steps: 1  # Interval of logging.
-     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    reload_dataloaders_every_epoch: true
-     resume_from_checkpoint: null
- 
-+    # gpus: 0 # the number of gpus, 0 for CPU
-+    # num_nodes: 1
-+    # max_epochs: 8
-+    # max_steps: null # precedence over max_epochs
-+    # accumulate_grad_batches: 1 # accumulates grads every k batches
-+    # gradient_clip_val: 0
-+    # amp_level: O0 # O1/O2 for mixed precision
-+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # # accelerator: ddp
-+    # checkpoint_callback: false  # Provided by exp_manager
-+    # logger: false #false  # Provided by exp_manager
-+    # log_every_n_steps: 1  # Interval of logging.
-+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    # reload_dataloaders_every_epoch: true
-+    # resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /home/nxingyu2/data # /root/data # 
--tmp_path: /home/nxingyu2/data/tmp # /tmp # 
-+base_path: /home/nxingyu/data # /root/data # 
-+tmp_path: /home/nxingyu/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
--    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-     unfrozen: 0
--    maximum_unfrozen: 2
-+    maximum_unfrozen: 1
-     unfreeze_step: 1
-     # unfreeze_every: 3
-     punct_label_ids:
-@@ -58,10 +58,10 @@ model:
-         - "—"
-         - "…"
- 
--    punct_class_weights: true
-+    punct_class_weights: false
-     
-     dataset:
--        data_dir: /home/nxingyu2/data # /root/data # 
-+        data_dir: /home/nxingyu/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -106,6 +106,12 @@ model:
-         config: null
-         # unfrozen_layers: 1
- 
-+    mlp:
-+        num_fc_layers: 2
-+        fc_dropout: 0.1
-+        log_softmax: false
-+        activation: 'relu'
-+        
-     punct_head:
-         punct_num_fc_layers: 1
-         fc_dropout: 0.1
-@@ -122,17 +128,19 @@ model:
-         use_transformer_init: true
-         loss: 'cel'
-         gamma: 0.1 # coefficient of gradient reversal
-+        pooling: 'mean'
-+        idx_conditioned_on: 0
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 3
-+        alpha: 4
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: novograd
-+        name: adamw
-         lr: 1e-3
-         weight_decay: 0.00
-         sched:
-diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
-index d4ff927..618351a 100644
---- a/experiment/core/layers/sequence_classifier.py
-+++ b/experiment/core/layers/sequence_classifier.py
-@@ -1,3 +1,4 @@
-+import torch
- from torch import nn
- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
- from core.utils import transformer_weights_init
-@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
-         log_softmax: bool = True,
-         dropout: float = 0.0,
-         use_transformer_init: bool = True,
--        idx_conditioned_on: int = 0,
-+        pooling: str = 'mean', # mean, max, mean_max, token
-+        idx_conditioned_on: int = None,
-     ):
-         """
-         Initializes the SequenceClassifier module.
-@@ -37,6 +39,7 @@ class SequenceClassifier(nn.Module):
-         super().__init__()
-         self.log_softmax = log_softmax
-         self._idx_conditioned_on = idx_conditioned_on
-+        self.pooling = pooling
-         self.mlp = MultiLayerPerceptron(
-             hidden_size=hidden_size,
-             num_classes=num_classes,
-@@ -48,7 +51,24 @@ class SequenceClassifier(nn.Module):
-         if use_transformer_init:
-             self.apply(lambda module: transformer_weights_init(module, xavier=False))
- 
--    def forward(self, hidden_states):
-+    def forward(self, hidden_states, subtoken_mask=None):
-         hidden_states = self.dropout(hidden_states)
--        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
-+        if self.pooling=='token':
-+            pooled = hidden_states[:, self._idx_conditioned_on]
-+        else:
-+            if subtoken_mask==None:
-+                ct=hidden_states.shape[1] # Seq len
-+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
-+            else:
-+                ct = torch.sum(subtoken_mask,axis=1)
-+            pooled_sum = torch.sum(hidden_states,axis=1)
-+            pp(ct.shape,pooled_sum.shape)
-+            if self.pooling=='mean' or self.pooling == 'mean_max':
-+                pooled_mean = pooled_sum//ct
-+            if self.pooling=='max' or self.pooling=='mean_max':
-+                pooled_max = torch.max(hidden_states,axis=1)[0]
-+            pooled=pooled_mean if self.pooling=='mean' else \
-+                pooled_max if self.pooling=='max' else \
-+                    torch.cat([pooled_mean,pooled_max],axis=-1)
-+        logits = self.mlp(pooled)
-         return logits
-diff --git a/experiment/info.log b/experiment/info.log
-index 2471fe9..e69de29 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,300 +0,0 @@
--[INFO] - GPU available: True, used: False
--[INFO] - TPU available: None, using: 0 TPU cores
--[INFO] - shuffling train set
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        184
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.23       0.34       0.53        594
--- (label_id: 3)                                          3.06      25.42       5.46         59
--. (label_id: 4)                                         47.22      12.98      20.36        524
--: (label_id: 5)                                          0.00       0.00       0.00         18
--; (label_id: 6)                                          0.00       0.00       0.00         13
--? (label_id: 7)                                          8.45       6.32       7.23         95
--— (label_id: 8)                                          0.00       0.00       0.00         12
--… (label_id: 9)                                          0.00       0.00       0.00          0
---------------------
--micro avg                                                6.05       6.05       6.05       1503
--macro avg                                                6.66       5.01       3.73       1503
--weighted avg                                            17.61       6.05       7.98       1503
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00         92
---------------------
--micro avg                                              100.00     100.00     100.00         92
--macro avg                                              100.00     100.00     100.00         92
--weighted avg                                           100.00     100.00     100.00         92
--
--[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 1.5848931924611143e-08
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
--will be used during training (effective maximum steps = 3192) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 3192
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        202
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.62       0.45       0.70        669
--- (label_id: 3)                                          3.48      27.27       6.17         66
--. (label_id: 4)                                         45.06      13.01      20.19        561
--: (label_id: 5)                                          1.52       6.67       2.47         15
--; (label_id: 6)                                          0.00       0.00       0.00         15
--? (label_id: 7)                                          8.70       7.32       7.95         82
--— (label_id: 8)                                          0.00       0.00       0.00         13
--… (label_id: 9)                                          0.00       0.00       0.00          1
---------------------
--micro avg                                                6.20       6.20       6.20       1628
--macro avg                                                6.04       5.47       3.75       1628
--weighted avg                                            16.79       6.20       7.92       1628
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        101
---------------------
--micro avg                                              100.00     100.00     100.00        101
--macro avg                                              100.00     100.00     100.00        101
--weighted avg                                           100.00     100.00     100.00        101
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.34       0.73       0.46       4402
--! (label_id: 1)                                          0.42      13.95       0.82        129
--, (label_id: 2)                                          2.53       0.64       1.03      15243
--- (label_id: 3)                                          2.45      21.03       4.38       1322
--. (label_id: 4)                                         44.00      11.40      18.11      12542
--: (label_id: 5)                                          0.43       1.41       0.65        354
--; (label_id: 6)                                          0.00       0.00       0.00        163
--? (label_id: 7)                                          4.16       6.27       5.00       1117
--— (label_id: 8)                                          3.00       0.61       1.02        488
--… (label_id: 9)                                          0.97       6.17       1.68         81
---------------------
--micro avg                                                5.41       5.41       5.41      35841
--macro avg                                                5.83       6.22       3.32      35841
--weighted avg                                            16.78       5.41       7.18      35841
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2201
---------------------
--micro avg                                              100.00     100.00     100.00       2201
--macro avg                                              100.00     100.00     100.00       2201
--weighted avg                                           100.00     100.00     100.00       2201
--
--[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.20       0.43       0.27       4226
--! (label_id: 1)                                          0.44      14.17       0.86        127
--, (label_id: 2)                                          1.93       0.49       0.78      14611
--- (label_id: 3)                                          2.23      19.56       4.01       1237
--. (label_id: 4)                                         43.37      11.25      17.86      11977
--: (label_id: 5)                                          0.68       2.34       1.05        342
--; (label_id: 6)                                          0.00       0.00       0.00        129
--? (label_id: 7)                                          5.16       7.47       6.10       1058
--— (label_id: 8)                                          2.15       0.49       0.80        409
--… (label_id: 9)                                          0.69       4.23       1.19         71
---------------------
--micro avg                                                5.23       5.23       5.23      34187
--macro avg                                                5.68       6.04       3.29      34187
--weighted avg                                            16.32       5.23       6.98      34187
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2113
---------------------
--micro avg                                              100.00     100.00     100.00       2113
--macro avg                                              100.00     100.00     100.00       2113
--weighted avg                                           100.00     100.00     100.00       2113
--
--[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.61       0.39       4228
--! (label_id: 1)                                          0.30       8.28       0.58        145
--, (label_id: 2)                                          2.27       0.58       0.92      14495
--- (label_id: 3)                                          2.64      21.78       4.70       1327
--. (label_id: 4)                                         44.87      11.66      18.51      12193
--: (label_id: 5)                                          0.60       1.93       0.91        362
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.19       6.40       5.07       1078
--— (label_id: 8)                                          1.16       0.22       0.37        459
--… (label_id: 9)                                          0.85       4.17       1.41         96
---------------------
--micro avg                                                5.54       5.54       5.54      34547
--macro avg                                                5.72       5.56       3.29      34547
--weighted avg                                            17.08       5.54       7.33      34547
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2114
---------------------
--micro avg                                              100.00     100.00     100.00       2114
--macro avg                                              100.00     100.00     100.00       2114
--weighted avg                                           100.00     100.00     100.00       2114
--
--[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.29       0.63       0.40       4444
--! (label_id: 1)                                          0.38      10.67       0.74        150
--, (label_id: 2)                                          2.32       0.59       0.94      15290
--- (label_id: 3)                                          2.34      20.28       4.19       1292
--. (label_id: 4)                                         43.85      11.68      18.44      12599
--: (label_id: 5)                                          0.41       1.28       0.62        392
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.24       6.30       5.07       1111
--— (label_id: 8)                                          0.00       0.00       0.00        456
--… (label_id: 9)                                          0.38       2.41       0.65         83
---------------------
--micro avg                                                5.40       5.40       5.40      35981
--macro avg                                                5.42       5.38       3.11      35981
--weighted avg                                            16.59       5.40       7.22      35981
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2222
---------------------
--micro avg                                              100.00     100.00     100.00       2222
--macro avg                                              100.00     100.00     100.00       2222
--weighted avg                                           100.00     100.00     100.00       2222
--
--[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.35       0.73       0.48       3844
--! (label_id: 1)                                          0.54      14.62       1.04        130
--, (label_id: 2)                                          2.32       0.59       0.94      13056
--- (label_id: 3)                                          2.67      22.28       4.77       1194
--. (label_id: 4)                                         44.45      11.95      18.84      10791
--: (label_id: 5)                                          0.84       3.21       1.33        280
--; (label_id: 6)                                          0.00       0.00       0.00        140
--? (label_id: 7)                                          4.17       6.56       5.10        914
--— (label_id: 8)                                          0.00       0.00       0.00        401
--… (label_id: 9)                                          0.48       2.63       0.81         76
---------------------
--micro avg                                                5.68       5.68       5.68      30826
--macro avg                                                5.58       6.26       3.33      30826
--weighted avg                                            16.82       5.68       7.41      30826
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1922
---------------------
--micro avg                                              100.00     100.00     100.00       1922
--macro avg                                              100.00     100.00     100.00       1922
--weighted avg                                           100.00     100.00     100.00       1922
--
--[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.60       0.39       3970
--! (label_id: 1)                                          0.35      10.66       0.68        122
--, (label_id: 2)                                          2.09       0.53       0.85      13469
--- (label_id: 3)                                          2.29      19.32       4.10       1201
--. (label_id: 4)                                         43.43      11.24      17.86      11227
--: (label_id: 5)                                          0.63       2.30       0.99        304
--; (label_id: 6)                                          0.00       0.00       0.00        141
--? (label_id: 7)                                          4.52       6.86       5.45       1006
--— (label_id: 8)                                          1.15       0.23       0.38        444
--… (label_id: 9)                                          0.45       2.67       0.78         75
---------------------
--micro avg                                                5.26       5.26       5.26      31959
--macro avg                                                5.52       5.44       3.15      31959
--weighted avg                                            16.42       5.26       7.02      31959
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1985
---------------------
--micro avg                                              100.00     100.00     100.00       1985
--macro avg                                              100.00     100.00     100.00       1985
--weighted avg                                           100.00     100.00     100.00       1985
--
--[INFO] - Epoch 5, step 2394: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.23       0.48       0.31       4126
--! (label_id: 1)                                          0.29       9.40       0.56        117
--, (label_id: 2)                                          1.91       0.49       0.77      14019
--- (label_id: 3)                                          2.52      22.59       4.53       1164
--. (label_id: 4)                                         44.15      11.65      18.44      11789
--: (label_id: 5)                                          0.72       2.41       1.11        332
--; (label_id: 6)                                          0.56       0.61       0.58        165
--? (label_id: 7)                                          3.89       6.53       4.88        980
--— (label_id: 8)                                          2.30       0.47       0.77        430
--… (label_id: 9)                                          1.18       8.33       2.07         60
---------------------
--micro avg                                                5.47       5.47       5.47      33182
--macro avg                                                5.77       6.30       3.40      33182
--weighted avg                                            16.77       5.47       7.25      33182
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2063
---------------------
--micro avg                                              100.00     100.00     100.00       2063
--macro avg                                              100.00     100.00     100.00       2063
--weighted avg                                           100.00     100.00     100.00       2063
--
--[INFO] - Epoch 6, step 2793: val_loss was not in top 3
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index fa37b4c..43fc93d 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             self.hparams.model.punct_class_weights=None
- 
-+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
-+        self.mlp = MultiLayerPerceptron(
-+            self.transformer.config.hidden_size,
-+            self.transformer.config.hidden_size,
-+            num_layers=self.hparams.model.mlp.num_fc_layers, 
-+            activation=self.hparams.model.mlp.activation, 
-+            log_softmax=self.hparams.model.mlp.log_softmax
-+        )
-+
-         self.punct_classifier = TokenClassifier(
-             hidden_size=self.transformer.config.hidden_size,
-             num_classes=len(self.labels_to_ids),
-@@ -80,6 +89,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-             log_softmax=self.hparams.model.domain_head.log_softmax,
-             dropout=self.hparams.model.domain_head.fc_dropout,
-             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
-+            pooling=self.hparams.model.domain_head.pooling,
-+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
-         )
- 
-         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
-@@ -119,14 +130,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
-         self.freeze()
- 
--    def forward(self, input_ids, attention_mask, domain_ids=None):
-+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
-         hidden_states = self.transformer(
-             input_ids=input_ids, attention_mask=attention_mask
-         )[0]
-+        hidden_states = self.dropout(hidden_states)
-+        hidden_states = self.mlp(hidden_states)
-         punct_logits = self.punct_classifier(hidden_states=hidden_states)
-         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
-         domain_logits = self.domain_classifier(
--            hidden_states=reverse_grad_hidden_states)
-+            hidden_states=reverse_grad_hidden_states,
-+            subtoken_mask=subtoken_mask)
-         return punct_logits, domain_logits
- 
-     def _make_step(self, batch):
-@@ -136,7 +150,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         punct_labels = batch['labels']
-         domain_labels = batch['domain']
-         punct_logits, domain_logits = self(
--            input_ids=input_ids, attention_mask=attention_mask
-+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
-         )
-         punctuation_loss = self.punctuation_loss(
-             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/lightning_logs.txt
deleted file mode 100644
index 9fa336d..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/lightning_logs.txt
+++ /dev/null
@@ -1,24 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-Using native 16bit precision.
-Global seed set to 42
-initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | dropout             | Dropout              | 0     
-2 | mlp                 | MultiLayerPerceptron | 131 K 
-3 | punct_classifier    | TokenClassifier      | 2.6 K 
-4 | domain_classifier   | SequenceClassifier   | 257   
-5 | punctuation_loss    | FocalDiceLoss        | 0     
-6 | domain_loss         | CrossEntropyLoss     | 0     
-7 | agg_loss            | AggregatorLoss       | 0     
-8 | punct_class_report  | ClassificationReport | 0     
-9 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-167 K     Trainable params
-13.4 M    Non-trainable params
-13.6 M    Total params
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/nemo_error_log.txt
deleted file mode 100644
index d318902..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/nemo_error_log.txt
+++ /dev/null
@@ -1,7 +0,0 @@
-[NeMo W 2021-02-08 11:02:01 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 11:02:10 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:02:10 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 4c89259..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,9 +0,0 @@
-[NeMo I 2021-02-08 11:02:01 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01
-[NeMo I 2021-02-08 11:02:01 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-08 11:02:01 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 11:02:10 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:02:10 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/git-info.log
deleted file mode 100644
index 9aebfb8..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/git-info.log
+++ /dev/null
@@ -1,625 +0,0 @@
-commit hash: 939a671c8c117db6975316767ced5d95449e2b27
-diff --git a/README.md b/README.md
-index d52dba5..0b403d3 100644
---- a/README.md
-+++ b/README.md
-@@ -333,4 +333,71 @@ label                                                precision    recall       f
-  'punct_f1': 0.5858508944511414,
-  'punct_precision': 0.30176490545272827,
-  'punct_recall': 10.0,
-- 'test_loss': 0.8140875697135925}
-\ No newline at end of file
-+ 'test_loss': 0.8140875697135925}
-+
-+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
-+
-+ (label_id: 0)                                          62.15     100.00      76.66       5154
-+! (label_id: 1)                                          0.00       0.00       0.00        108
-+, (label_id: 2)                                          0.00       0.00       0.00      18022
-+- (label_id: 3)                                          0.00       0.00       0.00       1557
-+. (label_id: 4)                                         41.74      94.01      57.81      15164
-+: (label_id: 5)                                          0.00       0.00       0.00        319
-+; (label_id: 6)                                          0.00       0.00       0.00         88
-+? (label_id: 7)                                          0.00       0.00       0.00       1217
-+ (label_id: 8)                                          0.00       0.00       0.00        752
-+… (label_id: 9)                                          0.00       0.00       0.00         67
-+-------------------
-+micro avg                                               45.72      45.72      45.72      42448
-+macro avg                                               10.39      19.40      13.45      42448
-+weighted avg                                            22.46      45.72      29.96      42448
-+{ 'punct_f1': 13.446383476257324,
-+ 'punct_precision': 10.388500213623047,
-+ 'punct_recall': 19.400554656982422,
-+ 'test_loss': 0.44148480892181396}
-+
-+
-+ ## Log for 8/2/2021
-+
-+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
-+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
-+
-+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
-+End frozen 
-+micro avg                                               41.42      41.42      41.42      33406
-+macro avg                                               11.01      13.54      11.07      33406
-+weighted avg                                            34.88      41.42      34.20      33406
-+
-+1st layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               36.65      36.65      36.65      33463
-+macro avg                                               10.71       9.91       8.26      33463
-+weighted avg                                            34.32      36.65      31.08      33463
-+
-+2nd layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               35.72      35.72      35.72      42448
-+macro avg                                                3.57      10.00       5.26      42448
-+weighted avg                                            12.76      35.72      18.81      42448
-+
-+{'punct_f1': 5.264181137084961,
-+ 'punct_precision': 3.572371006011963,
-+ 'punct_recall': 10.0,
-+ 'test_loss': 18.49854850769043}
-+
-+
-+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
-+alpha from 3->4 seems to reduce convergence rate.
-+
-+micro avg                                               50.98      50.98      50.98      33463
-+macro avg                                               25.99      25.38      23.38      33463
-+weighted avg                                            50.31      50.98      48.27      33463
-+
-+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
-+micro avg                                               58.55      58.55      58.55      39340
-+macro avg                                               30.02      29.74      29.52      39340
-+weighted avg                                            57.91      58.55      57.51      39340
-+
-+still increasing?!
-+{'punct_f1': 29.523975372314453,
-+ 'punct_precision': 30.015613555908203,
-+ 'punct_recall': 29.738296508789062,
-+ 'test_loss': 0.3690211772918701}
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index fe58670..a4a012a 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,49 +1,49 @@
- seed: 42
- trainer:
--    # gpus: 1 # the number of gpus, 0 for CPU
--    # num_nodes: 1
--    # max_epochs: 2
--    # max_steps: null # precedence over max_epochs
--    # accumulate_grad_batches: 4 # accumulates grads every k batches
--    # gradient_clip_val: 0
--    # amp_level: O1 # O1/O2 for mixed precision
--    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
--    # checkpoint_callback: false  # Provided by exp_manager
--    # logger: false #false  # Provided by exp_manager
--    # log_every_n_steps: 1  # Interval of logging.
--    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # resume_from_checkpoint: null
--
--    gpus: 0 # the number of gpus, 0 for CPU
-+    gpus: 1 # the number of gpus, 0 for CPU
-     num_nodes: 1
--    max_epochs: 8
-+    max_epochs: 10
-     max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 1 # accumulates grads every k batches
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-     gradient_clip_val: 0
--    amp_level: O0 # O1/O2 for mixed precision
--    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
-+    amp_level: O1 # O1/O2 for mixed precision
-+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    accelerator: ddp
-     checkpoint_callback: false  # Provided by exp_manager
-     logger: false #false  # Provided by exp_manager
-     log_every_n_steps: 1  # Interval of logging.
-     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    reload_dataloaders_every_epoch: true
-     resume_from_checkpoint: null
- 
-+    # gpus: 0 # the number of gpus, 0 for CPU
-+    # num_nodes: 1
-+    # max_epochs: 8
-+    # max_steps: null # precedence over max_epochs
-+    # accumulate_grad_batches: 1 # accumulates grads every k batches
-+    # gradient_clip_val: 0
-+    # amp_level: O0 # O1/O2 for mixed precision
-+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # # accelerator: ddp
-+    # checkpoint_callback: false  # Provided by exp_manager
-+    # logger: false #false  # Provided by exp_manager
-+    # log_every_n_steps: 1  # Interval of logging.
-+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    # reload_dataloaders_every_epoch: true
-+    # resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /home/nxingyu2/data # /root/data # 
--tmp_path: /home/nxingyu2/data/tmp # /tmp # 
-+base_path: /home/nxingyu/data # /root/data # 
-+tmp_path: /home/nxingyu/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
--    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-     unfrozen: 0
--    maximum_unfrozen: 2
-+    maximum_unfrozen: 1
-     unfreeze_step: 1
-     # unfreeze_every: 3
-     punct_label_ids:
-@@ -58,10 +58,10 @@ model:
-         - "—"
-         - "…"
- 
--    punct_class_weights: true
-+    punct_class_weights: false
-     
-     dataset:
--        data_dir: /home/nxingyu2/data # /root/data # 
-+        data_dir: /home/nxingyu/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -106,6 +106,12 @@ model:
-         config: null
-         # unfrozen_layers: 1
- 
-+    mlp:
-+        num_fc_layers: 2
-+        fc_dropout: 0.1
-+        log_softmax: false
-+        activation: 'relu'
-+        
-     punct_head:
-         punct_num_fc_layers: 1
-         fc_dropout: 0.1
-@@ -122,17 +128,19 @@ model:
-         use_transformer_init: true
-         loss: 'cel'
-         gamma: 0.1 # coefficient of gradient reversal
-+        pooling: 'mean'
-+        idx_conditioned_on: 0
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 3
-+        alpha: 4
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: novograd
-+        name: adamw
-         lr: 1e-3
-         weight_decay: 0.00
-         sched:
-diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
-index d4ff927..f1c68d0 100644
---- a/experiment/core/layers/sequence_classifier.py
-+++ b/experiment/core/layers/sequence_classifier.py
-@@ -1,3 +1,4 @@
-+import torch
- from torch import nn
- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
- from core.utils import transformer_weights_init
-@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
-         log_softmax: bool = True,
-         dropout: float = 0.0,
-         use_transformer_init: bool = True,
--        idx_conditioned_on: int = 0,
-+        pooling: str = 'mean', # mean, max, mean_max, token
-+        idx_conditioned_on: int = None,
-     ):
-         """
-         Initializes the SequenceClassifier module.
-@@ -37,6 +39,7 @@ class SequenceClassifier(nn.Module):
-         super().__init__()
-         self.log_softmax = log_softmax
-         self._idx_conditioned_on = idx_conditioned_on
-+        self.pooling = pooling
-         self.mlp = MultiLayerPerceptron(
-             hidden_size=hidden_size,
-             num_classes=num_classes,
-@@ -48,7 +51,24 @@ class SequenceClassifier(nn.Module):
-         if use_transformer_init:
-             self.apply(lambda module: transformer_weights_init(module, xavier=False))
- 
--    def forward(self, hidden_states):
-+    def forward(self, hidden_states, subtoken_mask=None):
-         hidden_states = self.dropout(hidden_states)
--        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
-+        if self.pooling=='token':
-+            pooled = hidden_states[:, self._idx_conditioned_on]
-+        else:
-+            if subtoken_mask==None:
-+                ct=hidden_states.shape[1] # Seq len
-+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
-+            else:
-+                ct = torch.sum(subtoken_mask,axis=1).unsqueeze(1)
-+            pooled_sum = torch.sum(hidden_states,axis=1)
-+            pp(ct.shape,pooled_sum.shape)
-+            if self.pooling=='mean' or self.pooling == 'mean_max':
-+                pooled_mean = pooled_sum//ct
-+            if self.pooling=='max' or self.pooling=='mean_max':
-+                pooled_max = torch.max(hidden_states,axis=1)[0]
-+            pooled=pooled_mean if self.pooling=='mean' else \
-+                pooled_max if self.pooling=='max' else \
-+                    torch.cat([pooled_mean,pooled_max],axis=-1)
-+        logits = self.mlp(pooled)
-         return logits
-diff --git a/experiment/info.log b/experiment/info.log
-index 2471fe9..e69de29 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,300 +0,0 @@
--[INFO] - GPU available: True, used: False
--[INFO] - TPU available: None, using: 0 TPU cores
--[INFO] - shuffling train set
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        184
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.23       0.34       0.53        594
--- (label_id: 3)                                          3.06      25.42       5.46         59
--. (label_id: 4)                                         47.22      12.98      20.36        524
--: (label_id: 5)                                          0.00       0.00       0.00         18
--; (label_id: 6)                                          0.00       0.00       0.00         13
--? (label_id: 7)                                          8.45       6.32       7.23         95
--— (label_id: 8)                                          0.00       0.00       0.00         12
--… (label_id: 9)                                          0.00       0.00       0.00          0
---------------------
--micro avg                                                6.05       6.05       6.05       1503
--macro avg                                                6.66       5.01       3.73       1503
--weighted avg                                            17.61       6.05       7.98       1503
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00         92
---------------------
--micro avg                                              100.00     100.00     100.00         92
--macro avg                                              100.00     100.00     100.00         92
--weighted avg                                           100.00     100.00     100.00         92
--
--[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 1.5848931924611143e-08
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
--will be used during training (effective maximum steps = 3192) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 3192
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        202
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.62       0.45       0.70        669
--- (label_id: 3)                                          3.48      27.27       6.17         66
--. (label_id: 4)                                         45.06      13.01      20.19        561
--: (label_id: 5)                                          1.52       6.67       2.47         15
--; (label_id: 6)                                          0.00       0.00       0.00         15
--? (label_id: 7)                                          8.70       7.32       7.95         82
--— (label_id: 8)                                          0.00       0.00       0.00         13
--… (label_id: 9)                                          0.00       0.00       0.00          1
---------------------
--micro avg                                                6.20       6.20       6.20       1628
--macro avg                                                6.04       5.47       3.75       1628
--weighted avg                                            16.79       6.20       7.92       1628
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        101
---------------------
--micro avg                                              100.00     100.00     100.00        101
--macro avg                                              100.00     100.00     100.00        101
--weighted avg                                           100.00     100.00     100.00        101
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.34       0.73       0.46       4402
--! (label_id: 1)                                          0.42      13.95       0.82        129
--, (label_id: 2)                                          2.53       0.64       1.03      15243
--- (label_id: 3)                                          2.45      21.03       4.38       1322
--. (label_id: 4)                                         44.00      11.40      18.11      12542
--: (label_id: 5)                                          0.43       1.41       0.65        354
--; (label_id: 6)                                          0.00       0.00       0.00        163
--? (label_id: 7)                                          4.16       6.27       5.00       1117
--— (label_id: 8)                                          3.00       0.61       1.02        488
--… (label_id: 9)                                          0.97       6.17       1.68         81
---------------------
--micro avg                                                5.41       5.41       5.41      35841
--macro avg                                                5.83       6.22       3.32      35841
--weighted avg                                            16.78       5.41       7.18      35841
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2201
---------------------
--micro avg                                              100.00     100.00     100.00       2201
--macro avg                                              100.00     100.00     100.00       2201
--weighted avg                                           100.00     100.00     100.00       2201
--
--[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.20       0.43       0.27       4226
--! (label_id: 1)                                          0.44      14.17       0.86        127
--, (label_id: 2)                                          1.93       0.49       0.78      14611
--- (label_id: 3)                                          2.23      19.56       4.01       1237
--. (label_id: 4)                                         43.37      11.25      17.86      11977
--: (label_id: 5)                                          0.68       2.34       1.05        342
--; (label_id: 6)                                          0.00       0.00       0.00        129
--? (label_id: 7)                                          5.16       7.47       6.10       1058
--— (label_id: 8)                                          2.15       0.49       0.80        409
--… (label_id: 9)                                          0.69       4.23       1.19         71
---------------------
--micro avg                                                5.23       5.23       5.23      34187
--macro avg                                                5.68       6.04       3.29      34187
--weighted avg                                            16.32       5.23       6.98      34187
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2113
---------------------
--micro avg                                              100.00     100.00     100.00       2113
--macro avg                                              100.00     100.00     100.00       2113
--weighted avg                                           100.00     100.00     100.00       2113
--
--[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.61       0.39       4228
--! (label_id: 1)                                          0.30       8.28       0.58        145
--, (label_id: 2)                                          2.27       0.58       0.92      14495
--- (label_id: 3)                                          2.64      21.78       4.70       1327
--. (label_id: 4)                                         44.87      11.66      18.51      12193
--: (label_id: 5)                                          0.60       1.93       0.91        362
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.19       6.40       5.07       1078
--— (label_id: 8)                                          1.16       0.22       0.37        459
--… (label_id: 9)                                          0.85       4.17       1.41         96
---------------------
--micro avg                                                5.54       5.54       5.54      34547
--macro avg                                                5.72       5.56       3.29      34547
--weighted avg                                            17.08       5.54       7.33      34547
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2114
---------------------
--micro avg                                              100.00     100.00     100.00       2114
--macro avg                                              100.00     100.00     100.00       2114
--weighted avg                                           100.00     100.00     100.00       2114
--
--[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.29       0.63       0.40       4444
--! (label_id: 1)                                          0.38      10.67       0.74        150
--, (label_id: 2)                                          2.32       0.59       0.94      15290
--- (label_id: 3)                                          2.34      20.28       4.19       1292
--. (label_id: 4)                                         43.85      11.68      18.44      12599
--: (label_id: 5)                                          0.41       1.28       0.62        392
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.24       6.30       5.07       1111
--— (label_id: 8)                                          0.00       0.00       0.00        456
--… (label_id: 9)                                          0.38       2.41       0.65         83
---------------------
--micro avg                                                5.40       5.40       5.40      35981
--macro avg                                                5.42       5.38       3.11      35981
--weighted avg                                            16.59       5.40       7.22      35981
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2222
---------------------
--micro avg                                              100.00     100.00     100.00       2222
--macro avg                                              100.00     100.00     100.00       2222
--weighted avg                                           100.00     100.00     100.00       2222
--
--[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.35       0.73       0.48       3844
--! (label_id: 1)                                          0.54      14.62       1.04        130
--, (label_id: 2)                                          2.32       0.59       0.94      13056
--- (label_id: 3)                                          2.67      22.28       4.77       1194
--. (label_id: 4)                                         44.45      11.95      18.84      10791
--: (label_id: 5)                                          0.84       3.21       1.33        280
--; (label_id: 6)                                          0.00       0.00       0.00        140
--? (label_id: 7)                                          4.17       6.56       5.10        914
--— (label_id: 8)                                          0.00       0.00       0.00        401
--… (label_id: 9)                                          0.48       2.63       0.81         76
---------------------
--micro avg                                                5.68       5.68       5.68      30826
--macro avg                                                5.58       6.26       3.33      30826
--weighted avg                                            16.82       5.68       7.41      30826
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1922
---------------------
--micro avg                                              100.00     100.00     100.00       1922
--macro avg                                              100.00     100.00     100.00       1922
--weighted avg                                           100.00     100.00     100.00       1922
--
--[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.60       0.39       3970
--! (label_id: 1)                                          0.35      10.66       0.68        122
--, (label_id: 2)                                          2.09       0.53       0.85      13469
--- (label_id: 3)                                          2.29      19.32       4.10       1201
--. (label_id: 4)                                         43.43      11.24      17.86      11227
--: (label_id: 5)                                          0.63       2.30       0.99        304
--; (label_id: 6)                                          0.00       0.00       0.00        141
--? (label_id: 7)                                          4.52       6.86       5.45       1006
--— (label_id: 8)                                          1.15       0.23       0.38        444
--… (label_id: 9)                                          0.45       2.67       0.78         75
---------------------
--micro avg                                                5.26       5.26       5.26      31959
--macro avg                                                5.52       5.44       3.15      31959
--weighted avg                                            16.42       5.26       7.02      31959
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1985
---------------------
--micro avg                                              100.00     100.00     100.00       1985
--macro avg                                              100.00     100.00     100.00       1985
--weighted avg                                           100.00     100.00     100.00       1985
--
--[INFO] - Epoch 5, step 2394: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.23       0.48       0.31       4126
--! (label_id: 1)                                          0.29       9.40       0.56        117
--, (label_id: 2)                                          1.91       0.49       0.77      14019
--- (label_id: 3)                                          2.52      22.59       4.53       1164
--. (label_id: 4)                                         44.15      11.65      18.44      11789
--: (label_id: 5)                                          0.72       2.41       1.11        332
--; (label_id: 6)                                          0.56       0.61       0.58        165
--? (label_id: 7)                                          3.89       6.53       4.88        980
--— (label_id: 8)                                          2.30       0.47       0.77        430
--… (label_id: 9)                                          1.18       8.33       2.07         60
---------------------
--micro avg                                                5.47       5.47       5.47      33182
--macro avg                                                5.77       6.30       3.40      33182
--weighted avg                                            16.77       5.47       7.25      33182
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2063
---------------------
--micro avg                                              100.00     100.00     100.00       2063
--macro avg                                              100.00     100.00     100.00       2063
--weighted avg                                           100.00     100.00     100.00       2063
--
--[INFO] - Epoch 6, step 2793: val_loss was not in top 3
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index fa37b4c..43fc93d 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             self.hparams.model.punct_class_weights=None
- 
-+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
-+        self.mlp = MultiLayerPerceptron(
-+            self.transformer.config.hidden_size,
-+            self.transformer.config.hidden_size,
-+            num_layers=self.hparams.model.mlp.num_fc_layers, 
-+            activation=self.hparams.model.mlp.activation, 
-+            log_softmax=self.hparams.model.mlp.log_softmax
-+        )
-+
-         self.punct_classifier = TokenClassifier(
-             hidden_size=self.transformer.config.hidden_size,
-             num_classes=len(self.labels_to_ids),
-@@ -80,6 +89,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-             log_softmax=self.hparams.model.domain_head.log_softmax,
-             dropout=self.hparams.model.domain_head.fc_dropout,
-             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
-+            pooling=self.hparams.model.domain_head.pooling,
-+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
-         )
- 
-         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
-@@ -119,14 +130,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
-         self.freeze()
- 
--    def forward(self, input_ids, attention_mask, domain_ids=None):
-+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
-         hidden_states = self.transformer(
-             input_ids=input_ids, attention_mask=attention_mask
-         )[0]
-+        hidden_states = self.dropout(hidden_states)
-+        hidden_states = self.mlp(hidden_states)
-         punct_logits = self.punct_classifier(hidden_states=hidden_states)
-         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
-         domain_logits = self.domain_classifier(
--            hidden_states=reverse_grad_hidden_states)
-+            hidden_states=reverse_grad_hidden_states,
-+            subtoken_mask=subtoken_mask)
-         return punct_logits, domain_logits
- 
-     def _make_step(self, batch):
-@@ -136,7 +150,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         punct_labels = batch['labels']
-         domain_labels = batch['domain']
-         punct_logits, domain_logits = self(
--            input_ids=input_ids, attention_mask=attention_mask
-+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
-         )
-         punctuation_loss = self.punctuation_loss(
-             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/lightning_logs.txt
deleted file mode 100644
index 9fa336d..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/lightning_logs.txt
+++ /dev/null
@@ -1,24 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-Using native 16bit precision.
-Global seed set to 42
-initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | dropout             | Dropout              | 0     
-2 | mlp                 | MultiLayerPerceptron | 131 K 
-3 | punct_classifier    | TokenClassifier      | 2.6 K 
-4 | domain_classifier   | SequenceClassifier   | 257   
-5 | punctuation_loss    | FocalDiceLoss        | 0     
-6 | domain_loss         | CrossEntropyLoss     | 0     
-7 | agg_loss            | AggregatorLoss       | 0     
-8 | punct_class_report  | ClassificationReport | 0     
-9 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-167 K     Trainable params
-13.4 M    Non-trainable params
-13.6 M    Total params
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/nemo_error_log.txt
deleted file mode 100644
index 93e0364..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/nemo_error_log.txt
+++ /dev/null
@@ -1,10 +0,0 @@
-[NeMo W 2021-02-08 11:03:15 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 11:03:24 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:03:24 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:03:26 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 6c8beed..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,12 +0,0 @@
-[NeMo I 2021-02-08 11:03:15 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15
-[NeMo I 2021-02-08 11:03:15 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-08 11:03:15 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 11:03:24 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:03:24 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:03:26 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/git-info.log
deleted file mode 100644
index 4010e76..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/git-info.log
+++ /dev/null
@@ -1,624 +0,0 @@
-commit hash: 939a671c8c117db6975316767ced5d95449e2b27
-diff --git a/README.md b/README.md
-index d52dba5..0b403d3 100644
---- a/README.md
-+++ b/README.md
-@@ -333,4 +333,71 @@ label                                                precision    recall       f
-  'punct_f1': 0.5858508944511414,
-  'punct_precision': 0.30176490545272827,
-  'punct_recall': 10.0,
-- 'test_loss': 0.8140875697135925}
-\ No newline at end of file
-+ 'test_loss': 0.8140875697135925}
-+
-+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
-+
-+ (label_id: 0)                                          62.15     100.00      76.66       5154
-+! (label_id: 1)                                          0.00       0.00       0.00        108
-+, (label_id: 2)                                          0.00       0.00       0.00      18022
-+- (label_id: 3)                                          0.00       0.00       0.00       1557
-+. (label_id: 4)                                         41.74      94.01      57.81      15164
-+: (label_id: 5)                                          0.00       0.00       0.00        319
-+; (label_id: 6)                                          0.00       0.00       0.00         88
-+? (label_id: 7)                                          0.00       0.00       0.00       1217
-+ (label_id: 8)                                          0.00       0.00       0.00        752
-+… (label_id: 9)                                          0.00       0.00       0.00         67
-+-------------------
-+micro avg                                               45.72      45.72      45.72      42448
-+macro avg                                               10.39      19.40      13.45      42448
-+weighted avg                                            22.46      45.72      29.96      42448
-+{ 'punct_f1': 13.446383476257324,
-+ 'punct_precision': 10.388500213623047,
-+ 'punct_recall': 19.400554656982422,
-+ 'test_loss': 0.44148480892181396}
-+
-+
-+ ## Log for 8/2/2021
-+
-+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
-+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
-+
-+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
-+End frozen 
-+micro avg                                               41.42      41.42      41.42      33406
-+macro avg                                               11.01      13.54      11.07      33406
-+weighted avg                                            34.88      41.42      34.20      33406
-+
-+1st layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               36.65      36.65      36.65      33463
-+macro avg                                               10.71       9.91       8.26      33463
-+weighted avg                                            34.32      36.65      31.08      33463
-+
-+2nd layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               35.72      35.72      35.72      42448
-+macro avg                                                3.57      10.00       5.26      42448
-+weighted avg                                            12.76      35.72      18.81      42448
-+
-+{'punct_f1': 5.264181137084961,
-+ 'punct_precision': 3.572371006011963,
-+ 'punct_recall': 10.0,
-+ 'test_loss': 18.49854850769043}
-+
-+
-+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
-+alpha from 3->4 seems to reduce convergence rate.
-+
-+micro avg                                               50.98      50.98      50.98      33463
-+macro avg                                               25.99      25.38      23.38      33463
-+weighted avg                                            50.31      50.98      48.27      33463
-+
-+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
-+micro avg                                               58.55      58.55      58.55      39340
-+macro avg                                               30.02      29.74      29.52      39340
-+weighted avg                                            57.91      58.55      57.51      39340
-+
-+still increasing?!
-+{'punct_f1': 29.523975372314453,
-+ 'punct_precision': 30.015613555908203,
-+ 'punct_recall': 29.738296508789062,
-+ 'test_loss': 0.3690211772918701}
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index fe58670..a4a012a 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,49 +1,49 @@
- seed: 42
- trainer:
--    # gpus: 1 # the number of gpus, 0 for CPU
--    # num_nodes: 1
--    # max_epochs: 2
--    # max_steps: null # precedence over max_epochs
--    # accumulate_grad_batches: 4 # accumulates grads every k batches
--    # gradient_clip_val: 0
--    # amp_level: O1 # O1/O2 for mixed precision
--    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
--    # checkpoint_callback: false  # Provided by exp_manager
--    # logger: false #false  # Provided by exp_manager
--    # log_every_n_steps: 1  # Interval of logging.
--    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # resume_from_checkpoint: null
--
--    gpus: 0 # the number of gpus, 0 for CPU
-+    gpus: 1 # the number of gpus, 0 for CPU
-     num_nodes: 1
--    max_epochs: 8
-+    max_epochs: 10
-     max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 1 # accumulates grads every k batches
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-     gradient_clip_val: 0
--    amp_level: O0 # O1/O2 for mixed precision
--    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
-+    amp_level: O1 # O1/O2 for mixed precision
-+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    accelerator: ddp
-     checkpoint_callback: false  # Provided by exp_manager
-     logger: false #false  # Provided by exp_manager
-     log_every_n_steps: 1  # Interval of logging.
-     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    reload_dataloaders_every_epoch: true
-     resume_from_checkpoint: null
- 
-+    # gpus: 0 # the number of gpus, 0 for CPU
-+    # num_nodes: 1
-+    # max_epochs: 8
-+    # max_steps: null # precedence over max_epochs
-+    # accumulate_grad_batches: 1 # accumulates grads every k batches
-+    # gradient_clip_val: 0
-+    # amp_level: O0 # O1/O2 for mixed precision
-+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # # accelerator: ddp
-+    # checkpoint_callback: false  # Provided by exp_manager
-+    # logger: false #false  # Provided by exp_manager
-+    # log_every_n_steps: 1  # Interval of logging.
-+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    # reload_dataloaders_every_epoch: true
-+    # resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /home/nxingyu2/data # /root/data # 
--tmp_path: /home/nxingyu2/data/tmp # /tmp # 
-+base_path: /home/nxingyu/data # /root/data # 
-+tmp_path: /home/nxingyu/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
--    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-     unfrozen: 0
--    maximum_unfrozen: 2
-+    maximum_unfrozen: 1
-     unfreeze_step: 1
-     # unfreeze_every: 3
-     punct_label_ids:
-@@ -58,10 +58,10 @@ model:
-         - "—"
-         - "…"
- 
--    punct_class_weights: true
-+    punct_class_weights: false
-     
-     dataset:
--        data_dir: /home/nxingyu2/data # /root/data # 
-+        data_dir: /home/nxingyu/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -106,6 +106,12 @@ model:
-         config: null
-         # unfrozen_layers: 1
- 
-+    mlp:
-+        num_fc_layers: 2
-+        fc_dropout: 0.1
-+        log_softmax: false
-+        activation: 'relu'
-+        
-     punct_head:
-         punct_num_fc_layers: 1
-         fc_dropout: 0.1
-@@ -122,17 +128,19 @@ model:
-         use_transformer_init: true
-         loss: 'cel'
-         gamma: 0.1 # coefficient of gradient reversal
-+        pooling: 'mean'
-+        idx_conditioned_on: 0
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 3
-+        alpha: 4
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: novograd
-+        name: adamw
-         lr: 1e-3
-         weight_decay: 0.00
-         sched:
-diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
-index d4ff927..f84c6e0 100644
---- a/experiment/core/layers/sequence_classifier.py
-+++ b/experiment/core/layers/sequence_classifier.py
-@@ -1,3 +1,4 @@
-+import torch
- from torch import nn
- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
- from core.utils import transformer_weights_init
-@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
-         log_softmax: bool = True,
-         dropout: float = 0.0,
-         use_transformer_init: bool = True,
--        idx_conditioned_on: int = 0,
-+        pooling: str = 'mean', # mean, max, mean_max, token
-+        idx_conditioned_on: int = None,
-     ):
-         """
-         Initializes the SequenceClassifier module.
-@@ -37,6 +39,7 @@ class SequenceClassifier(nn.Module):
-         super().__init__()
-         self.log_softmax = log_softmax
-         self._idx_conditioned_on = idx_conditioned_on
-+        self.pooling = pooling
-         self.mlp = MultiLayerPerceptron(
-             hidden_size=hidden_size,
-             num_classes=num_classes,
-@@ -48,7 +51,23 @@ class SequenceClassifier(nn.Module):
-         if use_transformer_init:
-             self.apply(lambda module: transformer_weights_init(module, xavier=False))
- 
--    def forward(self, hidden_states):
-+    def forward(self, hidden_states, subtoken_mask=None):
-         hidden_states = self.dropout(hidden_states)
--        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
-+        if self.pooling=='token':
-+            pooled = hidden_states[:, self._idx_conditioned_on]
-+        else:
-+            if subtoken_mask==None:
-+                ct=hidden_states.shape[1] # Seq len
-+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
-+            else:
-+                ct = torch.sum(subtoken_mask,axis=1).unsqueeze(1)
-+            pooled_sum = torch.sum(hidden_states,axis=1)            
-+            if self.pooling=='mean' or self.pooling == 'mean_max':
-+                pooled_mean = torch.div(pooled_sum,ct)
-+            if self.pooling=='max' or self.pooling=='mean_max':
-+                pooled_max = torch.max(hidden_states,axis=1)[0]
-+            pooled=pooled_mean if self.pooling=='mean' else \
-+                pooled_max if self.pooling=='max' else \
-+                    torch.cat([pooled_mean,pooled_max],axis=-1)
-+        logits = self.mlp(pooled)
-         return logits
-diff --git a/experiment/info.log b/experiment/info.log
-index 2471fe9..e69de29 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,300 +0,0 @@
--[INFO] - GPU available: True, used: False
--[INFO] - TPU available: None, using: 0 TPU cores
--[INFO] - shuffling train set
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        184
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.23       0.34       0.53        594
--- (label_id: 3)                                          3.06      25.42       5.46         59
--. (label_id: 4)                                         47.22      12.98      20.36        524
--: (label_id: 5)                                          0.00       0.00       0.00         18
--; (label_id: 6)                                          0.00       0.00       0.00         13
--? (label_id: 7)                                          8.45       6.32       7.23         95
--— (label_id: 8)                                          0.00       0.00       0.00         12
--… (label_id: 9)                                          0.00       0.00       0.00          0
---------------------
--micro avg                                                6.05       6.05       6.05       1503
--macro avg                                                6.66       5.01       3.73       1503
--weighted avg                                            17.61       6.05       7.98       1503
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00         92
---------------------
--micro avg                                              100.00     100.00     100.00         92
--macro avg                                              100.00     100.00     100.00         92
--weighted avg                                           100.00     100.00     100.00         92
--
--[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 1.5848931924611143e-08
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
--will be used during training (effective maximum steps = 3192) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 3192
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        202
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.62       0.45       0.70        669
--- (label_id: 3)                                          3.48      27.27       6.17         66
--. (label_id: 4)                                         45.06      13.01      20.19        561
--: (label_id: 5)                                          1.52       6.67       2.47         15
--; (label_id: 6)                                          0.00       0.00       0.00         15
--? (label_id: 7)                                          8.70       7.32       7.95         82
--— (label_id: 8)                                          0.00       0.00       0.00         13
--… (label_id: 9)                                          0.00       0.00       0.00          1
---------------------
--micro avg                                                6.20       6.20       6.20       1628
--macro avg                                                6.04       5.47       3.75       1628
--weighted avg                                            16.79       6.20       7.92       1628
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        101
---------------------
--micro avg                                              100.00     100.00     100.00        101
--macro avg                                              100.00     100.00     100.00        101
--weighted avg                                           100.00     100.00     100.00        101
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.34       0.73       0.46       4402
--! (label_id: 1)                                          0.42      13.95       0.82        129
--, (label_id: 2)                                          2.53       0.64       1.03      15243
--- (label_id: 3)                                          2.45      21.03       4.38       1322
--. (label_id: 4)                                         44.00      11.40      18.11      12542
--: (label_id: 5)                                          0.43       1.41       0.65        354
--; (label_id: 6)                                          0.00       0.00       0.00        163
--? (label_id: 7)                                          4.16       6.27       5.00       1117
--— (label_id: 8)                                          3.00       0.61       1.02        488
--… (label_id: 9)                                          0.97       6.17       1.68         81
---------------------
--micro avg                                                5.41       5.41       5.41      35841
--macro avg                                                5.83       6.22       3.32      35841
--weighted avg                                            16.78       5.41       7.18      35841
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2201
---------------------
--micro avg                                              100.00     100.00     100.00       2201
--macro avg                                              100.00     100.00     100.00       2201
--weighted avg                                           100.00     100.00     100.00       2201
--
--[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.20       0.43       0.27       4226
--! (label_id: 1)                                          0.44      14.17       0.86        127
--, (label_id: 2)                                          1.93       0.49       0.78      14611
--- (label_id: 3)                                          2.23      19.56       4.01       1237
--. (label_id: 4)                                         43.37      11.25      17.86      11977
--: (label_id: 5)                                          0.68       2.34       1.05        342
--; (label_id: 6)                                          0.00       0.00       0.00        129
--? (label_id: 7)                                          5.16       7.47       6.10       1058
--— (label_id: 8)                                          2.15       0.49       0.80        409
--… (label_id: 9)                                          0.69       4.23       1.19         71
---------------------
--micro avg                                                5.23       5.23       5.23      34187
--macro avg                                                5.68       6.04       3.29      34187
--weighted avg                                            16.32       5.23       6.98      34187
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2113
---------------------
--micro avg                                              100.00     100.00     100.00       2113
--macro avg                                              100.00     100.00     100.00       2113
--weighted avg                                           100.00     100.00     100.00       2113
--
--[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.61       0.39       4228
--! (label_id: 1)                                          0.30       8.28       0.58        145
--, (label_id: 2)                                          2.27       0.58       0.92      14495
--- (label_id: 3)                                          2.64      21.78       4.70       1327
--. (label_id: 4)                                         44.87      11.66      18.51      12193
--: (label_id: 5)                                          0.60       1.93       0.91        362
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.19       6.40       5.07       1078
--— (label_id: 8)                                          1.16       0.22       0.37        459
--… (label_id: 9)                                          0.85       4.17       1.41         96
---------------------
--micro avg                                                5.54       5.54       5.54      34547
--macro avg                                                5.72       5.56       3.29      34547
--weighted avg                                            17.08       5.54       7.33      34547
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2114
---------------------
--micro avg                                              100.00     100.00     100.00       2114
--macro avg                                              100.00     100.00     100.00       2114
--weighted avg                                           100.00     100.00     100.00       2114
--
--[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.29       0.63       0.40       4444
--! (label_id: 1)                                          0.38      10.67       0.74        150
--, (label_id: 2)                                          2.32       0.59       0.94      15290
--- (label_id: 3)                                          2.34      20.28       4.19       1292
--. (label_id: 4)                                         43.85      11.68      18.44      12599
--: (label_id: 5)                                          0.41       1.28       0.62        392
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.24       6.30       5.07       1111
--— (label_id: 8)                                          0.00       0.00       0.00        456
--… (label_id: 9)                                          0.38       2.41       0.65         83
---------------------
--micro avg                                                5.40       5.40       5.40      35981
--macro avg                                                5.42       5.38       3.11      35981
--weighted avg                                            16.59       5.40       7.22      35981
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2222
---------------------
--micro avg                                              100.00     100.00     100.00       2222
--macro avg                                              100.00     100.00     100.00       2222
--weighted avg                                           100.00     100.00     100.00       2222
--
--[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.35       0.73       0.48       3844
--! (label_id: 1)                                          0.54      14.62       1.04        130
--, (label_id: 2)                                          2.32       0.59       0.94      13056
--- (label_id: 3)                                          2.67      22.28       4.77       1194
--. (label_id: 4)                                         44.45      11.95      18.84      10791
--: (label_id: 5)                                          0.84       3.21       1.33        280
--; (label_id: 6)                                          0.00       0.00       0.00        140
--? (label_id: 7)                                          4.17       6.56       5.10        914
--— (label_id: 8)                                          0.00       0.00       0.00        401
--… (label_id: 9)                                          0.48       2.63       0.81         76
---------------------
--micro avg                                                5.68       5.68       5.68      30826
--macro avg                                                5.58       6.26       3.33      30826
--weighted avg                                            16.82       5.68       7.41      30826
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1922
---------------------
--micro avg                                              100.00     100.00     100.00       1922
--macro avg                                              100.00     100.00     100.00       1922
--weighted avg                                           100.00     100.00     100.00       1922
--
--[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.60       0.39       3970
--! (label_id: 1)                                          0.35      10.66       0.68        122
--, (label_id: 2)                                          2.09       0.53       0.85      13469
--- (label_id: 3)                                          2.29      19.32       4.10       1201
--. (label_id: 4)                                         43.43      11.24      17.86      11227
--: (label_id: 5)                                          0.63       2.30       0.99        304
--; (label_id: 6)                                          0.00       0.00       0.00        141
--? (label_id: 7)                                          4.52       6.86       5.45       1006
--— (label_id: 8)                                          1.15       0.23       0.38        444
--… (label_id: 9)                                          0.45       2.67       0.78         75
---------------------
--micro avg                                                5.26       5.26       5.26      31959
--macro avg                                                5.52       5.44       3.15      31959
--weighted avg                                            16.42       5.26       7.02      31959
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1985
---------------------
--micro avg                                              100.00     100.00     100.00       1985
--macro avg                                              100.00     100.00     100.00       1985
--weighted avg                                           100.00     100.00     100.00       1985
--
--[INFO] - Epoch 5, step 2394: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.23       0.48       0.31       4126
--! (label_id: 1)                                          0.29       9.40       0.56        117
--, (label_id: 2)                                          1.91       0.49       0.77      14019
--- (label_id: 3)                                          2.52      22.59       4.53       1164
--. (label_id: 4)                                         44.15      11.65      18.44      11789
--: (label_id: 5)                                          0.72       2.41       1.11        332
--; (label_id: 6)                                          0.56       0.61       0.58        165
--? (label_id: 7)                                          3.89       6.53       4.88        980
--— (label_id: 8)                                          2.30       0.47       0.77        430
--… (label_id: 9)                                          1.18       8.33       2.07         60
---------------------
--micro avg                                                5.47       5.47       5.47      33182
--macro avg                                                5.77       6.30       3.40      33182
--weighted avg                                            16.77       5.47       7.25      33182
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2063
---------------------
--micro avg                                              100.00     100.00     100.00       2063
--macro avg                                              100.00     100.00     100.00       2063
--weighted avg                                           100.00     100.00     100.00       2063
--
--[INFO] - Epoch 6, step 2793: val_loss was not in top 3
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index fa37b4c..43fc93d 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             self.hparams.model.punct_class_weights=None
- 
-+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
-+        self.mlp = MultiLayerPerceptron(
-+            self.transformer.config.hidden_size,
-+            self.transformer.config.hidden_size,
-+            num_layers=self.hparams.model.mlp.num_fc_layers, 
-+            activation=self.hparams.model.mlp.activation, 
-+            log_softmax=self.hparams.model.mlp.log_softmax
-+        )
-+
-         self.punct_classifier = TokenClassifier(
-             hidden_size=self.transformer.config.hidden_size,
-             num_classes=len(self.labels_to_ids),
-@@ -80,6 +89,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-             log_softmax=self.hparams.model.domain_head.log_softmax,
-             dropout=self.hparams.model.domain_head.fc_dropout,
-             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
-+            pooling=self.hparams.model.domain_head.pooling,
-+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
-         )
- 
-         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
-@@ -119,14 +130,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
-         self.freeze()
- 
--    def forward(self, input_ids, attention_mask, domain_ids=None):
-+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
-         hidden_states = self.transformer(
-             input_ids=input_ids, attention_mask=attention_mask
-         )[0]
-+        hidden_states = self.dropout(hidden_states)
-+        hidden_states = self.mlp(hidden_states)
-         punct_logits = self.punct_classifier(hidden_states=hidden_states)
-         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
-         domain_logits = self.domain_classifier(
--            hidden_states=reverse_grad_hidden_states)
-+            hidden_states=reverse_grad_hidden_states,
-+            subtoken_mask=subtoken_mask)
-         return punct_logits, domain_logits
- 
-     def _make_step(self, batch):
-@@ -136,7 +150,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         punct_labels = batch['labels']
-         domain_labels = batch['domain']
-         punct_logits, domain_logits = self(
--            input_ids=input_ids, attention_mask=attention_mask
-+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
-         )
-         punctuation_loss = self.punctuation_loss(
-             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/lightning_logs.txt
deleted file mode 100644
index 0037c7a..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/lightning_logs.txt
+++ /dev/null
@@ -1,25 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-Using native 16bit precision.
-Global seed set to 42
-initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | dropout             | Dropout              | 0     
-2 | mlp                 | MultiLayerPerceptron | 131 K 
-3 | punct_classifier    | TokenClassifier      | 2.6 K 
-4 | domain_classifier   | SequenceClassifier   | 257   
-5 | punctuation_loss    | FocalDiceLoss        | 0     
-6 | domain_loss         | CrossEntropyLoss     | 0     
-7 | agg_loss            | AggregatorLoss       | 0     
-8 | punct_class_report  | ClassificationReport | 0     
-9 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-167 K     Trainable params
-13.4 M    Non-trainable params
-13.6 M    Total params
-LR finder stopped early due to diverging loss.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/nemo_error_log.txt
deleted file mode 100644
index 0a0389f..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/nemo_error_log.txt
+++ /dev/null
@@ -1,13 +0,0 @@
-[NeMo W 2021-02-08 11:04:26 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 11:04:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:04:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:04:37 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:04:42 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 312482b..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,15 +0,0 @@
-[NeMo I 2021-02-08 11:04:26 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26
-[NeMo I 2021-02-08 11:04:26 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-08 11:04:26 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 11:04:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:04:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:04:37 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:04:42 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/git-info.log
deleted file mode 100644
index 13fff12..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/git-info.log
+++ /dev/null
@@ -1,627 +0,0 @@
-commit hash: 939a671c8c117db6975316767ced5d95449e2b27
-diff --git a/README.md b/README.md
-index d52dba5..0b403d3 100644
---- a/README.md
-+++ b/README.md
-@@ -333,4 +333,71 @@ label                                                precision    recall       f
-  'punct_f1': 0.5858508944511414,
-  'punct_precision': 0.30176490545272827,
-  'punct_recall': 10.0,
-- 'test_loss': 0.8140875697135925}
-\ No newline at end of file
-+ 'test_loss': 0.8140875697135925}
-+
-+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
-+
-+ (label_id: 0)                                          62.15     100.00      76.66       5154
-+! (label_id: 1)                                          0.00       0.00       0.00        108
-+, (label_id: 2)                                          0.00       0.00       0.00      18022
-+- (label_id: 3)                                          0.00       0.00       0.00       1557
-+. (label_id: 4)                                         41.74      94.01      57.81      15164
-+: (label_id: 5)                                          0.00       0.00       0.00        319
-+; (label_id: 6)                                          0.00       0.00       0.00         88
-+? (label_id: 7)                                          0.00       0.00       0.00       1217
-+ (label_id: 8)                                          0.00       0.00       0.00        752
-+… (label_id: 9)                                          0.00       0.00       0.00         67
-+-------------------
-+micro avg                                               45.72      45.72      45.72      42448
-+macro avg                                               10.39      19.40      13.45      42448
-+weighted avg                                            22.46      45.72      29.96      42448
-+{ 'punct_f1': 13.446383476257324,
-+ 'punct_precision': 10.388500213623047,
-+ 'punct_recall': 19.400554656982422,
-+ 'test_loss': 0.44148480892181396}
-+
-+
-+ ## Log for 8/2/2021
-+
-+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
-+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
-+
-+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
-+End frozen 
-+micro avg                                               41.42      41.42      41.42      33406
-+macro avg                                               11.01      13.54      11.07      33406
-+weighted avg                                            34.88      41.42      34.20      33406
-+
-+1st layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               36.65      36.65      36.65      33463
-+macro avg                                               10.71       9.91       8.26      33463
-+weighted avg                                            34.32      36.65      31.08      33463
-+
-+2nd layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               35.72      35.72      35.72      42448
-+macro avg                                                3.57      10.00       5.26      42448
-+weighted avg                                            12.76      35.72      18.81      42448
-+
-+{'punct_f1': 5.264181137084961,
-+ 'punct_precision': 3.572371006011963,
-+ 'punct_recall': 10.0,
-+ 'test_loss': 18.49854850769043}
-+
-+
-+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
-+alpha from 3->4 seems to reduce convergence rate.
-+
-+micro avg                                               50.98      50.98      50.98      33463
-+macro avg                                               25.99      25.38      23.38      33463
-+weighted avg                                            50.31      50.98      48.27      33463
-+
-+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
-+micro avg                                               58.55      58.55      58.55      39340
-+macro avg                                               30.02      29.74      29.52      39340
-+weighted avg                                            57.91      58.55      57.51      39340
-+
-+still increasing?!
-+{'punct_f1': 29.523975372314453,
-+ 'punct_precision': 30.015613555908203,
-+ 'punct_recall': 29.738296508789062,
-+ 'test_loss': 0.3690211772918701}
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index fe58670..91dbc8c 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,49 +1,49 @@
- seed: 42
- trainer:
--    # gpus: 1 # the number of gpus, 0 for CPU
--    # num_nodes: 1
--    # max_epochs: 2
--    # max_steps: null # precedence over max_epochs
--    # accumulate_grad_batches: 4 # accumulates grads every k batches
--    # gradient_clip_val: 0
--    # amp_level: O1 # O1/O2 for mixed precision
--    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
--    # checkpoint_callback: false  # Provided by exp_manager
--    # logger: false #false  # Provided by exp_manager
--    # log_every_n_steps: 1  # Interval of logging.
--    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # resume_from_checkpoint: null
--
--    gpus: 0 # the number of gpus, 0 for CPU
-+    gpus: 1 # the number of gpus, 0 for CPU
-     num_nodes: 1
--    max_epochs: 8
-+    max_epochs: 10
-     max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 1 # accumulates grads every k batches
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-     gradient_clip_val: 0
--    amp_level: O0 # O1/O2 for mixed precision
--    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
-+    amp_level: O1 # O1/O2 for mixed precision
-+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    accelerator: ddp
-     checkpoint_callback: false  # Provided by exp_manager
-     logger: false #false  # Provided by exp_manager
-     log_every_n_steps: 1  # Interval of logging.
-     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    reload_dataloaders_every_epoch: true
-     resume_from_checkpoint: null
- 
-+    # gpus: 0 # the number of gpus, 0 for CPU
-+    # num_nodes: 1
-+    # max_epochs: 8
-+    # max_steps: null # precedence over max_epochs
-+    # accumulate_grad_batches: 1 # accumulates grads every k batches
-+    # gradient_clip_val: 0
-+    # amp_level: O0 # O1/O2 for mixed precision
-+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # # accelerator: ddp
-+    # checkpoint_callback: false  # Provided by exp_manager
-+    # logger: false #false  # Provided by exp_manager
-+    # log_every_n_steps: 1  # Interval of logging.
-+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    # reload_dataloaders_every_epoch: true
-+    # resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /home/nxingyu2/data # /root/data # 
--tmp_path: /home/nxingyu2/data/tmp # /tmp # 
-+base_path: /home/nxingyu/data # /root/data # 
-+tmp_path: /home/nxingyu/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
--    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-     unfrozen: 0
--    maximum_unfrozen: 2
-+    maximum_unfrozen: 1
-     unfreeze_step: 1
-     # unfreeze_every: 3
-     punct_label_ids:
-@@ -58,10 +58,10 @@ model:
-         - "—"
-         - "…"
- 
--    punct_class_weights: true
-+    punct_class_weights: false
-     
-     dataset:
--        data_dir: /home/nxingyu2/data # /root/data # 
-+        data_dir: /home/nxingyu/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -106,6 +106,12 @@ model:
-         config: null
-         # unfrozen_layers: 1
- 
-+    mlp:
-+        num_fc_layers: 2
-+        fc_dropout: 0.1
-+        log_softmax: false
-+        activation: 'relu'
-+        
-     punct_head:
-         punct_num_fc_layers: 1
-         fc_dropout: 0.1
-@@ -122,17 +128,19 @@ model:
-         use_transformer_init: true
-         loss: 'cel'
-         gamma: 0.1 # coefficient of gradient reversal
-+        pooling: 'mean_max'
-+        idx_conditioned_on: 0
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 3
-+        alpha: 4
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: novograd
-+        name: adamw
-         lr: 1e-3
-         weight_decay: 0.00
-         sched:
-diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
-index d4ff927..f9927ac 100644
---- a/experiment/core/layers/sequence_classifier.py
-+++ b/experiment/core/layers/sequence_classifier.py
-@@ -1,3 +1,4 @@
-+import torch
- from torch import nn
- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
- from core.utils import transformer_weights_init
-@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
-         log_softmax: bool = True,
-         dropout: float = 0.0,
-         use_transformer_init: bool = True,
--        idx_conditioned_on: int = 0,
-+        pooling: str = 'mean', # mean, max, mean_max, token
-+        idx_conditioned_on: int = None,
-     ):
-         """
-         Initializes the SequenceClassifier module.
-@@ -37,8 +39,9 @@ class SequenceClassifier(nn.Module):
-         super().__init__()
-         self.log_softmax = log_softmax
-         self._idx_conditioned_on = idx_conditioned_on
-+        self.pooling = pooling
-         self.mlp = MultiLayerPerceptron(
--            hidden_size=hidden_size,
-+            hidden_size=(hidden_size*2 if pooling=='mean_max' else hidden_size),
-             num_classes=num_classes,
-             num_layers=num_layers,
-             activation=activation,
-@@ -48,7 +51,23 @@ class SequenceClassifier(nn.Module):
-         if use_transformer_init:
-             self.apply(lambda module: transformer_weights_init(module, xavier=False))
- 
--    def forward(self, hidden_states):
-+    def forward(self, hidden_states, subtoken_mask=None):
-         hidden_states = self.dropout(hidden_states)
--        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
-+        if self.pooling=='token':
-+            pooled = hidden_states[:, self._idx_conditioned_on]
-+        else:
-+            if subtoken_mask==None:
-+                ct=hidden_states.shape[1] # Seq len
-+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
-+            else:
-+                ct = torch.sum(subtoken_mask,axis=1).unsqueeze(1)
-+            pooled_sum = torch.sum(hidden_states,axis=1)            
-+            if self.pooling=='mean' or self.pooling == 'mean_max':
-+                pooled_mean = torch.div(pooled_sum,ct)
-+            if self.pooling=='max' or self.pooling=='mean_max':
-+                pooled_max = torch.max(hidden_states,axis=1)[0]
-+            pooled=pooled_mean if self.pooling=='mean' else \
-+                pooled_max if self.pooling=='max' else \
-+                    torch.cat([pooled_mean,pooled_max],axis=-1)
-+        logits = self.mlp(pooled)
-         return logits
-diff --git a/experiment/info.log b/experiment/info.log
-index 2471fe9..e69de29 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,300 +0,0 @@
--[INFO] - GPU available: True, used: False
--[INFO] - TPU available: None, using: 0 TPU cores
--[INFO] - shuffling train set
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        184
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.23       0.34       0.53        594
--- (label_id: 3)                                          3.06      25.42       5.46         59
--. (label_id: 4)                                         47.22      12.98      20.36        524
--: (label_id: 5)                                          0.00       0.00       0.00         18
--; (label_id: 6)                                          0.00       0.00       0.00         13
--? (label_id: 7)                                          8.45       6.32       7.23         95
--— (label_id: 8)                                          0.00       0.00       0.00         12
--… (label_id: 9)                                          0.00       0.00       0.00          0
---------------------
--micro avg                                                6.05       6.05       6.05       1503
--macro avg                                                6.66       5.01       3.73       1503
--weighted avg                                            17.61       6.05       7.98       1503
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00         92
---------------------
--micro avg                                              100.00     100.00     100.00         92
--macro avg                                              100.00     100.00     100.00         92
--weighted avg                                           100.00     100.00     100.00         92
--
--[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 1.5848931924611143e-08
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
--will be used during training (effective maximum steps = 3192) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 3192
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        202
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.62       0.45       0.70        669
--- (label_id: 3)                                          3.48      27.27       6.17         66
--. (label_id: 4)                                         45.06      13.01      20.19        561
--: (label_id: 5)                                          1.52       6.67       2.47         15
--; (label_id: 6)                                          0.00       0.00       0.00         15
--? (label_id: 7)                                          8.70       7.32       7.95         82
--— (label_id: 8)                                          0.00       0.00       0.00         13
--… (label_id: 9)                                          0.00       0.00       0.00          1
---------------------
--micro avg                                                6.20       6.20       6.20       1628
--macro avg                                                6.04       5.47       3.75       1628
--weighted avg                                            16.79       6.20       7.92       1628
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        101
---------------------
--micro avg                                              100.00     100.00     100.00        101
--macro avg                                              100.00     100.00     100.00        101
--weighted avg                                           100.00     100.00     100.00        101
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.34       0.73       0.46       4402
--! (label_id: 1)                                          0.42      13.95       0.82        129
--, (label_id: 2)                                          2.53       0.64       1.03      15243
--- (label_id: 3)                                          2.45      21.03       4.38       1322
--. (label_id: 4)                                         44.00      11.40      18.11      12542
--: (label_id: 5)                                          0.43       1.41       0.65        354
--; (label_id: 6)                                          0.00       0.00       0.00        163
--? (label_id: 7)                                          4.16       6.27       5.00       1117
--— (label_id: 8)                                          3.00       0.61       1.02        488
--… (label_id: 9)                                          0.97       6.17       1.68         81
---------------------
--micro avg                                                5.41       5.41       5.41      35841
--macro avg                                                5.83       6.22       3.32      35841
--weighted avg                                            16.78       5.41       7.18      35841
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2201
---------------------
--micro avg                                              100.00     100.00     100.00       2201
--macro avg                                              100.00     100.00     100.00       2201
--weighted avg                                           100.00     100.00     100.00       2201
--
--[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.20       0.43       0.27       4226
--! (label_id: 1)                                          0.44      14.17       0.86        127
--, (label_id: 2)                                          1.93       0.49       0.78      14611
--- (label_id: 3)                                          2.23      19.56       4.01       1237
--. (label_id: 4)                                         43.37      11.25      17.86      11977
--: (label_id: 5)                                          0.68       2.34       1.05        342
--; (label_id: 6)                                          0.00       0.00       0.00        129
--? (label_id: 7)                                          5.16       7.47       6.10       1058
--— (label_id: 8)                                          2.15       0.49       0.80        409
--… (label_id: 9)                                          0.69       4.23       1.19         71
---------------------
--micro avg                                                5.23       5.23       5.23      34187
--macro avg                                                5.68       6.04       3.29      34187
--weighted avg                                            16.32       5.23       6.98      34187
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2113
---------------------
--micro avg                                              100.00     100.00     100.00       2113
--macro avg                                              100.00     100.00     100.00       2113
--weighted avg                                           100.00     100.00     100.00       2113
--
--[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.61       0.39       4228
--! (label_id: 1)                                          0.30       8.28       0.58        145
--, (label_id: 2)                                          2.27       0.58       0.92      14495
--- (label_id: 3)                                          2.64      21.78       4.70       1327
--. (label_id: 4)                                         44.87      11.66      18.51      12193
--: (label_id: 5)                                          0.60       1.93       0.91        362
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.19       6.40       5.07       1078
--— (label_id: 8)                                          1.16       0.22       0.37        459
--… (label_id: 9)                                          0.85       4.17       1.41         96
---------------------
--micro avg                                                5.54       5.54       5.54      34547
--macro avg                                                5.72       5.56       3.29      34547
--weighted avg                                            17.08       5.54       7.33      34547
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2114
---------------------
--micro avg                                              100.00     100.00     100.00       2114
--macro avg                                              100.00     100.00     100.00       2114
--weighted avg                                           100.00     100.00     100.00       2114
--
--[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.29       0.63       0.40       4444
--! (label_id: 1)                                          0.38      10.67       0.74        150
--, (label_id: 2)                                          2.32       0.59       0.94      15290
--- (label_id: 3)                                          2.34      20.28       4.19       1292
--. (label_id: 4)                                         43.85      11.68      18.44      12599
--: (label_id: 5)                                          0.41       1.28       0.62        392
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.24       6.30       5.07       1111
--— (label_id: 8)                                          0.00       0.00       0.00        456
--… (label_id: 9)                                          0.38       2.41       0.65         83
---------------------
--micro avg                                                5.40       5.40       5.40      35981
--macro avg                                                5.42       5.38       3.11      35981
--weighted avg                                            16.59       5.40       7.22      35981
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2222
---------------------
--micro avg                                              100.00     100.00     100.00       2222
--macro avg                                              100.00     100.00     100.00       2222
--weighted avg                                           100.00     100.00     100.00       2222
--
--[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.35       0.73       0.48       3844
--! (label_id: 1)                                          0.54      14.62       1.04        130
--, (label_id: 2)                                          2.32       0.59       0.94      13056
--- (label_id: 3)                                          2.67      22.28       4.77       1194
--. (label_id: 4)                                         44.45      11.95      18.84      10791
--: (label_id: 5)                                          0.84       3.21       1.33        280
--; (label_id: 6)                                          0.00       0.00       0.00        140
--? (label_id: 7)                                          4.17       6.56       5.10        914
--— (label_id: 8)                                          0.00       0.00       0.00        401
--… (label_id: 9)                                          0.48       2.63       0.81         76
---------------------
--micro avg                                                5.68       5.68       5.68      30826
--macro avg                                                5.58       6.26       3.33      30826
--weighted avg                                            16.82       5.68       7.41      30826
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1922
---------------------
--micro avg                                              100.00     100.00     100.00       1922
--macro avg                                              100.00     100.00     100.00       1922
--weighted avg                                           100.00     100.00     100.00       1922
--
--[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.60       0.39       3970
--! (label_id: 1)                                          0.35      10.66       0.68        122
--, (label_id: 2)                                          2.09       0.53       0.85      13469
--- (label_id: 3)                                          2.29      19.32       4.10       1201
--. (label_id: 4)                                         43.43      11.24      17.86      11227
--: (label_id: 5)                                          0.63       2.30       0.99        304
--; (label_id: 6)                                          0.00       0.00       0.00        141
--? (label_id: 7)                                          4.52       6.86       5.45       1006
--— (label_id: 8)                                          1.15       0.23       0.38        444
--… (label_id: 9)                                          0.45       2.67       0.78         75
---------------------
--micro avg                                                5.26       5.26       5.26      31959
--macro avg                                                5.52       5.44       3.15      31959
--weighted avg                                            16.42       5.26       7.02      31959
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1985
---------------------
--micro avg                                              100.00     100.00     100.00       1985
--macro avg                                              100.00     100.00     100.00       1985
--weighted avg                                           100.00     100.00     100.00       1985
--
--[INFO] - Epoch 5, step 2394: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.23       0.48       0.31       4126
--! (label_id: 1)                                          0.29       9.40       0.56        117
--, (label_id: 2)                                          1.91       0.49       0.77      14019
--- (label_id: 3)                                          2.52      22.59       4.53       1164
--. (label_id: 4)                                         44.15      11.65      18.44      11789
--: (label_id: 5)                                          0.72       2.41       1.11        332
--; (label_id: 6)                                          0.56       0.61       0.58        165
--? (label_id: 7)                                          3.89       6.53       4.88        980
--— (label_id: 8)                                          2.30       0.47       0.77        430
--… (label_id: 9)                                          1.18       8.33       2.07         60
---------------------
--micro avg                                                5.47       5.47       5.47      33182
--macro avg                                                5.77       6.30       3.40      33182
--weighted avg                                            16.77       5.47       7.25      33182
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2063
---------------------
--micro avg                                              100.00     100.00     100.00       2063
--macro avg                                              100.00     100.00     100.00       2063
--weighted avg                                           100.00     100.00     100.00       2063
--
--[INFO] - Epoch 6, step 2793: val_loss was not in top 3
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index fa37b4c..43fc93d 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             self.hparams.model.punct_class_weights=None
- 
-+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
-+        self.mlp = MultiLayerPerceptron(
-+            self.transformer.config.hidden_size,
-+            self.transformer.config.hidden_size,
-+            num_layers=self.hparams.model.mlp.num_fc_layers, 
-+            activation=self.hparams.model.mlp.activation, 
-+            log_softmax=self.hparams.model.mlp.log_softmax
-+        )
-+
-         self.punct_classifier = TokenClassifier(
-             hidden_size=self.transformer.config.hidden_size,
-             num_classes=len(self.labels_to_ids),
-@@ -80,6 +89,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-             log_softmax=self.hparams.model.domain_head.log_softmax,
-             dropout=self.hparams.model.domain_head.fc_dropout,
-             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
-+            pooling=self.hparams.model.domain_head.pooling,
-+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
-         )
- 
-         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
-@@ -119,14 +130,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
-         self.freeze()
- 
--    def forward(self, input_ids, attention_mask, domain_ids=None):
-+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
-         hidden_states = self.transformer(
-             input_ids=input_ids, attention_mask=attention_mask
-         )[0]
-+        hidden_states = self.dropout(hidden_states)
-+        hidden_states = self.mlp(hidden_states)
-         punct_logits = self.punct_classifier(hidden_states=hidden_states)
-         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
-         domain_logits = self.domain_classifier(
--            hidden_states=reverse_grad_hidden_states)
-+            hidden_states=reverse_grad_hidden_states,
-+            subtoken_mask=subtoken_mask)
-         return punct_logits, domain_logits
- 
-     def _make_step(self, batch):
-@@ -136,7 +150,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         punct_labels = batch['labels']
-         domain_labels = batch['domain']
-         punct_logits, domain_logits = self(
--            input_ids=input_ids, attention_mask=attention_mask
-+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
-         )
-         punctuation_loss = self.punctuation_loss(
-             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/lightning_logs.txt
deleted file mode 100644
index 031cd06..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/lightning_logs.txt
+++ /dev/null
@@ -1,25 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-Using native 16bit precision.
-Global seed set to 42
-initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | dropout             | Dropout              | 0     
-2 | mlp                 | MultiLayerPerceptron | 131 K 
-3 | punct_classifier    | TokenClassifier      | 2.6 K 
-4 | domain_classifier   | SequenceClassifier   | 513   
-5 | punctuation_loss    | FocalDiceLoss        | 0     
-6 | domain_loss         | CrossEntropyLoss     | 0     
-7 | agg_loss            | AggregatorLoss       | 0     
-8 | punct_class_report  | ClassificationReport | 0     
-9 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-167 K     Trainable params
-13.4 M    Non-trainable params
-13.6 M    Total params
-LR finder stopped early due to diverging loss.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/nemo_error_log.txt
deleted file mode 100644
index a3259fc..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/nemo_error_log.txt
+++ /dev/null
@@ -1,13 +0,0 @@
-[NeMo W 2021-02-08 11:06:05 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 11:06:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:06:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:06:17 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:06:21 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index d641997..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,15 +0,0 @@
-[NeMo I 2021-02-08 11:06:05 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05
-[NeMo I 2021-02-08 11:06:05 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-08 11:06:05 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 11:06:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:06:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:06:17 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:06:21 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/events.out.tfevents.1612753856.Titan.19011.0 b/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/events.out.tfevents.1612753856.Titan.19011.0
deleted file mode 100644
index 481eef2..0000000
Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/events.out.tfevents.1612753856.Titan.19011.0 and /dev/null differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/git-info.log
deleted file mode 100644
index daa2b33..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/git-info.log
+++ /dev/null
@@ -1,627 +0,0 @@
-commit hash: 939a671c8c117db6975316767ced5d95449e2b27
-diff --git a/README.md b/README.md
-index d52dba5..0b403d3 100644
---- a/README.md
-+++ b/README.md
-@@ -333,4 +333,71 @@ label                                                precision    recall       f
-  'punct_f1': 0.5858508944511414,
-  'punct_precision': 0.30176490545272827,
-  'punct_recall': 10.0,
-- 'test_loss': 0.8140875697135925}
-\ No newline at end of file
-+ 'test_loss': 0.8140875697135925}
-+
-+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
-+
-+ (label_id: 0)                                          62.15     100.00      76.66       5154
-+! (label_id: 1)                                          0.00       0.00       0.00        108
-+, (label_id: 2)                                          0.00       0.00       0.00      18022
-+- (label_id: 3)                                          0.00       0.00       0.00       1557
-+. (label_id: 4)                                         41.74      94.01      57.81      15164
-+: (label_id: 5)                                          0.00       0.00       0.00        319
-+; (label_id: 6)                                          0.00       0.00       0.00         88
-+? (label_id: 7)                                          0.00       0.00       0.00       1217
-+ (label_id: 8)                                          0.00       0.00       0.00        752
-+… (label_id: 9)                                          0.00       0.00       0.00         67
-+-------------------
-+micro avg                                               45.72      45.72      45.72      42448
-+macro avg                                               10.39      19.40      13.45      42448
-+weighted avg                                            22.46      45.72      29.96      42448
-+{ 'punct_f1': 13.446383476257324,
-+ 'punct_precision': 10.388500213623047,
-+ 'punct_recall': 19.400554656982422,
-+ 'test_loss': 0.44148480892181396}
-+
-+
-+ ## Log for 8/2/2021
-+
-+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
-+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
-+
-+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
-+End frozen 
-+micro avg                                               41.42      41.42      41.42      33406
-+macro avg                                               11.01      13.54      11.07      33406
-+weighted avg                                            34.88      41.42      34.20      33406
-+
-+1st layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               36.65      36.65      36.65      33463
-+macro avg                                               10.71       9.91       8.26      33463
-+weighted avg                                            34.32      36.65      31.08      33463
-+
-+2nd layer best lr 1e-10, set to 0.007943282347242822
-+micro avg                                               35.72      35.72      35.72      42448
-+macro avg                                                3.57      10.00       5.26      42448
-+weighted avg                                            12.76      35.72      18.81      42448
-+
-+{'punct_f1': 5.264181137084961,
-+ 'punct_precision': 3.572371006011963,
-+ 'punct_recall': 10.0,
-+ 'test_loss': 18.49854850769043}
-+
-+
-+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
-+alpha from 3->4 seems to reduce convergence rate.
-+
-+micro avg                                               50.98      50.98      50.98      33463
-+macro avg                                               25.99      25.38      23.38      33463
-+weighted avg                                            50.31      50.98      48.27      33463
-+
-+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
-+micro avg                                               58.55      58.55      58.55      39340
-+macro avg                                               30.02      29.74      29.52      39340
-+weighted avg                                            57.91      58.55      57.51      39340
-+
-+still increasing?!
-+{'punct_f1': 29.523975372314453,
-+ 'punct_precision': 30.015613555908203,
-+ 'punct_recall': 29.738296508789062,
-+ 'test_loss': 0.3690211772918701}
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index fe58670..a4a012a 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,49 +1,49 @@
- seed: 42
- trainer:
--    # gpus: 1 # the number of gpus, 0 for CPU
--    # num_nodes: 1
--    # max_epochs: 2
--    # max_steps: null # precedence over max_epochs
--    # accumulate_grad_batches: 4 # accumulates grads every k batches
--    # gradient_clip_val: 0
--    # amp_level: O1 # O1/O2 for mixed precision
--    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
--    # checkpoint_callback: false  # Provided by exp_manager
--    # logger: false #false  # Provided by exp_manager
--    # log_every_n_steps: 1  # Interval of logging.
--    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # resume_from_checkpoint: null
--
--    gpus: 0 # the number of gpus, 0 for CPU
-+    gpus: 1 # the number of gpus, 0 for CPU
-     num_nodes: 1
--    max_epochs: 8
-+    max_epochs: 10
-     max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 1 # accumulates grads every k batches
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-     gradient_clip_val: 0
--    amp_level: O0 # O1/O2 for mixed precision
--    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
-+    amp_level: O1 # O1/O2 for mixed precision
-+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    accelerator: ddp
-     checkpoint_callback: false  # Provided by exp_manager
-     logger: false #false  # Provided by exp_manager
-     log_every_n_steps: 1  # Interval of logging.
-     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    reload_dataloaders_every_epoch: true
-     resume_from_checkpoint: null
- 
-+    # gpus: 0 # the number of gpus, 0 for CPU
-+    # num_nodes: 1
-+    # max_epochs: 8
-+    # max_steps: null # precedence over max_epochs
-+    # accumulate_grad_batches: 1 # accumulates grads every k batches
-+    # gradient_clip_val: 0
-+    # amp_level: O0 # O1/O2 for mixed precision
-+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # # accelerator: ddp
-+    # checkpoint_callback: false  # Provided by exp_manager
-+    # logger: false #false  # Provided by exp_manager
-+    # log_every_n_steps: 1  # Interval of logging.
-+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    # reload_dataloaders_every_epoch: true
-+    # resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /home/nxingyu2/data # /root/data # 
--tmp_path: /home/nxingyu2/data/tmp # /tmp # 
-+base_path: /home/nxingyu/data # /root/data # 
-+tmp_path: /home/nxingyu/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
--    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-     unfrozen: 0
--    maximum_unfrozen: 2
-+    maximum_unfrozen: 1
-     unfreeze_step: 1
-     # unfreeze_every: 3
-     punct_label_ids:
-@@ -58,10 +58,10 @@ model:
-         - "—"
-         - "…"
- 
--    punct_class_weights: true
-+    punct_class_weights: false
-     
-     dataset:
--        data_dir: /home/nxingyu2/data # /root/data # 
-+        data_dir: /home/nxingyu/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -106,6 +106,12 @@ model:
-         config: null
-         # unfrozen_layers: 1
- 
-+    mlp:
-+        num_fc_layers: 2
-+        fc_dropout: 0.1
-+        log_softmax: false
-+        activation: 'relu'
-+        
-     punct_head:
-         punct_num_fc_layers: 1
-         fc_dropout: 0.1
-@@ -122,17 +128,19 @@ model:
-         use_transformer_init: true
-         loss: 'cel'
-         gamma: 0.1 # coefficient of gradient reversal
-+        pooling: 'mean'
-+        idx_conditioned_on: 0
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 3
-+        alpha: 4
-         macro_average: true
- 
-     focal_loss: 
-         gamma: 5
- 
-     optim:
--        name: novograd
-+        name: adamw
-         lr: 1e-3
-         weight_decay: 0.00
-         sched:
-diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
-index d4ff927..f9927ac 100644
---- a/experiment/core/layers/sequence_classifier.py
-+++ b/experiment/core/layers/sequence_classifier.py
-@@ -1,3 +1,4 @@
-+import torch
- from torch import nn
- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
- from core.utils import transformer_weights_init
-@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
-         log_softmax: bool = True,
-         dropout: float = 0.0,
-         use_transformer_init: bool = True,
--        idx_conditioned_on: int = 0,
-+        pooling: str = 'mean', # mean, max, mean_max, token
-+        idx_conditioned_on: int = None,
-     ):
-         """
-         Initializes the SequenceClassifier module.
-@@ -37,8 +39,9 @@ class SequenceClassifier(nn.Module):
-         super().__init__()
-         self.log_softmax = log_softmax
-         self._idx_conditioned_on = idx_conditioned_on
-+        self.pooling = pooling
-         self.mlp = MultiLayerPerceptron(
--            hidden_size=hidden_size,
-+            hidden_size=(hidden_size*2 if pooling=='mean_max' else hidden_size),
-             num_classes=num_classes,
-             num_layers=num_layers,
-             activation=activation,
-@@ -48,7 +51,23 @@ class SequenceClassifier(nn.Module):
-         if use_transformer_init:
-             self.apply(lambda module: transformer_weights_init(module, xavier=False))
- 
--    def forward(self, hidden_states):
-+    def forward(self, hidden_states, subtoken_mask=None):
-         hidden_states = self.dropout(hidden_states)
--        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
-+        if self.pooling=='token':
-+            pooled = hidden_states[:, self._idx_conditioned_on]
-+        else:
-+            if subtoken_mask==None:
-+                ct=hidden_states.shape[1] # Seq len
-+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
-+            else:
-+                ct = torch.sum(subtoken_mask,axis=1).unsqueeze(1)
-+            pooled_sum = torch.sum(hidden_states,axis=1)            
-+            if self.pooling=='mean' or self.pooling == 'mean_max':
-+                pooled_mean = torch.div(pooled_sum,ct)
-+            if self.pooling=='max' or self.pooling=='mean_max':
-+                pooled_max = torch.max(hidden_states,axis=1)[0]
-+            pooled=pooled_mean if self.pooling=='mean' else \
-+                pooled_max if self.pooling=='max' else \
-+                    torch.cat([pooled_mean,pooled_max],axis=-1)
-+        logits = self.mlp(pooled)
-         return logits
-diff --git a/experiment/info.log b/experiment/info.log
-index 2471fe9..e69de29 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,300 +0,0 @@
--[INFO] - GPU available: True, used: False
--[INFO] - TPU available: None, using: 0 TPU cores
--[INFO] - shuffling train set
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        184
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.23       0.34       0.53        594
--- (label_id: 3)                                          3.06      25.42       5.46         59
--. (label_id: 4)                                         47.22      12.98      20.36        524
--: (label_id: 5)                                          0.00       0.00       0.00         18
--; (label_id: 6)                                          0.00       0.00       0.00         13
--? (label_id: 7)                                          8.45       6.32       7.23         95
--— (label_id: 8)                                          0.00       0.00       0.00         12
--… (label_id: 9)                                          0.00       0.00       0.00          0
---------------------
--micro avg                                                6.05       6.05       6.05       1503
--macro avg                                                6.66       5.01       3.73       1503
--weighted avg                                            17.61       6.05       7.98       1503
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00         92
---------------------
--micro avg                                              100.00     100.00     100.00         92
--macro avg                                              100.00     100.00     100.00         92
--weighted avg                                           100.00     100.00     100.00         92
--
--[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 1.5848931924611143e-08
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
--will be used during training (effective maximum steps = 3192) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 3192
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        202
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.62       0.45       0.70        669
--- (label_id: 3)                                          3.48      27.27       6.17         66
--. (label_id: 4)                                         45.06      13.01      20.19        561
--: (label_id: 5)                                          1.52       6.67       2.47         15
--; (label_id: 6)                                          0.00       0.00       0.00         15
--? (label_id: 7)                                          8.70       7.32       7.95         82
--— (label_id: 8)                                          0.00       0.00       0.00         13
--… (label_id: 9)                                          0.00       0.00       0.00          1
---------------------
--micro avg                                                6.20       6.20       6.20       1628
--macro avg                                                6.04       5.47       3.75       1628
--weighted avg                                            16.79       6.20       7.92       1628
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        101
---------------------
--micro avg                                              100.00     100.00     100.00        101
--macro avg                                              100.00     100.00     100.00        101
--weighted avg                                           100.00     100.00     100.00        101
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.34       0.73       0.46       4402
--! (label_id: 1)                                          0.42      13.95       0.82        129
--, (label_id: 2)                                          2.53       0.64       1.03      15243
--- (label_id: 3)                                          2.45      21.03       4.38       1322
--. (label_id: 4)                                         44.00      11.40      18.11      12542
--: (label_id: 5)                                          0.43       1.41       0.65        354
--; (label_id: 6)                                          0.00       0.00       0.00        163
--? (label_id: 7)                                          4.16       6.27       5.00       1117
--— (label_id: 8)                                          3.00       0.61       1.02        488
--… (label_id: 9)                                          0.97       6.17       1.68         81
---------------------
--micro avg                                                5.41       5.41       5.41      35841
--macro avg                                                5.83       6.22       3.32      35841
--weighted avg                                            16.78       5.41       7.18      35841
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2201
---------------------
--micro avg                                              100.00     100.00     100.00       2201
--macro avg                                              100.00     100.00     100.00       2201
--weighted avg                                           100.00     100.00     100.00       2201
--
--[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.20       0.43       0.27       4226
--! (label_id: 1)                                          0.44      14.17       0.86        127
--, (label_id: 2)                                          1.93       0.49       0.78      14611
--- (label_id: 3)                                          2.23      19.56       4.01       1237
--. (label_id: 4)                                         43.37      11.25      17.86      11977
--: (label_id: 5)                                          0.68       2.34       1.05        342
--; (label_id: 6)                                          0.00       0.00       0.00        129
--? (label_id: 7)                                          5.16       7.47       6.10       1058
--— (label_id: 8)                                          2.15       0.49       0.80        409
--… (label_id: 9)                                          0.69       4.23       1.19         71
---------------------
--micro avg                                                5.23       5.23       5.23      34187
--macro avg                                                5.68       6.04       3.29      34187
--weighted avg                                            16.32       5.23       6.98      34187
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2113
---------------------
--micro avg                                              100.00     100.00     100.00       2113
--macro avg                                              100.00     100.00     100.00       2113
--weighted avg                                           100.00     100.00     100.00       2113
--
--[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.61       0.39       4228
--! (label_id: 1)                                          0.30       8.28       0.58        145
--, (label_id: 2)                                          2.27       0.58       0.92      14495
--- (label_id: 3)                                          2.64      21.78       4.70       1327
--. (label_id: 4)                                         44.87      11.66      18.51      12193
--: (label_id: 5)                                          0.60       1.93       0.91        362
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.19       6.40       5.07       1078
--— (label_id: 8)                                          1.16       0.22       0.37        459
--… (label_id: 9)                                          0.85       4.17       1.41         96
---------------------
--micro avg                                                5.54       5.54       5.54      34547
--macro avg                                                5.72       5.56       3.29      34547
--weighted avg                                            17.08       5.54       7.33      34547
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2114
---------------------
--micro avg                                              100.00     100.00     100.00       2114
--macro avg                                              100.00     100.00     100.00       2114
--weighted avg                                           100.00     100.00     100.00       2114
--
--[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.29       0.63       0.40       4444
--! (label_id: 1)                                          0.38      10.67       0.74        150
--, (label_id: 2)                                          2.32       0.59       0.94      15290
--- (label_id: 3)                                          2.34      20.28       4.19       1292
--. (label_id: 4)                                         43.85      11.68      18.44      12599
--: (label_id: 5)                                          0.41       1.28       0.62        392
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.24       6.30       5.07       1111
--— (label_id: 8)                                          0.00       0.00       0.00        456
--… (label_id: 9)                                          0.38       2.41       0.65         83
---------------------
--micro avg                                                5.40       5.40       5.40      35981
--macro avg                                                5.42       5.38       3.11      35981
--weighted avg                                            16.59       5.40       7.22      35981
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2222
---------------------
--micro avg                                              100.00     100.00     100.00       2222
--macro avg                                              100.00     100.00     100.00       2222
--weighted avg                                           100.00     100.00     100.00       2222
--
--[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.35       0.73       0.48       3844
--! (label_id: 1)                                          0.54      14.62       1.04        130
--, (label_id: 2)                                          2.32       0.59       0.94      13056
--- (label_id: 3)                                          2.67      22.28       4.77       1194
--. (label_id: 4)                                         44.45      11.95      18.84      10791
--: (label_id: 5)                                          0.84       3.21       1.33        280
--; (label_id: 6)                                          0.00       0.00       0.00        140
--? (label_id: 7)                                          4.17       6.56       5.10        914
--— (label_id: 8)                                          0.00       0.00       0.00        401
--… (label_id: 9)                                          0.48       2.63       0.81         76
---------------------
--micro avg                                                5.68       5.68       5.68      30826
--macro avg                                                5.58       6.26       3.33      30826
--weighted avg                                            16.82       5.68       7.41      30826
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1922
---------------------
--micro avg                                              100.00     100.00     100.00       1922
--macro avg                                              100.00     100.00     100.00       1922
--weighted avg                                           100.00     100.00     100.00       1922
--
--[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.60       0.39       3970
--! (label_id: 1)                                          0.35      10.66       0.68        122
--, (label_id: 2)                                          2.09       0.53       0.85      13469
--- (label_id: 3)                                          2.29      19.32       4.10       1201
--. (label_id: 4)                                         43.43      11.24      17.86      11227
--: (label_id: 5)                                          0.63       2.30       0.99        304
--; (label_id: 6)                                          0.00       0.00       0.00        141
--? (label_id: 7)                                          4.52       6.86       5.45       1006
--— (label_id: 8)                                          1.15       0.23       0.38        444
--… (label_id: 9)                                          0.45       2.67       0.78         75
---------------------
--micro avg                                                5.26       5.26       5.26      31959
--macro avg                                                5.52       5.44       3.15      31959
--weighted avg                                            16.42       5.26       7.02      31959
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1985
---------------------
--micro avg                                              100.00     100.00     100.00       1985
--macro avg                                              100.00     100.00     100.00       1985
--weighted avg                                           100.00     100.00     100.00       1985
--
--[INFO] - Epoch 5, step 2394: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.23       0.48       0.31       4126
--! (label_id: 1)                                          0.29       9.40       0.56        117
--, (label_id: 2)                                          1.91       0.49       0.77      14019
--- (label_id: 3)                                          2.52      22.59       4.53       1164
--. (label_id: 4)                                         44.15      11.65      18.44      11789
--: (label_id: 5)                                          0.72       2.41       1.11        332
--; (label_id: 6)                                          0.56       0.61       0.58        165
--? (label_id: 7)                                          3.89       6.53       4.88        980
--— (label_id: 8)                                          2.30       0.47       0.77        430
--… (label_id: 9)                                          1.18       8.33       2.07         60
---------------------
--micro avg                                                5.47       5.47       5.47      33182
--macro avg                                                5.77       6.30       3.40      33182
--weighted avg                                            16.77       5.47       7.25      33182
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2063
---------------------
--micro avg                                              100.00     100.00     100.00       2063
--macro avg                                              100.00     100.00     100.00       2063
--weighted avg                                           100.00     100.00     100.00       2063
--
--[INFO] - Epoch 6, step 2793: val_loss was not in top 3
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index fa37b4c..43fc93d 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             self.hparams.model.punct_class_weights=None
- 
-+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
-+        self.mlp = MultiLayerPerceptron(
-+            self.transformer.config.hidden_size,
-+            self.transformer.config.hidden_size,
-+            num_layers=self.hparams.model.mlp.num_fc_layers, 
-+            activation=self.hparams.model.mlp.activation, 
-+            log_softmax=self.hparams.model.mlp.log_softmax
-+        )
-+
-         self.punct_classifier = TokenClassifier(
-             hidden_size=self.transformer.config.hidden_size,
-             num_classes=len(self.labels_to_ids),
-@@ -80,6 +89,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-             log_softmax=self.hparams.model.domain_head.log_softmax,
-             dropout=self.hparams.model.domain_head.fc_dropout,
-             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
-+            pooling=self.hparams.model.domain_head.pooling,
-+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
-         )
- 
-         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
-@@ -119,14 +130,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
-         self.freeze()
- 
--    def forward(self, input_ids, attention_mask, domain_ids=None):
-+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
-         hidden_states = self.transformer(
-             input_ids=input_ids, attention_mask=attention_mask
-         )[0]
-+        hidden_states = self.dropout(hidden_states)
-+        hidden_states = self.mlp(hidden_states)
-         punct_logits = self.punct_classifier(hidden_states=hidden_states)
-         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
-         domain_logits = self.domain_classifier(
--            hidden_states=reverse_grad_hidden_states)
-+            hidden_states=reverse_grad_hidden_states,
-+            subtoken_mask=subtoken_mask)
-         return punct_logits, domain_logits
- 
-     def _make_step(self, batch):
-@@ -136,7 +150,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         punct_labels = batch['labels']
-         domain_labels = batch['domain']
-         punct_logits, domain_logits = self(
--            input_ids=input_ids, attention_mask=attention_mask
-+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
-         )
-         punctuation_loss = self.punctuation_loss(
-             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/hparams.yaml
deleted file mode 100644
index ebcb726..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/hparams.yaml
+++ /dev/null
@@ -1,114 +0,0 @@
-seed: 42
-trainer:
-  gpus: 1
-  num_nodes: 1
-  max_epochs: 10
-  max_steps: null
-  accumulate_grad_batches: 4
-  gradient_clip_val: 0
-  amp_level: O1
-  precision: 16
-  accelerator: ddp
-  checkpoint_callback: false
-  logger: false
-  log_every_n_steps: 1
-  val_check_interval: 1.0
-  resume_from_checkpoint: null
-exp_manager:
-  exp_dir: /home/nxingyu/project/
-  name: Punctuation_with_Domain_discriminator
-  create_tensorboard_logger: true
-  create_checkpoint_callback: true
-base_path: /home/nxingyu/data
-tmp_path: /home/nxingyu/data/tmp
-model:
-  nemo_path: null
-  transformer_path: google/electra-small-discriminator
-  unfrozen: 0
-  maximum_unfrozen: 1
-  unfreeze_step: 1
-  punct_label_ids:
-  - ''
-  - '!'
-  - ','
-  - '-'
-  - .
-  - ':'
-  - ;
-  - '?'
-  - —
-  - …
-  punct_class_weights: false
-  dataset:
-    data_dir: /home/nxingyu/data
-    labelled:
-    - /home/nxingyu/data/ted_talks_processed
-    unlabelled: null
-    max_seq_length: 128
-    pad_label: ''
-    ignore_extra_tokens: false
-    ignore_start_end: false
-    use_cache: false
-    num_workers: 0
-    pin_memory: true
-    drop_last: false
-    num_labels: 10
-    num_domains: 1
-    test_unlabelled: true
-    train_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 8
-    validation_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 8
-  tokenizer:
-    tokenizer_name: google/electra-small-discriminator
-    vocab_file: null
-    tokenizer_model: null
-    special_tokens: null
-  language_model:
-    pretrained_model_name: google/electra-small-discriminator
-    lm_checkpoint: null
-    config_file: null
-    config: null
-  mlp:
-    num_fc_layers: 2
-    fc_dropout: 0.1
-    log_softmax: false
-    activation: relu
-  punct_head:
-    punct_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: dice
-  domain_head:
-    domain_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: cel
-    gamma: 0.1
-    pooling: mean
-    idx_conditioned_on: 0
-  dice_loss:
-    epsilon: 0.01
-    alpha: 4
-    macro_average: true
-  focal_loss:
-    gamma: 5
-  optim:
-    name: adamw
-    lr: 0.001
-    weight_decay: 0.0
-    sched:
-      name: WarmupAnnealing
-      warmup_steps: null
-      warmup_ratio: 0.1
-      last_epoch: -1
-      monitor: val_loss
-      reduce_on_plateau: false
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/lightning_logs.txt
deleted file mode 100644
index d3676e6..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/lightning_logs.txt
+++ /dev/null
@@ -1,46 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-Using native 16bit precision.
-Global seed set to 42
-initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | dropout             | Dropout              | 0     
-2 | mlp                 | MultiLayerPerceptron | 131 K 
-3 | punct_classifier    | TokenClassifier      | 2.6 K 
-4 | domain_classifier   | SequenceClassifier   | 257   
-5 | punctuation_loss    | FocalDiceLoss        | 0     
-6 | domain_loss         | CrossEntropyLoss     | 0     
-7 | agg_loss            | AggregatorLoss       | 0     
-8 | punct_class_report  | ClassificationReport | 0     
-9 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-167 K     Trainable params
-13.4 M    Non-trainable params
-13.6 M    Total params
-Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/lr_find_temp_model.ckpt
-Global seed set to 42
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | dropout             | Dropout              | 0     
-2 | mlp                 | MultiLayerPerceptron | 131 K 
-3 | punct_classifier    | TokenClassifier      | 2.6 K 
-4 | domain_classifier   | SequenceClassifier   | 257   
-5 | punctuation_loss    | FocalDiceLoss        | 0     
-6 | domain_loss         | CrossEntropyLoss     | 0     
-7 | agg_loss            | AggregatorLoss       | 0     
-8 | punct_class_report  | ClassificationReport | 0     
-9 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-167 K     Trainable params
-13.4 M    Non-trainable params
-13.6 M    Total params
-Epoch 0, global step 100: val_loss reached 0.87292 (best 0.87292), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=0.ckpt" as top 3
-Epoch 1, global step 200: val_loss reached 0.80471 (best 0.80471), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.80-epoch=1.ckpt" as top 3
-Epoch 2, global step 300: val_loss reached 0.80542 (best 0.80471), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.81-epoch=2.ckpt" as top 3
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/nemo_error_log.txt
deleted file mode 100644
index 5b76cfb..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/nemo_error_log.txt
+++ /dev/null
@@ -1,28 +0,0 @@
-[NeMo W 2021-02-08 11:07:07 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 11:07:16 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:07:16 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:07:18 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:10:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:10:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:10:57 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:11:01 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-    
-[NeMo W 2021-02-08 11:16:19 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7fe326ec1700> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-08 11:16:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7fe326ea8040> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 45ada6c..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,30 +0,0 @@
-[NeMo I 2021-02-08 11:07:07 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07
-[NeMo I 2021-02-08 11:07:07 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-08 11:07:07 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 11:07:16 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:07:16 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:07:18 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:10:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:10:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:10:57 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:11:01 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-    
-[NeMo W 2021-02-08 11:16:19 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7fe326ec1700> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-08 11:16:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7fe326ea8040> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/events.out.tfevents.1612754918.Titan.21528.0 b/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/events.out.tfevents.1612754918.Titan.21528.0
deleted file mode 100644
index 9596b1a..0000000
Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/events.out.tfevents.1612754918.Titan.21528.0 and /dev/null differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/git-info.log
deleted file mode 100644
index e04e39e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/git-info.log
+++ /dev/null
@@ -1,699 +0,0 @@
-commit hash: 939a671c8c117db6975316767ced5d95449e2b27
-diff --git a/README.md b/README.md
-index d52dba5..51ab30b 100644
---- a/README.md
-+++ b/README.md
-@@ -291,21 +291,22 @@ weighted avg         |  43.14   | 33.52 |  29.43  |  67486
-  0 layer not too much improvement, 1 layer pretty decent.
-  alpha 5 seems too high. to try full run 4 next.
- layer 0 * 8 + layer 1 * 3
--
--  (label_id: 0)                                           0.00       0.00       0.00       5704
--! (label_id: 1)                                          0.00       0.00       0.00        110
--, (label_id: 2)                                          0.00       0.00       0.00      19711
--- (label_id: 3)                                          6.82      29.32      11.07       1702
--. (label_id: 4)                                         37.30      83.82      51.62      18406
--: (label_id: 5)                                          0.00       0.00       0.00        379
--; (label_id: 6)                                          0.00       0.00       0.00        190
--? (label_id: 7)                                          6.71       1.31       2.20       1446
--— (label_id: 8)                                          0.00       0.00       0.00       1227
--… (label_id: 9)                                          0.00       0.00       0.00         86
---------------------
--micro avg                                               32.57      32.57      32.57      48961
--macro avg                                                5.08      11.44       6.49      48961
--weighted avg                                            14.46      32.57      19.86      48961
-+label                |  precision | recall |   f1   |    support
-+---|---|---|---|---
-+  (label_id: 0)        | 0.00     | 0.00    | 0.00  | 5704
-+! (label_id: 1)        | 0.00     | 0.00    | 0.00  | 110
-+, (label_id: 2)        | 0.00     | 0.00    | 0.00  | 19711
-+- (label_id: 3)        | 6.82     | 29.32   | 11.07 | 1702
-+. (label_id: 4)        | 37.30    | 83.82   | 51.62 | 18406
-+: (label_id: 5)        | 0.00     | 0.00    | 0.00  | 379
-+; (label_id: 6)        | 0.00     | 0.00    | 0.00  | 190
-+? (label_id: 7)        | 6.71     | 1.31    | 2.20  | 1446
-+— (label_id: 8)        | 0.00     | 0.00    | 0.00  | 1227
-+… (label_id: 9)        | 0.00     | 0.00    | 0.00  | 86
-+-------------------||||
-+micro avg              | 32.57    | 32.57   | 32.57 | 48961
-+macro avg              | 5.08     | 11.44   | 6.49  | 48961
-+weighted avg           | 14.46    | 32.57   | 19.86 | 48961
- 
-  {'punct_f1': 6.104840278625488,
-  'punct_precision': 4.423948764801025,
-@@ -318,19 +319,96 @@ lr 0 : 0.008413951416451957
- 1: 0.00031622776601683794 ** too high. to adjust the min to 1e-10?
- 2: 0.00031622776601683794
- 
--label                                                precision    recall       f1           support
-- (label_id: 0)                                           0.00       0.00       0.00       7470
--! (label_id: 1)                                          0.00       0.00       0.00        148
--, (label_id: 2)                                          0.00       0.00       0.00      28513
--- (label_id: 3)                                          3.02     100.00       5.86       2074
--. (label_id: 4)                                          0.00       0.00       0.00      25120
--: (label_id: 5)                                          0.00       0.00       0.00        570
--; (label_id: 6)                                          0.00       0.00       0.00        534
--? (label_id: 7)                                          0.00       0.00       0.00       2085
--— (label_id: 8)                                          0.00       0.00       0.00       2073
--… (label_id: 9)                                          0.00       0.00       0.00        142
--
-- 'punct_f1': 0.5858508944511414,
-+label                |  precision | recall |   f1   |    support
-+---|---|---|---|---
-+ (label_id: 0)       |   0.00  | 0.00     | 0.00  | 7470
-+! (label_id: 1)      |   0.00  | 0.00     | 0.00  | 148
-+, (label_id: 2)      |   0.00  | 0.00     | 0.00  | 28513
-+- (label_id: 3)      |   3.02  | 100.00   | 5.86  | 2074
-+. (label_id: 4)      |   0.00  | 0.00     | 0.00  | 25120
-+: (label_id: 5)      |   0.00  | 0.00     | 0.00  | 570
-+; (label_id: 6)      |   0.00  | 0.00     | 0.00  | 534
-+? (label_id: 7)      |   0.00  | 0.00     | 0.00  | 2085
-+— (label_id: 8)      |   0.00  | 0.00     | 0.00  | 2073
-+… (label_id: 9)      |   0.00  | 0.00     | 0.00  | 142
-+
-+ {'punct_f1': 0.5858508944511414,
-  'punct_precision': 0.30176490545272827,
-  'punct_recall': 10.0,
-- 'test_loss': 0.8140875697135925}
-\ No newline at end of file
-+ 'test_loss': 0.8140875697135925}
-+
-+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
-+
-+label                |  precision | recall |   f1   |    support
-+---|---|---|---|---
-+ (label_id: 0)      | 62.15  | 100.00   | 76.66   | 5154
-+! (label_id: 1)     | 0.00   | 0.00     | 0.00    | 108
-+, (label_id: 2)     | 0.00   | 0.00     | 0.00    | 18022
-+- (label_id: 3)     | 0.00   | 0.00     | 0.00    | 1557
-+. (label_id: 4)     | 41.74  | 94.01    | 57.81   | 15164
-+: (label_id: 5)     | 0.00   | 0.00     | 0.00    | 319
-+; (label_id: 6)     | 0.00   | 0.00     | 0.00    | 88
-+? (label_id: 7)     | 0.00   | 0.00     | 0.00    | 1217
-+ (label_id: 8)      | 0.00   | 0.00     | 0.00    | 752
-+… (label_id: 9)     | 0.00   | 0.00     | 0.00    | 67
-+-------------------||||
-+micro avg           | 45.72 | 45.72 | 45.72 | 42448
-+macro avg           | 10.39 | 19.40 | 13.45 | 42448
-+weighted avg        | 22.46 | 45.72 | 29.96 | 42448
-+
-+{ 'punct_f1': 13.446383476257324,
-+ 'punct_precision': 10.388500213623047,
-+ 'punct_recall': 19.400554656982422,
-+ 'test_loss': 0.44148480892181396}
-+
-+
-+ ## Log for 8/2/2021
-+
-+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
-+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
-+
-+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
-+End frozen 
-+micro avg      |  41.42 |   41.42  |    41.42   |   33406
-+macro avg      |  11.01 |   13.54  |    11.07   |   33406
-+weighted avg   |  34.88 |   41.42  |    34.20   |   33406
-+
-+1st layer best lr 1e-10, set to 0.007943282347242822
-+micro avg            |       36.65  |    36.65  |    36.65  |    33463
-+macro avg            |       10.71  |     9.91  |     8.26  |    33463
-+weighted avg         |       34.32  |    36.65  |    31.08  |    33463
-+
-+2nd layer best lr 1e-10, set to 0.007943282347242822
-+micro avg        |   35.72  |    35.72  |    35.72  |    42448
-+macro avg        |    3.57  |    10.00  |     5.26  |    42448
-+weighted avg     |   12.76  |    35.72  |    18.81  |    42448
-+
-+{'punct_f1': 5.264181137084961,
-+ 'punct_precision': 3.572371006011963,
-+ 'punct_recall': 10.0,
-+ 'test_loss': 18.49854850769043}
-+
-+
-+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
-+alpha from 3->4 seems to reduce convergence rate.
-+
-+micro avg        |   50.98  | 50.98  |  50.98    |  33463
-+macro avg        |   25.99  | 25.38  |  23.38    |  33463
-+weighted avg     |   50.31  | 50.98  |  48.27    |  33463
-+
-+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
-+micro avg     |  58.55  |  58.55 |  58.55 | 39340
-+macro avg     |  30.02  |  29.74 |  29.52 | 39340
-+weighted avg  |  57.91  |  58.55 |  57.51 | 39340
-+
-+still increasing?!
-+{'punct_f1': 29.523975372314453,
-+ 'punct_precision': 30.015613555908203,
-+ 'punct_recall': 29.738296508789062,
-+ 'test_loss': 0.3690211772918701}
-+
-+
-+### Implemented mlp 2 layer before classifier, 
-+
-+adamw mean 2 layer domain, dice, alpha 4 10 batch, accgrad 4 2021-02-08_11-07-07/
-+frozen lr 0.0025118864315095825 best: 0.01,
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index fe58670..b137ae8 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,49 +1,49 @@
- seed: 42
- trainer:
--    # gpus: 1 # the number of gpus, 0 for CPU
--    # num_nodes: 1
--    # max_epochs: 2
--    # max_steps: null # precedence over max_epochs
--    # accumulate_grad_batches: 4 # accumulates grads every k batches
--    # gradient_clip_val: 0
--    # amp_level: O1 # O1/O2 for mixed precision
--    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
--    # checkpoint_callback: false  # Provided by exp_manager
--    # logger: false #false  # Provided by exp_manager
--    # log_every_n_steps: 1  # Interval of logging.
--    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # resume_from_checkpoint: null
--
--    gpus: 0 # the number of gpus, 0 for CPU
-+    gpus: 1 # the number of gpus, 0 for CPU
-     num_nodes: 1
--    max_epochs: 8
-+    max_epochs: 10
-     max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 1 # accumulates grads every k batches
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-     gradient_clip_val: 0
--    amp_level: O0 # O1/O2 for mixed precision
--    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
-+    amp_level: O1 # O1/O2 for mixed precision
-+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    accelerator: ddp
-     checkpoint_callback: false  # Provided by exp_manager
-     logger: false #false  # Provided by exp_manager
-     log_every_n_steps: 1  # Interval of logging.
-     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    reload_dataloaders_every_epoch: true
-     resume_from_checkpoint: null
- 
-+    # gpus: 0 # the number of gpus, 0 for CPU
-+    # num_nodes: 1
-+    # max_epochs: 8
-+    # max_steps: null # precedence over max_epochs
-+    # accumulate_grad_batches: 1 # accumulates grads every k batches
-+    # gradient_clip_val: 0
-+    # amp_level: O0 # O1/O2 for mixed precision
-+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # # accelerator: ddp
-+    # checkpoint_callback: false  # Provided by exp_manager
-+    # logger: false #false  # Provided by exp_manager
-+    # log_every_n_steps: 1  # Interval of logging.
-+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    # reload_dataloaders_every_epoch: true
-+    # resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /home/nxingyu2/data # /root/data # 
--tmp_path: /home/nxingyu2/data/tmp # /tmp # 
-+base_path: /home/nxingyu/data # /root/data # 
-+tmp_path: /home/nxingyu/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
--    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-     unfrozen: 0
--    maximum_unfrozen: 2
-+    maximum_unfrozen: 1
-     unfreeze_step: 1
-     # unfreeze_every: 3
-     punct_label_ids:
-@@ -58,10 +58,10 @@ model:
-         - "—"
-         - "…"
- 
--    punct_class_weights: true
-+    punct_class_weights: false
-     
-     dataset:
--        data_dir: /home/nxingyu2/data # /root/data # 
-+        data_dir: /home/nxingyu/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -106,6 +106,12 @@ model:
-         config: null
-         # unfrozen_layers: 1
- 
-+    mlp:
-+        num_fc_layers: 1
-+        fc_dropout: 0.1
-+        log_softmax: false
-+        activation: 'relu'
-+        
-     punct_head:
-         punct_num_fc_layers: 1
-         fc_dropout: 0.1
-@@ -122,6 +128,8 @@ model:
-         use_transformer_init: true
-         loss: 'cel'
-         gamma: 0.1 # coefficient of gradient reversal
-+        pooling: 'mean'
-+        idx_conditioned_on: 0
-     
-     dice_loss:
-         epsilon: 0.01
-@@ -132,7 +140,7 @@ model:
-         gamma: 5
- 
-     optim:
--        name: novograd
-+        name: adamw
-         lr: 1e-3
-         weight_decay: 0.00
-         sched:
-diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
-index d4ff927..f9927ac 100644
---- a/experiment/core/layers/sequence_classifier.py
-+++ b/experiment/core/layers/sequence_classifier.py
-@@ -1,3 +1,4 @@
-+import torch
- from torch import nn
- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
- from core.utils import transformer_weights_init
-@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
-         log_softmax: bool = True,
-         dropout: float = 0.0,
-         use_transformer_init: bool = True,
--        idx_conditioned_on: int = 0,
-+        pooling: str = 'mean', # mean, max, mean_max, token
-+        idx_conditioned_on: int = None,
-     ):
-         """
-         Initializes the SequenceClassifier module.
-@@ -37,8 +39,9 @@ class SequenceClassifier(nn.Module):
-         super().__init__()
-         self.log_softmax = log_softmax
-         self._idx_conditioned_on = idx_conditioned_on
-+        self.pooling = pooling
-         self.mlp = MultiLayerPerceptron(
--            hidden_size=hidden_size,
-+            hidden_size=(hidden_size*2 if pooling=='mean_max' else hidden_size),
-             num_classes=num_classes,
-             num_layers=num_layers,
-             activation=activation,
-@@ -48,7 +51,23 @@ class SequenceClassifier(nn.Module):
-         if use_transformer_init:
-             self.apply(lambda module: transformer_weights_init(module, xavier=False))
- 
--    def forward(self, hidden_states):
-+    def forward(self, hidden_states, subtoken_mask=None):
-         hidden_states = self.dropout(hidden_states)
--        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
-+        if self.pooling=='token':
-+            pooled = hidden_states[:, self._idx_conditioned_on]
-+        else:
-+            if subtoken_mask==None:
-+                ct=hidden_states.shape[1] # Seq len
-+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
-+            else:
-+                ct = torch.sum(subtoken_mask,axis=1).unsqueeze(1)
-+            pooled_sum = torch.sum(hidden_states,axis=1)            
-+            if self.pooling=='mean' or self.pooling == 'mean_max':
-+                pooled_mean = torch.div(pooled_sum,ct)
-+            if self.pooling=='max' or self.pooling=='mean_max':
-+                pooled_max = torch.max(hidden_states,axis=1)[0]
-+            pooled=pooled_mean if self.pooling=='mean' else \
-+                pooled_max if self.pooling=='max' else \
-+                    torch.cat([pooled_mean,pooled_max],axis=-1)
-+        logits = self.mlp(pooled)
-         return logits
-diff --git a/experiment/info.log b/experiment/info.log
-index 2471fe9..e69de29 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,300 +0,0 @@
--[INFO] - GPU available: True, used: False
--[INFO] - TPU available: None, using: 0 TPU cores
--[INFO] - shuffling train set
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        184
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.23       0.34       0.53        594
--- (label_id: 3)                                          3.06      25.42       5.46         59
--. (label_id: 4)                                         47.22      12.98      20.36        524
--: (label_id: 5)                                          0.00       0.00       0.00         18
--; (label_id: 6)                                          0.00       0.00       0.00         13
--? (label_id: 7)                                          8.45       6.32       7.23         95
--— (label_id: 8)                                          0.00       0.00       0.00         12
--… (label_id: 9)                                          0.00       0.00       0.00          0
---------------------
--micro avg                                                6.05       6.05       6.05       1503
--macro avg                                                6.66       5.01       3.73       1503
--weighted avg                                            17.61       6.05       7.98       1503
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00         92
---------------------
--micro avg                                              100.00     100.00     100.00         92
--macro avg                                              100.00     100.00     100.00         92
--weighted avg                                           100.00     100.00     100.00         92
--
--[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 1.5848931924611143e-08
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
--will be used during training (effective maximum steps = 3192) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 3192
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        202
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.62       0.45       0.70        669
--- (label_id: 3)                                          3.48      27.27       6.17         66
--. (label_id: 4)                                         45.06      13.01      20.19        561
--: (label_id: 5)                                          1.52       6.67       2.47         15
--; (label_id: 6)                                          0.00       0.00       0.00         15
--? (label_id: 7)                                          8.70       7.32       7.95         82
--— (label_id: 8)                                          0.00       0.00       0.00         13
--… (label_id: 9)                                          0.00       0.00       0.00          1
---------------------
--micro avg                                                6.20       6.20       6.20       1628
--macro avg                                                6.04       5.47       3.75       1628
--weighted avg                                            16.79       6.20       7.92       1628
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        101
---------------------
--micro avg                                              100.00     100.00     100.00        101
--macro avg                                              100.00     100.00     100.00        101
--weighted avg                                           100.00     100.00     100.00        101
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.34       0.73       0.46       4402
--! (label_id: 1)                                          0.42      13.95       0.82        129
--, (label_id: 2)                                          2.53       0.64       1.03      15243
--- (label_id: 3)                                          2.45      21.03       4.38       1322
--. (label_id: 4)                                         44.00      11.40      18.11      12542
--: (label_id: 5)                                          0.43       1.41       0.65        354
--; (label_id: 6)                                          0.00       0.00       0.00        163
--? (label_id: 7)                                          4.16       6.27       5.00       1117
--— (label_id: 8)                                          3.00       0.61       1.02        488
--… (label_id: 9)                                          0.97       6.17       1.68         81
---------------------
--micro avg                                                5.41       5.41       5.41      35841
--macro avg                                                5.83       6.22       3.32      35841
--weighted avg                                            16.78       5.41       7.18      35841
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2201
---------------------
--micro avg                                              100.00     100.00     100.00       2201
--macro avg                                              100.00     100.00     100.00       2201
--weighted avg                                           100.00     100.00     100.00       2201
--
--[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.20       0.43       0.27       4226
--! (label_id: 1)                                          0.44      14.17       0.86        127
--, (label_id: 2)                                          1.93       0.49       0.78      14611
--- (label_id: 3)                                          2.23      19.56       4.01       1237
--. (label_id: 4)                                         43.37      11.25      17.86      11977
--: (label_id: 5)                                          0.68       2.34       1.05        342
--; (label_id: 6)                                          0.00       0.00       0.00        129
--? (label_id: 7)                                          5.16       7.47       6.10       1058
--— (label_id: 8)                                          2.15       0.49       0.80        409
--… (label_id: 9)                                          0.69       4.23       1.19         71
---------------------
--micro avg                                                5.23       5.23       5.23      34187
--macro avg                                                5.68       6.04       3.29      34187
--weighted avg                                            16.32       5.23       6.98      34187
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2113
---------------------
--micro avg                                              100.00     100.00     100.00       2113
--macro avg                                              100.00     100.00     100.00       2113
--weighted avg                                           100.00     100.00     100.00       2113
--
--[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.61       0.39       4228
--! (label_id: 1)                                          0.30       8.28       0.58        145
--, (label_id: 2)                                          2.27       0.58       0.92      14495
--- (label_id: 3)                                          2.64      21.78       4.70       1327
--. (label_id: 4)                                         44.87      11.66      18.51      12193
--: (label_id: 5)                                          0.60       1.93       0.91        362
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.19       6.40       5.07       1078
--— (label_id: 8)                                          1.16       0.22       0.37        459
--… (label_id: 9)                                          0.85       4.17       1.41         96
---------------------
--micro avg                                                5.54       5.54       5.54      34547
--macro avg                                                5.72       5.56       3.29      34547
--weighted avg                                            17.08       5.54       7.33      34547
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2114
---------------------
--micro avg                                              100.00     100.00     100.00       2114
--macro avg                                              100.00     100.00     100.00       2114
--weighted avg                                           100.00     100.00     100.00       2114
--
--[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.29       0.63       0.40       4444
--! (label_id: 1)                                          0.38      10.67       0.74        150
--, (label_id: 2)                                          2.32       0.59       0.94      15290
--- (label_id: 3)                                          2.34      20.28       4.19       1292
--. (label_id: 4)                                         43.85      11.68      18.44      12599
--: (label_id: 5)                                          0.41       1.28       0.62        392
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.24       6.30       5.07       1111
--— (label_id: 8)                                          0.00       0.00       0.00        456
--… (label_id: 9)                                          0.38       2.41       0.65         83
---------------------
--micro avg                                                5.40       5.40       5.40      35981
--macro avg                                                5.42       5.38       3.11      35981
--weighted avg                                            16.59       5.40       7.22      35981
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2222
---------------------
--micro avg                                              100.00     100.00     100.00       2222
--macro avg                                              100.00     100.00     100.00       2222
--weighted avg                                           100.00     100.00     100.00       2222
--
--[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.35       0.73       0.48       3844
--! (label_id: 1)                                          0.54      14.62       1.04        130
--, (label_id: 2)                                          2.32       0.59       0.94      13056
--- (label_id: 3)                                          2.67      22.28       4.77       1194
--. (label_id: 4)                                         44.45      11.95      18.84      10791
--: (label_id: 5)                                          0.84       3.21       1.33        280
--; (label_id: 6)                                          0.00       0.00       0.00        140
--? (label_id: 7)                                          4.17       6.56       5.10        914
--— (label_id: 8)                                          0.00       0.00       0.00        401
--… (label_id: 9)                                          0.48       2.63       0.81         76
---------------------
--micro avg                                                5.68       5.68       5.68      30826
--macro avg                                                5.58       6.26       3.33      30826
--weighted avg                                            16.82       5.68       7.41      30826
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1922
---------------------
--micro avg                                              100.00     100.00     100.00       1922
--macro avg                                              100.00     100.00     100.00       1922
--weighted avg                                           100.00     100.00     100.00       1922
--
--[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.60       0.39       3970
--! (label_id: 1)                                          0.35      10.66       0.68        122
--, (label_id: 2)                                          2.09       0.53       0.85      13469
--- (label_id: 3)                                          2.29      19.32       4.10       1201
--. (label_id: 4)                                         43.43      11.24      17.86      11227
--: (label_id: 5)                                          0.63       2.30       0.99        304
--; (label_id: 6)                                          0.00       0.00       0.00        141
--? (label_id: 7)                                          4.52       6.86       5.45       1006
--— (label_id: 8)                                          1.15       0.23       0.38        444
--… (label_id: 9)                                          0.45       2.67       0.78         75
---------------------
--micro avg                                                5.26       5.26       5.26      31959
--macro avg                                                5.52       5.44       3.15      31959
--weighted avg                                            16.42       5.26       7.02      31959
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1985
---------------------
--micro avg                                              100.00     100.00     100.00       1985
--macro avg                                              100.00     100.00     100.00       1985
--weighted avg                                           100.00     100.00     100.00       1985
--
--[INFO] - Epoch 5, step 2394: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.23       0.48       0.31       4126
--! (label_id: 1)                                          0.29       9.40       0.56        117
--, (label_id: 2)                                          1.91       0.49       0.77      14019
--- (label_id: 3)                                          2.52      22.59       4.53       1164
--. (label_id: 4)                                         44.15      11.65      18.44      11789
--: (label_id: 5)                                          0.72       2.41       1.11        332
--; (label_id: 6)                                          0.56       0.61       0.58        165
--? (label_id: 7)                                          3.89       6.53       4.88        980
--— (label_id: 8)                                          2.30       0.47       0.77        430
--… (label_id: 9)                                          1.18       8.33       2.07         60
---------------------
--micro avg                                                5.47       5.47       5.47      33182
--macro avg                                                5.77       6.30       3.40      33182
--weighted avg                                            16.77       5.47       7.25      33182
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2063
---------------------
--micro avg                                              100.00     100.00     100.00       2063
--macro avg                                              100.00     100.00     100.00       2063
--weighted avg                                           100.00     100.00     100.00       2063
--
--[INFO] - Epoch 6, step 2793: val_loss was not in top 3
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index fa37b4c..43fc93d 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         else:
-             self.hparams.model.punct_class_weights=None
- 
-+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
-+        self.mlp = MultiLayerPerceptron(
-+            self.transformer.config.hidden_size,
-+            self.transformer.config.hidden_size,
-+            num_layers=self.hparams.model.mlp.num_fc_layers, 
-+            activation=self.hparams.model.mlp.activation, 
-+            log_softmax=self.hparams.model.mlp.log_softmax
-+        )
-+
-         self.punct_classifier = TokenClassifier(
-             hidden_size=self.transformer.config.hidden_size,
-             num_classes=len(self.labels_to_ids),
-@@ -80,6 +89,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-             log_softmax=self.hparams.model.domain_head.log_softmax,
-             dropout=self.hparams.model.domain_head.fc_dropout,
-             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
-+            pooling=self.hparams.model.domain_head.pooling,
-+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
-         )
- 
-         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
-@@ -119,14 +130,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
-         self.freeze()
- 
--    def forward(self, input_ids, attention_mask, domain_ids=None):
-+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
-         hidden_states = self.transformer(
-             input_ids=input_ids, attention_mask=attention_mask
-         )[0]
-+        hidden_states = self.dropout(hidden_states)
-+        hidden_states = self.mlp(hidden_states)
-         punct_logits = self.punct_classifier(hidden_states=hidden_states)
-         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
-         domain_logits = self.domain_classifier(
--            hidden_states=reverse_grad_hidden_states)
-+            hidden_states=reverse_grad_hidden_states,
-+            subtoken_mask=subtoken_mask)
-         return punct_logits, domain_logits
- 
-     def _make_step(self, batch):
-@@ -136,7 +150,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         punct_labels = batch['labels']
-         domain_labels = batch['domain']
-         punct_logits, domain_logits = self(
--            input_ids=input_ids, attention_mask=attention_mask
-+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
-         )
-         punctuation_loss = self.punctuation_loss(
-             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/hparams.yaml
deleted file mode 100644
index 8fe49f4..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/hparams.yaml
+++ /dev/null
@@ -1,114 +0,0 @@
-seed: 42
-trainer:
-  gpus: 1
-  num_nodes: 1
-  max_epochs: 10
-  max_steps: null
-  accumulate_grad_batches: 4
-  gradient_clip_val: 0
-  amp_level: O1
-  precision: 16
-  accelerator: ddp
-  checkpoint_callback: false
-  logger: false
-  log_every_n_steps: 1
-  val_check_interval: 1.0
-  resume_from_checkpoint: null
-exp_manager:
-  exp_dir: /home/nxingyu/project/
-  name: Punctuation_with_Domain_discriminator
-  create_tensorboard_logger: true
-  create_checkpoint_callback: true
-base_path: /home/nxingyu/data
-tmp_path: /home/nxingyu/data/tmp
-model:
-  nemo_path: null
-  transformer_path: google/electra-small-discriminator
-  unfrozen: 0
-  maximum_unfrozen: 1
-  unfreeze_step: 1
-  punct_label_ids:
-  - ''
-  - '!'
-  - ','
-  - '-'
-  - .
-  - ':'
-  - ;
-  - '?'
-  - —
-  - …
-  punct_class_weights: false
-  dataset:
-    data_dir: /home/nxingyu/data
-    labelled:
-    - /home/nxingyu/data/ted_talks_processed
-    unlabelled: null
-    max_seq_length: 128
-    pad_label: ''
-    ignore_extra_tokens: false
-    ignore_start_end: false
-    use_cache: false
-    num_workers: 0
-    pin_memory: true
-    drop_last: false
-    num_labels: 10
-    num_domains: 1
-    test_unlabelled: true
-    train_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 8
-    validation_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 8
-  tokenizer:
-    tokenizer_name: google/electra-small-discriminator
-    vocab_file: null
-    tokenizer_model: null
-    special_tokens: null
-  language_model:
-    pretrained_model_name: google/electra-small-discriminator
-    lm_checkpoint: null
-    config_file: null
-    config: null
-  mlp:
-    num_fc_layers: 1
-    fc_dropout: 0.1
-    log_softmax: false
-    activation: relu
-  punct_head:
-    punct_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: dice
-  domain_head:
-    domain_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: cel
-    gamma: 0.1
-    pooling: mean
-    idx_conditioned_on: 0
-  dice_loss:
-    epsilon: 0.01
-    alpha: 3
-    macro_average: true
-  focal_loss:
-    gamma: 5
-  optim:
-    name: adamw
-    lr: 0.001
-    weight_decay: 0.0
-    sched:
-      name: WarmupAnnealing
-      warmup_steps: null
-      warmup_ratio: 0.1
-      last_epoch: -1
-      monitor: val_loss
-      reduce_on_plateau: false
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/lightning_logs.txt
deleted file mode 100644
index 40113aa..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/lightning_logs.txt
+++ /dev/null
@@ -1,43 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
-Using native 16bit precision.
-Global seed set to 42
-initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | dropout             | Dropout              | 0     
-2 | mlp                 | MultiLayerPerceptron | 65.8 K
-3 | punct_classifier    | TokenClassifier      | 2.6 K 
-4 | domain_classifier   | SequenceClassifier   | 257   
-5 | punctuation_loss    | FocalDiceLoss        | 0     
-6 | domain_loss         | CrossEntropyLoss     | 0     
-7 | agg_loss            | AggregatorLoss       | 0     
-8 | punct_class_report  | ClassificationReport | 0     
-9 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-101 K     Trainable params
-13.4 M    Non-trainable params
-13.6 M    Total params
-Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/lr_find_temp_model.ckpt
-Global seed set to 42
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | dropout             | Dropout              | 0     
-2 | mlp                 | MultiLayerPerceptron | 65.8 K
-3 | punct_classifier    | TokenClassifier      | 2.6 K 
-4 | domain_classifier   | SequenceClassifier   | 257   
-5 | punctuation_loss    | FocalDiceLoss        | 0     
-6 | domain_loss         | CrossEntropyLoss     | 0     
-7 | agg_loss            | AggregatorLoss       | 0     
-8 | punct_class_report  | ClassificationReport | 0     
-9 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-101 K     Trainable params
-13.4 M    Non-trainable params
-13.6 M    Total params
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/nemo_error_log.txt
deleted file mode 100644
index e026dd1..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/nemo_error_log.txt
+++ /dev/null
@@ -1,22 +0,0 @@
-[NeMo W 2021-02-08 11:24:45 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 11:24:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:24:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:24:58 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:28:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:28:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:28:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:28:44 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index b012129..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,24 +0,0 @@
-[NeMo I 2021-02-08 11:24:45 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45
-[NeMo I 2021-02-08 11:24:45 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-08 11:24:45 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 11:24:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:24:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:24:58 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:28:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:28:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:28:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 11:28:44 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-    
diff --git a/README.md b/README.md
index 51ab30b..beca3ef 100644
--- a/README.md
+++ b/README.md
@@ -167,17 +167,18 @@ label            | precision    | recall   | f1     | support
 … (label_id: 9)  | 0.00         | 0.00     | 0.00   | 66
 
 ###Focal DistilBERT gamma 3 0 unfrozen ted
-label                                                precision    recall       f1           support   
- (label_id: 0)                                         100.00      51.29      67.80       4118
-! (label_id: 1)                                          0.00       0.00       0.00         91
-, (label_id: 2)                                          0.00       0.00       0.00      13953
-- (label_id: 3)                                         94.27      46.49      62.27       1310
-. (label_id: 4)                                         39.51      99.94      56.63      12142
-: (label_id: 5)                                          0.00       0.00       0.00        254
-; (label_id: 6)                                          0.00       0.00       0.00         79
-? (label_id: 7)                                          0.00       0.00       0.00        905
-— (label_id: 8)                                          0.00       0.00       0.00        566
-… (label_id: 9)                                          0.00       0.00       0.00         52
+label               | precision   | recall | f1    | support   
+---|---|---|---|---
+ (label_id: 0)      | 100.00      | 51.29  | 67.80 | 4118
+! (label_id: 1)     | 0.00        | 0.00   | 0.00  | 91
+, (label_id: 2)     | 0.00        | 0.00   | 0.00  | 13953
+\- (label_id: 3)    | 94.27       | 46.49  | 62.27 | 1310
+. (label_id: 4)     | 39.51       | 99.94  | 56.63 | 12142
+: (label_id: 5)     | 0.00        | 0.00   | 0.00  | 254
+; (label_id: 6)     | 0.00        | 0.00   | 0.00  | 79
+? (label_id: 7)     | 0.00        | 0.00   | 0.00  | 905
+— (label_id: 8)     | 0.00        | 0.00   | 0.00  | 566
+… (label_id: 9)     | 0.00        | 0.00   | 0.00  | 52
 
 ## Observations
 - CRF tends to perform better on higher proportion classes like blank, comma and period without class weights.
@@ -195,7 +196,7 @@ label                 |   precision  |  recall |    f1    |      support
  (label_id: 0)        |      79.50   |   29.94 |   43.50  |    5026
 ! (label_id: 1)       |       6.84   |   20.59 |   10.27  |     102
 , (label_id: 2)       |      50.70   |   60.09 |   55.00  |   17571
-- (label_id: 3)       |      64.45   |   82.11 |   72.22  |    1526
+\- (label_id: 3)       |      64.45   |   82.11 |   72.22  |    1526
 . (label_id: 4)       |      57.40   |   49.43 |   53.12  |   14767
 : (label_id: 5)       |      17.86   |   31.83 |   22.89  |     289
 ; (label_id: 6)       |       1.50   |    5.88 |    2.39  |      85
@@ -220,7 +221,7 @@ label                 |   precision  |  recall |    f1    |      support
  (label_id: 0)             |     0.00  |  0.00 |   0.00  |  5026
 ! (label_id: 1)            |     0.00  |  0.00 |   0.00  |   102
 , (label_id: 2)            |    42.79  | 47.54 |  45.04  | 17571
-- (label_id: 3)            |    73.63  | 80.87 |  77.08  |  1526
+\- (label_id: 3)            |    73.63  | 80.87 |  77.08  |  1526
 . (label_id: 4)            |    47.36  | 55.16 |  50.96  | 14767
 : (label_id: 5)            |    10.88  | 27.68 |  15.62  |   289
 ; (label_id: 6)            |     0.00  |  0.00 |   0.00  |    85
@@ -245,7 +246,7 @@ label                  |  precision | recall |   f1   |     support
  (label_id: 0)         |     59.35  |  52.35 |  55.63 |   7314
 ! (label_id: 1)        |      0.00  |   0.00 |   0.00 |    154
 , (label_id: 2)        |     44.15  |  82.80 |  57.59 |  28180
-- (label_id: 3)        |      3.91  |   2.02 |   2.66 |   1933
+\- (label_id: 3)        |      3.91  |   2.02 |   2.66 |   1933
 . (label_id: 4)        |     39.91  |  11.64 |  18.02 |  24624
 : (label_id: 5)        |      0.00  |   0.00 |   0.00 |    522
 ; (label_id: 6)        |      0.00  |   0.00 |   0.00 |    485
@@ -268,7 +269,7 @@ label                |  precision | recall |   f1   |    support
  (label_id: 0)       |  62.32   | 99.78 |  76.72  |   7314
 ! (label_id: 1)      |   0.00   |  0.00 |   0.00  |    154
 , (label_id: 2)      |  49.81   |  4.72 |   8.62  |  28180
-- (label_id: 3)      |   5.91   | 28.35 |   9.78  |   1933
+\- (label_id: 3)      |   5.91   | 28.35 |   9.78  |   1933
 . (label_id: 4)      |  41.80   | 52.40 |  46.50  |  24624
 : (label_id: 5)      |   0.94   |  4.02 |   1.53  |    522
 ; (label_id: 6)      |   0.00   |  0.00 |   0.00  |    485
@@ -296,7 +297,7 @@ label                |  precision | recall |   f1   |    support
   (label_id: 0)        | 0.00     | 0.00    | 0.00  | 5704
 ! (label_id: 1)        | 0.00     | 0.00    | 0.00  | 110
 , (label_id: 2)        | 0.00     | 0.00    | 0.00  | 19711
-- (label_id: 3)        | 6.82     | 29.32   | 11.07 | 1702
+\- (label_id: 3)        | 6.82     | 29.32   | 11.07 | 1702
 . (label_id: 4)        | 37.30    | 83.82   | 51.62 | 18406
 : (label_id: 5)        | 0.00     | 0.00    | 0.00  | 379
 ; (label_id: 6)        | 0.00     | 0.00    | 0.00  | 190
@@ -324,7 +325,7 @@ label                |  precision | recall |   f1   |    support
  (label_id: 0)       |   0.00  | 0.00     | 0.00  | 7470
 ! (label_id: 1)      |   0.00  | 0.00     | 0.00  | 148
 , (label_id: 2)      |   0.00  | 0.00     | 0.00  | 28513
-- (label_id: 3)      |   3.02  | 100.00   | 5.86  | 2074
+\- (label_id: 3)      |   3.02  | 100.00   | 5.86  | 2074
 . (label_id: 4)      |   0.00  | 0.00     | 0.00  | 25120
 : (label_id: 5)      |   0.00  | 0.00     | 0.00  | 570
 ; (label_id: 6)      |   0.00  | 0.00     | 0.00  | 534
@@ -344,7 +345,7 @@ label                |  precision | recall |   f1   |    support
  (label_id: 0)      | 62.15  | 100.00   | 76.66   | 5154
 ! (label_id: 1)     | 0.00   | 0.00     | 0.00    | 108
 , (label_id: 2)     | 0.00   | 0.00     | 0.00    | 18022
-- (label_id: 3)     | 0.00   | 0.00     | 0.00    | 1557
+\- (label_id: 3)     | 0.00   | 0.00     | 0.00    | 1557
 . (label_id: 4)     | 41.74  | 94.01    | 57.81   | 15164
 : (label_id: 5)     | 0.00   | 0.00     | 0.00    | 319
 ; (label_id: 6)     | 0.00   | 0.00     | 0.00    | 88
@@ -412,3 +413,39 @@ still increasing?!
 
 adamw mean 2 layer domain, dice, alpha 4 10 batch, accgrad 4 2021-02-08_11-07-07/
 frozen lr 0.0025118864315095825 best: 0.01,
+
+unfreeze 0.07943282347242822 best lr 1e-10
+
+ep 6 
+micro avg    | 64.21 | 64.21 | 64.21 | 33835
+macro avg    | 36.55 | 37.56 | 36.71 | 33835
+weighted avg | 63.77 | 64.21 | 63.91 | 33835
+
+{'punct_f1': 38.96394729614258,
+ 'punct_precision': 38.412635803222656,
+ 'punct_recall': 40.2258415222168,
+ 'test_loss': 0.2748030722141266}
+
+
+ ### CEL
+
+  (label_id: 0)                                         100.00     100.00     100.00       5564
+! (label_id: 1)                                          0.00       0.00       0.00        148
+, (label_id: 2)                                         69.27      76.77      72.83      19606
+\- (label_id: 3)                                         87.16      75.17      80.72       1788
+. (label_id: 4)                                         65.71      68.86      67.25      16090
+: (label_id: 5)                                          0.00       0.00       0.00        368
+; (label_id: 6)                                          0.00       0.00       0.00        202
+? (label_id: 7)                                         47.76      17.08      25.16       1370
+ (label_id: 8)                                          0.00       0.00       0.00        934
+… (label_id: 9)                                          0.00       0.00       0.00        122
+-------------------
+micro avg                                               72.03      72.03      72.03      46192
+macro avg                                               36.99      33.79      34.60      46192
+weighted avg                                            69.12      72.03      70.25      46192
+
+{'punct_f1': 34.595890045166016,
+ 'punct_precision': 36.98928451538086,
+ 'punct_recall': 33.78831481933594,
+ 'test_loss': 0.2638570964336395}
+
diff --git a/experiment/config.yaml b/experiment/config.yaml
index b137ae8..070bc4f 100644
--- a/experiment/config.yaml
+++ b/experiment/config.yaml
@@ -63,8 +63,10 @@ model:
     dataset:
         data_dir: /home/nxingyu/data # /root/data # 
         labelled:
-            - ${base_path}/ted_talks_processed #
+            # - ${base_path}/ted_talks_processed #
+            - ${base_path}/open_subtitles_processed #  
         unlabelled:
+            - ${base_path}/ted_talks_processed #
             # - ${base_path}/open_subtitles_processed #  
             # parameters for dataset preprocessing
         max_seq_length: 128
@@ -73,11 +75,11 @@ model:
         ignore_start_end: false
         use_cache: false
         # shared among dataloaders
-        num_workers:  0
+        num_workers:  4
         pin_memory: true
         drop_last: false
         num_labels: 10
-        num_domains: 1
+        num_domains: 2
         test_unlabelled: true
 
         train_ds:
@@ -105,12 +107,6 @@ model:
         config_file: null # json file, precedence over config
         config: null
         # unfrozen_layers: 1
-
-    mlp:
-        num_fc_layers: 1
-        fc_dropout: 0.1
-        log_softmax: false
-        activation: 'relu'
         
     punct_head:
         punct_num_fc_layers: 1
@@ -127,8 +123,8 @@ model:
         log_softmax: false
         use_transformer_init: true
         loss: 'cel'
-        gamma: 0.1 # coefficient of gradient reversal
-        pooling: 'mean'
+        gamma: 0 #0.1 # coefficient of gradient reversal
+        pooling: 'mean_max' # 'mean' mean_max
         idx_conditioned_on: 0
     
     dice_loss:
@@ -137,7 +133,7 @@ model:
         macro_average: true
 
     focal_loss: 
-        gamma: 5
+        gamma: 1
 
     optim:
         name: adamw
diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
index f9927ac..8053ecc 100644
--- a/experiment/core/layers/sequence_classifier.py
+++ b/experiment/core/layers/sequence_classifier.py
@@ -51,17 +51,18 @@ class SequenceClassifier(nn.Module):
         if use_transformer_init:
             self.apply(lambda module: transformer_weights_init(module, xavier=False))
 
-    def forward(self, hidden_states, subtoken_mask=None):
+    def forward(self, hidden_states, attention_mask=None):
         hidden_states = self.dropout(hidden_states)
         if self.pooling=='token':
             pooled = hidden_states[:, self._idx_conditioned_on]
         else:
-            if subtoken_mask==None:
+            if attention_mask is None:
                 ct=hidden_states.shape[1] # Seq len
-                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
             else:
-                ct = torch.sum(subtoken_mask,axis=1).unsqueeze(1)
-            pooled_sum = torch.sum(hidden_states,axis=1)            
+                hidden_states=hidden_states*attention_mask.unsqueeze(2) # remove subtoken or padding contribution.
+                ct = torch.sum(attention_mask,axis=1).unsqueeze(1)
+            pooled_sum = torch.sum(hidden_states,axis=1)
+
             if self.pooling=='mean' or self.pooling == 'mean_max':
                 pooled_mean = torch.div(pooled_sum,ct)
             if self.pooling=='max' or self.pooling=='mean_max':
diff --git a/experiment/data/__init__.py b/experiment/data/__init__.py
index 9b0816f..818d8e3 100644
--- a/experiment/data/__init__.py
+++ b/experiment/data/__init__.py
@@ -1,3 +1,3 @@
-from data.punctuation_dataset import PunctuationDomainDataset, PunctuationDomainDatasets, PunctuationInferenceDataset
+from data.punctuation_dataset_multi import PunctuationDomainDataset, PunctuationDomainDatasets, PunctuationInferenceDataset
 from data.punctuation_datamodule import PunctuationDataModule
 
diff --git a/experiment/data/punctuation_dataset.py b/experiment/data/punctuation_dataset.py
index 8374d29..bfd015c 100644
--- a/experiment/data/punctuation_dataset.py
+++ b/experiment/data/punctuation_dataset.py
@@ -36,7 +36,9 @@ class PunctuationDomainDataset(IterableDataset):
         labelled=True,
         randomize=True,
         target_file='',
-        tmp_path='~/data/tmp'
+        tmp_path='~/data/tmp',
+        start=0,
+        end=-1,
     ):
         if not (os.path.exists(csv_file)):
             raise FileNotFoundError(
@@ -148,7 +150,9 @@ class PunctuationDomainDatasets(IterableDataset):
                  tokenizer,
                  randomize:bool=True,
                  data_id='',
-                 tmp_path='~/data/tmp'):
+                 tmp_path='~/data/tmp',
+                 start=0,
+                 end=-1,):
         self.num_labelled=len(labelled)
         self.datasets = []
         self.iterables=[]
diff --git a/experiment/info.log b/experiment/info.log
index bd124fd..e69de29 100644
Binary files a/experiment/info.log and b/experiment/info.log differ
diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
index 43fc93d..6a54496 100644
--- a/experiment/models/punctuation_domain_model.py
+++ b/experiment/models/punctuation_domain_model.py
@@ -62,15 +62,6 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
         else:
             self.hparams.model.punct_class_weights=None
 
-        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
-        self.mlp = MultiLayerPerceptron(
-            self.transformer.config.hidden_size,
-            self.transformer.config.hidden_size,
-            num_layers=self.hparams.model.mlp.num_fc_layers, 
-            activation=self.hparams.model.mlp.activation, 
-            log_softmax=self.hparams.model.mlp.log_softmax
-        )
-
         self.punct_classifier = TokenClassifier(
             hidden_size=self.transformer.config.hidden_size,
             num_classes=len(self.labels_to_ids),
@@ -134,13 +125,11 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
         hidden_states = self.transformer(
             input_ids=input_ids, attention_mask=attention_mask
         )[0]
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.mlp(hidden_states)
         punct_logits = self.punct_classifier(hidden_states=hidden_states)
         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
         domain_logits = self.domain_classifier(
             hidden_states=reverse_grad_hidden_states,
-            subtoken_mask=subtoken_mask)
+            attention_mask=attention_mask)
         return punct_logits, domain_logits
 
     def _make_step(self, batch):
@@ -669,6 +658,13 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
 
         for layer in list(encoder.layer)[n:]:
             set_requires_grad_for_module(layer, True)
+        
+        # Set output layer to true.
+        last_iter=iter(encoder.layer[-1].children())
+        last = next(last_iter)
+        for last in last_iter:
+            continue
+        set_requires_grad_for_module(last, True)
 
     def freeze(self) -> None:
         try:
