commit hash: 5bbbaef9beee82324f41576fa8eca0ad2c24e5ae
diff --git a/experiment/Untitled.ipynb b/experiment/Untitled.ipynb
deleted file mode 100644
index f5bb089..0000000
--- a/experiment/Untitled.ipynb
+++ /dev/null
@@ -1,136 +0,0 @@
-{
- "cells": [
-  {
-   "cell_type": "code",
-   "execution_count": 1,
-   "id": "modern-amplifier",
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "14:59:48.20 LOG:\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "shuffling <data.punctuation_dataset_multi.PunctuationDomainDataset object at 0x7f44ea05e5b0>\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "14:59:48.28 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
-      "14:59:48.34 LOG:\n",
-      "14:59:48.63 .... f\"1st {n} encoder layers of transformer frozen\" = '1st 11 encoder layers of transformer frozen'\n",
-      "GPU available: True, used: False\n",
-      "TPU available: None, using: 0 TPU cores\n",
-      "[NeMo W 2021-02-09 14:59:48 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
-      "      warnings.warn(*args, **kwargs)\n",
-      "    \n"
-     ]
-    }
-   ],
-   "source": [
-    "import hydra\n",
-    "import numpy as np\n",
-    "import pytorch_lightning as pl\n",
-    "import torch\n",
-    "from omegaconf import DictConfig, OmegaConf\n",
-    "from transformers import AutoTokenizer\n",
-    "\n",
-    "from data import PunctuationDataModule, PunctuationInferenceDataset\n",
-    "import os\n",
-    "from models import PunctuationDomainModel\n",
-    "\n",
-    "from nemo.utils.exp_manager import exp_manager\n",
-    "from time import time\n",
-    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
-    "\n",
-    "import atexit\n",
-    "from copy import deepcopy\n",
-    "import snoop\n",
-    "snoop.install()\n",
-    "\n",
-    "from hydra.experimental import initialize, initialize_config_module, initialize_config_dir, compose\n",
-    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
-    "initialize()\n",
-    "cfg=compose(\n",
-    "    config_name=\"test_config.yaml\", \n",
-    ")\n",
-    "cfg.model.punct_label_ids=OmegaConf.create(sorted(cfg.model.punct_label_ids))\n",
-    "labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
-    "ids_to_labels = {_[0]:_[1] for _ in enumerate(cfg.model.punct_label_ids)}\n",
-    "\n",
-    "model = PunctuationDomainModel.load_from_checkpoint(\n",
-    "    checkpoint_path=\"/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_14-05-14/checkpoints/Punctuation_with_Domain_discriminator-last.ckpt\")\n",
-    "\n",
-    "trainer = pl.Trainer(**cfg.trainer)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 8,
-   "id": "hairy-proxy",
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "[' we bought four shirts, one pen, and a mug from the nvidia gear store in santa clara                                                                                                            ',\n",
-       " ' what can i do for you today?                                                                                                                        ',\n",
-       " ' , how are you? ,                                                                                                                           ',\n",
-       " ' good morning. everyone? how have your weekends been? its a really great day?                                                                                                                  ']"
-      ]
-     },
-     "execution_count": 8,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "queries = [\n",
-    "    'we bought four shirts one pen and a mug from the nvidia gear store in santa clara',\n",
-    "    'what can i do for you today',\n",
-    "    'how are you',\n",
-    "    'good morning everyone how have your weekends been its a really great day'\n",
-    "]\n",
-    "inference_results = model.add_punctuation(queries)\n",
-    "inference_results"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "id": "employed-station",
-   "metadata": {},
-   "outputs": [],
-   "source": []
-  }
- ],
- "metadata": {
-  "kernelspec": {
-   "display_name": "Python 3",
-   "language": "python",
-   "name": "python3"
-  },
-  "language_info": {
-   "codemirror_mode": {
-    "name": "ipython",
-    "version": 3
-   },
-   "file_extension": ".py",
-   "mimetype": "text/x-python",
-   "name": "python",
-   "nbconvert_exporter": "python",
-   "pygments_lexer": "ipython3",
-   "version": "3.8.5"
-  }
- },
- "nbformat": 4,
- "nbformat_minor": 5
-}
diff --git a/experiment/config.yaml b/experiment/config.yaml
index 8226922..6b576c7 100644
--- a/experiment/config.yaml
+++ b/experiment/config.yaml
@@ -1,43 +1,43 @@
 seed: 42
 trainer:
-    gpus: 1 # the number of gpus, 0 for CPU
+    # gpus: 1 # the number of gpus, 0 for CPU
+    # num_nodes: 1
+    # max_epochs: 15
+    # max_steps: null # precedence over max_epochs
+    # accumulate_grad_batches: 4 # accumulates grads every k batches
+    # gradient_clip_val: 0
+    # amp_level: O1 # O1/O2 for mixed precision
+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
+    # accelerator: ddp
+    # checkpoint_callback: false  # Provided by exp_manager
+    # logger: false #false  # Provided by exp_manager
+    # log_every_n_steps: 1  # Interval of logging.
+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
+    # resume_from_checkpoint: null
+
+    gpus: 0 # the number of gpus, 0 for CPU
     num_nodes: 1
     max_epochs: 15
     max_steps: null # precedence over max_epochs
     accumulate_grad_batches: 4 # accumulates grads every k batches
     gradient_clip_val: 0
-    amp_level: O1 # O1/O2 for mixed precision
-    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-    accelerator: ddp
+    amp_level: O0 # O1/O2 for mixed precision
+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
+    # accelerator: ddp
     checkpoint_callback: false  # Provided by exp_manager
     logger: false #false  # Provided by exp_manager
     log_every_n_steps: 1  # Interval of logging.
     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
+    reload_dataloaders_every_epoch: true
     resume_from_checkpoint: null
 
-    # gpus: 0 # the number of gpus, 0 for CPU
-    # num_nodes: 1
-    # max_epochs: 8
-    # max_steps: null # precedence over max_epochs
-    # accumulate_grad_batches: 1 # accumulates grads every k batches
-    # gradient_clip_val: 0
-    # amp_level: O0 # O1/O2 for mixed precision
-    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-    # # accelerator: ddp
-    # checkpoint_callback: false  # Provided by exp_manager
-    # logger: false #false  # Provided by exp_manager
-    # log_every_n_steps: 1  # Interval of logging.
-    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-    # reload_dataloaders_every_epoch: true
-    # resume_from_checkpoint: null
-
 exp_manager:
-    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
     name: Punctuation_with_Domain_discriminator  # The name of your model
     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
     create_checkpoint_callback: true 
-base_path: /home/nxingyu/data # /root/data # 
-tmp_path: /home/nxingyu/data/tmp # /tmp # 
+base_path: /home/nxingyu2/data # /root/data # 
+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
 
 model:
     nemo_path: null
@@ -61,7 +61,7 @@ model:
     punct_class_weights: false
     
     dataset:
-        data_dir: /home/nxingyu/data # /root/data # 
+        data_dir: /home/nxingyu2/data # /root/data # 
         labelled:
             # - ${base_path}/ted_talks_processed #
             - ${base_path}/open_subtitles_processed #  
@@ -81,6 +81,7 @@ model:
         num_labels: 10
         num_domains: 1
         test_unlabelled: true
+        attach_label_to_end: false # false if attach to start
 
         train_ds:
             shuffle: true
diff --git a/experiment/core/utils.py b/experiment/core/utils.py
index ce7436b..a2a708d 100644
--- a/experiment/core/utils.py
+++ b/experiment/core/utils.py
@@ -92,19 +92,19 @@ def subword_tokenize(tokenizer,tokens):
     subwords = list(map(tokenizer.tokenize, tokens))
     subword_lengths = list(map(len, subwords))
     subwords = list(flatten(subwords))
-    # token_start_idxs = np.cumsum([0]+subword_lengths[:-1])
+    token_start_idxs = np.cumsum([0]+subword_lengths[:-1])
     token_end_idxs = np.cumsum([0]+subword_lengths[:-1])+np.array(subword_lengths)-1
-    return subwords, token_end_idxs
+    return subwords, token_start_idxs,token_end_idxs
 
-def chunk_to_len(max_seq_length,tokenizer,tokens,labels=None):
-    subwords,token_end_idxs = subword_tokenize(tokenizer,tokens)
-    teim=token_end_idxs%(max_seq_length-2)
+def chunk_to_len(max_seq_length,tokenizer,attach_label_to_end,tokens,labels=None):
+    subwords,token_start_idxs,token_end_idxs = subword_tokenize(tokenizer,tokens)
+    teim=token_end_idxs%(max_seq_length-2) if attach_label_to_end else token_start_idxs%(max_seq_length-2)
     breakpoints=(np.argwhere(teim[1:]<teim[:-1]).flatten()+1).tolist()
-    split_token_end_idxs=np.array_split(token_end_idxs,breakpoints)
+    split_token_idxs=np.array_split(token_end_idxs,breakpoints) if attach_label_to_end else np.array_split(token_start_idxs,breakpoints)
     split_subwords=np.array_split(subwords,np.arange(max_seq_length-2,len(subwords),max_seq_length-2))
     ids=[pad_to_len(max_seq_length,tokenizer.convert_tokens_to_ids(['[CLS]']+list(_)+['[SEP]'])) for _ in split_subwords]
     masks=[]
-    for _ in split_token_end_idxs:
+    for _ in split_token_idxs:
         masks.append(position_to_mask(max_seq_length,_).copy())
     padded_labels=None
     if labels!=None:
@@ -112,12 +112,12 @@ def chunk_to_len(max_seq_length,tokenizer,tokens,labels=None):
         padded_labels=[pad_to_len(max_seq_length,align_labels_to_mask(*_)) for _ in zip(masks,split_labels)]
     return ids,masks,padded_labels
     
-def chunk_to_len_batch(max_seq_length,tokenizer,tokens,labels=None,labelled=True,ignore_index=-100):
+def chunk_to_len_batch(max_seq_length,tokenizer,tokens,labels=None,labelled=True,ignore_index=-100, attach_label_to_end=True):
     batch_ids=[]
     batch_masks=[]
     batch_labels=[]
     for i,_ in enumerate(zip(tokens,tokens) if labels==None else zip(tokens,labels)):
-        a,b,c=chunk_to_len(max_seq_length,tokenizer,*_) if labels else chunk_to_len(max_seq_length,tokenizer,_[0])
+        a,b,c=chunk_to_len(max_seq_length,tokenizer,attach_label_to_end,*_) if labels else chunk_to_len(max_seq_length,tokenizer,attach_label_to_end,_[0])
         batch_ids.extend(a)
         batch_masks.extend(b)
         if labelled==True:
diff --git a/experiment/data/punctuation_datamodule.py b/experiment/data/punctuation_datamodule.py
index 8711456..fb69299 100644
--- a/experiment/data/punctuation_datamodule.py
+++ b/experiment/data/punctuation_datamodule.py
@@ -24,7 +24,8 @@ class PunctuationDataModule(LightningDataModule):
             seed: int = 42,
             data_id: str = '',
             tmp_path:str = '~/data/tmp',
-            test_unlabelled:bool = True
+            test_unlabelled:bool = True,
+            attach_label_to_end:bool = True,
             ):
         #unlabelled=[], batch_size = 256, max_seq_length = 256, num_workers=1):
         super().__init__()
@@ -52,6 +53,7 @@ class PunctuationDataModule(LightningDataModule):
         self.data_id=data_id
         self.tmp_path=tmp_path
         self.test_unlabelled=test_unlabelled
+        self.attach_label_to_end=attach_label_to_end
     
     def reset(self):
         # self.setup('fit')
@@ -70,7 +72,8 @@ class PunctuationDataModule(LightningDataModule):
                     tokenizer=self.tokenizer,
                     randomize=self.train_shuffle,
                     data_id=self.data_id,
-                    tmp_path=self.tmp_path)
+                    tmp_path=self.tmp_path,
+                    attach_label_to_end=self.attach_label_to_end)
             self.val_dataset = PunctuationDomainDatasets(split='dev',
                     num_samples=self.val_batch_size,
                     max_seq_length=self.max_seq_length,
@@ -80,7 +83,8 @@ class PunctuationDataModule(LightningDataModule):
                     tokenizer=self.tokenizer,
                     randomize=self.val_shuffle,
                     data_id=self.data_id,
-                    tmp_path=self.tmp_path)
+                    tmp_path=self.tmp_path,
+                    attach_label_to_end=self.attach_label_to_end)
         if stage=='test' or stage is None:
             if (len(self.unlabelled)>0) and self.test_unlabelled:
                 self.test_dataset = PunctuationDomainDatasets(split='test',
@@ -92,7 +96,8 @@ class PunctuationDataModule(LightningDataModule):
                     tokenizer=self.tokenizer,
                     randomize=self.val_shuffle,
                     data_id=self.data_id,
-                    tmp_path=self.tmp_path
+                    tmp_path=self.tmp_path,
+                    attach_label_to_end=self.attach_label_to_end
                     )
             else: self.test_dataset = PunctuationDomainDatasets(split='test',
                     num_samples=self.val_batch_size,
@@ -103,7 +108,8 @@ class PunctuationDataModule(LightningDataModule):
                     tokenizer=self.tokenizer,
                     randomize=self.val_shuffle,
                     data_id=self.data_id,
-                    tmp_path=self.tmp_path
+                    tmp_path=self.tmp_path,
+                    attach_label_to_end=self.attach_label_to_end
                     )
 
         logging.info(f"shuffling train set")
diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
index 31923de..6a0452f 100644
--- a/experiment/data/punctuation_dataset_multi.py
+++ b/experiment/data/punctuation_dataset_multi.py
@@ -39,6 +39,7 @@ class PunctuationDomainDataset(IterableDataset):
         tmp_path='~/data/tmp',
         start=0,
         end=-1,
+        attach_label_to_end=True,
     ):
         if not (os.path.exists(csv_file)):
             raise FileNotFoundError(
@@ -63,6 +64,7 @@ class PunctuationDomainDataset(IterableDataset):
         self.randomize=randomize
         self.target_file=target_file
         self.tmp_path=tmp_path
+        self.attach_label_to_end=attach_label_to_end
         if not (os.path.exists(self.target_file)):
             os.system(f'cp {self.csv_file} {self.target_file}')
 
@@ -152,6 +154,7 @@ class PunctuationDomainDatasets(IterableDataset):
                  randomize:bool=True,
                  data_id='',
                  tmp_path='~/data/tmp',
+                 attach_label_to_end=True,
                  ):
         worker_info = get_worker_info()
         self.num_workers=1 if worker_info is None else worker_info.num_workers
@@ -177,7 +180,8 @@ class PunctuationDomainDatasets(IterableDataset):
                     punct_label_ids=punct_label_ids,domain=i,labelled=True,
                     randomize=randomize,
                     target_file=f'{target}.{split}.{data_id}.csv',
-                    tmp_path=tmp_path)
+                    tmp_path=tmp_path,
+                    attach_label_to_end=attach_label_to_end)
             self.datasets.append(dataset)
             self.iterables.append(cycle(dataset))
             
@@ -189,7 +193,8 @@ class PunctuationDomainDatasets(IterableDataset):
                     punct_label_ids=punct_label_ids,domain=len(labelled)+i,labelled=False,
                     randomize=randomize,
                     target_file=f'{target}.{split}.{data_id}.csv',
-                    tmp_path=tmp_path)
+                    tmp_path=tmp_path,
+                    attach_label_to_end=attach_label_to_end)
             self.datasets.append(dataset)
             self.iterables.append(cycle(dataset))
 
@@ -261,12 +266,13 @@ class PunctuationInferenceDataset(Dataset):
             "labels": NeuralType(('B', 'T'), ChannelType()),
         }
 
-    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], num_samples:int=256, degree:int = 0):
+    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], num_samples:int=256, degree:int = 0, attach_label_to_end:bool=True):
         """ Initializes BertPunctuationInferDataset. """
         self.degree=degree
         self.punct_label_ids=punct_label_ids
         chunked=chunk_examples_with_degree(self.degree, self.punct_label_ids)(queries)
         self.features = chunk_to_len_batch(max_seq_length, tokenizer,chunked['texts'],chunked['tags'],True)
+        self.attach_label_to_end=attach_label_to_end
         # self.all_input_ids = features['input_ids']
         # self.all_attention_mask = features['attention_mask']
         # self.all_subtoken_mask = features['subtoken_mask']
diff --git a/experiment/info.log b/experiment/info.log
index a9aa9db..5024c4f 100644
--- a/experiment/info.log
+++ b/experiment/info.log
@@ -1,43 +1,2 @@
-[INFO] - shuffling train set
-[INFO] - Optimizer config = AdamW (
-Parameter Group 0
-    amsgrad: False
-    betas: (0.9, 0.999)
-    eps: 1e-08
-    lr: 0.01
-    weight_decay: 0.0
-)
-[INFO] - Scheduler "<core.optim.lr_scheduler.CosineAnnealing object at 0x7f68053f0940>" 
-will be used during training (effective maximum steps = 79575) - 
-Parameters : 
-(warmup_steps: null
-warmup_ratio: 0.1
-min_lr: 1.0e-10
-last_epoch: -1
-max_steps: 79575
-)
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          80.97      20.28      32.44       9732
-! (label_id: 1)                                          1.45       2.63       1.87        152
-, (label_id: 2)                                          5.44      27.46       9.08        772
-- (label_id: 3)                                          1.70      53.57       3.30         56
-. (label_id: 4)                                          3.12       0.85       1.34       1642
-: (label_id: 5)                                          0.00       0.00       0.00          0
-; (label_id: 6)                                          0.00       0.00       0.00          0
-? (label_id: 7)                                          1.42      22.94       2.67        218
-— (label_id: 8)                                          0.00       0.00       0.00          0
-… (label_id: 9)                                          0.93       2.04       1.28         98
--------------------
-micro avg                                               18.04      18.04      18.04      12670
-macro avg                                               13.58      18.54       7.42      12670
-weighted avg                                            62.99      18.04      25.74      12670
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00        116
--------------------
-micro avg                                              100.00     100.00     100.00        116
-macro avg                                              100.00     100.00     100.00        116
-weighted avg                                           100.00     100.00     100.00        116
-
+[INFO] - GPU available: True, used: False
+[INFO] - TPU available: None, using: 0 TPU cores
diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
index 5b5d668..09eb9d5 100644
--- a/experiment/models/punctuation_domain_model.py
+++ b/experiment/models/punctuation_domain_model.py
@@ -514,7 +514,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
             seed=self._cfg.seed,
             data_id=self.data_id,
             tmp_path=self.hparams.tmp_path,
-            test_unlabelled=data_config.test_unlabelled
+            test_unlabelled=data_config.test_unlabelled,
+            attach_label_to_end=data_config.attach_label_to_end,
         )
         self.dm.setup()
         self._train_dl=self.dm.train_dataloader
@@ -722,7 +723,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
             tokenizer= self.tokenizer,
             queries=queries, 
             max_seq_length=self.hparams.model.dataset.max_seq_length,
-            punct_label_ids=self._cfg.model.punct_label_ids)
+            punct_label_ids=self._cfg.model.punct_label_ids,
+            attach_label_to_end=self._cfg.model.dataset.attach_label_to_end)
         batch=ds[0]
         attention_mask = batch['attention_mask']
         subtoken_mask = batch['subtoken_mask']
diff --git a/experiment/test_config.yaml b/experiment/test_config.yaml
index a179869..0cc65e7 100644
--- a/experiment/test_config.yaml
+++ b/experiment/test_config.yaml
@@ -32,15 +32,16 @@ trainer:
     resume_from_checkpoint: null
 
 exp_manager:
-    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
     name: Punctuation_with_Domain_discriminator  # The name of your model
     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
     create_checkpoint_callback: true 
-    restore_path: /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/
-    override_config_path: /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/hparams.yaml
+    restore_path: /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/
+    # 2021-02-08_11-57-58
+    override_config_path: /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/hparams.yaml
 
-base_path: /home/nxingyu/data # /root/data # 
-tmp_path: /home/nxingyu/data/tmp # /tmp # 
+base_path: /home/nxingyu2/data # /root/data # 
+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
 
 model:
     nemo_path: null
@@ -84,6 +85,7 @@ model:
         num_labels: 10
         num_domains: 2
         test_unlabelled: true
+        attach_label_to_end: false
 
         train_ds:
             shuffle: true
