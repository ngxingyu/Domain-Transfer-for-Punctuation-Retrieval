commit hash: b4eb0170684a284ad305919aa718f16d67c3b65e
diff --git a/experiment/config.yaml b/experiment/config.yaml
index 4555062..053824d 100644
--- a/experiment/config.yaml
+++ b/experiment/config.yaml
@@ -51,23 +51,24 @@ model:
         - ","
         - "."
         - "?"
-        # - "-"
-        # - "!"
-        # - ":"
-        # - "—"
+        - "-"
+        - "!"
+        - ":"
+        - "…"
 
+        # - "—"
         # - ";"
-        # - "…"
 
     label_map:
-        "-": ","
-        ":": ","
-        "—": ","
-        "!": "."
+        # "-": ","
+        # "!": "."
+        # ":": ","
+        # "…": "."
 
-        "…": "."
+        "—": ","
         ";": "."
 
+        
     no_space_label: '#'
     test_chunk_percent: 0.5
 
@@ -76,9 +77,9 @@ model:
     dataset:
         data_dir: /home/nxingyu/data # /root/data # 
         labelled:
-            - ${base_path}/ted2010 #
+            # - ${base_path}/ted2010 #
             # - ${base_path}/ted_talks_processed #
-            # - ${base_path}/open_subtitles_processed #  
+            - ${base_path}/open_subtitles_processed #  
         unlabelled:
             # - ${base_path}/ted_talks_processed #
             # - ${base_path}/open_subtitles_processed #  
@@ -100,8 +101,8 @@ model:
         train_ds:
             shuffle: true
             num_samples: -1
-            batch_size: 16
-            manual_len: 40000 #default 0 84074
+            batch_size: 8 #16
+            manual_len: 20000 #default 0 84074
 
         validation_ds:
             # if evaluation data is not in the model.dataset.data_dir as the training data or multiple datasets are used for
@@ -109,7 +110,7 @@ model:
             # ds_item: null # expected format: [PATH_TO_DEV1,PATH_TO_DEV2] (Note no space between the paths and square brackets)
             shuffle: true
             num_samples: -1
-            batch_size: 16
+            batch_size: 4 #16
 
     tokenizer:
         tokenizer_name: ${model.language_model.pretrained_model_name} # or sentencepiece
@@ -125,12 +126,12 @@ model:
         # unfrozen_layers: 1
     
     punct_head:
-        punct_num_fc_layers: 3
+        punct_num_fc_layers: 0
         fc_dropout: 0.1
         activation: 'gelu'
         log_softmax: false
         use_transformer_init: true
-        loss: 'dice'
+        loss: 'crf'
         bilstm: false
 
     domain_head:
diff --git a/experiment/core/utils.py b/experiment/core/utils.py
index 567f04b..2d3c063 100644
--- a/experiment/core/utils.py
+++ b/experiment/core/utils.py
@@ -46,6 +46,7 @@ def view_aligned(texts,tags,tokenizer,labels_to_ids):
             )]
         )))) for _ in zip(texts,tags)]
 
+def text2masks(n, labels_to_ids,label_map):
     def text2masks(text):
         '''Converts single paragraph of text into a list of words and corresponding punctuation based on the degree requested.'''
         labels=''.join(labels_to_ids.keys())
@@ -73,26 +74,24 @@ def view_aligned(texts,tags,tokenizer,labels_to_ids):
             if p!='':
                 wordlist.append(p)
                 punctlist.append(0)
-        wordlist=['']*pad_start_and_end+wordlist+['']*pad_start_and_end
-        punctlist=['']*pad_start_and_end+wordlist+[0]*pad_start_and_end
         return(wordlist,punctlist)
     return text2masks
 
-def chunk_examples_with_degree(n, labels_to_ids,label_map,pad_start_and_end):
+def chunk_examples_with_degree(n, labels_to_ids,label_map):
     '''Ensure batched=True if using dataset.map or ensure the examples are wrapped in lists.'''
     def chunk_examples(examples):
         output={}
         output['texts']=[]
         output['tags']=[]
         for sentence in examples:
-            text,tag=text2masks(n, labels_to_ids, label_map,pad_start_and_end)(sentence)
+            text,tag=text2masks(n, labels_to_ids, label_map)(sentence)
             output['texts'].append(text)
             output['tags'].append(tag)
             # output['tags'].append([0]+tag if text[0]!='' else tag) # [0]+tag so that in all case, the first tag refers to [CLS]
             # not necessary since all the leading punctuations are stripped
         return output
     return chunk_examples
-assert(chunk_examples_with_degree(0,{'': 0, '!': 1, ',': 2, '-': 3, '.': 4, ':': 5, '?': 6, '—': 7},{'…':'.',';':'.'},0)(['Hello!Bye…'])=={'texts': [['Hello', 'Bye']], 'tags': [[1, 4]]})
+assert(chunk_examples_with_degree(0,{'': 0, '!': 1, ',': 2, '-': 3, '.': 4, ':': 5, '?': 6, '—': 7},{'…':'.',';':'.'})(['Hello!Bye…'])=={'texts': [['Hello', 'Bye']], 'tags': [[1, 4]]})
 
 def subword_tokenize(tokenizer,tokens):
     subwords = list(map(tokenizer.tokenize, tokens))
diff --git a/experiment/info.log b/experiment/info.log
index 105cc77..e69de29 100644
--- a/experiment/info.log
+++ b/experiment/info.log
@@ -1,764 +0,0 @@
-[INFO] - GPU available: True, used: True
-[INFO] - TPU available: None, using: 0 TPU cores
-[INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
-[INFO] - Using native 16bit precision.
-[INFO] - shuffling train set
-[INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-[INFO] - Optimizer config = AdamW (
-Parameter Group 0
-    amsgrad: False
-    betas: (0.9, 0.999)
-    eps: 1e-08
-    lr: 0.02
-    weight_decay: 0.0
-)
-[INFO] - Scheduler "<core.optim.lr_scheduler.CosineAnnealing object at 0x7f8622566e80>" 
-will be used during training (effective maximum steps = 3750) - 
-Parameters : 
-(warmup_steps: null
-warmup_ratio: 0.1
-min_lr: 1.0e-08
-last_epoch: -1
-max_steps: 3750
-)
-[INFO] - 
-  | Name                       | Type                 | Params
---------------------------------------------------------------------
-0 | transformer                | ElectraModel         | 108 M 
-1 | punct_classifier           | TokenClassifier      | 6.9 K 
-2 | domain_classifier          | SequenceClassifier   | 4.7 M 
-3 | punctuation_loss           | LinearChainCRF       | 99    
-4 | domain_loss                | CrossEntropyLoss     | 0     
-5 | agg_loss                   | AggregatorLoss       | 0     
-6 | punct_class_report         | ClassificationReport | 0     
-7 | chunked_punct_class_report | ClassificationReport | 0     
-8 | domain_class_report        | ClassificationReport | 0     
---------------------------------------------------------------------
-4.7 M     Trainable params
-108 M     Non-trainable params
-113 M     Total params
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          55.42       0.10       0.20      89602
-! (label_id: 1)                                          1.48       1.63       1.55        858
-# (label_id: 2)                                         18.33       6.47       9.57      17576
-, (label_id: 3)                                          3.20       1.88       2.37       7660
-- (label_id: 4)                                          0.58       6.34       1.06        662
-. (label_id: 5)                                          6.14      31.53      10.28      14950
-: (label_id: 6)                                          0.00       0.00       0.00         10
-? (label_id: 7)                                          1.68       1.42       1.54       2672
-— (label_id: 8)                                          0.00       0.00       0.00          0
--------------------
-micro avg                                                4.61       4.61       4.61     133990
-macro avg                                               10.85       6.17       3.32     133990
-weighted avg                                            40.38       4.61       2.72     133990
-
--------------------
-                       !           #           ,           -           .           :           ?           —
-       92.00         2.00        30.00         4.00         0.00        32.00         0.00         6.00         0.00
-      550.00        14.00       112.00        70.00        10.00       172.00         0.00        20.00         0.00
-     3552.00        58.00      1138.00       580.00        34.00       746.00         0.00       100.00         0.00
-     3062.00        32.00       712.00       144.00        28.00       434.00         0.00        84.00         0.00
-     3834.00        84.00       926.00       710.00        42.00      1384.00         0.00       280.00         0.00
-    61182.00       188.00      7050.00      2474.00       194.00      4714.00         2.00       982.00         0.00
-     1376.00         0.00      5720.00         4.00         2.00        32.00         0.00         2.00         0.00
-     1064.00        66.00       286.00       348.00        70.00       384.00         0.00        38.00         0.00
-    14890.00       414.00      1602.00      3326.00       282.00      7052.00         8.00      1160.00         0.00
--------------------
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       1072
--------------------
-micro avg                                              100.00     100.00     100.00       1072
-macro avg                                              100.00     100.00     100.00       1072
-weighted avg                                           100.00     100.00     100.00       1072
-
--------------------
-           0
-     1072.00
--------------------
-
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          95.66      97.53      96.59   32695556
-! (label_id: 1)                                         46.72       0.80       1.57     594128
-# (label_id: 2)                                         99.22      99.11      99.16    6532600
-, (label_id: 3)                                         53.46      37.16      43.84    2688848
-- (label_id: 4)                                         47.86      26.80      34.36     260520
-. (label_id: 5)                                         60.75      78.33      68.43    5279872
-: (label_id: 6)                                         75.05      38.90      51.24      14048
-? (label_id: 7)                                         70.44      36.34      47.95    1306712
-— (label_id: 8)                                          0.00       0.00       0.00         16
--------------------
-micro avg                                               89.22      89.22      89.22   49372300
-macro avg                                               61.02      46.11      49.24   49372300
-weighted avg                                            88.59      89.22      88.27   49372300
-
--------------------
-                       !           #           ,           -           .           :           ?           —
- 31887728.00     40504.00     50096.00    551448.00    107368.00    558784.00      3416.00    134016.00         0.00
-      496.00      4736.00         0.00      1240.00        40.00      3208.00         0.00       416.00         0.00
-    41624.00       640.00   6474232.00      1728.00      2288.00      3824.00        40.00       520.00         0.00
-   165912.00    112616.00       656.00    999112.00     14488.00    481320.00       608.00     94144.00         0.00
-    46512.00      1880.00       480.00      6544.00     69808.00     19784.00       104.00       760.00         0.00
-   503720.00    417608.00      6144.00   1076392.00     61456.00   4135944.00      4368.00    601960.00        16.00
-      448.00         0.00         8.00       880.00       160.00       320.00      5464.00         0.00         0.00
-    49048.00     16144.00       984.00     51504.00      4912.00     76688.00        48.00    474896.00         0.00
-        0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
--------------------
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00     393864
--------------------
-micro avg                                              100.00     100.00     100.00     393864
-macro avg                                              100.00     100.00     100.00     393864
-weighted avg                                           100.00     100.00     100.00     393864
-
--------------------
-           0
-   393864.00
--------------------
-
-[INFO] - Epoch 0, global step 624: val_loss reached 39.13414 (best 39.13414), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-21_15-37-04/checkpoints/Punctuation_with_Domain_discriminator---val_loss=39.13-epoch=0.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          95.69      97.59      96.63   32695556
-! (label_id: 1)                                         49.22       0.85       1.67     594128
-# (label_id: 2)                                         99.36      99.24      99.30    6532600
-, (label_id: 3)                                         53.14      37.83      44.20    2688848
-- (label_id: 4)                                         46.01      28.17      34.94     260520
-. (label_id: 5)                                         61.05      78.02      68.50    5279872
-: (label_id: 6)                                         72.72      42.20      53.41      14048
-? (label_id: 7)                                         71.20      36.27      48.05    1306712
-— (label_id: 8)                                          0.00       0.00       0.00         16
--------------------
-micro avg                                               89.29      89.29      89.29   49372300
-macro avg                                               60.93      46.68      49.63   49372300
-weighted avg                                            88.68      89.29      88.35   49372300
-
--------------------
-                       !           #           ,           -           .           :           ?           —
- 31906840.00     39872.00     42424.00    551472.00    105208.00    560232.00      3056.00    133912.00         0.00
-      480.00      5040.00         0.00      1248.00        32.00      3128.00         0.00       312.00         0.00
-    32624.00       624.00   6482832.00      1800.00      2184.00      3680.00        48.00       544.00         0.00
-   167432.00    118496.00       624.00   1017288.00     14344.00    497248.00       600.00     98264.00         0.00
-    48600.00      3232.00       544.00     10168.00     73384.00     22304.00       136.00      1144.00         0.00
-   491656.00    411104.00      5392.00   1056296.00     60656.00   4119144.00      4232.00    598624.00        16.00
-      608.00         0.00         0.00       936.00       264.00       400.00      5928.00        16.00         0.00
-    47296.00     15760.00       784.00     49640.00      4448.00     73736.00        48.00    473896.00         0.00
-        0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
--------------------
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00     393864
--------------------
-micro avg                                              100.00     100.00     100.00     393864
-macro avg                                              100.00     100.00     100.00     393864
-weighted avg                                           100.00     100.00     100.00     393864
-
--------------------
-           0
-   393864.00
--------------------
-
-[INFO] - Epoch 1, global step 1249: val_loss reached 38.60223 (best 38.60223), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-21_15-37-04/checkpoints/Punctuation_with_Domain_discriminator---val_loss=38.60-epoch=1.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          96.03      97.33      96.67   32695556
-! (label_id: 1)                                         48.26       1.75       3.38     594128
-# (label_id: 2)                                         99.23      99.34      99.28    6532600
-, (label_id: 3)                                         54.39      36.47      43.66    2688848
-- (label_id: 4)                                         45.30      29.78      35.93     260520
-. (label_id: 5)                                         61.10      78.05      68.54    5279872
-: (label_id: 6)                                         72.36      44.42      55.05      14048
-? (label_id: 7)                                         62.38      45.18      52.41    1306712
-— (label_id: 8)                                          0.00       0.00       0.00         16
--------------------
-micro avg                                               89.32      89.32      89.32   49372300
-macro avg                                               59.89      48.04      50.55   49372300
-weighted avg                                            88.71      89.32      88.50   49372300
-
--------------------
-                       !           #           ,           -           .           :           ?           —
- 31821584.00     35096.00     34912.00    510624.00     98672.00    516712.00      2736.00    116296.00         0.00
-     1216.00     10408.00         0.00      2816.00        88.00      6504.00         0.00       536.00         0.00
-    40064.00       752.00   6489584.00      2152.00      2544.00      4448.00        40.00       680.00         0.00
-   162944.00    111856.00       640.00    980584.00     13880.00    448792.00       552.00     83512.00         0.00
-    55792.00      3248.00       488.00      9760.00     77576.00     23144.00       144.00      1088.00         0.00
-   534952.00    406952.00      6040.00   1096960.00     60344.00   4121048.00      4216.00    514152.00        16.00
-      680.00         0.00         0.00       920.00       296.00       472.00      6240.00        16.00         0.00
-    78248.00     25816.00       936.00     85032.00      7120.00    158752.00       120.00    590432.00         0.00
-        0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
--------------------
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00     393864
--------------------
-micro avg                                              100.00     100.00     100.00     393864
-macro avg                                              100.00     100.00     100.00     393864
-weighted avg                                           100.00     100.00     100.00     393864
-
--------------------
-           0
-   393864.00
--------------------
-
-[INFO] - Epoch 2, global step 1874: val_loss reached 38.55209 (best 38.55209), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-21_15-37-04/checkpoints/Punctuation_with_Domain_discriminator---val_loss=38.55-epoch=2.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          95.23      97.90      96.55   32695556
-! (label_id: 1)                                         42.07       5.46       9.67     594128
-# (label_id: 2)                                         99.45      99.23      99.34    6532600
-, (label_id: 3)                                         56.91      32.50      41.37    2688848
-- (label_id: 4)                                         48.29      27.41      34.97     260520
-. (label_id: 5)                                         60.76      78.90      68.65    5279872
-: (label_id: 6)                                         75.19      44.02      55.53      14048
-? (label_id: 7)                                         73.09      34.56      46.93    1306712
-— (label_id: 8)                                          0.00       0.00       0.00         16
--------------------
-micro avg                                               89.31      89.31      89.31   49372300
-macro avg                                               61.22      46.66      50.33   49372300
-weighted avg                                            88.54      89.31      88.23   49372300
-
--------------------
-                       !           #           ,           -           .           :           ?           —
- 32009576.00     46312.00     43264.00    627064.00    109416.00    624936.00      3048.00    148480.00         0.00
-     4488.00     32440.00         8.00     15216.00       616.00     21392.00        56.00      2896.00         0.00
-    27224.00       624.00   6482408.00      1624.00      2096.00      3392.00        32.00       576.00         0.00
-   104248.00     87456.00       408.00    873904.00     11080.00    383224.00       440.00     74824.00         0.00
-    44120.00      2640.00       456.00      8632.00     71408.00     19640.00        80.00       896.00         0.00
-   463664.00    411120.00      5320.00   1116560.00     61648.00   4165640.00      4168.00    627456.00        16.00
-      552.00         0.00         0.00       752.00       320.00       392.00      6184.00        24.00         0.00
-    41632.00     13536.00       736.00     45096.00      3936.00     61256.00        40.00    451560.00         0.00
-        0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
--------------------
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00     393864
--------------------
-micro avg                                              100.00     100.00     100.00     393864
-macro avg                                              100.00     100.00     100.00     393864
-weighted avg                                           100.00     100.00     100.00     393864
-
--------------------
-           0
-   393864.00
--------------------
-
-[INFO] - Epoch 3, global step 2499: val_loss reached 38.22637 (best 38.22637), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-21_15-37-04/checkpoints/Punctuation_with_Domain_discriminator---val_loss=38.23-epoch=3.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          95.39      97.84      96.60   32695556
-! (label_id: 1)                                         34.51      12.96      18.85     594128
-# (label_id: 2)                                         99.46      99.24      99.35    6532600
-, (label_id: 3)                                         53.87      38.53      44.93    2688848
-- (label_id: 4)                                         52.33      25.83      34.58     260520
-. (label_id: 5)                                         62.62      75.60      68.50    5279872
-: (label_id: 6)                                         75.19      45.22      56.47      14048
-? (label_id: 7)                                         71.84      36.25      48.19    1306712
-— (label_id: 8)                                          0.00       0.00       0.00         16
--------------------
-micro avg                                               89.37      89.37      89.37   49372300
-macro avg                                               60.58      47.94      51.94   49372300
-weighted avg                                            88.57      89.37      88.59   49372300
-
--------------------
-                       !           #           ,           -           .           :           ?           —
- 31989448.00     43024.00     42856.00    592656.00    110128.00    611656.00      2832.00    144520.00         0.00
-    14264.00     77024.00        56.00     47352.00      3184.00     70136.00       128.00     11048.00         0.00
-    27464.00       560.00   6483024.00      1536.00      2136.00      3256.00        32.00       496.00         0.00
-   147440.00    105808.00       560.00   1035992.00     15488.00    513248.00       616.00    103816.00         0.00
-    38752.00      1336.00       352.00      3984.00     67280.00     16320.00        56.00       496.00         0.00
-   433448.00    350832.00      4944.00    959448.00     57248.00   3991464.00      3992.00    572544.00        16.00
-      504.00         0.00         0.00       720.00       464.00       360.00      6352.00        48.00         0.00
-    44128.00     15544.00       808.00     47160.00      4592.00     73432.00        40.00    473744.00         0.00
-        0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
--------------------
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00     393864
--------------------
-micro avg                                              100.00     100.00     100.00     393864
-macro avg                                              100.00     100.00     100.00     393864
-weighted avg                                           100.00     100.00     100.00     393864
-
--------------------
-           0
-   393864.00
--------------------
-
-[INFO] - Epoch 4, global step 3124: val_loss reached 38.05367 (best 38.05367), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-21_15-37-04/checkpoints/Punctuation_with_Domain_discriminator---val_loss=38.05-epoch=4.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          95.35      97.89      96.60   32695556
-! (label_id: 1)                                         34.52      12.85      18.73     594128
-# (label_id: 2)                                         99.46      99.25      99.36    6532600
-, (label_id: 3)                                         54.58      37.10      44.18    2688848
-- (label_id: 4)                                         53.42      24.19      33.30     260520
-. (label_id: 5)                                         62.51      75.97      68.59    5279872
-: (label_id: 6)                                         75.60      45.16      56.54      14048
-? (label_id: 7)                                         70.71      37.62      49.11    1306712
-— (label_id: 8)                                          0.00       0.00       0.00         16
--------------------
-micro avg                                               89.39      89.39      89.39   49372300
-macro avg                                               60.68      47.78      51.82   49372300
-weighted avg                                            88.55      89.39      88.57   49372300
-
--------------------
-                       !           #           ,           -           .           :           ?           —
- 32004140.00     43136.00     42224.00    599088.00    116984.00    614248.00      2904.00    143256.00         0.00
-    13808.00     76368.00        64.00     46880.00      3032.00     69976.00       136.00     10960.00         0.00
-    27440.00       480.00   6483664.00      1528.00      2056.00      3096.00        24.00       496.00         0.00
-   134688.00    101528.00       568.00    997616.00     14616.00    480392.00       568.00     97808.00         0.00
-    29728.00      1528.00       312.00      5064.00     63024.00     17696.00        32.00       584.00         0.00
-   437512.00    354536.00      4896.00    986976.00     55720.00   4011312.00      3984.00    561968.00        16.00
-      488.00         0.00         0.00       696.00       464.00       376.00      6344.00        24.00         0.00
-    47760.00     16552.00       872.00     51000.00      4624.00     82776.00        56.00    491616.00         0.00
-        0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
--------------------
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00     393864
--------------------
-micro avg                                              100.00     100.00     100.00     393864
-macro avg                                              100.00     100.00     100.00     393864
-weighted avg                                           100.00     100.00     100.00     393864
-
--------------------
-           0
-   393864.00
--------------------
-
-[INFO] - Epoch 5, global step 3749: val_loss reached 37.89869 (best 37.89869), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-21_15-37-04/checkpoints/Punctuation_with_Domain_discriminator---val_loss=37.90-epoch=5.ckpt" as top 3
-[INFO] - Saving latest checkpoint...
-[INFO] - Optimizer config = AdamW (
-Parameter Group 0
-    amsgrad: False
-    betas: (0.9, 0.999)
-    eps: 1e-08
-    lr: 0.001
-    weight_decay: 0.0
-)
-[INFO] - Scheduler "<core.optim.lr_scheduler.CosineAnnealing object at 0x7f8610251370>" 
-will be used during training (effective maximum steps = 3750) - 
-Parameters : 
-(warmup_steps: null
-warmup_ratio: 0.1
-min_lr: 1.0e-08
-last_epoch: -1
-max_steps: 3750
-)
-[INFO] - 
-  | Name                       | Type                 | Params
---------------------------------------------------------------------
-0 | transformer                | ElectraModel         | 108 M 
-1 | punct_classifier           | TokenClassifier      | 6.9 K 
-2 | domain_classifier          | SequenceClassifier   | 4.7 M 
-3 | punctuation_loss           | LinearChainCRF       | 99    
-4 | domain_loss                | CrossEntropyLoss     | 0     
-5 | agg_loss                   | AggregatorLoss       | 0     
-6 | punct_class_report         | ClassificationReport | 0     
-7 | chunked_punct_class_report | ClassificationReport | 0     
-8 | domain_class_report        | ClassificationReport | 0     
---------------------------------------------------------------------
-11.8 M    Trainable params
-101 M     Non-trainable params
-113 M     Total params
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          95.08      98.13      96.58      89602
-! (label_id: 1)                                         14.29       4.66       7.03        858
-# (label_id: 2)                                         99.46      99.35      99.41      17576
-, (label_id: 3)                                         56.13      34.31      42.59       7660
-- (label_id: 4)                                         50.00      29.00      36.71        662
-. (label_id: 5)                                         66.19      75.68      70.62      14950
-: (label_id: 6)                                        100.00      60.00      75.00         10
-? (label_id: 7)                                         69.14      39.07      49.93       2672
-— (label_id: 8)                                          0.00       0.00       0.00          0
--------------------
-micro avg                                               90.02      90.02      90.02     133990
-macro avg                                               68.79      55.03      59.73     133990
-weighted avg                                            88.95      90.02      89.17     133990
-
--------------------
-                       !           #           ,           -           .           :           ?           —
-    87928.00        78.00       102.00      1998.00       322.00      1784.00         0.00       266.00         0.00
-       20.00        40.00         0.00        60.00         4.00       148.00         0.00         8.00         0.00
-       54.00         0.00     17462.00         2.00         2.00        36.00         0.00         0.00         0.00
-      336.00       144.00         0.00      2628.00        22.00      1346.00         0.00       206.00         0.00
-       70.00         8.00         0.00        14.00       192.00       100.00         0.00         0.00         0.00
-     1090.00       562.00        12.00      2854.00       110.00     11314.00         4.00      1148.00         0.00
-        0.00         0.00         0.00         0.00         0.00         0.00         6.00         0.00         0.00
-      104.00        26.00         0.00       104.00        10.00       222.00         0.00      1044.00         0.00
-        0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
--------------------
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       1072
--------------------
-micro avg                                              100.00     100.00     100.00       1072
-macro avg                                              100.00     100.00     100.00       1072
-weighted avg                                           100.00     100.00     100.00       1072
-
--------------------
-           0
-     1072.00
--------------------
-
-[INFO] - Internal process exited
-                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     [INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.62      99.16      98.38   32597898
-! (label_id: 1)                                         54.65       9.25      15.83     249790
-# (label_id: 2)                                         98.98      92.63      95.70    2657770
-, (label_id: 3)                                         63.36      54.46      58.58    1092581
-- (label_id: 4)                                         55.67      44.63      49.54     101638
-. (label_id: 5)                                         72.09      75.28      73.65    2193139
-: (label_id: 6)                                         88.28      60.72      71.95       6848
-? (label_id: 7)                                         76.07      67.94      71.78     547390
-— (label_id: 8)                                          0.00       0.00       0.00        354
--------------------
-micro avg                                               95.00      95.00      95.00   39447408
-macro avg                                               67.41      56.01      59.49   39447408
-weighted avg                                            94.66      95.00      94.70   39447408
-
--------------------
-                       !           #           ,           -           .           :           ?           —
- 32324414.00     17139.00    195052.00    212940.00     33520.00    282195.00      1162.00     47331.00        32.00
-     1630.00     23117.00        10.00      4917.00       283.00     11176.00         1.00      1167.00         0.00
-    24763.00        51.00   2461773.00       126.00       122.00       293.00         2.00        47.00         1.00
-    88501.00     37724.00       188.00    595017.00      4616.00    183715.00       286.00     28976.00        27.00
-    17851.00      1340.00        14.00      2999.00     45358.00     12974.00        95.00       833.00        12.00
-   117633.00    158480.00       610.00    248319.00     15662.00   1650906.00      1098.00     97125.00       206.00
-      154.00         7.00         0.00       111.00       116.00       155.00      4158.00         9.00         0.00
-    22972.00     11932.00       123.00     28152.00      1961.00     51725.00        46.00    371902.00        76.00
-        0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
--------------------
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         99.08      87.58      92.97     158827
-1 (label_id: 1)                                         88.87      99.19      93.74     158827
--------------------
-micro avg                                               93.38      93.38      93.38     317654
-macro avg                                               93.97      93.38      93.36     317654
-weighted avg                                            93.97      93.38      93.36     317654
-
--------------------
-           0           1
-   139098.00      1294.00
-    19729.00    157533.00
--------------------
-
-[INFO] - Epoch 1, global step 4999: val_loss reached 18.97545 (best 18.97545), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-20_22-21-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=18.98-epoch=1.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.63      99.18      98.40   32604048
-! (label_id: 1)                                         52.97      12.27      19.92     247810
-# (label_id: 2)                                         99.04      92.66      95.74    2654572
-, (label_id: 3)                                         62.91      56.68      59.63    1091194
-- (label_id: 4)                                         58.06      44.15      50.16     101035
-. (label_id: 5)                                         73.30      74.39      73.84    2196152
-: (label_id: 6)                                         87.81      57.30      69.34       6585
-? (label_id: 7)                                         76.27      69.09      72.50     545024
-— (label_id: 8)                                          0.00       0.00       0.00        326
--------------------
-micro avg                                               95.07      95.07      95.07   39446748
-macro avg                                               67.55      56.19      59.95   39446748
-weighted avg                                            94.73      95.07      94.80   39446748
-
--------------------
-                       !           #           ,           -           .           :           ?           —
- 32335532.00     17308.00    193752.00    207430.00     31980.00    285644.00      1227.00     45961.00        21.00
-     2098.00     30396.00        22.00      6503.00       441.00     16082.00         9.00      1831.00         3.00
-    23273.00        47.00   2459766.00       122.00       112.00       270.00         0.00        68.00         0.00
-    93735.00     38362.00       228.00    618513.00      5226.00    196668.00       340.00     30152.00        21.00
-    18509.00      1039.00        12.00      2329.00     44609.00      9670.00        84.00       558.00        20.00
-   109156.00    148686.00       690.00    228599.00     16669.00   1633776.00      1088.00     89888.00       189.00
-      128.00         3.00         0.00       136.00       112.00       140.00      3773.00         5.00         0.00
-    21590.00     11969.00       102.00     27562.00      1886.00     53902.00        64.00    376561.00        72.00
-        0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
--------------------
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         99.18      87.08      92.74     158827
-1 (label_id: 1)                                         88.49      99.28      93.58     158827
--------------------
-micro avg                                               93.18      93.18      93.18     317654
-macro avg                                               93.84      93.18      93.16     317654
-weighted avg                                            93.84      93.18      93.16     317654
-
--------------------
-           0           1
-   138314.00      1137.00
-    20513.00    157690.00
--------------------
-
-[INFO] - Epoch 2, global step 5624: val_loss reached 18.89849 (best 18.89849), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-20_22-21-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=18.90-epoch=2.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.82      99.16      98.49   32600588
-! (label_id: 1)                                         54.08      12.31      20.05     250346
-# (label_id: 2)                                         98.93      93.62      96.20    2655305
-, (label_id: 3)                                         60.33      61.30      60.81    1091409
-- (label_id: 4)                                         61.37      42.81      50.43     101727
-. (label_id: 5)                                         74.72      73.55      74.13    2194529
-: (label_id: 6)                                         91.55      60.05      72.53       6804
-? (label_id: 7)                                         77.73      69.22      73.23     545464
-— (label_id: 8)                                          0.00       0.00       0.00        294
--------------------
-micro avg                                               95.20      95.20      95.20   39446468
-macro avg                                               68.50      56.89      60.65   39446468
-weighted avg                                            94.92      95.20      94.96   39446468
-
--------------------
-                       !           #           ,           -           .           :           ?           —
- 32327616.00     15660.00    168579.00    193547.00     32248.00    265632.00      1117.00     42172.00        18.00
-     1938.00     30812.00        18.00      5371.00       427.00     16440.00        10.00      1959.00         5.00
-    26057.00        53.00   2485974.00       153.00       133.00       333.00         0.00        87.00         0.00
-   107013.00     48488.00       191.00    668992.00      6078.00    241162.00       390.00     36458.00        25.00
-    16238.00       805.00        13.00      1800.00     43546.00      8008.00        90.00       455.00         3.00
-   100675.00    142883.00       433.00    196758.00     17347.00   1614009.00      1055.00     86758.00       179.00
-      116.00         2.00         0.00        66.00        81.00       110.00      4086.00         2.00         0.00
-    20910.00     11643.00        97.00     24722.00      1867.00     48835.00        56.00    377573.00        64.00
-        0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
--------------------
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         99.00      88.74      93.59     158827
-1 (label_id: 1)                                         89.80      99.10      94.22     158827
--------------------
-micro avg                                               93.92      93.92      93.92     317654
-macro avg                                               94.40      93.92      93.91     317654
-weighted avg                                            94.40      93.92      93.91     317654
-
--------------------
-           0           1
-   140944.00      1424.00
-    17883.00    157403.00
--------------------
-
-[INFO] - Epoch 3, global step 6249: val_loss reached 18.10233 (best 18.10233), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-20_22-21-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=18.10-epoch=3.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          98.03      99.13      98.58   32604730
-! (label_id: 1)                                         42.49      27.37      33.29     248124
-# (label_id: 2)                                         98.69      94.81      96.71    2653528
-, (label_id: 3)                                         62.01      60.83      61.41    1090510
-- (label_id: 4)                                         61.24      44.75      51.71     101381
-. (label_id: 5)                                         75.37      73.57      74.46    2196142
-: (label_id: 6)                                         91.43      60.91      73.11       6757
-? (label_id: 7)                                         78.98      68.59      73.42     547175
-— (label_id: 8)                                          0.00       0.00       0.00        318
--------------------
-micro avg                                               95.34      95.34      95.34   39448664
-macro avg                                               67.58      58.89      62.52   39448664
-weighted avg                                            95.10      95.34      95.19   39448664
-
--------------------
-                       !           #           ,           -           .           :           ?           —
- 32321456.00     14229.00    136908.00    182055.00     30079.00    244636.00      1074.00     41973.00        25.00
-     6719.00     67912.00        36.00     20837.00      1173.00     57297.00        11.00      5855.00         6.00
-    32561.00        57.00   2515797.00       163.00       116.00       383.00         1.00        78.00         0.00
-   105519.00     36501.00       210.00    663322.00      5633.00    223830.00       329.00     34342.00        30.00
-    16639.00       785.00        14.00      2162.00     45371.00      8532.00        81.00       498.00         8.00
-   103126.00    118283.00       480.00    198563.00     17197.00   1615693.00      1098.00     89097.00       185.00
-      124.00         0.00         0.00        66.00       105.00        88.00      4116.00         3.00         0.00
-    18579.00     10357.00        83.00     23342.00      1707.00     45683.00        47.00    375329.00        64.00
-        0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
--------------------
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         98.72      90.65      94.51     158827
-1 (label_id: 1)                                         91.36      98.82      94.94     158827
--------------------
-micro avg                                               94.73      94.73      94.73     317654
-macro avg                                               95.04      94.73      94.73     317654
-weighted avg                                            95.04      94.73      94.73     317654
-
--------------------
-           0           1
-   143975.00      1874.00
-    14852.00    156953.00
--------------------
-
-[INFO] - Epoch 4, global step 6874: val_loss reached 17.37220 (best 17.37220), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-20_22-21-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=17.37-epoch=4.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.97      99.12      98.55   32602862
-! (label_id: 1)                                         42.68      26.46      32.67     248510
-# (label_id: 2)                                         98.58      94.69      96.60    2656961
-, (label_id: 3)                                         64.36      56.65      60.26    1091100
-- (label_id: 4)                                         63.76      41.81      50.50     101520
-. (label_id: 5)                                         73.96      75.36      74.65    2194822
-: (label_id: 6)                                         78.84      60.40      68.40       6657
-? (label_id: 7)                                         77.80      69.45      73.39     545189
-— (label_id: 8)                                          0.00       0.00       0.00        381
--------------------
-micro avg                                               95.31      95.31      95.31   39448004
-macro avg                                               66.44      58.22      61.67   39448004
-weighted avg                                            95.03      95.31      95.13   39448004
-
--------------------
-                       !           #           ,           -           .           :           ?           —
- 32317560.00     14478.00    140290.00    193588.00     30920.00    247496.00      1080.00     41005.00        28.00
-     6542.00     65755.00        33.00     21446.00      1120.00     53688.00        18.00      5460.00        13.00
-    35389.00        53.00   2515865.00       180.00       136.00       340.00         0.00        92.00         0.00
-    90771.00     32424.00       144.00    618151.00      4992.00    184962.00       280.00     28708.00        32.00
-    15454.00       799.00        20.00      1714.00     42446.00      5684.00        54.00       393.00         7.00
-   115628.00    124086.00       514.00    230104.00     19894.00   1654096.00      1155.00     90861.00       221.00
-      272.00         7.00         0.00       241.00       201.00       345.00      4021.00        13.00         0.00
-    21229.00     10908.00        95.00     25676.00      1811.00     48211.00        49.00    378657.00        80.00
-        0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
--------------------
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         98.59      90.52      94.38     158827
-1 (label_id: 1)                                         91.23      98.70      94.82     158827
--------------------
-micro avg                                               94.61      94.61      94.61     317654
-macro avg                                               94.91      94.61      94.60     317654
-weighted avg                                            94.91      94.61      94.60     317654
-
--------------------
-           0           1
-   143765.00      2059.00
-    15062.00    156768.00
--------------------
-
-[INFO] - Epoch 5, global step 7499: val_loss reached 17.65734 (best 17.37220), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-20_22-21-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=17.66-epoch=5.ckpt" as top 3
-[INFO] - Optimizer config = AdamW (
-Parameter Group 0
-    amsgrad: False
-    betas: (0.9, 0.999)
-    eps: 1e-08
-    lr: 0.0004
-    weight_decay: 0.0
-)
-[INFO] - Scheduler "<core.optim.lr_scheduler.CosineAnnealing object at 0x7fab3a266d00>" 
-will be used during training (effective maximum steps = 3750) - 
-Parameters : 
-(warmup_steps: null
-warmup_ratio: 0.1
-min_lr: 1.0e-08
-last_epoch: -1
-max_steps: 3750
-)
-[INFO] - 
-  | Name                       | Type                 | Params
---------------------------------------------------------------------
-0 | transformer                | ElectraModel         | 108 M 
-1 | punct_classifier           | TokenClassifier      | 6.9 K 
-2 | domain_classifier          | SequenceClassifier   | 4.7 M 
-3 | punctuation_loss           | LinearChainCRF       | 99    
-4 | domain_loss                | CrossEntropyLoss     | 0     
-5 | agg_loss                   | AggregatorLoss       | 0     
-6 | punct_class_report         | ClassificationReport | 0     
-7 | chunked_punct_class_report | ClassificationReport | 0     
-8 | domain_class_report        | ClassificationReport | 0     
---------------------------------------------------------------------
-18.9 M    Trainable params
-94.7 M    Non-trainable params
-113 M     Total params
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          96.39      99.32      97.83      86266
-! (label_id: 1)                                         50.00      29.03      36.73        534
-# (label_id: 2)                                         98.80      83.35      90.42       6602
-, (label_id: 3)                                         65.02      47.04      54.59       2687
-- (label_id: 4)                                         63.86      33.23      43.71        319
-. (label_id: 5)                                         73.31      67.70      70.40       4880
-: (label_id: 6)                                         85.71      37.50      52.17         16
-? (label_id: 7)                                         79.48      72.20      75.66       1025
-— (label_id: 8)                                          0.00       0.00       0.00          0
--------------------
-micro avg                                               94.56      94.56      94.56     102329
-macro avg                                               76.57      58.67      65.19     102329
-weighted avg                                            94.10      94.56      94.19     102329
-
--------------------
-                       !           #           ,           -           .           :           ?           —
-    85682.00        30.00      1098.00       824.00       164.00      1023.00         5.00        68.00         0.00
-       10.00       155.00         0.00        36.00         0.00        96.00         0.00        13.00         0.00
-       66.00         0.00      5503.00         0.00         1.00         0.00         0.00         0.00         0.00
-      195.00        74.00         0.00      1264.00         8.00       346.00         0.00        57.00         0.00
-       25.00         1.00         0.00         2.00       106.00        32.00         0.00         0.00         0.00
-      246.00       261.00         1.00       506.00        37.00      3304.00         5.00       147.00         0.00
-        0.00         0.00         0.00         0.00         0.00         1.00         6.00         0.00         0.00
-       42.00        13.00         0.00        55.00         3.00        78.00         0.00       740.00         0.00
-        0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
--------------------
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         98.39      73.49      84.14        415
-1 (label_id: 1)                                         78.85      98.80      87.70        415
--------------------
-micro avg                                               86.14      86.14      86.14        830
-macro avg                                               88.62      86.14      85.92        830
-weighted avg                                            88.62      86.14      85.92        830
-
--------------------
-           0           1
-      305.00         5.00
-      110.00       410.00
--------------------
-
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.89      99.09      98.49   32604960
-! (label_id: 1)                                         52.61      14.88      23.20     247798
-# (label_id: 2)                                         98.77      93.63      96.13    2656047
-, (label_id: 3)                                         63.37      58.27      60.71    1089163
-- (label_id: 4)                                         57.70      46.43      51.46     101032
-. (label_id: 5)                                         73.88      75.47      74.67    2197231
-: (label_id: 6)                                         87.48      60.01      71.19       6624
-? (label_id: 7)                                         75.00      71.92      73.42     546169
-— (label_id: 8)                                          0.00       0.00       0.00        346
--------------------
-micro avg                                               95.23      95.23      95.23   39449368
-macro avg                                               67.41      57.74      61.03   39449368
-weighted avg                                            94.95      95.23      95.01   39449368
-
--------------------
-                       !           #           ,           -           .           :           ?           —
- 32309032.00     14436.00    168374.00    186568.00     29362.00    255686.00      1040.00     41483.00        17.00
-     2660.00     36884.00        27.00      7975.00       514.00     20030.00         8.00      2008.00         1.00
-    30195.00        54.00   2486834.00       140.00       116.00       353.00         0.00        77.00         0.00
-   104325.00     37436.00       159.00    634641.00      5214.00    191338.00       325.00     27969.00        28.00
-    20937.00       844.00        16.00      2174.00     46909.00      9737.00        69.00       587.00        23.00
-   113530.00    144989.00       530.00    227893.00     16722.00   1658204.00      1141.00     81245.00       192.00
-      168.00         5.00         0.00       102.00       128.00       164.00      3975.00         2.00         0.00
-    24099.00     13150.00       107.00     29670.00      2067.00     61719.00        66.00    392798.00        85.00
-        0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
--------------------
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         98.61      89.69      93.94     158827
-1 (label_id: 1)                                         90.54      98.74      94.46     158827
--------------------
-micro avg                                               94.21      94.21      94.21     317654
-macro avg                                               94.58      94.21      94.20     317654
-weighted avg                                            94.58      94.21      94.20     317654
-
--------------------
-           0           1
-   142449.00      2006.00
-    16378.00    156821.00
--------------------
-
-[INFO] - Epoch 0, step 8124: val_loss was not in top 3
-[INFO] - Internal process exited
