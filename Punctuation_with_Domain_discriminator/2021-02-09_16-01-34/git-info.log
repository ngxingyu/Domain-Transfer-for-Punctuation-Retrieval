commit hash: 836632f0fdebe90f93105efab295fe4cd83af4ca
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0 b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0
index c3ff071..0e8637d 100644
Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0 differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt
index 7f6eddf..a1b895b 100644
--- a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt
+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt
@@ -39,3 +39,53 @@ Global seed set to 42
 66.4 M    Total params
 Epoch 0, global step 200: val_loss reached 0.26631 (best 0.26631), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=0.ckpt" as top 3
 Epoch 1, global step 400: val_loss reached 0.26336 (best 0.26336), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=1.ckpt" as top 3
+Epoch 2, global step 600: val_loss reached 0.26109 (best 0.26109), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=2.ckpt" as top 3
+Epoch 3, global step 800: val_loss reached 0.25805 (best 0.25805), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=3.ckpt" as top 3
+Epoch 4, global step 1000: val_loss reached 0.25547 (best 0.25547), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=4.ckpt" as top 3
+Epoch 5, global step 1200: val_loss reached 0.25339 (best 0.25339), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.25-epoch=5.ckpt" as top 3
+Epoch 6, global step 1400: val_loss reached 0.24810 (best 0.24810), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.25-epoch=6.ckpt" as top 3
+Epoch 7, global step 1600: val_loss reached 0.24127 (best 0.24127), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=7.ckpt" as top 3
+Epoch 8, global step 1800: val_loss reached 0.23864 (best 0.23864), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=8.ckpt" as top 3
+Epoch 9, global step 2000: val_loss reached 0.23646 (best 0.23646), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=9.ckpt" as top 3
+Global seed set to 42
+
+  | Name                | Type                 | Params
+-------------------------------------------------------------
+0 | transformer         | DistilBertModel      | 66.4 M
+1 | punct_classifier    | TokenClassifier      | 7.7 K 
+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
+3 | punctuation_loss    | FocalDiceLoss        | 0     
+4 | domain_loss         | CrossEntropyLoss     | 0     
+5 | agg_loss            | AggregatorLoss       | 0     
+6 | punct_class_report  | ClassificationReport | 0     
+7 | domain_class_report | ClassificationReport | 0     
+-------------------------------------------------------------
+7.1 M     Trainable params
+59.3 M    Non-trainable params
+66.4 M    Total params
+LR finder stopped early due to diverging loss.
+Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lr_find_temp_model.ckpt
+Failed to compute suggesting for `lr`. There might not be enough points.
+Traceback (most recent call last):
+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
+    min_grad = np.gradient(loss).argmin()
+  File "<__array_function__ internals>", line 5, in gradient
+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
+    raise ValueError(
+ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
+Global seed set to 42
+
+  | Name                | Type                 | Params
+-------------------------------------------------------------
+0 | transformer         | DistilBertModel      | 66.4 M
+1 | punct_classifier    | TokenClassifier      | 7.7 K 
+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
+3 | punctuation_loss    | FocalDiceLoss        | 0     
+4 | domain_loss         | CrossEntropyLoss     | 0     
+5 | agg_loss            | AggregatorLoss       | 0     
+6 | punct_class_report  | ClassificationReport | 0     
+7 | domain_class_report | ClassificationReport | 0     
+-------------------------------------------------------------
+7.1 M     Trainable params
+59.3 M    Non-trainable params
+66.4 M    Total params
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt
index 2d9ccfb..a9e0895 100644
--- a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt
+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt
@@ -14,3 +14,14 @@
 [NeMo W 2021-02-09 15:11:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fa1136ae0a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
       warnings.warn(warn_msg)
     
+[NeMo W 2021-02-09 15:25:45 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
+      warnings.warn(*args, **kwargs)
+    
+Failed to compute suggesting for `lr`. There might not be enough points.
+Traceback (most recent call last):
+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
+    min_grad = np.gradient(loss).argmin()
+  File "<__array_function__ internals>", line 5, in gradient
+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
+    raise ValueError(
+ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt
index 80a7030..dfff301 100644
--- a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt
+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt
@@ -16,3 +16,6 @@
 [NeMo W 2021-02-09 15:11:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fa1136ae0a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
       warnings.warn(warn_msg)
     
+[NeMo W 2021-02-09 15:25:45 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
+      warnings.warn(*args, **kwargs)
+    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0
index bb4d846..53377de 100644
Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
index 2869be1..0f4c210 100644
--- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
@@ -37,3 +37,55 @@ Global seed set to 42
 10.9 K    Trainable params
 66.4 M    Non-trainable params
 66.4 M    Total params
+Epoch 0, global step 200: val_loss reached 108.49911 (best 108.49911), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=108.50-epoch=0.ckpt" as top 3
+Epoch 1, global step 400: val_loss reached 155.59180 (best 108.49911), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=155.59-epoch=1.ckpt" as top 3
+Epoch 2, global step 600: val_loss reached 46.96625 (best 46.96625), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=46.97-epoch=2.ckpt" as top 3
+Epoch 3, global step 800: val_loss reached 63.33499 (best 46.96625), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=63.33-epoch=3.ckpt" as top 3
+Epoch 4, global step 1000: val_loss reached 33.09097 (best 33.09097), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=33.09-epoch=4.ckpt" as top 3
+Epoch 5, global step 1200: val_loss reached 30.02221 (best 30.02221), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=30.02-epoch=5.ckpt" as top 3
+Epoch 6, global step 1400: val_loss reached 31.40887 (best 30.02221), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=31.41-epoch=6.ckpt" as top 3
+Epoch 7, step 1600: val_loss was not in top 3
+Epoch 8, global step 1800: val_loss reached 28.61345 (best 28.61345), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=28.61-epoch=8.ckpt" as top 3
+Epoch 9, global step 2000: val_loss reached 26.27475 (best 26.27475), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=26.27-epoch=9.ckpt" as top 3
+Global seed set to 42
+
+  | Name                | Type                 | Params
+-------------------------------------------------------------
+0 | transformer         | DistilBertModel      | 66.4 M
+1 | punct_classifier    | TokenClassifier      | 7.7 K 
+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
+3 | punctuation_loss    | LinearChainCRF       | 120   
+4 | domain_loss         | CrossEntropyLoss     | 0     
+5 | agg_loss            | AggregatorLoss       | 0     
+6 | punct_class_report  | ClassificationReport | 0     
+7 | domain_class_report | ClassificationReport | 0     
+-------------------------------------------------------------
+7.1 M     Trainable params
+59.3 M    Non-trainable params
+66.4 M    Total params
+LR finder stopped early due to diverging loss.
+Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lr_find_temp_model.ckpt
+Failed to compute suggesting for `lr`. There might not be enough points.
+Traceback (most recent call last):
+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
+    min_grad = np.gradient(loss).argmin()
+  File "<__array_function__ internals>", line 5, in gradient
+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
+    raise ValueError(
+ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
+Global seed set to 42
+
+  | Name                | Type                 | Params
+-------------------------------------------------------------
+0 | transformer         | DistilBertModel      | 66.4 M
+1 | punct_classifier    | TokenClassifier      | 7.7 K 
+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
+3 | punctuation_loss    | LinearChainCRF       | 120   
+4 | domain_loss         | CrossEntropyLoss     | 0     
+5 | agg_loss            | AggregatorLoss       | 0     
+6 | punct_class_report  | ClassificationReport | 0     
+7 | domain_class_report | ClassificationReport | 0     
+-------------------------------------------------------------
+7.1 M     Trainable params
+59.3 M    Non-trainable params
+66.4 M    Total params
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
index 568694f..cab9655 100644
--- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
@@ -23,3 +23,20 @@
 [NeMo W 2021-02-09 15:11:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
     
+[NeMo W 2021-02-09 15:16:02 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce8610> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
+[NeMo W 2021-02-09 15:16:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce40a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
+[NeMo W 2021-02-09 15:57:31 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
+      warnings.warn(*args, **kwargs)
+    
+Failed to compute suggesting for `lr`. There might not be enough points.
+Traceback (most recent call last):
+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
+    min_grad = np.gradient(loss).argmin()
+  File "<__array_function__ internals>", line 5, in gradient
+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
+    raise ValueError(
+ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
index 7533c2c..d19ea43 100644
--- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
@@ -25,3 +25,12 @@
 [NeMo W 2021-02-09 15:11:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
     
+[NeMo W 2021-02-09 15:16:02 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce8610> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
+[NeMo W 2021-02-09 15:16:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce40a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
+[NeMo W 2021-02-09 15:57:31 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
+      warnings.warn(*args, **kwargs)
+    
diff --git a/experiment/config.yaml b/experiment/config.yaml
index 0aeaa8b..cb177ef 100644
--- a/experiment/config.yaml
+++ b/experiment/config.yaml
@@ -2,7 +2,7 @@ seed: 42
 trainer:
     gpus: 1 # the number of gpus, 0 for CPU
     num_nodes: 1
-    max_epochs: 10
+    max_epochs: 2
     max_steps: null # precedence over max_epochs
     accumulate_grad_batches: 4 # accumulates grads every k batches
     gradient_clip_val: 0
@@ -41,7 +41,7 @@ tmp_path: /home/nxingyu/data/tmp # /tmp #
 
 model:
     nemo_path: null
-    transformer_path: distilbert-base-uncased #google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
     unfrozen: 0
     maximum_unfrozen: 1
     unfreeze_step: 1
@@ -75,7 +75,7 @@ model:
         ignore_start_end: false
         use_cache: false
         # shared among dataloaders
-        num_workers:  2
+        num_workers:  4
         pin_memory: true
         drop_last: true
         num_labels: 10
@@ -114,7 +114,7 @@ model:
         activation: 'relu'
         log_softmax: false
         use_transformer_init: true
-        loss: 'crf'
+        loss: 'dice'
 
     domain_head:
         domain_num_fc_layers: 1
diff --git a/experiment/data/punctuation_datamodule.py b/experiment/data/punctuation_datamodule.py
index 20a4093..8711456 100644
--- a/experiment/data/punctuation_datamodule.py
+++ b/experiment/data/punctuation_datamodule.py
@@ -108,7 +108,8 @@ class PunctuationDataModule(LightningDataModule):
 
         logging.info(f"shuffling train set")
         # self.train_dataset.shuffle(randomize=False)
-        self.train_dataset.shuffle(randomize=True, seed=self.seed)
+        if (self.train_shuffle):
+            self.train_dataset.shuffle(randomize=True, seed=self.seed)
 
     def train_dataloader(self):
         return DataLoader(self.train_dataset,batch_size=None,num_workers=self.num_workers,pin_memory=self.pin_memory,drop_last=self.drop_last)
diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
index 88a268f..31923de 100644
--- a/experiment/data/punctuation_dataset_multi.py
+++ b/experiment/data/punctuation_dataset_multi.py
@@ -165,8 +165,8 @@ class PunctuationDomainDatasets(IterableDataset):
         for path in labelled+unlabelled:
             self.ds_lengths.append(int(subprocess.Popen(['wc', '-l', f'{path}.{split}.csv'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0]))
         self.max_length=max(self.ds_lengths)
-        self.len=int(self.max_length/num_samples)
         self.per_worker=int(self.max_length/self.num_workers)
+        self.len=int(self.per_worker/num_samples)
         self.class_weights=None
 
         for i,path in enumerate(labelled):
diff --git a/experiment/info.log b/experiment/info.log
index ff2d606..e69de29 100644
--- a/experiment/info.log
+++ b/experiment/info.log
@@ -1,133 +0,0 @@
-[INFO] - shuffling train set
-[INFO] - Optimizer config = AdamW (
-Parameter Group 0
-    amsgrad: False
-    betas: (0.9, 0.999)
-    eps: 1e-08
-    lr: 0.009261935523740748
-    weight_decay: 0.0
-)
-[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f596c78a3d0>" 
-will be used during training (effective maximum steps = 80) - 
-Parameters : 
-(warmup_steps: null
-warmup_ratio: 0.1
-last_epoch: -1
-max_steps: 80
-)
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          80.85       4.94       9.30       1540
-! (label_id: 1)                                          0.00       0.00       0.00          2
-, (label_id: 2)                                          7.69       1.41       2.38        142
-- (label_id: 3)                                          1.67      20.00       3.08         20
-. (label_id: 4)                                          6.82      12.50       8.82         96
-: (label_id: 5)                                          0.00       0.00       0.00          0
-; (label_id: 6)                                          0.00       0.00       0.00          0
-? (label_id: 7)                                          0.00       0.00       0.00          4
-— (label_id: 8)                                          0.00       0.00       0.00          0
-… (label_id: 9)                                          0.00       0.00       0.00          0
--------------------
-micro avg                                                5.21       5.21       5.21       1804
-macro avg                                               16.17       6.47       3.93       1804
-weighted avg                                            70.01       5.21       8.63       1804
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00         20
--------------------
-micro avg                                              100.00     100.00     100.00         20
-macro avg                                              100.00     100.00     100.00         20
-weighted avg                                           100.00     100.00     100.00         20
-
-[INFO] - Optimizer config = AdamW (
-Parameter Group 0
-    amsgrad: False
-    betas: (0.9, 0.999)
-    eps: 1e-08
-    lr: 0.0024506370946974477
-    weight_decay: 0.0
-)
-[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f596c570f40>" 
-will be used during training (effective maximum steps = 2000) - 
-Parameters : 
-(warmup_steps: null
-warmup_ratio: 0.1
-last_epoch: -1
-max_steps: 2000
-)
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          80.85       4.94       9.30       1540
-! (label_id: 1)                                          0.00       0.00       0.00          2
-, (label_id: 2)                                          7.69       1.41       2.38        142
-- (label_id: 3)                                          1.67      20.00       3.08         20
-. (label_id: 4)                                          6.82      12.50       8.82         96
-: (label_id: 5)                                          0.00       0.00       0.00          0
-; (label_id: 6)                                          0.00       0.00       0.00          0
-? (label_id: 7)                                          0.00       0.00       0.00          4
-— (label_id: 8)                                          0.00       0.00       0.00          0
-… (label_id: 9)                                          0.00       0.00       0.00          0
--------------------
-micro avg                                                5.21       5.21       5.21       1804
-macro avg                                               16.17       6.47       3.93       1804
-weighted avg                                            70.01       5.21       8.63       1804
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00         20
--------------------
-micro avg                                              100.00     100.00     100.00         20
-macro avg                                              100.00     100.00     100.00         20
-weighted avg                                           100.00     100.00     100.00         20
-
-NFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          90.44      95.02      92.67     179900
-! (label_id: 1)                                          0.00       0.00       0.00         52
-, (label_id: 2)                                         21.92       9.58      13.34      13074
-- (label_id: 3)                                         44.94       6.69      11.64       1062
-. (label_id: 4)                                         28.64      30.78      29.67      11077
-: (label_id: 5)                                          0.00       0.00       0.00        330
-; (label_id: 6)                                          0.00       0.00       0.00        108
-? (label_id: 7)                                          0.00       0.00       0.00        899
-— (label_id: 8)                                          0.00       0.00       0.00        276
-… (label_id: 9)                                          0.00       0.00       0.00         28
--------------------
-micro avg                                               84.95      84.95      84.95     206806
-macro avg                                               18.59      14.21      14.73     206806
-weighted avg                                            81.82      84.95      83.11     206806
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       1959
--------------------
-micro avg                                              100.00     100.00     100.00       1959
-macro avg                                              100.00     100.00     100.00       1959
-weighted avg                                           100.00     100.00     100.00       1959
-
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          90.62      94.69      92.61     179900
-! (label_id: 1)                                          0.00       0.00       0.00         52
-, (label_id: 2)                                         21.80      10.36      14.04      13074
-- (label_id: 3)                                         44.94       6.69      11.64       1062
-. (label_id: 4)                                         28.46      32.05      30.15      11077
-: (label_id: 5)                                          0.00       0.00       0.00        330
-; (label_id: 6)                                          0.00       0.00       0.00        108
-? (label_id: 7)                                          0.00       0.00       0.00        899
-— (label_id: 8)                                          0.00       0.00       0.00        276
-… (label_id: 9)                                          0.00       0.00       0.00         28
--------------------
-micro avg                                               84.77      84.77      84.77     206806
-macro avg                                               18.58      14.38      14.84     206806
-weighted avg                                            81.97      84.77      83.12     206806
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       1959
--------------------
-micro avg                                              100.00     100.00     100.00       1959
-macro avg                                              100.00     100.00     100.00       1959
-weighted avg                                           100.00     100.00     100.00       1959
-
diff --git a/experiment/main.py b/experiment/main.py
index 6b15e25..1dd39ff 100644
--- a/experiment/main.py
+++ b/experiment/main.py
@@ -37,9 +37,27 @@ def main(cfg: DictConfig)->None:
     exp_manager(trainer, cfg.exp_manager)
     model = PunctuationDomainModel(cfg=cfg, trainer=trainer, data_id = data_id)
     
+    lr_finder_dm=PunctuationDataModule(
+            tokenizer= cfg.model.transformer_path,
+            labelled= list(cfg.model.dataset.labelled),
+            unlabelled= list(cfg.model.dataset.unlabelled),
+            punct_label_ids= {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)},
+            train_batch_size= cfg.model.dataset.train_ds.batch_size,
+            max_seq_length= cfg.model.dataset.max_seq_length,
+            val_batch_size= cfg.model.dataset.validation_ds.batch_size,
+            num_workers= 1,
+            pin_memory= False,
+            train_shuffle= False,
+            val_shuffle= False,
+            seed=cfg.seed,
+            data_id=data_id,
+            tmp_path=cfg.tmp_path,
+            test_unlabelled=False,
+        )
+
     while(model.hparams.model.unfrozen<=cfg.model.maximum_unfrozen and model.hparams.model.unfrozen>=0):
         trainer.current_epoch=0
-        lr_finder = trainer.tuner.lr_find(model,min_lr=1e-8, max_lr=0.5, num_training=80) #, early_stop_threshold=None
+        lr_finder = trainer.tuner.lr_find(model,datamodule=lr_finder_dm,min_lr=1e-8, max_lr=0.5, num_training=80) #, early_stop_threshold=None
         # Results can be found in
         pp(lr_finder.results)
         new_lr = lr_finder.suggestion()
