commit hash: e06c976eca0f0f1ece5dd302964c113374b09762
diff --git a/experiment/Inference.ipynb b/experiment/Inference.ipynb
index 99790f8..2554b62 100644
--- a/experiment/Inference.ipynb
+++ b/experiment/Inference.ipynb
@@ -7,10 +7,10 @@
    "metadata": {},
    "outputs": [
     {
-     "output_type": "stream",
      "name": "stdout",
+     "output_type": "stream",
      "text": [
-      "/home/nxingyu2/project/experiment\n"
+      "/home/nxingyu2/project/experiment\r\n"
      ]
     }
    ],
@@ -37,29 +37,28 @@
     "\n",
     "from hydra.experimental import initialize, initialize_config_module, initialize_config_dir, compose\n",
     "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
-    "initialize(config_path=\"project/experiment\")\n",
+    "initialize() #config_path=\"project/experiment\"\n",
     "!pwd\n",
     "\n",
     "cfg=compose(\n",
     "    config_name=\"test_config.yaml\", \n",
     ")\n",
     "cfg.model.punct_label_ids=OmegaConf.create(sorted(cfg.model.punct_label_ids))\n",
-    "labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
-    "ids_to_labels = {_[0]:_[1] for _ in enumerate(cfg.model.punct_label_ids)}\n",
-    "\n",
-    "# model = PunctuationDomainModel.load_from_checkpoint(\n",
-    "#     checkpoint_path=\"/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator-last.ckpt\")\n",
+    "# labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
+    "# ids_to_labels = {_[0]:_[1] for _ in enumerate(cfg.model.punct_label_ids)}\n",
     "\n",
+    "# model = PunctuationDomainModel.load_from_checkpoint( #TEDend2021-02-11_07-57-33  # TEDstart2021-02-11_07-55-58\n",
+    "#     checkpoint_path=\"/home/nxingyu2/project/Punctuation_with_Domain_discriminator/TEDend2021-02-11_07-57-33/checkpoints/Punctuation_with_Domain_discriminator-last.ckpt\")\n",
     "# trainer = pl.Trainer(**cfg.trainer)"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 11,
+   "execution_count": 3,
+   "id": "loose-assignment",
    "metadata": {},
    "outputs": [
     {
-     "output_type": "execute_result",
      "data": {
       "text/plain": [
        "{'input_ids': tensor([[ 101, 7570, 9541, 9541, 9541,  102,    0,    0,    0,    0,    0,    0,\n",
@@ -86,7 +85,7 @@
        "          False, False, False, False, False, False, False, False, False, False,\n",
        "          False, False, False, False, False, False, False, False, False, False,\n",
        "          False, False, False, False, False, False, False, False]]),\n",
-       " 'subtoken_mask': tensor([[ True, False, False, False,  True,  True, False, False, False, False,\n",
+       " 'subtoken_mask': tensor([[ True,  True, False, False, False, False, False, False, False, False,\n",
        "          False, False, False, False, False, False, False, False, False, False,\n",
        "          False, False, False, False, False, False, False, False, False, False,\n",
        "          False, False, False, False, False, False, False, False, False, False,\n",
@@ -99,7 +98,7 @@
        "          False, False, False, False, False, False, False, False, False, False,\n",
        "          False, False, False, False, False, False, False, False, False, False,\n",
        "          False, False, False, False, False, False, False, False]]),\n",
-       " 'labels': tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
+       " 'labels': tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
@@ -107,8 +106,9 @@
        "          0, 0, 0, 0, 0, 0, 0, 0]])}"
       ]
      },
+     "execution_count": 3,
      "metadata": {},
-     "execution_count": 11
+     "output_type": "execute_result"
     }
    ],
    "source": [
@@ -120,32 +120,73 @@
     "]\n",
     "cfg.model.punct_label_ids=OmegaConf.create(sorted(cfg.model.punct_label_ids))\n",
     "labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
-    "ds=PunctuationInferenceDataset(tokenizer=AutoTokenizer.from_pretrained('google/electra-small-discriminator'), queries=queries, max_seq_length=128, punct_label_ids=labels_to_ids, degree=0, attach_label_to_end=True)\n",
+    "ds=PunctuationInferenceDataset(tokenizer=AutoTokenizer.from_pretrained('google/electra-small-discriminator'), queries=queries, max_seq_length=128, punct_label_ids=labels_to_ids, degree=0, attach_label_to_end=False)\n",
     "ds[0]"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 8,
+   "execution_count": 2,
+   "id": "serious-block",
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "['', '!', ',', '-', '.', ':', ';', '?', '—', '…']"
+      ]
+     },
+     "execution_count": 2,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "model._cfg.model.punct_label_ids"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 3,
    "id": "hairy-proxy",
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
-       "[' we bought four shirts, one pen, and a mug from the nvidia gear store in santa clara                                                                                                            ',\n",
+       "[' hooooooo                                                                                                                           ',\n",
        " ' what can i do for you today?                                                                                                                        ',\n",
-       " ' , how are you? ,                                                                                                                           ',\n",
-       " ' good morning. everyone? how have your weekends been? its a really great day?                                                                                                                  ']"
+       " ' , how? are you?                                                                                                                            ',\n",
+       " ' firstly, development policy in africa. in any case, in the acp countries, employment policy in madeira, the canaries guadeloupe martinique and crete, regional policy in the ultra- peripheral areas— human rights, which mr barthet mayer mentioned earlier, since dollar bananas are after all slavery, bananas, the product of human exploitation by three multinationals payments of ecu 50 per month, instead of ecu 50 per day in guadeloupe or martinique. it also brings into question budgetary policy. because the european union is after all, making a present of ecu 1, 9 billion to three multinationals where are the financial interests of the european union         ',\n",
+       " ' plans for this weekend, include turning wine into water. the small white buoys marked the location of hundreds of crab pots. he said he was not there yesterday. however many people saw him there today, arrived with a crash of my car through the garage door. the lyrics of the song sounded like fingernails on a chalkboard the guinea fowl flies through the air with all the grace of a turtle. they ran around the corner, to find that they had traveled back in time,                                      ',\n",
+       " ' good morning? everyone? how have your weekends? been? its a really great day? thank you.                                                                                                                ',\n",
+       " ' first of all, i too agree that tourism- related action must include employment, training and education. as you know, after the european conference on tourism and employment in luxemborg we set up a high- level group, whose mission was to examine how best tourism could contribute towards employment— the first stage,                                                                          ']"
       ]
      },
-     "execution_count": 8,
+     "execution_count": 3,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
+    "# cfg.model.punct_label_ids=OmegaConf.create(sorted(cfg.model.punct_label_ids))\n",
+    "# labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
     "\n",
+    "queries = [\n",
+    "    'Hooooooo!',\n",
+    "    'what can i do for you today',\n",
+    "    'how are you',\n",
+    "    'firstly development policy in africa in any case in the acp countries employment policy in madeira the canaries guadeloupe martinique and crete regional policy in the ultra-peripheral areas human rights which mr barthet-mayer mentioned earlier since dollar bananas are after all slavery bananas the product of human exploitation by three multinationals payments of ecu 50 per month instead of ecu 50 per day in guadeloupe or martinique it also brings into question budgetary policy because the european union is after all making a present of ecu 1.9 billion to three multinationals where are the financial interests of the european union',\n",
+    "    '''Plans for this weekend include turning wine into water.\n",
+    "The small white buoys marked the location of hundreds of crab pots.\n",
+    "He said he was not there yesterday; however, many people saw him there.\n",
+    "Today arrived with a crash of my car through the garage door.\n",
+    "The lyrics of the song sounded like fingernails on a chalkboard.\n",
+    "The Guinea fowl flies through the air with all the grace of a turtle.\n",
+    "They ran around the corner to find that they had traveled back in time.''',\n",
+    "    'good morning everyone how have your weekends been its a really great day thank you',\n",
+    "    'first of all i too agree that tourism related action must include employment training and education as you know after the european conference on tourism and employment in luxemborg we set up a high-level group whose mission was to examine how best tourism could contribute towards employment the first stage'\n",
+    "]\n",
     "inference_results = model.add_punctuation(queries)\n",
     "inference_results"
    ]
@@ -175,9 +216,9 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.8.5-final"
+   "version": "3.8.5"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 5
-}
\ No newline at end of file
+}
diff --git a/experiment/config.yaml b/experiment/config.yaml
index 2aa798f..48ec77d 100644
--- a/experiment/config.yaml
+++ b/experiment/config.yaml
@@ -1,36 +1,36 @@
 seed: 42
 trainer:
-    # gpus: 1 # the number of gpus, 0 for CPU
-    # num_nodes: 1
-    # max_epochs: 15
-    # max_steps: null # precedence over max_epochs
-    # accumulate_grad_batches: 4 # accumulates grads every k batches
-    # gradient_clip_val: 0
-    # amp_level: O1 # O1/O2 for mixed precision
-    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-    # accelerator: ddp
-    # checkpoint_callback: false  # Provided by exp_manager
-    # logger: false #false  # Provided by exp_manager
-    # log_every_n_steps: 1  # Interval of logging.
-    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-    # resume_from_checkpoint: null
-
-    gpus: 0 # the number of gpus, 0 for CPU
+    gpus: 1 # the number of gpus, 0 for CPU
     num_nodes: 1
-    max_epochs: 8
+    max_epochs: 15
     max_steps: null # precedence over max_epochs
     accumulate_grad_batches: 4 # accumulates grads every k batches
     gradient_clip_val: 0
-    amp_level: O0 # O1/O2 for mixed precision
-    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-    # accelerator: ddp
+    amp_level: O1 # O1/O2 for mixed precision
+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
+    accelerator: ddp
     checkpoint_callback: false  # Provided by exp_manager
     logger: false #false  # Provided by exp_manager
     log_every_n_steps: 1  # Interval of logging.
     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-    reload_dataloaders_every_epoch: true
     resume_from_checkpoint: null
 
+    # gpus: 0 # the number of gpus, 0 for CPU
+    # num_nodes: 1
+    # max_epochs: 8
+    # max_steps: null # precedence over max_epochs
+    # accumulate_grad_batches: 4 # accumulates grads every k batches
+    # gradient_clip_val: 0
+    # amp_level: O0 # O1/O2 for mixed precision
+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
+    # # accelerator: ddp
+    # checkpoint_callback: false  # Provided by exp_manager
+    # logger: false #false  # Provided by exp_manager
+    # log_every_n_steps: 1  # Interval of logging.
+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
+    # reload_dataloaders_every_epoch: true
+    # resume_from_checkpoint: null
+
 exp_manager:
     exp_dir: /home/nxingyu2/project/ # /root/project # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
     name: Punctuation_with_Domain_discriminator  # The name of your model
@@ -81,7 +81,7 @@ model:
         num_labels: 10
         num_domains: 1
         test_unlabelled: true
-        attach_label_to_end: false # false if attach to start
+        attach_label_to_end: none # false if attach to start none if dont mask
 
         train_ds:
             shuffle: true
diff --git a/experiment/core/classification_report.py b/experiment/core/classification_report.py
index 7ff6282..9cca979 100644
--- a/experiment/core/classification_report.py
+++ b/experiment/core/classification_report.py
@@ -15,6 +15,7 @@
 from typing import Any, Dict, Optional
 
 import torch
+from torch.nn.functional import one_hot
 from pytorch_lightning.metrics import Metric
 from pytorch_lightning.metrics.utils import METRIC_EPS
 
@@ -82,11 +83,15 @@ class ClassificationReport(Metric):
         self.add_state(
             "num_examples_per_class", default=torch.zeros(num_classes), dist_reduce_fx='sum', persistent=False
         )
+        self.add_state("cm", default=torch.zeros((num_classes,num_classes)), dist_reduce_fx="sum", persistent=False)
 
     def update(self, predictions: torch.Tensor, labels: torch.Tensor):
         TP = []
         FN = []
         FP = []
+        CM = torch.zeros((self.num_classes,self.num_classes),dtype=torch.long).to(predictions.device)
+        print(CM.shape,predictions.shape,labels.shape)
+        CM.index_add_(0, predictions, one_hot(labels,num_classes=self.num_classes))
         for label_id in range(self.num_classes):
             current_label = labels == label_id
             label_predicted = predictions == label_id
@@ -98,11 +103,13 @@ class ClassificationReport(Metric):
         tp = torch.tensor(TP).to(predictions.device)
         fn = torch.tensor(FN).to(predictions.device)
         fp = torch.tensor(FP).to(predictions.device)
+        # cm = torch.tensor(CM).to(predictions.device)
         num_examples_per_class = tp + fn
 
         self.tp += tp
         self.fn += fn
         self.fp += fp
+        self.cm += CM
         self.num_examples_per_class += num_examples_per_class
 
     def compute(self):
@@ -156,6 +163,9 @@ class ClassificationReport(Metric):
             + '\n'
         )
 
+        report += "\n-------------------\n"
+        report += self.cm.__str__()
+
         self.total_examples = total_examples
 
         if self.mode == 'macro':
diff --git a/experiment/core/utils.py b/experiment/core/utils.py
index c9470b8..72fe2ab 100644
--- a/experiment/core/utils.py
+++ b/experiment/core/utils.py
@@ -38,7 +38,7 @@ def align_labels_to_mask(mask,labels):
     return m1.tolist()
 
 def view_aligned(texts,tags,tokenizer,labels_to_ids):
-        return [re.sub(r'( ?\[((PAD)|(CLS)|(SEP))\] ?)',' ',re.sub(' ##','',' '.join(
+        return [re.sub(r'( ?\[((PAD)|(CLS)|(SEP))\] ?)',' ',re.sub('[.?!,;:\-—… ]+##','',' '.join(
             [_[0]+_[1] for _ in list(
                 zip(tokenizer.convert_ids_to_tokens(_[0]),
                     [labels_to_ids[id] for id in _[1].tolist()])
@@ -113,7 +113,11 @@ def chunk_to_len(max_seq_length,tokenizer,attach_label_to_end,tokens,labels=None
         padded_labels=[pad_to_len(max_seq_length,align_labels_to_mask(*_)) for _ in zip(masks,split_labels)]
     return ids,masks,padded_labels
     
-def chunk_to_len_batch(max_seq_length,tokenizer,tokens,labels=None,labelled=True,ignore_index=-100, attach_label_to_end=True):
+def chunk_to_len_batch(max_seq_length,tokenizer,tokens,labels=None,labelled=True,ignore_index=-100, attach_label_to_end=None):
+    no_mask=False
+    if attach_label_to_end is None:
+        no_mask=True
+        attach_label_to_end=True
     batch_ids=[]
     batch_masks=[]
     batch_labels=[]
@@ -126,8 +130,11 @@ def chunk_to_len_batch(max_seq_length,tokenizer,tokens,labels=None,labelled=True
     output = {'input_ids': torch.as_tensor(batch_ids, dtype=torch.long),
               'attention_mask': torch.as_tensor(batch_ids, dtype=torch.bool),
               'subtoken_mask': torch.as_tensor(batch_masks,dtype=torch.bool)}
-    output['subtoken_mask']|=((output['input_ids']==101)|(output['input_ids']==102))
-    output['subtoken_mask']&=labelled
+    if no_mask:
+        output['subtoken_mask']=output['attention_mask']&(output['input_ids']!=102)
+    else:
+        output['subtoken_mask']|=(output['input_ids']==101)  # dont want end token |(output['input_ids']==102)
+        output['subtoken_mask']&=labelled
     output['labels']=torch.as_tensor(batch_labels,dtype=torch.long) if labelled==True else torch.zeros_like(output['input_ids'],dtype=torch.long)
     return output
 
diff --git a/experiment/data/punctuation_datamodule.py b/experiment/data/punctuation_datamodule.py
index fb69299..71a1f7f 100644
--- a/experiment/data/punctuation_datamodule.py
+++ b/experiment/data/punctuation_datamodule.py
@@ -25,7 +25,7 @@ class PunctuationDataModule(LightningDataModule):
             data_id: str = '',
             tmp_path:str = '~/data/tmp',
             test_unlabelled:bool = True,
-            attach_label_to_end:bool = True,
+            attach_label_to_end:bool = None,
             ):
         #unlabelled=[], batch_size = 256, max_seq_length = 256, num_workers=1):
         super().__init__()
diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
index 97f2fb2..4c9f67a 100644
--- a/experiment/data/punctuation_dataset_multi.py
+++ b/experiment/data/punctuation_dataset_multi.py
@@ -39,7 +39,7 @@ class PunctuationDomainDataset(IterableDataset):
         tmp_path='~/data/tmp',
         start=0,
         end=-1,
-        attach_label_to_end=True,
+        attach_label_to_end=None,
     ):
         if not (os.path.exists(csv_file)):
             raise FileNotFoundError(
@@ -154,7 +154,7 @@ class PunctuationDomainDatasets(IterableDataset):
                  randomize:bool=True,
                  data_id='',
                  tmp_path='~/data/tmp',
-                 attach_label_to_end=True,
+                 attach_label_to_end=None,
                  ):
         worker_info = get_worker_info()
         self.num_workers=1 if worker_info is None else worker_info.num_workers
@@ -266,7 +266,7 @@ class PunctuationInferenceDataset(Dataset):
             "labels": NeuralType(('B', 'T'), ChannelType()),
         }
 
-    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], num_samples:int=256, degree:int = 0, attach_label_to_end:bool=True):
+    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], num_samples:int=256, degree:int = 0, attach_label_to_end:bool=None):
         """ Initializes BertPunctuationInferDataset. """
         self.degree=degree
         self.punct_label_ids=punct_label_ids
diff --git a/experiment/info.log b/experiment/info.log
index 85bc297..104c5dd 100644
--- a/experiment/info.log
+++ b/experiment/info.log
@@ -1,192 +1,4 @@
-[INFO] - GPU available: True, used: False
+[INFO] - GPU available: True, used: True
 [INFO] - TPU available: None, using: 0 TPU cores
-[INFO] - shuffling train set
-[INFO] - Global seed set to 42
-[INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-[INFO] - Optimizer config = AdamW (
-Parameter Group 0
-    amsgrad: False
-    betas: (0.9, 0.999)
-    eps: 1e-08
-    lr: 0.01
-    weight_decay: 0.0
-)
-[INFO] - Scheduler "<core.optim.lr_scheduler.CosineAnnealing object at 0x7f13e0b714f0>" 
-will be used during training (effective maximum steps = 1600) - 
-Parameters : 
-(warmup_steps: null
-warmup_ratio: 0.1
-min_lr: 1.0e-10
-last_epoch: -1
-max_steps: 1600
-)
-[INFO] - 
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 513   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-299 K     Trainable params
-13.2 M    Non-trainable params
-13.5 M    Total params
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          91.10      19.21      31.73       3196
-! (label_id: 1)                                          0.00       0.00       0.00          0
-, (label_id: 2)                                          5.61      37.37       9.76        198
-- (label_id: 3)                                          0.79       9.09       1.46         22
-. (label_id: 4)                                          0.00       0.00       0.00        212
-: (label_id: 5)                                          0.00       0.00       0.00          4
-; (label_id: 6)                                          0.00       0.00       0.00          4
-? (label_id: 7)                                          0.96      62.50       1.88         16
-— (label_id: 8)                                          0.00       0.00       0.00         20
-… (label_id: 9)                                          0.00       0.00       0.00          0
--------------------
-micro avg                                               19.06      19.06      19.06       3672
-macro avg                                               12.31      16.02       5.60       3672
-weighted avg                                            79.60      19.06      28.16       3672
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00         34
--------------------
-micro avg                                              100.00     100.00     100.00         34
-macro avg                                              100.00     100.00     100.00         34
-weighted avg                                           100.00     100.00     100.00         34
-
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.76      93.83      95.75     303856
-! (label_id: 1)                                          0.00       0.00       0.00        155
-, (label_id: 2)                                         38.85      59.39      46.98      23143
-- (label_id: 3)                                         72.00      53.11      61.13       1830
-. (label_id: 4)                                         58.36      60.43      59.38      20164
-: (label_id: 5)                                          0.00       0.00       0.00        439
-; (label_id: 6)                                          0.00       0.00       0.00        176
-? (label_id: 7)                                         24.31      36.42      29.15       1590
-— (label_id: 8)                                          6.84       6.23       6.52       1509
-… (label_id: 9)                                          0.00       0.00       0.00        111
--------------------
-micro avg                                               88.58      88.58      88.58     352973
-macro avg                                               29.81      30.94      29.89     352973
-weighted avg                                            90.55      88.58      89.38     352973
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       3181
--------------------
-micro avg                                              100.00     100.00     100.00       3181
-macro avg                                              100.00     100.00     100.00       3181
-weighted avg                                           100.00     100.00     100.00       3181
-
-[INFO] - Epoch 0, global step 199: val_loss reached 0.27337 (best 0.27337), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=0.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.63      94.98      96.29     303856
-! (label_id: 1)                                          0.00       0.00       0.00        155
-, (label_id: 2)                                         43.70      57.68      49.73      23143
-- (label_id: 3)                                         75.76      51.58      61.38       1830
-. (label_id: 4)                                         58.49      63.93      61.09      20164
-: (label_id: 5)                                          0.00       0.00       0.00        439
-; (label_id: 6)                                          0.00       0.00       0.00        176
-? (label_id: 7)                                         35.78      37.42      36.58       1590
-— (label_id: 8)                                          5.88       7.22       6.48       1509
-… (label_id: 9)                                          0.00       0.00       0.00        111
--------------------
-micro avg                                               89.67      89.67      89.67     352973
-macro avg                                               31.72      31.28      31.15     352973
-weighted avg                                            90.83      89.67      90.15     352973
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       3181
--------------------
-micro avg                                              100.00     100.00     100.00       3181
-macro avg                                              100.00     100.00     100.00       3181
-weighted avg                                           100.00     100.00     100.00       3181
-
-[INFO] - Epoch 1, global step 399: val_loss reached 0.26732 (best 0.26732), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=1.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.23      95.39      96.30     303856
-! (label_id: 1)                                          0.00       0.00       0.00        155
-, (label_id: 2)                                         47.20      48.78      47.98      23143
-- (label_id: 3)                                         69.68      55.25      61.63       1830
-. (label_id: 4)                                         55.03      67.55      60.65      20164
-: (label_id: 5)                                          0.00       0.00       0.00        439
-; (label_id: 6)                                          0.00       0.00       0.00        176
-? (label_id: 7)                                         28.81      45.91      35.40       1590
-— (label_id: 8)                                          8.17      11.93       9.70       1509
-… (label_id: 9)                                          0.00       0.00       0.00        111
--------------------
-micro avg                                               89.72      89.72      89.72     352973
-macro avg                                               30.61      32.48      31.17     352973
-weighted avg                                            90.47      89.72      90.03     352973
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       3181
--------------------
-micro avg                                              100.00     100.00     100.00       3181
-macro avg                                              100.00     100.00     100.00       3181
-weighted avg                                           100.00     100.00     100.00       3181
-
-[INFO] - Epoch 2, global step 599: val_loss reached 0.26594 (best 0.26594), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=2.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.60      95.36      96.46     303856
-! (label_id: 1)                                          0.00       0.00       0.00        155
-, (label_id: 2)                                         45.38      58.10      50.96      23143
-- (label_id: 3)                                         70.17      59.40      64.34       1830
-. (label_id: 4)                                         60.79      63.42      62.08      20164
-: (label_id: 5)                                          0.00       0.00       0.00        439
-; (label_id: 6)                                          0.00       0.00       0.00        176
-? (label_id: 7)                                         30.77      51.57      38.54       1590
-— (label_id: 8)                                         10.69       8.48       9.46       1509
-… (label_id: 9)                                          0.00       0.00       0.00        111
--------------------
-micro avg                                               90.10      90.10      90.10     352973
-macro avg                                               31.54      33.63      32.18     352973
-weighted avg                                            91.01      90.10      90.48     352973
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       3181
--------------------
-micro avg                                              100.00     100.00     100.00       3181
-macro avg                                              100.00     100.00     100.00       3181
-weighted avg                                           100.00     100.00     100.00       3181
-
-[INFO] - Epoch 3, global step 799: val_loss reached 0.25942 (best 0.25942), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=3.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.63      95.55      96.58     303856
-! (label_id: 1)                                          0.00       0.00       0.00        155
-, (label_id: 2)                                         46.84      57.30      51.54      23143
-- (label_id: 3)                                         75.90      57.49      65.42       1830
-. (label_id: 4)                                         59.34      67.15      63.00      20164
-: (label_id: 5)                                          0.00       0.00       0.00        439
-; (label_id: 6)                                          0.00       0.00       0.00        176
-? (label_id: 7)                                         33.55      48.62      39.70       1590
-— (label_id: 8)                                         12.65       6.36       8.47       1509
-… (label_id: 9)                                          0.00       0.00       0.00        111
--------------------
-micro avg                                               90.39      90.39      90.39     352973
-macro avg                                               32.59      33.25      32.47     352973
-weighted avg                                            91.10      90.39      90.67     352973
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       3181
--------------------
-micro avg                                              100.00     100.00     100.00       3181
-macro avg                                              100.00     100.00     100.00       3181
-weighted avg                                           100.00     100.00     100.00       3181
-
-[INFO] - Epoch 4, global step 999: val_loss reached 0.26189 (best 0.25942), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=4.ckpt" as top 3
+[INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
+[INFO] - Using native 16bit precision.
diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
index 20e9816..d21c38f 100644
--- a/experiment/models/punctuation_domain_model.py
+++ b/experiment/models/punctuation_domain_model.py
@@ -171,7 +171,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
 
         self.log('lr', lr, prog_bar=True)
         self.log('train_loss', loss)
-        self.log('gamma', self.grad_reverse.scale)
+        self.log('gamma', self.grad_reverse.scale,logger=True)
 
         return {'loss': loss, 'lr': lr}
 
@@ -724,7 +724,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
             tokenizer= self.tokenizer,
             queries=queries, 
             max_seq_length=self.hparams.model.dataset.max_seq_length,
-            punct_label_ids=self._cfg.model.punct_label_ids,
+            punct_label_ids=self.labels_to_ids,
             attach_label_to_end=self._cfg.model.dataset.attach_label_to_end)
         batch=ds[0]
         attention_mask = batch['attention_mask']
