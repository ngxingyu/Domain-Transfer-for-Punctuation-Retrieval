commit hash: 26a3ff9e02d410033d6c3f8e23c40345e212c555
diff --git a/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/events.out.tfevents.1614589587.intern-instance.32451.0 b/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/events.out.tfevents.1614589587.intern-instance.32451.0
index c8797d8..d4103bc 100644
Binary files a/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/events.out.tfevents.1614589587.intern-instance.32451.0 and b/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/events.out.tfevents.1614589587.intern-instance.32451.0 differ
diff --git a/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/lightning_logs.txt
index 164a666..a1abf97 100644
--- a/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/lightning_logs.txt
+++ b/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/lightning_logs.txt
@@ -21,3 +21,91 @@ initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
 5.4 K     Trainable params
 108 M     Non-trainable params
 108 M     Total params
+Epoch 0, global step 49: val_loss reached 95.42175 (best 95.42175), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=95.42-epoch=0.ckpt" as top 3
+Epoch 1, global step 99: val_loss reached 69.13660 (best 69.13660), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=69.14-epoch=1.ckpt" as top 3
+Epoch 2, global step 149: val_loss reached 61.99438 (best 61.99438), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=61.99-epoch=2.ckpt" as top 3
+Epoch 3, global step 199: val_loss reached 60.18487 (best 60.18487), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=60.18-epoch=3.ckpt" as top 3
+Epoch 4, global step 249: val_loss reached 59.47455 (best 59.47455), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=59.47-epoch=4.ckpt" as top 3
+Epoch 5, global step 299: val_loss reached 59.16112 (best 59.16112), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=59.16-epoch=5.ckpt" as top 3
+Epoch 6, global step 349: val_loss reached 59.05064 (best 59.05064), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=59.05-epoch=6.ckpt" as top 3
+Epoch 7, global step 399: val_loss reached 59.03148 (best 59.03148), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=59.03-epoch=7.ckpt" as top 3
+Saving latest checkpoint...
+Global seed set to 42
+
+  | Name                       | Type                 | Params
+--------------------------------------------------------------------
+0 | transformer                | ElectraModel         | 108 M 
+1 | punct_classifier           | TokenClassifier      | 3.8 K 
+2 | domain_classifier          | SequenceClassifier   | 1.5 K 
+3 | punctuation_loss           | LinearChainCRF       | 35    
+4 | domain_loss                | CrossEntropyLoss     | 0     
+5 | agg_loss                   | AggregatorLoss       | 0     
+6 | punct_class_report         | ClassificationReport | 0     
+7 | chunked_punct_class_report | ClassificationReport | 0     
+8 | domain_class_report        | ClassificationReport | 0     
+--------------------------------------------------------------------
+7.1 M     Trainable params
+101 M     Non-trainable params
+108 M     Total params
+Epoch 0, global step 449: val_loss reached 59.03037 (best 59.03037), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=59.03-epoch=0.ckpt" as top 3
+Epoch 1, global step 499: val_loss reached 59.02941 (best 59.02941), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=59.03-epoch=1.ckpt" as top 3
+Epoch 2, global step 549: val_loss reached 59.02843 (best 59.02843), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=59.03-epoch=2.ckpt" as top 3
+Epoch 3, global step 599: val_loss reached 59.02758 (best 59.02758), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=59.03-epoch=3.ckpt" as top 3
+Epoch 4, global step 649: val_loss reached 59.02651 (best 59.02651), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=59.03-epoch=4.ckpt" as top 3
+Epoch 5, global step 699: val_loss reached 59.02562 (best 59.02562), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=59.03-epoch=5.ckpt" as top 3
+Epoch 6, global step 749: val_loss reached 59.02467 (best 59.02467), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=59.02-epoch=6.ckpt" as top 3
+Epoch 7, global step 799: val_loss reached 59.02380 (best 59.02380), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=59.02-epoch=7.ckpt" as top 3
+Global seed set to 42
+
+  | Name                       | Type                 | Params
+--------------------------------------------------------------------
+0 | transformer                | ElectraModel         | 108 M 
+1 | punct_classifier           | TokenClassifier      | 3.8 K 
+2 | domain_classifier          | SequenceClassifier   | 1.5 K 
+3 | punctuation_loss           | LinearChainCRF       | 35    
+4 | domain_loss                | CrossEntropyLoss     | 0     
+5 | agg_loss                   | AggregatorLoss       | 0     
+6 | punct_class_report         | ClassificationReport | 0     
+7 | chunked_punct_class_report | ClassificationReport | 0     
+8 | domain_class_report        | ClassificationReport | 0     
+--------------------------------------------------------------------
+14.2 M    Trainable params
+94.7 M    Non-trainable params
+108 M     Total params
+Epoch 0, global step 849: val_loss reached 59.02153 (best 59.02153), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=59.02-epoch=0.ckpt" as top 3
+Epoch 1, global step 899: val_loss reached 59.01865 (best 59.01865), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=59.02-epoch=1.ckpt" as top 3
+Epoch 2, global step 949: val_loss reached 59.01544 (best 59.01544), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=59.02-epoch=2.ckpt" as top 3
+Epoch 3, global step 999: val_loss reached 59.01217 (best 59.01217), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=59.01-epoch=3.ckpt" as top 3
+Epoch 4, global step 1049: val_loss reached 59.00918 (best 59.00918), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=59.01-epoch=4.ckpt" as top 3
+Epoch 5, global step 1099: val_loss reached 59.00644 (best 59.00644), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=59.01-epoch=5.ckpt" as top 3
+Epoch 6, global step 1149: val_loss reached 59.00360 (best 59.00360), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=59.00-epoch=6.ckpt" as top 3
+Epoch 7, global step 1199: val_loss reached 59.00091 (best 59.00091), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=59.00-epoch=7.ckpt" as top 3
+Global seed set to 42
+
+  | Name                       | Type                 | Params
+--------------------------------------------------------------------
+0 | transformer                | ElectraModel         | 108 M 
+1 | punct_classifier           | TokenClassifier      | 3.8 K 
+2 | domain_classifier          | SequenceClassifier   | 1.5 K 
+3 | punctuation_loss           | LinearChainCRF       | 35    
+4 | domain_loss                | CrossEntropyLoss     | 0     
+5 | agg_loss                   | AggregatorLoss       | 0     
+6 | punct_class_report         | ClassificationReport | 0     
+7 | chunked_punct_class_report | ClassificationReport | 0     
+8 | domain_class_report        | ClassificationReport | 0     
+--------------------------------------------------------------------
+21.3 M    Trainable params
+87.6 M    Non-trainable params
+108 M     Total params
+Epoch 0, global step 1249: val_loss reached 58.99876 (best 58.99876), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=59.00-epoch=0.ckpt" as top 3
+Epoch 1, global step 1299: val_loss reached 58.99594 (best 58.99594), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=59.00-epoch=1.ckpt" as top 3
+Epoch 2, global step 1349: val_loss reached 58.99242 (best 58.99242), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=58.99-epoch=2.ckpt" as top 3
+Epoch 3, global step 1399: val_loss reached 58.98897 (best 58.98897), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=58.99-epoch=3.ckpt" as top 3
+Epoch 4, global step 1449: val_loss reached 58.98552 (best 58.98552), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=58.99-epoch=4.ckpt" as top 3
+Epoch 5, global step 1499: val_loss reached 58.98222 (best 58.98222), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=58.98-epoch=5.ckpt" as top 3
+Epoch 6, global step 1549: val_loss reached 58.97903 (best 58.97903), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=58.98-epoch=6.ckpt" as top 3
+Epoch 7, global step 1599: val_loss reached 58.97577 (best 58.97577), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/checkpoints/Punctuation_with_Domain_discriminator---val_loss=58.98-epoch=7.ckpt" as top 3
+GPU available: True, used: True
+TPU available: None, using: 0 TPU cores
+Using environment variable NODE_RANK for node rank (0).
+LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
diff --git a/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/nemo_error_log.txt
index 7fdcdc9..6522f4a 100644
--- a/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/nemo_error_log.txt
+++ b/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/nemo_error_log.txt
@@ -23,3 +23,12 @@
 [NeMo W 2021-03-01 17:06:34 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
     
+[NeMo W 2021-03-01 17:08:26 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fc79ec7e850> was reported to be 199 (when accessing len(dataloader)), but 200 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
+[NeMo W 2021-03-01 17:08:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fc79ec80bb0> was reported to be 25 (when accessing len(dataloader)), but 26 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
+[NeMo W 2021-03-01 18:27:59 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fc77c08e8b0> was reported to be 25 (when accessing len(dataloader)), but 26 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
diff --git a/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/nemo_log_globalrank-0_localrank-0.txt
index 6165c90..e1953c2 100644
--- a/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/nemo_log_globalrank-0_localrank-0.txt
+++ b/Punctuation_with_Domain_discriminator/2021-03-01_17-06-18/nemo_log_globalrank-0_localrank-0.txt
@@ -25,3 +25,12 @@
 [NeMo W 2021-03-01 17:06:34 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
     
+[NeMo W 2021-03-01 17:08:26 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fc79ec7e850> was reported to be 199 (when accessing len(dataloader)), but 200 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
+[NeMo W 2021-03-01 17:08:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fc79ec80bb0> was reported to be 25 (when accessing len(dataloader)), but 26 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
+[NeMo W 2021-03-01 18:27:59 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fc77c08e8b0> was reported to be 25 (when accessing len(dataloader)), but 26 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
diff --git a/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/events.out.tfevents.1614589596.intern-instance.32758.0 b/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/events.out.tfevents.1614589596.intern-instance.32758.0
index ef4a709..a4601b1 100644
Binary files a/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/events.out.tfevents.1614589596.intern-instance.32758.0 and b/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/events.out.tfevents.1614589596.intern-instance.32758.0 differ
diff --git a/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/lightning_logs.txt
index 7fec6d7..fff5479 100644
--- a/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/lightning_logs.txt
+++ b/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/lightning_logs.txt
@@ -21,3 +21,91 @@ initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
 5.4 K     Trainable params
 108 M     Non-trainable params
 108 M     Total params
+Epoch 0, global step 49: val_loss reached 78.70628 (best 78.70628), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=78.71-epoch=0.ckpt" as top 3
+Epoch 1, global step 99: val_loss reached 54.97641 (best 54.97641), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=54.98-epoch=1.ckpt" as top 3
+Epoch 2, global step 149: val_loss reached 51.84042 (best 51.84042), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=51.84-epoch=2.ckpt" as top 3
+Epoch 3, global step 199: val_loss reached 50.42594 (best 50.42594), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=50.43-epoch=3.ckpt" as top 3
+Epoch 4, global step 249: val_loss reached 50.21295 (best 50.21295), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=50.21-epoch=4.ckpt" as top 3
+Epoch 5, global step 299: val_loss reached 46.51522 (best 46.51522), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=46.52-epoch=5.ckpt" as top 3
+Epoch 6, global step 349: val_loss reached 45.61020 (best 45.61020), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=45.61-epoch=6.ckpt" as top 3
+Epoch 7, global step 399: val_loss reached 45.42730 (best 45.42730), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=45.43-epoch=7.ckpt" as top 3
+Saving latest checkpoint...
+Global seed set to 42
+
+  | Name                       | Type                 | Params
+--------------------------------------------------------------------
+0 | transformer                | ElectraModel         | 108 M 
+1 | punct_classifier           | TokenClassifier      | 3.8 K 
+2 | domain_classifier          | SequenceClassifier   | 1.5 K 
+3 | punctuation_loss           | LinearChainCRF       | 35    
+4 | domain_loss                | CrossEntropyLoss     | 0     
+5 | agg_loss                   | AggregatorLoss       | 0     
+6 | punct_class_report         | ClassificationReport | 0     
+7 | chunked_punct_class_report | ClassificationReport | 0     
+8 | domain_class_report        | ClassificationReport | 0     
+--------------------------------------------------------------------
+7.1 M     Trainable params
+101 M     Non-trainable params
+108 M     Total params
+Epoch 0, global step 449: val_loss reached 45.31381 (best 45.31381), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=45.31-epoch=0.ckpt" as top 3
+Epoch 1, global step 499: val_loss reached 45.16925 (best 45.16925), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=45.17-epoch=1.ckpt" as top 3
+Epoch 2, step 549: val_loss was not in top 3
+Epoch 3, global step 599: val_loss reached 45.17898 (best 45.16925), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=45.18-epoch=3.ckpt" as top 3
+Epoch 4, step 649: val_loss was not in top 3
+Epoch 5, step 699: val_loss was not in top 3
+Epoch 6, step 749: val_loss was not in top 3
+Epoch 7, step 799: val_loss was not in top 3
+Global seed set to 42
+
+  | Name                       | Type                 | Params
+--------------------------------------------------------------------
+0 | transformer                | ElectraModel         | 108 M 
+1 | punct_classifier           | TokenClassifier      | 3.8 K 
+2 | domain_classifier          | SequenceClassifier   | 1.5 K 
+3 | punctuation_loss           | LinearChainCRF       | 35    
+4 | domain_loss                | CrossEntropyLoss     | 0     
+5 | agg_loss                   | AggregatorLoss       | 0     
+6 | punct_class_report         | ClassificationReport | 0     
+7 | chunked_punct_class_report | ClassificationReport | 0     
+8 | domain_class_report        | ClassificationReport | 0     
+--------------------------------------------------------------------
+14.2 M    Trainable params
+94.7 M    Non-trainable params
+108 M     Total params
+Epoch 0, global step 849: val_loss reached 45.29802 (best 45.16925), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=45.30-epoch=0.ckpt" as top 3
+Epoch 1, global step 899: val_loss reached 45.15104 (best 45.15104), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=45.15-epoch=1.ckpt" as top 3
+Epoch 2, step 949: val_loss was not in top 3
+Epoch 3, global step 999: val_loss reached 45.15690 (best 45.15104), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=45.16-epoch=3.ckpt" as top 3
+Epoch 4, step 1049: val_loss was not in top 3
+Epoch 5, step 1099: val_loss was not in top 3
+Epoch 6, step 1149: val_loss was not in top 3
+Epoch 7, step 1199: val_loss was not in top 3
+Global seed set to 42
+
+  | Name                       | Type                 | Params
+--------------------------------------------------------------------
+0 | transformer                | ElectraModel         | 108 M 
+1 | punct_classifier           | TokenClassifier      | 3.8 K 
+2 | domain_classifier          | SequenceClassifier   | 1.5 K 
+3 | punctuation_loss           | LinearChainCRF       | 35    
+4 | domain_loss                | CrossEntropyLoss     | 0     
+5 | agg_loss                   | AggregatorLoss       | 0     
+6 | punct_class_report         | ClassificationReport | 0     
+7 | chunked_punct_class_report | ClassificationReport | 0     
+8 | domain_class_report        | ClassificationReport | 0     
+--------------------------------------------------------------------
+21.3 M    Trainable params
+87.6 M    Non-trainable params
+108 M     Total params
+Epoch 0, step 1249: val_loss was not in top 3
+Epoch 1, global step 1299: val_loss reached 45.11864 (best 45.11864), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=45.12-epoch=1.ckpt" as top 3
+Epoch 2, step 1349: val_loss was not in top 3
+Epoch 3, global step 1399: val_loss reached 45.12158 (best 45.11864), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=45.12-epoch=3.ckpt" as top 3
+Epoch 4, step 1449: val_loss was not in top 3
+Epoch 5, step 1499: val_loss was not in top 3
+Epoch 6, step 1549: val_loss was not in top 3
+Epoch 7, step 1599: val_loss was not in top 3
+GPU available: True, used: True
+TPU available: None, using: 0 TPU cores
+Using environment variable NODE_RANK for node rank (0).
+LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
diff --git a/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/nemo_error_log.txt
index f3b1181..3129874 100644
--- a/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/nemo_error_log.txt
+++ b/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/nemo_error_log.txt
@@ -20,3 +20,12 @@
 [NeMo W 2021-03-01 17:07:28 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
     
+[NeMo W 2021-03-01 17:10:21 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f279df95a30> was reported to be 199 (when accessing len(dataloader)), but 200 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
+[NeMo W 2021-03-01 17:10:50 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f279df67070> was reported to be 25 (when accessing len(dataloader)), but 26 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
+[NeMo W 2021-03-01 19:23:53 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f277c0c2eb0> was reported to be 7 (when accessing len(dataloader)), but 8 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
diff --git a/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/nemo_log_globalrank-0_localrank-0.txt
index 1c4bae7..2b3f345 100644
--- a/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/nemo_log_globalrank-0_localrank-0.txt
+++ b/Punctuation_with_Domain_discriminator/2021-03-01_17-06-26/nemo_log_globalrank-0_localrank-0.txt
@@ -22,3 +22,12 @@
 [NeMo W 2021-03-01 17:07:28 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
     
+[NeMo W 2021-03-01 17:10:21 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f279df95a30> was reported to be 199 (when accessing len(dataloader)), but 200 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
+[NeMo W 2021-03-01 17:10:50 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f279df67070> was reported to be 25 (when accessing len(dataloader)), but 26 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
+[NeMo W 2021-03-01 19:23:53 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f277c0c2eb0> was reported to be 7 (when accessing len(dataloader)), but 8 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
diff --git a/experiment/config.yaml b/experiment/config.yaml
index 6d38b7f..f22c470 100644
--- a/experiment/config.yaml
+++ b/experiment/config.yaml
@@ -4,7 +4,7 @@ trainer:
     num_nodes: 1
     max_epochs: 8
     max_steps: null # precedence over max_epochs
-    accumulate_grad_batches: 4 # accumulates grads every k batches
+    accumulate_grad_batches: 16 # accumulates grads every k batches
     gradient_clip_val: 0
     amp_level: O1 # O1/O2 for mixed precision
     precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
@@ -46,7 +46,7 @@ model:
     unfrozen: 0
     maximum_unfrozen: 3
     unfreeze_step: 1
-    test_every_layer: true
+    test_every_layer: false
     punct_label_ids:
         - ""
         - ","
@@ -84,7 +84,7 @@ model:
         unlabelled:
             # - ${base_path}/ted_talks_processed #
             # - ${base_path}/open_subtitles_processed #  
-            - ${base_path}/switchboardutt_processed
+            # - ${base_path}/switchboardutt_processed
         # parameters for dataset preprocessing
         max_seq_length: 128
         pad_label: ''
@@ -145,7 +145,7 @@ model:
         use_transformer_init: true
         loss: 'focal'
         gamma: 1 #0.1 # coefficient of gradient reversal
-        pooling: 'mean' # 'mean' # 'mean_max' # 'token'
+        pooling: 'token' # 'mean' # 'mean_max' # 'token'
         idx_conditioned_on: 0
         weight:
             - 0.5
@@ -170,11 +170,11 @@ model:
         - 1e-8
 
     gamma:
-        - 0.1
-        - 0.1
-        - 0.1
-        - 0.1
-        - 0.1
+        - 0.5
+        - 0.5
+        - 0.5
+        - 0.5
+        - 0.5
         # - 1e-0
         # - 5e-1
         # - 4e-1
diff --git a/experiment/info.log b/experiment/info.log
index 5d0d275..e2b8960 100644
--- a/experiment/info.log
+++ b/experiment/info.log
@@ -3,75 +3,3 @@
 [INFO] - TPU available: None, using: 0 TPU cores
 [INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
 [INFO] - Using native 16bit precision.
-[INFO] - shuffling train set
-[INFO] - Global seed set to 42
-[INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-[INFO] - Optimizer config = AdamW (
-Parameter Group 0
-    amsgrad: False
-    betas: (0.9, 0.999)
-    eps: 1e-08
-    lr: 0.01
-    weight_decay: 0.0
-)
-[INFO] - Scheduler "<core.optim.lr_scheduler.CosineAnnealing object at 0x7f2741533d30>" 
-will be used during training (effective maximum steps = 400) - 
-Parameters : 
-(warmup_steps: null
-warmup_ratio: 0.1
-min_lr: 1.0e-08
-last_epoch: -1
-max_steps: 400
-)
-[INFO] - 
-  | Name                       | Type                 | Params
---------------------------------------------------------------------
-0 | transformer                | ElectraModel         | 108 M 
-1 | punct_classifier           | TokenClassifier      | 3.8 K 
-2 | domain_classifier          | SequenceClassifier   | 1.5 K 
-3 | punctuation_loss           | LinearChainCRF       | 35    
-4 | domain_loss                | CrossEntropyLoss     | 0     
-5 | agg_loss                   | AggregatorLoss       | 0     
-6 | punct_class_report         | ClassificationReport | 0     
-7 | chunked_punct_class_report | ClassificationReport | 0     
-8 | domain_class_report        | ClassificationReport | 0     
---------------------------------------------------------------------
-5.4 K     Trainable params
-108 M     Non-trainable params
-108 M     Total params
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          86.85       0.98       1.94      35603
-# (label_id: 1)                                          0.00       0.00       0.00       1494
-, (label_id: 2)                                          3.02      87.79       5.84       1212
-. (label_id: 3)                                          0.00       0.00       0.00       1008
-? (label_id: 4)                                          0.30      13.10       0.59         84
--------------------
-micro avg                                                3.62       3.62       3.62      39401
-macro avg                                               18.03      20.37       1.67      39401
-weighted avg                                            78.57       3.62       1.94      39401
-
--------------------
-                       #           ,           .           ?
-      350.00        17.00        13.00        20.00         3.00
-       81.00         0.00         2.00        10.00         1.00
-    32073.00      1170.00      1064.00       863.00        69.00
-       25.00         0.00         2.00         0.00         0.00
-     3074.00       307.00       131.00       115.00        11.00
--------------------
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                          0.00       0.00       0.00          0
-1 (label_id: 1)                                          0.00       0.00       0.00        336
--------------------
-micro avg                                                0.00       0.00       0.00        336
-macro avg                                                0.00       0.00       0.00        336
-weighted avg                                             0.00       0.00       0.00        336
-
--------------------
-           0           1
-        0.00       336.00
-        0.00         0.00
--------------------
-
