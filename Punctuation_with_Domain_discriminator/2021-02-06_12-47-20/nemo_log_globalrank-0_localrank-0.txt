[NeMo W 2021-02-06 12:47:20 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
      warnings.warn(*args, **kwargs)
    
[NeMo I 2021-02-06 12:47:20 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20
[NeMo I 2021-02-06 12:47:20 exp_manager:519] TensorboardLogger has been set up
[NeMo W 2021-02-06 12:47:20 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
[NeMo W 2021-02-06 12:47:30 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
      warnings.warn(*args, **kwargs)
    
[NeMo W 2021-02-06 12:47:30 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
      warnings.warn(*args, **kwargs)
    
[NeMo W 2021-02-06 12:47:35 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
      warnings.warn(*args, **kwargs)
    
[NeMo W 2021-02-06 12:47:43 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtools/optim/radam.py:62: UserWarning: This overload of addcmul_ is deprecated:
    	addcmul_(Number value, Tensor tensor1, Tensor tensor2)
    Consider using one of the following signatures instead:
    	addcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/torch/csrc/utils/python_arg_parser.cpp:882.)
      exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
    
[NeMo W 2021-02-06 13:06:22 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f82e4ea6af0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
      warnings.warn(warn_msg)
    
[NeMo W 2021-02-06 13:06:50 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f82e4f64af0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
      warnings.warn(warn_msg)
    
[NeMo W 2021-02-06 14:00:15 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
      warnings.warn(*args, **kwargs)
    
[NeMo W 2021-02-06 14:00:27 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
      warnings.warn(*args, **kwargs)
    
