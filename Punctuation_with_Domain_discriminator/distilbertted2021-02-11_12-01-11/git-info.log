commit hash: e06c976eca0f0f1ece5dd302964c113374b09762
diff --git a/README.md b/README.md
index 8b23590..e1fff55 100644
--- a/README.md
+++ b/README.md
@@ -619,3 +619,22 @@ weighted avg                                            90.70      90.25      90
  'punct_precision': tensor(33.9651),
  'punct_recall': tensor(33.8733),
  'test_loss': tensor(0.2265)}
+
+TED end
+ 'punct_f1': tensor(32.2363, device='cuda:0'),
+ 'punct_precision': tensor(30.6842, device='cuda:0'),
+ 'punct_recall': tensor(36.3651, device='cuda:0'),
+ 'test_loss': tensor(0.2000, device='cuda:0')}
+
+TED start
+'punct_f1': tensor(32.0951, device='cuda:0'),
+'punct_precision': tensor(30.3402, device='cuda:0'),
+'punct_recall': tensor(36.4819, device='cuda:0'),
+'test_loss': tensor(0.1911, device='cuda:0')}
+
+
+TED None
+{'punct_f1': tensor(35.3044, device='cuda:0'),
+ 'punct_precision': tensor(34.8901, device='cuda:0'),
+ 'punct_recall': tensor(35.7895, device='cuda:0'),
+ 'test_loss': tensor(0.2175, device='cuda:0')}
diff --git a/experiment/Inference.ipynb b/experiment/Inference.ipynb
index 99790f8..3756810 100644
--- a/experiment/Inference.ipynb
+++ b/experiment/Inference.ipynb
@@ -2,15 +2,34 @@
  "cells": [
   {
    "cell_type": "code",
-   "execution_count": 1,
+   "execution_count": 10,
    "id": "modern-amplifier",
    "metadata": {},
    "outputs": [
     {
-     "output_type": "stream",
      "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "/home/nxingyu2/project/experiment\n",
+      "shuffling <data.punctuation_dataset_multi.PunctuationDomainDataset object at 0x7fb2c71e8790>\n"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
      "text": [
-      "/home/nxingyu2/project/experiment\n"
+      "11:23:21.60 LOG:\n",
+      "11:23:21.61 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
+      "11:23:21.62 LOG:\n",
+      "11:23:21.63 .... f\"1st {n} encoder layers of transformer frozen\" = '1st 11 encoder layers of transformer frozen'\n",
+      "GPU available: True, used: True\n",
+      "INFO:lightning:GPU available: True, used: True\n",
+      "TPU available: None, using: 0 TPU cores\n",
+      "INFO:lightning:TPU available: None, using: 0 TPU cores\n",
+      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
+      "INFO:lightning:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
+      "Using native 16bit precision.\n",
+      "INFO:lightning:Using native 16bit precision.\n"
      ]
     }
    ],
@@ -37,29 +56,191 @@
     "\n",
     "from hydra.experimental import initialize, initialize_config_module, initialize_config_dir, compose\n",
     "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
-    "initialize(config_path=\"project/experiment\")\n",
+    "# folder=\"TEDend2021-02-11_07-57-33\"\n",
+    "folder=\"TEDstart2021-02-11_07-55-58\"\n",
+    "# folder=\"TEDnone2021-02-11_10-14-34\"\n",
+    "initialize(config_path=\"../Punctuation_with_Domain_discriminator/\"+folder) #config_path=\"project/experiment\"\n",
     "!pwd\n",
-    "\n",
     "cfg=compose(\n",
-    "    config_name=\"test_config.yaml\", \n",
+    "    config_name=\"hparams.yaml\", \n",
     ")\n",
     "cfg.model.punct_label_ids=OmegaConf.create(sorted(cfg.model.punct_label_ids))\n",
-    "labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
-    "ids_to_labels = {_[0]:_[1] for _ in enumerate(cfg.model.punct_label_ids)}\n",
+    "# labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
+    "# ids_to_labels = {_[0]:_[1] for _ in enumerate(cfg.model.punct_label_ids)}\n",
+    "model = PunctuationDomainModel.load_from_checkpoint( #TEDend2021-02-11_07-57-33  # TEDstart2021-02-11_07-55-58\n",
+    "    checkpoint_path=f\"/home/nxingyu2/project/Punctuation_with_Domain_discriminator/{folder}/checkpoints/Punctuation_with_Domain_discriminator-last.ckpt\")\n",
+    "trainer = pl.Trainer(**cfg.trainer)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 9,
+   "id": "hairy-proxy",
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "[' hooooooo,                                                                                                                           ',\n",
+       " ' what can i do for you today?                                                                                                                        ',\n",
+       " ' ? how are you?                                                                                                                            ',\n",
+       " ' in guadeloupe or marti nique, it also brin gs into ques tion budgetary po licy. because the europe, an union is after all, making a present of ecu 1, 9 billion to three multinationals. where are the financial interests of the european union— firstly, development policy— in africa. in any case, in the acp, countries employ ment policy in madeira, the canaries— guadel, oupe, martinique and crete, regional, pol icy in the ultra- peripheral areas— human rights, which mr barthet mayer mentioned earlier, since dollar bananas are after all, slavery. bana nas, the product of human exploitation by three multinat ionals, payments of ec ',\n",
+       " ' u 50 per mon th. instead of ecu 50 per day                                                                                                                   ',\n",
+       " ' plans for this weekend, include turning wine into water. the small white buoys marked the location of hundreds of crab pots. he said he was not there yesterday. however, many people saw him there. today. arrived with a crash of my car through the garage door. the lyrics of the song sounded like fingernails on a chalkboard. the guinea fowl flies through the air with all the grace of a turtle. they ran around the corner, to find that they had traveled back in time,                                      ',\n",
+       " ' good morning. everyone? how have your weekends been? its a really great day? thank you.                                                                                                                ',\n",
+       " ' first of all, i too agree that tourism- related action must include employment training and education. as you know, after the european conference on tourism and employment in luxemborg, we set up a high- level group, whose mission was to examine how best tourism could contribute towards employment— the first stage,                                                                          ']"
+      ]
+     },
+     "execution_count": 9,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
     "\n",
-    "# model = PunctuationDomainModel.load_from_checkpoint(\n",
-    "#     checkpoint_path=\"/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator-last.ckpt\")\n",
+    "queries = [\n",
+    "    'Hooooooo!',\n",
+    "    'what can i do for you today',\n",
+    "    'how are you',\n",
+    "    ' in guadeloupe or marti nique it also brin gs into ques tion budgetary po licy because the europe an union is after all making a present of ecu 1.9 billion to three multinationals where are the financial interests of the european union firstly development policy in africa in any case in the acp countries employ ment policy in madeira the canaries guadel oupe martinique and crete regional pol icy in the ultra-peripheral areas human rights which mr barthet mayer mentioned earlier since dollar bananas are after all slavery bana nas the product of human exploitation by three multinat ionals payments of ecu 50 per mon th instead of ecu 50 per day',\n",
+    "    '''Plans for this weekend include turning wine into water.\n",
+    "The small white buoys marked the location of hundreds of crab pots.\n",
+    "He said he was not there yesterday; however, many people saw him there.\n",
+    "Today arrived with a crash of my car through the garage door.\n",
+    "The lyrics of the song sounded like fingernails on a chalkboard.\n",
+    "The Guinea fowl flies through the air with all the grace of a turtle.\n",
+    "They ran around the corner to find that they had traveled back in time.''',\n",
+    "    'good morning everyone how have your weekends been its a really great day thank you',\n",
+    "    'first of all i too agree that tourism related action must include employment training and education as you know after the european conference on tourism and employment in luxemborg we set up a high-level group whose mission was to examine how best tourism could contribute towards employment the first stage'\n",
+    "]\n",
+    "inference_results = model.add_punctuation(queries)\n",
+    "inference_results"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 20,
+   "id": "magnetic-approach",
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n"
+     ]
+    }
+   ],
+   "source": [
+    "for i in torch.zeros((10,10)):\n",
+    "    print(' '.join('{:5.2f}'.format(x) for x in i))"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 12,
+   "id": "dietary-violin",
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "[' hooooooo                                                                                                                           ',\n",
+       " ' what can i do for you today?                                                                                                                        ',\n",
+       " ' , how? are? you?                                                                                                                            ',\n",
+       " ' in guadeloupe or marti nique it also br-in gs into ques tion budgetary po. licy because the europe, an union is after all, making a present of ecu 1, 9 billion to three multinationals where are the financial interests of the european union? firstly development policy in africa. in any case, in the ac.p countries, employ men-t policy in madeira. the canaries guadel ou.pe martinique and crete, regional, pol, icy in the ultra- peripheral areas— human rights, which mr barthet mayer mentioned earlier, since dollar bananas are after all. slavery. bana nas. the product of human exploitation, by three multi-nat iona.ls payments of ec. ',\n",
+       " ' u 50 per mon, th. instead of ecu 50 per day                                                                                                                   ',\n",
+       " ' plans for this weekend, include turning wine into water. the small white bu.oys marked the location of hundreds of crab pots. he said he was not there yesterday. however many people saw him there today, arrived with a crash of my car through the garage door. the lyrics of the song sounded like fingernails on a chalkboard the guinea f-owl flies through the air with all the grace of a turtle. they ran around the corner, to find that they had traveled back in time,                                      ',\n",
+       " ' good morning? everyone? how have your weekends? been? its a really great day? thank you.                                                                                                                ',\n",
+       " ' first of all, i too agree that tourism- related action must include employment, training and education. as you know, after the european conference on tourism and employment in luxemborg we set up a high- level group, whose mission was to examine how best tourism could contribute towards employment— the first stage,                                                                          ']"
+      ]
+     },
+     "execution_count": 12,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "\n",
+    "queries = [\n",
+    "    'Hooooooo!',\n",
+    "    'what can i do for you today',\n",
+    "    'how are you',\n",
+    "    ' in guadeloupe or marti nique it also brin gs into ques tion budgetary po licy because the europe an union is after all making a present of ecu 1.9 billion to three multinationals where are the financial interests of the european union firstly development policy in africa in any case in the acp countries employ ment policy in madeira the canaries guadel oupe martinique and crete regional pol icy in the ultra-peripheral areas human rights which mr barthet mayer mentioned earlier since dollar bananas are after all slavery bana nas the product of human exploitation by three multinat ionals payments of ecu 50 per mon th instead of ecu 50 per day',\n",
+    "    '''Plans for this weekend include turning wine into water.\n",
+    "The small white buoys marked the location of hundreds of crab pots.\n",
+    "He said he was not there yesterday; however, many people saw him there.\n",
+    "Today arrived with a crash of my car through the garage door.\n",
+    "The lyrics of the song sounded like fingernails on a chalkboard.\n",
+    "The Guinea fowl flies through the air with all the grace of a turtle.\n",
+    "They ran around the corner to find that they had traveled back in time.''',\n",
+    "    'good morning everyone how have your weekends been its a really great day thank you',\n",
+    "    'first of all i too agree that tourism related action must include employment training and education as you know after the european conference on tourism and employment in luxemborg we set up a high-level group whose mission was to examine how best tourism could contribute towards employment the first stage'\n",
+    "]\n",
+    "inference_results = model.add_punctuation(queries)\n",
+    "inference_results"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 9,
+   "id": "residential-scene",
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "[' hooooooo,                                                                                                                           ',\n",
+       " ' what can i do for you today?                                                                                                                        ',\n",
+       " ' ? how are you?                                                                                                                            ',\n",
+       " ' firstly, development policy in africa. in any case, in the acp, countries, employment policy in madeira, the canaries— guadeloupe, martinique and crete, regional policy in the ultra- peripheral areas— human rights, which mr barthet mayer, mentioned earlier, since dollar bananas are after all, slavery. bananas, the product of human exploitation by three multinationals, payments of ecu 50 per month, instead of ecu 50 per day in guadeloupe or martinique. it also brings into question budgetary policy. because the european union is after all, making a present of ecu 1, 9 billion to three multinationals. where are the financial interests of the european union?         ',\n",
+       " ' plans for this weekend, include turning wine into water. the small white buoys marked the location of hundreds of crab pots. he said he was not there yesterday. however, many people saw him there. today. arrived with a crash of my car through the garage door. the lyrics of the song sounded like fingernails on a chalkboard. the guinea fowl flies through the air with all the grace of a turtle. they ran around the corner, to find that they had traveled back in time,                                      ',\n",
+       " ' good morning. everyone? how have your weekends been? its a really great day? thank you.                                                                                                                ',\n",
+       " ' first of all, i too agree that tourism- related action must include employment training and education. as you know, after the european conference on tourism and employment in luxemborg, we set up a high- level group, whose mission was to examine how best tourism could contribute towards employment— the first stage,                                                                          ']"
+      ]
+     },
+     "execution_count": 9,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
     "\n",
-    "# trainer = pl.Trainer(**cfg.trainer)"
+    "queries = [\n",
+    "    'Hooooooo!',\n",
+    "    'what can i do for you today',\n",
+    "    'how are you',\n",
+    "    'firstly development policy in africa in any case in the acp countries employment policy in madeira the canaries guadeloupe martinique and crete regional policy in the ultra-peripheral areas human rights which mr barthet-mayer mentioned earlier since dollar bananas are after all slavery bananas the product of human exploitation by three multinationals payments of ecu 50 per month instead of ecu 50 per day in guadeloupe or martinique it also brings into question budgetary policy because the european union is after all making a present of ecu 1.9 billion to three multinationals where are the financial interests of the european union',\n",
+    "    '''Plans for this weekend include turning wine into water.\n",
+    "The small white buoys marked the location of hundreds of crab pots.\n",
+    "He said he was not there yesterday; however, many people saw him there.\n",
+    "Today arrived with a crash of my car through the garage door.\n",
+    "The lyrics of the song sounded like fingernails on a chalkboard.\n",
+    "The Guinea fowl flies through the air with all the grace of a turtle.\n",
+    "They ran around the corner to find that they had traveled back in time.''',\n",
+    "    'good morning everyone how have your weekends been its a really great day thank you',\n",
+    "    'first of all i too agree that tourism related action must include employment training and education as you know after the european conference on tourism and employment in luxemborg we set up a high-level group whose mission was to examine how best tourism could contribute towards employment the first stage'\n",
+    "]\n",
+    "inference_results = model.add_punctuation(queries)\n",
+    "inference_results"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 11,
+   "execution_count": 3,
+   "id": "loose-assignment",
    "metadata": {},
    "outputs": [
     {
-     "output_type": "execute_result",
      "data": {
       "text/plain": [
        "{'input_ids': tensor([[ 101, 7570, 9541, 9541, 9541,  102,    0,    0,    0,    0,    0,    0,\n",
@@ -86,7 +267,7 @@
        "          False, False, False, False, False, False, False, False, False, False,\n",
        "          False, False, False, False, False, False, False, False, False, False,\n",
        "          False, False, False, False, False, False, False, False]]),\n",
-       " 'subtoken_mask': tensor([[ True, False, False, False,  True,  True, False, False, False, False,\n",
+       " 'subtoken_mask': tensor([[ True,  True, False, False, False, False, False, False, False, False,\n",
        "          False, False, False, False, False, False, False, False, False, False,\n",
        "          False, False, False, False, False, False, False, False, False, False,\n",
        "          False, False, False, False, False, False, False, False, False, False,\n",
@@ -99,7 +280,7 @@
        "          False, False, False, False, False, False, False, False, False, False,\n",
        "          False, False, False, False, False, False, False, False, False, False,\n",
        "          False, False, False, False, False, False, False, False]]),\n",
-       " 'labels': tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
+       " 'labels': tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
@@ -107,11 +288,15 @@
        "          0, 0, 0, 0, 0, 0, 0, 0]])}"
       ]
      },
+     "execution_count": 3,
      "metadata": {},
-     "execution_count": 11
+     "output_type": "execute_result"
     }
    ],
    "source": [
+    "# cfg.model.punct_label_ids=OmegaConf.create(sorted(cfg.model.punct_label_ids))\n",
+    "# labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
+    "\n",
     "queries = [\n",
     "    'Hooooooo!',\n",
     "    # 'what can i do for you today',\n",
@@ -120,36 +305,10 @@
     "]\n",
     "cfg.model.punct_label_ids=OmegaConf.create(sorted(cfg.model.punct_label_ids))\n",
     "labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
-    "ds=PunctuationInferenceDataset(tokenizer=AutoTokenizer.from_pretrained('google/electra-small-discriminator'), queries=queries, max_seq_length=128, punct_label_ids=labels_to_ids, degree=0, attach_label_to_end=True)\n",
+    "ds=PunctuationInferenceDataset(tokenizer=AutoTokenizer.from_pretrained('google/electra-small-discriminator'), queries=queries, max_seq_length=128, punct_label_ids=labels_to_ids, degree=0, attach_label_to_end=False)\n",
     "ds[0]"
    ]
   },
-  {
-   "cell_type": "code",
-   "execution_count": 8,
-   "id": "hairy-proxy",
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "[' we bought four shirts, one pen, and a mug from the nvidia gear store in santa clara                                                                                                            ',\n",
-       " ' what can i do for you today?                                                                                                                        ',\n",
-       " ' , how are you? ,                                                                                                                           ',\n",
-       " ' good morning. everyone? how have your weekends been? its a really great day?                                                                                                                  ']"
-      ]
-     },
-     "execution_count": 8,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "\n",
-    "inference_results = model.add_punctuation(queries)\n",
-    "inference_results"
-   ]
-  },
   {
    "cell_type": "code",
    "execution_count": null,
@@ -175,9 +334,9 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.8.5-final"
+   "version": "3.8.5"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 5
-}
\ No newline at end of file
+}
diff --git a/experiment/config.yaml b/experiment/config.yaml
index 2aa798f..9cfe485 100644
--- a/experiment/config.yaml
+++ b/experiment/config.yaml
@@ -1,36 +1,36 @@
 seed: 42
 trainer:
-    # gpus: 1 # the number of gpus, 0 for CPU
-    # num_nodes: 1
-    # max_epochs: 15
-    # max_steps: null # precedence over max_epochs
-    # accumulate_grad_batches: 4 # accumulates grads every k batches
-    # gradient_clip_val: 0
-    # amp_level: O1 # O1/O2 for mixed precision
-    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-    # accelerator: ddp
-    # checkpoint_callback: false  # Provided by exp_manager
-    # logger: false #false  # Provided by exp_manager
-    # log_every_n_steps: 1  # Interval of logging.
-    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-    # resume_from_checkpoint: null
-
-    gpus: 0 # the number of gpus, 0 for CPU
+    gpus: 1 # the number of gpus, 0 for CPU
     num_nodes: 1
-    max_epochs: 8
+    max_epochs: 15
     max_steps: null # precedence over max_epochs
     accumulate_grad_batches: 4 # accumulates grads every k batches
     gradient_clip_val: 0
-    amp_level: O0 # O1/O2 for mixed precision
-    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-    # accelerator: ddp
+    amp_level: O1 # O1/O2 for mixed precision
+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
+    accelerator: ddp
     checkpoint_callback: false  # Provided by exp_manager
     logger: false #false  # Provided by exp_manager
     log_every_n_steps: 1  # Interval of logging.
     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-    reload_dataloaders_every_epoch: true
     resume_from_checkpoint: null
 
+    # gpus: 0 # the number of gpus, 0 for CPU
+    # num_nodes: 1
+    # max_epochs: 8
+    # max_steps: null # precedence over max_epochs
+    # accumulate_grad_batches: 4 # accumulates grads every k batches
+    # gradient_clip_val: 0
+    # amp_level: O0 # O1/O2 for mixed precision
+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
+    # # accelerator: ddp
+    # checkpoint_callback: false  # Provided by exp_manager
+    # logger: false #false  # Provided by exp_manager
+    # log_every_n_steps: 1  # Interval of logging.
+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
+    # reload_dataloaders_every_epoch: true
+    # resume_from_checkpoint: null
+
 exp_manager:
     exp_dir: /home/nxingyu2/project/ # /root/project # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
     name: Punctuation_with_Domain_discriminator  # The name of your model
@@ -41,7 +41,7 @@ tmp_path: /home/nxingyu2/data/tmp # /tmp #
 
 model:
     nemo_path: null
-    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
+    transformer_path: distilbert-base-uncased # google/electra-small-discriminator #  filename to save the model and associated artifacts to .nemo file
     unfrozen: 0
     maximum_unfrozen: 1
     unfreeze_step: 1
@@ -81,7 +81,7 @@ model:
         num_labels: 10
         num_domains: 1
         test_unlabelled: true
-        attach_label_to_end: false # false if attach to start
+        attach_label_to_end: none # false if attach to start none if dont mask
 
         train_ds:
             shuffle: true
diff --git a/experiment/core/classification_report.py b/experiment/core/classification_report.py
index 7ff6282..1a710a2 100644
--- a/experiment/core/classification_report.py
+++ b/experiment/core/classification_report.py
@@ -15,6 +15,7 @@
 from typing import Any, Dict, Optional
 
 import torch
+from torch.nn.functional import one_hot
 from pytorch_lightning.metrics import Metric
 from pytorch_lightning.metrics.utils import METRIC_EPS
 
@@ -82,11 +83,14 @@ class ClassificationReport(Metric):
         self.add_state(
             "num_examples_per_class", default=torch.zeros(num_classes), dist_reduce_fx='sum', persistent=False
         )
+        self.add_state("cm", default=torch.zeros((num_classes,num_classes)), dist_reduce_fx="sum", persistent=False)
 
     def update(self, predictions: torch.Tensor, labels: torch.Tensor):
         TP = []
         FN = []
         FP = []
+        CM = torch.zeros((self.num_classes,self.num_classes),dtype=torch.long).to(predictions.device)
+        CM.index_add_(0, predictions, one_hot(labels,num_classes=self.num_classes))
         for label_id in range(self.num_classes):
             current_label = labels == label_id
             label_predicted = predictions == label_id
@@ -98,11 +102,13 @@ class ClassificationReport(Metric):
         tp = torch.tensor(TP).to(predictions.device)
         fn = torch.tensor(FN).to(predictions.device)
         fp = torch.tensor(FP).to(predictions.device)
+        # cm = torch.tensor(CM).to(predictions.device)
         num_examples_per_class = tp + fn
 
         self.tp += tp
         self.fn += fn
         self.fp += fp
+        self.cm += CM
         self.num_examples_per_class += num_examples_per_class
 
     def compute(self):
@@ -156,16 +162,19 @@ class ClassificationReport(Metric):
             + '\n'
         )
 
+        report += "\n-------------------\n"
+        cm = '\n'.join([(' '.join('{:5.2f}'.format(x) for x in i)) for i in self.cm])
+        report += cm
         self.total_examples = total_examples
 
         if self.mode == 'macro':
-            return macro_precision, macro_recall, macro_f1, report
+            return macro_precision, macro_recall, macro_f1, report, cm
         elif self.mode == 'weighted':
-            return weighted_precision, weighted_recall, weighted_f1, report
+            return weighted_precision, weighted_recall, weighted_f1, report, cm
         elif self.mode == 'micro':
-            return micro_precision, micro_recall, micro_f1, report
+            return micro_precision, micro_recall, micro_f1, report, cm
         elif self.mode == 'all':
-            return precision, recall, f1, report
+            return precision, recall, f1, report, cm
         else:
             raise ValueError(
                 f'{self.mode} mode is not supported. Choose "macro" to get aggregated numbers \
diff --git a/experiment/core/utils.py b/experiment/core/utils.py
index c9470b8..7f288e1 100644
--- a/experiment/core/utils.py
+++ b/experiment/core/utils.py
@@ -38,7 +38,7 @@ def align_labels_to_mask(mask,labels):
     return m1.tolist()
 
 def view_aligned(texts,tags,tokenizer,labels_to_ids):
-        return [re.sub(r'( ?\[((PAD)|(CLS)|(SEP))\] ?)',' ',re.sub(' ##','',' '.join(
+        return [re.sub(r'( ?\[((PAD)|(CLS)|(SEP))\] ?)',' ',re.sub(' +##','',' '.join( #[.?!,;:\-—… ]+
             [_[0]+_[1] for _ in list(
                 zip(tokenizer.convert_ids_to_tokens(_[0]),
                     [labels_to_ids[id] for id in _[1].tolist()])
@@ -113,7 +113,11 @@ def chunk_to_len(max_seq_length,tokenizer,attach_label_to_end,tokens,labels=None
         padded_labels=[pad_to_len(max_seq_length,align_labels_to_mask(*_)) for _ in zip(masks,split_labels)]
     return ids,masks,padded_labels
     
-def chunk_to_len_batch(max_seq_length,tokenizer,tokens,labels=None,labelled=True,ignore_index=-100, attach_label_to_end=True):
+def chunk_to_len_batch(max_seq_length,tokenizer,tokens,labels=None,labelled=True,ignore_index=-100, attach_label_to_end=None):
+    no_mask=False
+    if attach_label_to_end is None:
+        no_mask=True
+        attach_label_to_end=True
     batch_ids=[]
     batch_masks=[]
     batch_labels=[]
@@ -126,8 +130,11 @@ def chunk_to_len_batch(max_seq_length,tokenizer,tokens,labels=None,labelled=True
     output = {'input_ids': torch.as_tensor(batch_ids, dtype=torch.long),
               'attention_mask': torch.as_tensor(batch_ids, dtype=torch.bool),
               'subtoken_mask': torch.as_tensor(batch_masks,dtype=torch.bool)}
-    output['subtoken_mask']|=((output['input_ids']==101)|(output['input_ids']==102))
-    output['subtoken_mask']&=labelled
+    if no_mask:
+        output['subtoken_mask']=output['attention_mask']&(output['input_ids']!=102)
+    else:
+        output['subtoken_mask']|=(output['input_ids']==101)  # dont want end token |(output['input_ids']==102)
+        output['subtoken_mask']&=labelled
     output['labels']=torch.as_tensor(batch_labels,dtype=torch.long) if labelled==True else torch.zeros_like(output['input_ids'],dtype=torch.long)
     return output
 
diff --git a/experiment/data/punctuation_datamodule.py b/experiment/data/punctuation_datamodule.py
index fb69299..71a1f7f 100644
--- a/experiment/data/punctuation_datamodule.py
+++ b/experiment/data/punctuation_datamodule.py
@@ -25,7 +25,7 @@ class PunctuationDataModule(LightningDataModule):
             data_id: str = '',
             tmp_path:str = '~/data/tmp',
             test_unlabelled:bool = True,
-            attach_label_to_end:bool = True,
+            attach_label_to_end:bool = None,
             ):
         #unlabelled=[], batch_size = 256, max_seq_length = 256, num_workers=1):
         super().__init__()
diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
index 97f2fb2..4c9f67a 100644
--- a/experiment/data/punctuation_dataset_multi.py
+++ b/experiment/data/punctuation_dataset_multi.py
@@ -39,7 +39,7 @@ class PunctuationDomainDataset(IterableDataset):
         tmp_path='~/data/tmp',
         start=0,
         end=-1,
-        attach_label_to_end=True,
+        attach_label_to_end=None,
     ):
         if not (os.path.exists(csv_file)):
             raise FileNotFoundError(
@@ -154,7 +154,7 @@ class PunctuationDomainDatasets(IterableDataset):
                  randomize:bool=True,
                  data_id='',
                  tmp_path='~/data/tmp',
-                 attach_label_to_end=True,
+                 attach_label_to_end=None,
                  ):
         worker_info = get_worker_info()
         self.num_workers=1 if worker_info is None else worker_info.num_workers
@@ -266,7 +266,7 @@ class PunctuationInferenceDataset(Dataset):
             "labels": NeuralType(('B', 'T'), ChannelType()),
         }
 
-    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], num_samples:int=256, degree:int = 0, attach_label_to_end:bool=True):
+    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], num_samples:int=256, degree:int = 0, attach_label_to_end:bool=None):
         """ Initializes BertPunctuationInferDataset. """
         self.degree=degree
         self.punct_label_ids=punct_label_ids
diff --git a/experiment/info.log b/experiment/info.log
index 85bc297..104c5dd 100644
--- a/experiment/info.log
+++ b/experiment/info.log
@@ -1,192 +1,4 @@
-[INFO] - GPU available: True, used: False
+[INFO] - GPU available: True, used: True
 [INFO] - TPU available: None, using: 0 TPU cores
-[INFO] - shuffling train set
-[INFO] - Global seed set to 42
-[INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-[INFO] - Optimizer config = AdamW (
-Parameter Group 0
-    amsgrad: False
-    betas: (0.9, 0.999)
-    eps: 1e-08
-    lr: 0.01
-    weight_decay: 0.0
-)
-[INFO] - Scheduler "<core.optim.lr_scheduler.CosineAnnealing object at 0x7f13e0b714f0>" 
-will be used during training (effective maximum steps = 1600) - 
-Parameters : 
-(warmup_steps: null
-warmup_ratio: 0.1
-min_lr: 1.0e-10
-last_epoch: -1
-max_steps: 1600
-)
-[INFO] - 
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 513   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-299 K     Trainable params
-13.2 M    Non-trainable params
-13.5 M    Total params
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          91.10      19.21      31.73       3196
-! (label_id: 1)                                          0.00       0.00       0.00          0
-, (label_id: 2)                                          5.61      37.37       9.76        198
-- (label_id: 3)                                          0.79       9.09       1.46         22
-. (label_id: 4)                                          0.00       0.00       0.00        212
-: (label_id: 5)                                          0.00       0.00       0.00          4
-; (label_id: 6)                                          0.00       0.00       0.00          4
-? (label_id: 7)                                          0.96      62.50       1.88         16
-— (label_id: 8)                                          0.00       0.00       0.00         20
-… (label_id: 9)                                          0.00       0.00       0.00          0
--------------------
-micro avg                                               19.06      19.06      19.06       3672
-macro avg                                               12.31      16.02       5.60       3672
-weighted avg                                            79.60      19.06      28.16       3672
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00         34
--------------------
-micro avg                                              100.00     100.00     100.00         34
-macro avg                                              100.00     100.00     100.00         34
-weighted avg                                           100.00     100.00     100.00         34
-
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.76      93.83      95.75     303856
-! (label_id: 1)                                          0.00       0.00       0.00        155
-, (label_id: 2)                                         38.85      59.39      46.98      23143
-- (label_id: 3)                                         72.00      53.11      61.13       1830
-. (label_id: 4)                                         58.36      60.43      59.38      20164
-: (label_id: 5)                                          0.00       0.00       0.00        439
-; (label_id: 6)                                          0.00       0.00       0.00        176
-? (label_id: 7)                                         24.31      36.42      29.15       1590
-— (label_id: 8)                                          6.84       6.23       6.52       1509
-… (label_id: 9)                                          0.00       0.00       0.00        111
--------------------
-micro avg                                               88.58      88.58      88.58     352973
-macro avg                                               29.81      30.94      29.89     352973
-weighted avg                                            90.55      88.58      89.38     352973
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       3181
--------------------
-micro avg                                              100.00     100.00     100.00       3181
-macro avg                                              100.00     100.00     100.00       3181
-weighted avg                                           100.00     100.00     100.00       3181
-
-[INFO] - Epoch 0, global step 199: val_loss reached 0.27337 (best 0.27337), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=0.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.63      94.98      96.29     303856
-! (label_id: 1)                                          0.00       0.00       0.00        155
-, (label_id: 2)                                         43.70      57.68      49.73      23143
-- (label_id: 3)                                         75.76      51.58      61.38       1830
-. (label_id: 4)                                         58.49      63.93      61.09      20164
-: (label_id: 5)                                          0.00       0.00       0.00        439
-; (label_id: 6)                                          0.00       0.00       0.00        176
-? (label_id: 7)                                         35.78      37.42      36.58       1590
-— (label_id: 8)                                          5.88       7.22       6.48       1509
-… (label_id: 9)                                          0.00       0.00       0.00        111
--------------------
-micro avg                                               89.67      89.67      89.67     352973
-macro avg                                               31.72      31.28      31.15     352973
-weighted avg                                            90.83      89.67      90.15     352973
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       3181
--------------------
-micro avg                                              100.00     100.00     100.00       3181
-macro avg                                              100.00     100.00     100.00       3181
-weighted avg                                           100.00     100.00     100.00       3181
-
-[INFO] - Epoch 1, global step 399: val_loss reached 0.26732 (best 0.26732), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=1.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.23      95.39      96.30     303856
-! (label_id: 1)                                          0.00       0.00       0.00        155
-, (label_id: 2)                                         47.20      48.78      47.98      23143
-- (label_id: 3)                                         69.68      55.25      61.63       1830
-. (label_id: 4)                                         55.03      67.55      60.65      20164
-: (label_id: 5)                                          0.00       0.00       0.00        439
-; (label_id: 6)                                          0.00       0.00       0.00        176
-? (label_id: 7)                                         28.81      45.91      35.40       1590
-— (label_id: 8)                                          8.17      11.93       9.70       1509
-… (label_id: 9)                                          0.00       0.00       0.00        111
--------------------
-micro avg                                               89.72      89.72      89.72     352973
-macro avg                                               30.61      32.48      31.17     352973
-weighted avg                                            90.47      89.72      90.03     352973
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       3181
--------------------
-micro avg                                              100.00     100.00     100.00       3181
-macro avg                                              100.00     100.00     100.00       3181
-weighted avg                                           100.00     100.00     100.00       3181
-
-[INFO] - Epoch 2, global step 599: val_loss reached 0.26594 (best 0.26594), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=2.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.60      95.36      96.46     303856
-! (label_id: 1)                                          0.00       0.00       0.00        155
-, (label_id: 2)                                         45.38      58.10      50.96      23143
-- (label_id: 3)                                         70.17      59.40      64.34       1830
-. (label_id: 4)                                         60.79      63.42      62.08      20164
-: (label_id: 5)                                          0.00       0.00       0.00        439
-; (label_id: 6)                                          0.00       0.00       0.00        176
-? (label_id: 7)                                         30.77      51.57      38.54       1590
-— (label_id: 8)                                         10.69       8.48       9.46       1509
-… (label_id: 9)                                          0.00       0.00       0.00        111
--------------------
-micro avg                                               90.10      90.10      90.10     352973
-macro avg                                               31.54      33.63      32.18     352973
-weighted avg                                            91.01      90.10      90.48     352973
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       3181
--------------------
-micro avg                                              100.00     100.00     100.00       3181
-macro avg                                              100.00     100.00     100.00       3181
-weighted avg                                           100.00     100.00     100.00       3181
-
-[INFO] - Epoch 3, global step 799: val_loss reached 0.25942 (best 0.25942), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=3.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          97.63      95.55      96.58     303856
-! (label_id: 1)                                          0.00       0.00       0.00        155
-, (label_id: 2)                                         46.84      57.30      51.54      23143
-- (label_id: 3)                                         75.90      57.49      65.42       1830
-. (label_id: 4)                                         59.34      67.15      63.00      20164
-: (label_id: 5)                                          0.00       0.00       0.00        439
-; (label_id: 6)                                          0.00       0.00       0.00        176
-? (label_id: 7)                                         33.55      48.62      39.70       1590
-— (label_id: 8)                                         12.65       6.36       8.47       1509
-… (label_id: 9)                                          0.00       0.00       0.00        111
--------------------
-micro avg                                               90.39      90.39      90.39     352973
-macro avg                                               32.59      33.25      32.47     352973
-weighted avg                                            91.10      90.39      90.67     352973
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       3181
--------------------
-micro avg                                              100.00     100.00     100.00       3181
-macro avg                                              100.00     100.00     100.00       3181
-weighted avg                                           100.00     100.00     100.00       3181
-
-[INFO] - Epoch 4, global step 999: val_loss reached 0.26189 (best 0.25942), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=4.ckpt" as top 3
+[INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
+[INFO] - Using native 16bit precision.
diff --git a/experiment/main.py b/experiment/main.py
index 4ae03e6..51abddd 100644
--- a/experiment/main.py
+++ b/experiment/main.py
@@ -23,6 +23,7 @@ snoop.install()
 
 @hydra.main(config_name="config")
 def main(cfg: DictConfig)->None:
+    torch.set_printoptions(sci_mode=False)
     data_id = str(int(time()))
     def savecounter():
         # pp(os.system(f'rm -r {cfg.model.dataset.data_dir}/*.{data_id}.csv'))
diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
index 20e9816..e1651cf 100644
--- a/experiment/models/punctuation_domain_model.py
+++ b/experiment/models/punctuation_domain_model.py
@@ -171,7 +171,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
 
         self.log('lr', lr, prog_bar=True)
         self.log('train_loss', loss)
-        self.log('gamma', self.grad_reverse.scale)
+        self.log('gamma', self.grad_reverse.scale,logger=True)
 
         return {'loss': loss, 'lr': lr}
 
@@ -262,11 +262,11 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
         avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
 
         # calculate metrics and log classification report for Punctuation task
-        punct_precision, punct_recall, punct_f1, punct_report = self.punct_class_report.compute()
+        punct_precision, punct_recall, punct_f1, punct_report, punctuation_cm = self.punct_class_report.compute()
         logging.info(f'Punctuation report: {punct_report}')
 
         # calculate metrics and log classification report for domainalization task
-        domain_precision, domain_recall, domain_f1, domain_report = self.domain_class_report.compute()
+        domain_precision, domain_recall, domain_f1, domain_report, domain_cm = self.domain_class_report.compute()
         logging.info(f'Domain report: {domain_report}')
 
         self.log('val_loss', avg_loss, prog_bar=True)
@@ -276,6 +276,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
         self.log('domain_precision', domain_precision)
         self.log('domain_f1', domain_f1)
         self.log('domain_recall', domain_recall)
+        # self.log('punctuation_cm', punctuation_cm)
+        # self.log('domain_cm', domain_cm)
 
     def test_epoch_end(self, outputs):
         if outputs is not None and len(outputs) == 0:
@@ -298,11 +300,11 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
         avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()
 
         # calculate metrics and log classification report for Punctuation task
-        punct_precision, punct_recall, punct_f1, punct_report = self.punct_class_report.compute()
+        punct_precision, punct_recall, punct_f1, punct_report, punct_cm = self.punct_class_report.compute()
         logging.info(f'Punctuation report: {punct_report}')
 
         # calculate metrics and log classification report for domainalization task
-        domain_precision, domain_recall, domain_f1, domain_report = self.domain_class_report.compute()
+        domain_precision, domain_recall, domain_f1, domain_report, domain_cm = self.domain_class_report.compute()
         logging.info(f'Domain report: {domain_report}')
 
         self.log('test_loss', avg_loss, prog_bar=True)
@@ -312,6 +314,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
         self.log('domain_precision', domain_precision)
         self.log('domain_f1', domain_f1)
         self.log('domain_recall', domain_recall)
+        # self.log('punctuation_cm', punct_cm)
+        # self.log('domain_cm', domain_cm)
 
     def setup_optimization(self, optim_config: Optional[Union[DictConfig, Dict]] = None):
         """
@@ -724,7 +728,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
             tokenizer= self.tokenizer,
             queries=queries, 
             max_seq_length=self.hparams.model.dataset.max_seq_length,
-            punct_label_ids=self._cfg.model.punct_label_ids,
+            punct_label_ids=self.labels_to_ids,
             attach_label_to_end=self._cfg.model.dataset.attach_label_to_end)
         batch=ds[0]
         attention_mask = batch['attention_mask']
diff --git a/experiment/testing.py b/experiment/testing.py
index 667f776..272edb8 100644
--- a/experiment/testing.py
+++ b/experiment/testing.py
@@ -21,50 +21,9 @@ from copy import deepcopy
 import snoop
 snoop.install()
 
-# @hydra.main(config_name="config")
-# def main(cfg: DictConfig)->None:
-#     data_id = str(int(time()))
-#     def savecounter():
-#         # pp(os.system(f'rm -r {cfg.model.dataset.data_dir}/*.{data_id}.csv'))
-#         pp(os.system(f'rm -r {cfg.tmp_path}/*.{data_id}.csv'))
-#     atexit.register(savecounter)
-
-#     cfg.model.maximum_unfrozen=max(cfg.model.maximum_unfrozen,cfg.model.unfrozen)
-
-#     pp(cfg)
-#     pl.seed_everything(cfg.seed)
-#     trainer = pl.Trainer(**cfg.trainer)
-#     exp_manager(trainer, cfg.exp_manager)
-#     model = PunctuationDomainModel(cfg=cfg, trainer=trainer, data_id = data_id)
-    
-#     while(model.hparams.model.unfrozen<=cfg.model.maximum_unfrozen and model.hparams.model.unfrozen>=0):
-#         trainer.current_epoch=0
-#         lr_finder = trainer.tuner.lr_find(model,min_lr=1e-10, max_lr=1e-02, num_training=80, early_stop_threshold=None)
-#         # Results can be found in
-#         pp(lr_finder.results)
-#         new_lr = lr_finder.suggestion()
-#         model.hparams.model.optim.lr = new_lr
-#         model.dm.reset()
-#         trainer.current_epoch=0
-#         trainer.fit(model)
-#         try:
-#             model.unfreeze(cfg.model.unfreeze_step)
-#         except:
-#             pp('training complete.')
-#             break
-#     if cfg.model.nemo_path:
-#         model.save_to(cfg.model.nemo_path)
-
-    
-    
-#     gpu = 1 if cfg.trainer.gpus != 0 else 0
-#     # model.dm.setup('test')
-#     trainer = pl.Trainer(gpus=gpu)
-#     trainer.test(model,ckpt_path=None)
-
-
-@hydra.main(config_name="test_config")
+@hydra.main(config_path="../Punctuation_with_Domain_discriminator/TEDstart2021-02-11_07-55-58/",config_name="hparams.yaml")
 def main(cfg : DictConfig) -> None:
+    torch.set_printoptions(sci_mode=False)
     # trainer=pl.Trainer(**cfg.trainer)
     # exp_manager(trainer, cfg.get("exp_manager", None))
     # do_training = False
@@ -74,19 +33,22 @@ def main(cfg : DictConfig) -> None:
     #     if cfg.model.nemo_path:
     #         model.save_to(cfg.model.nemo_path)
     # gpu = 1 if cfg.trainer.gpus != 0 else 0
-    model = PunctuationDomainModel.restore_from(restore_path=cfg.exp_manager.restore_path, override_config_path=cfg.exp_manager.override_config_path, )
-    trainer = pl.Trainer(gpus=gpu)
-    model.set_trainer(trainer)
-    queries = [
-        'we bought four shirts one pen and a mug from the nvidia gear store in santa clara',
-        'what can i do for you today',
-        'how are you',
-    ]
-    inference_results = model.add_punctuation_capitalization(queries)
-
-    for query, result in zip(queries, inference_results):
-        logging.info(f'Query : {query}')
-        logging.info(f'Result: {result.strip()}\n')
+    # model = PunctuationDomainModel.restore_from(restore_path=cfg.exp_manager.restore_path, override_config_path=cfg.exp_manager.override_config_path, )
+    model = PunctuationDomainModel.load_from_checkpoint( #TEDend2021-02-11_07-57-33  # TEDstart2021-02-11_07-55-58
+    checkpoint_path="/home/nxingyu2/project/Punctuation_with_Domain_discriminator/TEDstart2021-02-11_07-55-58/checkpoints/Punctuation_with_Domain_discriminator-last.ckpt")
+    trainer = pl.Trainer(**cfg.trainer)
+    # trainer = pl.Trainer(gpus=gpu)
+    trainer.test(model,ckpt_path=None)
+    # queries = [
+    #     'we bought four shirts one pen and a mug from the nvidia gear store in santa clara',
+    #     'what can i do for you today',
+    #     'how are you',
+    # ]
+    # inference_results = model.add_punctuation_capitalization(queries)
+
+    # for query, result in zip(queries, inference_results):
+    #     logging.info(f'Query : {query}')
+    #     logging.info(f'Result: {result.strip()}\n')
 
 if __name__ == "__main__":
     main()
\ No newline at end of file
