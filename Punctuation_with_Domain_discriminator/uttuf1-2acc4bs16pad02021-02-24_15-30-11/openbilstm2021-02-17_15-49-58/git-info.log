commit hash: 49f437be4dc897ae49942253feec6e44513b972e
diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/events.out.tfevents.1613533775.intern-instance.25711.0 b/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/events.out.tfevents.1613533775.intern-instance.25711.0
index 72a6b47..5665c36 100644
Binary files a/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/events.out.tfevents.1613533775.intern-instance.25711.0 and b/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/events.out.tfevents.1613533775.intern-instance.25711.0 differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/hparams.yaml
index 8780f5e..80b2cf4 100644
--- a/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/hparams.yaml
+++ b/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/hparams.yaml
@@ -40,6 +40,7 @@ model:
     â€¦: .
     ;: .
   no_space_label: '#'
+  test_chunk_percent: 0.5
   punct_class_weights: false
   dataset:
     data_dir: /home/nxingyu2/data
@@ -54,7 +55,7 @@ model:
     num_workers: 4
     pin_memory: false
     drop_last: true
-    num_labels: 11
+    num_labels: 9
     num_domains: 1
     test_unlabelled: true
     attach_label_to_end: none
diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/lightning_logs.txt
index 6942473..8ca803c 100644
--- a/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/lightning_logs.txt
+++ b/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/lightning_logs.txt
@@ -56,3 +56,48 @@ Epoch 1, global step 1682: val_loss reached 0.19387 (best 0.19241), saving model
 Epoch 2, step 1781: val_loss was not in top 3
 Epoch 3, step 1880: val_loss was not in top 3
 Epoch 4, step 1979: val_loss was not in top 3
+Epoch 5, step 2078: val_loss was not in top 3
+Epoch 6, step 2177: val_loss was not in top 3
+Epoch 7, step 2276: val_loss was not in top 3
+Epoch 8, step 2375: val_loss was not in top 3
+Epoch 9, step 2474: val_loss was not in top 3
+Epoch 10, step 2573: val_loss was not in top 3
+Epoch 11, step 2672: val_loss was not in top 3
+Epoch 12, step 2771: val_loss was not in top 3
+Epoch 13, step 2870: val_loss was not in top 3
+Epoch 14, step 2969: val_loss was not in top 3
+
+  | Name                | Type                 | Params
+-------------------------------------------------------------
+0 | transformer         | ElectraModel         | 108 M 
+1 | punct_classifier    | TokenClassifier      | 1.2 M 
+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
+3 | punctuation_loss    | FocalDiceLoss        | 0     
+4 | bilstm              | LSTM                 | 7.1 M 
+5 | domain_loss         | CrossEntropyLoss     | 0     
+6 | agg_loss            | AggregatorLoss       | 0     
+7 | punct_class_report  | ClassificationReport | 0     
+8 | domain_class_report | ClassificationReport | 0     
+-------------------------------------------------------------
+22.5 M    Trainable params
+94.7 M    Non-trainable params
+117 M     Total params
+Epoch 0, step 3068: val_loss was not in top 3
+Epoch 1, step 3167: val_loss was not in top 3
+Epoch 2, step 3266: val_loss was not in top 3
+Epoch 3, step 3365: val_loss was not in top 3
+Epoch 4, step 3464: val_loss was not in top 3
+Epoch 5, step 3563: val_loss was not in top 3
+Epoch 6, step 3662: val_loss was not in top 3
+Epoch 7, step 3761: val_loss was not in top 3
+Epoch 8, step 3860: val_loss was not in top 3
+Epoch 9, step 3959: val_loss was not in top 3
+Epoch 10, step 4058: val_loss was not in top 3
+Epoch 11, step 4157: val_loss was not in top 3
+Epoch 12, step 4256: val_loss was not in top 3
+Epoch 13, step 4355: val_loss was not in top 3
+Epoch 14, step 4454: val_loss was not in top 3
+GPU available: True, used: True
+TPU available: None, using: 0 TPU cores
+Using environment variable NODE_RANK for node rank (0).
+LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/nemo_error_log.txt
index 01b5cea..950d6a0 100644
--- a/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/nemo_error_log.txt
+++ b/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/nemo_error_log.txt
@@ -8,3 +8,6 @@
 [NeMo W 2021-02-17 11:50:55 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f83b091f3d0> was reported to be 12 (when accessing len(dataloader)), but 13 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
       warnings.warn(warn_msg)
     
+[NeMo W 2021-02-17 13:03:56 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f83b0974a90> was reported to be 12 (when accessing len(dataloader)), but 13 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/nemo_log_globalrank-0_localrank-0.txt
index 76321e4..2cf0e0c 100644
--- a/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/nemo_log_globalrank-0_localrank-0.txt
+++ b/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/nemo_log_globalrank-0_localrank-0.txt
@@ -10,3 +10,6 @@
 [NeMo W 2021-02-17 11:50:55 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f83b091f3d0> was reported to be 12 (when accessing len(dataloader)), but 13 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
       warnings.warn(warn_msg)
     
+[NeMo W 2021-02-17 13:03:56 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f83b0974a90> was reported to be 12 (when accessing len(dataloader)), but 13 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/events.out.tfevents.1613535472.intern-instance.5941.0 b/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/events.out.tfevents.1613535472.intern-instance.5941.0
index 9b6538f..d09da1e 100644
Binary files a/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/events.out.tfevents.1613535472.intern-instance.5941.0 and b/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/events.out.tfevents.1613535472.intern-instance.5941.0 differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/lightning_logs.txt
index d56406b..c177ddb 100644
--- a/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/lightning_logs.txt
+++ b/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/lightning_logs.txt
@@ -19,3 +19,8 @@ initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
 7.7 M     Trainable params
 108 M     Non-trainable params
 116 M     Total params
+Epoch 0, global step 2626: val_loss reached 0.06840 (best 0.06840), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.07-epoch=0.ckpt" as top 3
+Epoch 1, global step 5253: val_loss reached 0.01696 (best 0.01696), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.02-epoch=1.ckpt" as top 3
+Epoch 2, global step 7880: val_loss reached 0.01890 (best 0.01696), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.02-epoch=2.ckpt" as top 3
+Saving latest checkpoint...
+Epoch 3, global step 8484: val_loss reached 0.01890 (best 0.01696), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.02-epoch=3.ckpt" as top 3
diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/nemo_error_log.txt
index d8819de..e75da19 100644
--- a/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/nemo_error_log.txt
+++ b/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/nemo_error_log.txt
@@ -2,3 +2,12 @@
 [NeMo W 2021-02-17 12:17:52 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
       warnings.warn(*args, **kwargs)
     
+[NeMo W 2021-02-17 13:01:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fc72c6c1850> was reported to be 2627 (when accessing len(dataloader)), but 2628 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
+[NeMo W 2021-02-17 13:06:36 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fc770032130> was reported to be 328 (when accessing len(dataloader)), but 329 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
+[NeMo W 2021-02-17 15:01:57 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
+      warnings.warn(*args, **kwargs)
+    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/nemo_log_globalrank-0_localrank-0.txt
index 8207f2f..e4933a9 100644
--- a/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/nemo_log_globalrank-0_localrank-0.txt
+++ b/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/nemo_log_globalrank-0_localrank-0.txt
@@ -4,3 +4,12 @@
 [NeMo W 2021-02-17 12:17:52 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
       warnings.warn(*args, **kwargs)
     
+[NeMo W 2021-02-17 13:01:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fc72c6c1850> was reported to be 2627 (when accessing len(dataloader)), but 2628 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
+[NeMo W 2021-02-17 13:06:36 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fc770032130> was reported to be 328 (when accessing len(dataloader)), but 329 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
+[NeMo W 2021-02-17 15:01:57 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
+      warnings.warn(*args, **kwargs)
+    
diff --git a/experiment/config.yaml b/experiment/config.yaml
index 9006e36..4e438d7 100644
--- a/experiment/config.yaml
+++ b/experiment/config.yaml
@@ -2,9 +2,9 @@ seed: 42
 trainer:
     gpus: 1 # the number of gpus, 0 for CPU
     num_nodes: 1
-    max_epochs: 15
+    max_epochs: 20
     max_steps: null # precedence over max_epochs
-    accumulate_grad_batches: 1 # accumulates grads every k batches
+    accumulate_grad_batches: 4 # accumulates grads every k batches
     gradient_clip_val: 0
     amp_level: O1 # O1/O2 for mixed precision
     precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
@@ -82,7 +82,7 @@ model:
         ignore_start_end: false
         use_cache: false
         # shared among dataloaders
-        num_workers:  4
+        num_workers:  8
         pin_memory: false
         drop_last: true
         num_labels: 9
@@ -94,7 +94,7 @@ model:
             shuffle: true
             num_samples: -1
             batch_size: 32
-            manual_len: 4000 #default 0 84074
+            manual_len: 3000 #default 0 84074
 
         validation_ds:
             # if evaluation data is not in the model.dataset.data_dir as the training data or multiple datasets are used for
@@ -118,13 +118,13 @@ model:
         # unfrozen_layers: 1
     
     punct_head:
-        punct_num_fc_layers: 2
+        punct_num_fc_layers: 3
         fc_dropout: 0.1
         activation: 'gelu'
         log_softmax: false
         use_transformer_init: true
         loss: 'dice'
-        bilstm: true
+        bilstm: false
 
     domain_head:
         domain_num_fc_layers: 1
@@ -147,7 +147,7 @@ model:
 
     frozen_lr:
         - 2e-2
-        - 1e-4
+        - 5e-4
         - 5e-6
         - 5e-7
         - 1e-7
diff --git a/experiment/core/utils.py b/experiment/core/utils.py
index 6226a01..2d3c063 100644
--- a/experiment/core/utils.py
+++ b/experiment/core/utils.py
@@ -26,7 +26,7 @@ def position_to_mask(max_seq_length:int,indices:list):
     try:
         o[np.array(indices)%(max_seq_length-2)+1]=1
     except:
-        pp('position_to_mask',np.array(indices)%(max_seq_length-2)+1)
+        # pp('empty array')
         o[(np.array(indices)%(max_seq_length-2)+1).astype(int)]=1
     return o
 
diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
index dda9cf5..cc59795 100644
--- a/experiment/data/punctuation_dataset_multi.py
+++ b/experiment/data/punctuation_dataset_multi.py
@@ -114,7 +114,8 @@ class PunctuationDomainDataset(IterableDataset):
         
 
     def __len__(self):
-        return self.len
+        pp('dataset')
+        return pp(self.len)
     
     def shuffle(self, randomize=True, seed=42):
         pp(os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)))
@@ -177,7 +178,10 @@ class PunctuationDomainDatasets(IterableDataset):
         self.label_map=label_map
         self.ds_lengths=[]
         for path in labelled+unlabelled:
-            self.ds_lengths.append(int(subprocess.Popen(['wc', '-l', f'{path}.{split}.csv'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0]))
+            if manual_len>0:
+                self.ds_lengths.append(min(manual_len,int(subprocess.Popen(['wc', '-l', f'{path}.{split}.csv'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0])))
+            else:
+                self.ds_lengths.append(int(subprocess.Popen(['wc', '-l', f'{path}.{split}.csv'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0]))
         self.max_length=max(self.ds_lengths) 
         self.per_worker=int(self.max_length/self.num_workers)
         self.len=int(self.per_worker/num_samples) 
@@ -245,7 +249,7 @@ class PunctuationDomainDatasets(IterableDataset):
             return {k:torch.cat([d[k] for d in ds], dim=0) for k in ['input_ids','attention_mask','subtoken_mask','labels','domain']}
 
     def __len__(self):
-        return self.len
+        return pp(self.len)
 
     def shuffle(self, randomize=True, seed=42):
         worker_info = get_worker_info()
diff --git a/experiment/info.log b/experiment/info.log
index 688df06..dc33065 100644
Binary files a/experiment/info.log and b/experiment/info.log differ
diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
index 71700f4..e689302 100644
--- a/experiment/models/punctuation_domain_model.py
+++ b/experiment/models/punctuation_domain_model.py
@@ -63,6 +63,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
                               for k,v in self.ids_to_labels.items()}
         self.label_map={k:v for k,v in self._cfg.model.label_map.items()}
         self.data_id=data_id
+        assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
         self.setup_datamodule()
 
         if (self.hparams.model.punct_class_weights==True and self.hparams.model.punct_head.loss!='crf'):
