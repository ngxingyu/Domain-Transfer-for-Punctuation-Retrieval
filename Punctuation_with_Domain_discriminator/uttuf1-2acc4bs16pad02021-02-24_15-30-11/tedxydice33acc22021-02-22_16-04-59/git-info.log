commit hash: 9024659c8f5fcb4fd7395119e5b1da1c126896e3
diff --git a/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/events.out.tfevents.1612938106.intern-instance.10766.0 b/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/events.out.tfevents.1612938106.intern-instance.10766.0
deleted file mode 100644
index 143b169..0000000
Binary files a/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/events.out.tfevents.1612938106.intern-instance.10766.0 and /dev/null differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/git-info.log
deleted file mode 100644
index d9c953a..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/git-info.log
+++ /dev/null
@@ -1,36897 +0,0 @@
-commit hash: 5a9219beb570f8aec7defdef6109bfa152f10c31
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/cmd-args.log
-deleted file mode 100644
-index 11a5d8e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/cmd-args.log
-+++ /dev/null
-@@ -1 +0,0 @@
--main.py
-\ No newline at end of file
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/events.out.tfevents.1612744901.Titan.7691.0 b/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/events.out.tfevents.1612744901.Titan.7691.0
-deleted file mode 100644
-index 11d4b59..0000000
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/events.out.tfevents.1612744901.Titan.7691.0 and /dev/null differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/git-info.log
-deleted file mode 100644
-index 2957aca..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/git-info.log
-+++ /dev/null
-@@ -1,456 +0,0 @@
--commit hash: 939a671c8c117db6975316767ced5d95449e2b27
--diff --git a/README.md b/README.md
--index d52dba5..3da2b86 100644
----- a/README.md
--+++ b/README.md
--@@ -333,4 +333,31 @@ label                                                precision    recall       f
--  'punct_f1': 0.5858508944511414,
--  'punct_precision': 0.30176490545272827,
--  'punct_recall': 10.0,
--- 'test_loss': 0.8140875697135925}
--\ No newline at end of file
--+ 'test_loss': 0.8140875697135925}
--+
--+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
--+
--+ (label_id: 0)                                          62.15     100.00      76.66       5154
--+! (label_id: 1)                                          0.00       0.00       0.00        108
--+, (label_id: 2)                                          0.00       0.00       0.00      18022
--+- (label_id: 3)                                          0.00       0.00       0.00       1557
--+. (label_id: 4)                                         41.74      94.01      57.81      15164
--+: (label_id: 5)                                          0.00       0.00       0.00        319
--+; (label_id: 6)                                          0.00       0.00       0.00         88
--+? (label_id: 7)                                          0.00       0.00       0.00       1217
--+ (label_id: 8)                                          0.00       0.00       0.00        752
--+… (label_id: 9)                                          0.00       0.00       0.00         67
--+-------------------
--+micro avg                                               45.72      45.72      45.72      42448
--+macro avg                                               10.39      19.40      13.45      42448
--+weighted avg                                            22.46      45.72      29.96      42448
--+{ 'punct_f1': 13.446383476257324,
--+ 'punct_precision': 10.388500213623047,
--+ 'punct_recall': 19.400554656982422,
--+ 'test_loss': 0.44148480892181396}
--+
--+
--+ ## Log for 8/2/2021
--+
--+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
--+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
--\ No newline at end of file
--diff --git a/experiment/config.yaml b/experiment/config.yaml
--index fe58670..4df2399 100644
----- a/experiment/config.yaml
--+++ b/experiment/config.yaml
--@@ -1,49 +1,49 @@
-- seed: 42
-- trainer:
---    # gpus: 1 # the number of gpus, 0 for CPU
---    # num_nodes: 1
---    # max_epochs: 2
---    # max_steps: null # precedence over max_epochs
---    # accumulate_grad_batches: 4 # accumulates grads every k batches
---    # gradient_clip_val: 0
---    # amp_level: O1 # O1/O2 for mixed precision
---    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
---    # accelerator: ddp
---    # checkpoint_callback: false  # Provided by exp_manager
---    # logger: false #false  # Provided by exp_manager
---    # log_every_n_steps: 1  # Interval of logging.
---    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
---    # resume_from_checkpoint: null
---
---    gpus: 0 # the number of gpus, 0 for CPU
--+    gpus: 1 # the number of gpus, 0 for CPU
--     num_nodes: 1
---    max_epochs: 8
--+    max_epochs: 10
--     max_steps: null # precedence over max_epochs
---    accumulate_grad_batches: 1 # accumulates grads every k batches
--+    accumulate_grad_batches: 4 # accumulates grads every k batches
--     gradient_clip_val: 0
---    amp_level: O0 # O1/O2 for mixed precision
---    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
---    # accelerator: ddp
--+    amp_level: O1 # O1/O2 for mixed precision
--+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--+    accelerator: ddp
--     checkpoint_callback: false  # Provided by exp_manager
--     logger: false #false  # Provided by exp_manager
--     log_every_n_steps: 1  # Interval of logging.
--     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
---    reload_dataloaders_every_epoch: true
--     resume_from_checkpoint: null
-- 
--+    # gpus: 0 # the number of gpus, 0 for CPU
--+    # num_nodes: 1
--+    # max_epochs: 8
--+    # max_steps: null # precedence over max_epochs
--+    # accumulate_grad_batches: 1 # accumulates grads every k batches
--+    # gradient_clip_val: 0
--+    # amp_level: O0 # O1/O2 for mixed precision
--+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--+    # # accelerator: ddp
--+    # checkpoint_callback: false  # Provided by exp_manager
--+    # logger: false #false  # Provided by exp_manager
--+    # log_every_n_steps: 1  # Interval of logging.
--+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--+    # reload_dataloaders_every_epoch: true
--+    # resume_from_checkpoint: null
--+
-- exp_manager:
---    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
--+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
--     name: Punctuation_with_Domain_discriminator  # The name of your model
--     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
--     create_checkpoint_callback: true 
---base_path: /home/nxingyu2/data # /root/data # 
---tmp_path: /home/nxingyu2/data/tmp # /tmp # 
--+base_path: /home/nxingyu/data # /root/data # 
--+tmp_path: /home/nxingyu/data/tmp # /tmp # 
-- 
-- model:
--     nemo_path: null
---    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
--+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
--     unfrozen: 0
---    maximum_unfrozen: 2
--+    maximum_unfrozen: 1
--     unfreeze_step: 1
--     # unfreeze_every: 3
--     punct_label_ids:
--@@ -58,10 +58,10 @@ model:
--         - "—"
--         - "…"
-- 
---    punct_class_weights: true
--+    punct_class_weights: false
--     
--     dataset:
---        data_dir: /home/nxingyu2/data # /root/data # 
--+        data_dir: /home/nxingyu/data # /root/data # 
--         labelled:
--             - ${base_path}/ted_talks_processed #
--         unlabelled:
--@@ -125,14 +125,14 @@ model:
--     
--     dice_loss:
--         epsilon: 0.01
---        alpha: 3
--+        alpha: 4
--         macro_average: true
-- 
--     focal_loss: 
--         gamma: 5
-- 
--     optim:
---        name: novograd
--+        name: adamw
--         lr: 1e-3
--         weight_decay: 0.00
--         sched:
--diff --git a/experiment/info.log b/experiment/info.log
--index 2471fe9..e69de29 100644
----- a/experiment/info.log
--+++ b/experiment/info.log
--@@ -1,300 +0,0 @@
---[INFO] - GPU available: True, used: False
---[INFO] - TPU available: None, using: 0 TPU cores
---[INFO] - shuffling train set
---[INFO] - Optimizer config = Novograd (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.95, 0.98)
---    eps: 1e-08
---    grad_averaging: False
---    lr: 0.001
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
---will be used during training (effective maximum steps = 80) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---last_epoch: -1
---max_steps: 80
---)
---[INFO] - 
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 108 M 
---1 | punct_classifier    | TokenClassifier      | 7.7 K 
---2 | domain_classifier   | SequenceClassifier   | 769   
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---8.5 K     Trainable params
---108 M     Non-trainable params
---108 M     Total params
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                           0.00       0.00       0.00        184
---! (label_id: 1)                                          0.00       0.00       0.00          4
---, (label_id: 2)                                          1.23       0.34       0.53        594
---- (label_id: 3)                                          3.06      25.42       5.46         59
---. (label_id: 4)                                         47.22      12.98      20.36        524
---: (label_id: 5)                                          0.00       0.00       0.00         18
---; (label_id: 6)                                          0.00       0.00       0.00         13
---? (label_id: 7)                                          8.45       6.32       7.23         95
---— (label_id: 8)                                          0.00       0.00       0.00         12
---… (label_id: 9)                                          0.00       0.00       0.00          0
----------------------
---micro avg                                                6.05       6.05       6.05       1503
---macro avg                                                6.66       5.01       3.73       1503
---weighted avg                                            17.61       6.05       7.98       1503
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00         92
----------------------
---micro avg                                              100.00     100.00     100.00         92
---macro avg                                              100.00     100.00     100.00         92
---weighted avg                                           100.00     100.00     100.00         92
---
---[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
---[INFO] - Optimizer config = Novograd (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.95, 0.98)
---    eps: 1e-08
---    grad_averaging: False
---    lr: 1.5848931924611143e-08
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
---will be used during training (effective maximum steps = 3192) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---last_epoch: -1
---max_steps: 3192
---)
---[INFO] - 
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 108 M 
---1 | punct_classifier    | TokenClassifier      | 7.7 K 
---2 | domain_classifier   | SequenceClassifier   | 769   
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---8.5 K     Trainable params
---108 M     Non-trainable params
---108 M     Total params
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                           0.00       0.00       0.00        202
---! (label_id: 1)                                          0.00       0.00       0.00          4
---, (label_id: 2)                                          1.62       0.45       0.70        669
---- (label_id: 3)                                          3.48      27.27       6.17         66
---. (label_id: 4)                                         45.06      13.01      20.19        561
---: (label_id: 5)                                          1.52       6.67       2.47         15
---; (label_id: 6)                                          0.00       0.00       0.00         15
---? (label_id: 7)                                          8.70       7.32       7.95         82
---— (label_id: 8)                                          0.00       0.00       0.00         13
---… (label_id: 9)                                          0.00       0.00       0.00          1
----------------------
---micro avg                                                6.20       6.20       6.20       1628
---macro avg                                                6.04       5.47       3.75       1628
---weighted avg                                            16.79       6.20       7.92       1628
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00        101
----------------------
---micro avg                                              100.00     100.00     100.00        101
---macro avg                                              100.00     100.00     100.00        101
---weighted avg                                           100.00     100.00     100.00        101
---
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                           0.34       0.73       0.46       4402
---! (label_id: 1)                                          0.42      13.95       0.82        129
---, (label_id: 2)                                          2.53       0.64       1.03      15243
---- (label_id: 3)                                          2.45      21.03       4.38       1322
---. (label_id: 4)                                         44.00      11.40      18.11      12542
---: (label_id: 5)                                          0.43       1.41       0.65        354
---; (label_id: 6)                                          0.00       0.00       0.00        163
---? (label_id: 7)                                          4.16       6.27       5.00       1117
---— (label_id: 8)                                          3.00       0.61       1.02        488
---… (label_id: 9)                                          0.97       6.17       1.68         81
----------------------
---micro avg                                                5.41       5.41       5.41      35841
---macro avg                                                5.83       6.22       3.32      35841
---weighted avg                                            16.78       5.41       7.18      35841
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       2201
----------------------
---micro avg                                              100.00     100.00     100.00       2201
---macro avg                                              100.00     100.00     100.00       2201
---weighted avg                                           100.00     100.00     100.00       2201
---
---[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                           0.20       0.43       0.27       4226
---! (label_id: 1)                                          0.44      14.17       0.86        127
---, (label_id: 2)                                          1.93       0.49       0.78      14611
---- (label_id: 3)                                          2.23      19.56       4.01       1237
---. (label_id: 4)                                         43.37      11.25      17.86      11977
---: (label_id: 5)                                          0.68       2.34       1.05        342
---; (label_id: 6)                                          0.00       0.00       0.00        129
---? (label_id: 7)                                          5.16       7.47       6.10       1058
---— (label_id: 8)                                          2.15       0.49       0.80        409
---… (label_id: 9)                                          0.69       4.23       1.19         71
----------------------
---micro avg                                                5.23       5.23       5.23      34187
---macro avg                                                5.68       6.04       3.29      34187
---weighted avg                                            16.32       5.23       6.98      34187
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       2113
----------------------
---micro avg                                              100.00     100.00     100.00       2113
---macro avg                                              100.00     100.00     100.00       2113
---weighted avg                                           100.00     100.00     100.00       2113
---
---[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                           0.28       0.61       0.39       4228
---! (label_id: 1)                                          0.30       8.28       0.58        145
---, (label_id: 2)                                          2.27       0.58       0.92      14495
---- (label_id: 3)                                          2.64      21.78       4.70       1327
---. (label_id: 4)                                         44.87      11.66      18.51      12193
---: (label_id: 5)                                          0.60       1.93       0.91        362
---; (label_id: 6)                                          0.00       0.00       0.00        164
---? (label_id: 7)                                          4.19       6.40       5.07       1078
---— (label_id: 8)                                          1.16       0.22       0.37        459
---… (label_id: 9)                                          0.85       4.17       1.41         96
----------------------
---micro avg                                                5.54       5.54       5.54      34547
---macro avg                                                5.72       5.56       3.29      34547
---weighted avg                                            17.08       5.54       7.33      34547
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       2114
----------------------
---micro avg                                              100.00     100.00     100.00       2114
---macro avg                                              100.00     100.00     100.00       2114
---weighted avg                                           100.00     100.00     100.00       2114
---
---[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                           0.29       0.63       0.40       4444
---! (label_id: 1)                                          0.38      10.67       0.74        150
---, (label_id: 2)                                          2.32       0.59       0.94      15290
---- (label_id: 3)                                          2.34      20.28       4.19       1292
---. (label_id: 4)                                         43.85      11.68      18.44      12599
---: (label_id: 5)                                          0.41       1.28       0.62        392
---; (label_id: 6)                                          0.00       0.00       0.00        164
---? (label_id: 7)                                          4.24       6.30       5.07       1111
---— (label_id: 8)                                          0.00       0.00       0.00        456
---… (label_id: 9)                                          0.38       2.41       0.65         83
----------------------
---micro avg                                                5.40       5.40       5.40      35981
---macro avg                                                5.42       5.38       3.11      35981
---weighted avg                                            16.59       5.40       7.22      35981
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       2222
----------------------
---micro avg                                              100.00     100.00     100.00       2222
---macro avg                                              100.00     100.00     100.00       2222
---weighted avg                                           100.00     100.00     100.00       2222
---
---[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                           0.35       0.73       0.48       3844
---! (label_id: 1)                                          0.54      14.62       1.04        130
---, (label_id: 2)                                          2.32       0.59       0.94      13056
---- (label_id: 3)                                          2.67      22.28       4.77       1194
---. (label_id: 4)                                         44.45      11.95      18.84      10791
---: (label_id: 5)                                          0.84       3.21       1.33        280
---; (label_id: 6)                                          0.00       0.00       0.00        140
---? (label_id: 7)                                          4.17       6.56       5.10        914
---— (label_id: 8)                                          0.00       0.00       0.00        401
---… (label_id: 9)                                          0.48       2.63       0.81         76
----------------------
---micro avg                                                5.68       5.68       5.68      30826
---macro avg                                                5.58       6.26       3.33      30826
---weighted avg                                            16.82       5.68       7.41      30826
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1922
----------------------
---micro avg                                              100.00     100.00     100.00       1922
---macro avg                                              100.00     100.00     100.00       1922
---weighted avg                                           100.00     100.00     100.00       1922
---
---[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                           0.28       0.60       0.39       3970
---! (label_id: 1)                                          0.35      10.66       0.68        122
---, (label_id: 2)                                          2.09       0.53       0.85      13469
---- (label_id: 3)                                          2.29      19.32       4.10       1201
---. (label_id: 4)                                         43.43      11.24      17.86      11227
---: (label_id: 5)                                          0.63       2.30       0.99        304
---; (label_id: 6)                                          0.00       0.00       0.00        141
---? (label_id: 7)                                          4.52       6.86       5.45       1006
---— (label_id: 8)                                          1.15       0.23       0.38        444
---… (label_id: 9)                                          0.45       2.67       0.78         75
----------------------
---micro avg                                                5.26       5.26       5.26      31959
---macro avg                                                5.52       5.44       3.15      31959
---weighted avg                                            16.42       5.26       7.02      31959
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1985
----------------------
---micro avg                                              100.00     100.00     100.00       1985
---macro avg                                              100.00     100.00     100.00       1985
---weighted avg                                           100.00     100.00     100.00       1985
---
---[INFO] - Epoch 5, step 2394: val_loss was not in top 3
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                           0.23       0.48       0.31       4126
---! (label_id: 1)                                          0.29       9.40       0.56        117
---, (label_id: 2)                                          1.91       0.49       0.77      14019
---- (label_id: 3)                                          2.52      22.59       4.53       1164
---. (label_id: 4)                                         44.15      11.65      18.44      11789
---: (label_id: 5)                                          0.72       2.41       1.11        332
---; (label_id: 6)                                          0.56       0.61       0.58        165
---? (label_id: 7)                                          3.89       6.53       4.88        980
---— (label_id: 8)                                          2.30       0.47       0.77        430
---… (label_id: 9)                                          1.18       8.33       2.07         60
----------------------
---micro avg                                                5.47       5.47       5.47      33182
---macro avg                                                5.77       6.30       3.40      33182
---weighted avg                                            16.77       5.47       7.25      33182
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       2063
----------------------
---micro avg                                              100.00     100.00     100.00       2063
---macro avg                                              100.00     100.00     100.00       2063
---weighted avg                                           100.00     100.00     100.00       2063
---
---[INFO] - Epoch 6, step 2793: val_loss was not in top 3
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/hparams.yaml
-deleted file mode 100644
-index 4d0b843..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/hparams.yaml
-+++ /dev/null
-@@ -1,107 +0,0 @@
--seed: 42
--trainer:
--  gpus: 1
--  num_nodes: 1
--  max_epochs: 10
--  max_steps: null
--  accumulate_grad_batches: 4
--  gradient_clip_val: 0
--  amp_level: O1
--  precision: 16
--  accelerator: ddp
--  checkpoint_callback: false
--  logger: false
--  log_every_n_steps: 1
--  val_check_interval: 1.0
--  resume_from_checkpoint: null
--exp_manager:
--  exp_dir: /home/nxingyu/project/
--  name: Punctuation_with_Domain_discriminator
--  create_tensorboard_logger: true
--  create_checkpoint_callback: true
--base_path: /home/nxingyu/data
--tmp_path: /home/nxingyu/data/tmp
--model:
--  nemo_path: null
--  transformer_path: google/electra-small-discriminator
--  unfrozen: 0
--  maximum_unfrozen: 1
--  unfreeze_step: 1
--  punct_label_ids:
--  - ''
--  - '!'
--  - ','
--  - '-'
--  - .
--  - ':'
--  - ;
--  - '?'
--  - —
--  - …
--  punct_class_weights: false
--  dataset:
--    data_dir: /home/nxingyu/data
--    labelled:
--    - /home/nxingyu/data/ted_talks_processed
--    unlabelled: null
--    max_seq_length: 128
--    pad_label: ''
--    ignore_extra_tokens: false
--    ignore_start_end: false
--    use_cache: false
--    num_workers: 0
--    pin_memory: true
--    drop_last: false
--    num_labels: 10
--    num_domains: 1
--    test_unlabelled: true
--    train_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 8
--    validation_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 8
--  tokenizer:
--    tokenizer_name: google/electra-small-discriminator
--    vocab_file: null
--    tokenizer_model: null
--    special_tokens: null
--  language_model:
--    pretrained_model_name: google/electra-small-discriminator
--    lm_checkpoint: null
--    config_file: null
--    config: null
--  punct_head:
--    punct_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: dice
--  domain_head:
--    domain_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: cel
--    gamma: 0.1
--  dice_loss:
--    epsilon: 0.01
--    alpha: 4
--    macro_average: true
--  focal_loss:
--    gamma: 5
--  optim:
--    name: adamw
--    lr: 0.001
--    weight_decay: 0.0
--    sched:
--      name: WarmupAnnealing
--      warmup_steps: null
--      warmup_ratio: 0.1
--      last_epoch: -1
--      monitor: val_loss
--      reduce_on_plateau: false
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/lightning_logs.txt
-deleted file mode 100644
-index e9f5aa6..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/lightning_logs.txt
-+++ /dev/null
-@@ -1,97 +0,0 @@
--Global seed set to 42
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
--Using native 16bit precision.
--Global seed set to 42
--initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 257   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--35.9 K    Trainable params
--13.4 M    Non-trainable params
--13.5 M    Total params
--Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/lr_find_temp_model.ckpt
--Global seed set to 42
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 257   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--35.9 K    Trainable params
--13.4 M    Non-trainable params
--13.5 M    Total params
--Epoch 0, global step 100: val_loss reached 0.54570 (best 0.54570), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.55-epoch=0.ckpt" as top 3
--Epoch 1, global step 200: val_loss reached 0.54727 (best 0.54570), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.55-epoch=1.ckpt" as top 3
--Epoch 2, global step 300: val_loss reached 0.54265 (best 0.54265), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.54-epoch=2.ckpt" as top 3
--Epoch 3, global step 400: val_loss reached 0.52936 (best 0.52936), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.53-epoch=3.ckpt" as top 3
--Epoch 4, global step 500: val_loss reached 0.52437 (best 0.52437), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.52-epoch=4.ckpt" as top 3
--Epoch 5, global step 600: val_loss reached 0.52797 (best 0.52437), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.53-epoch=5.ckpt" as top 3
--Epoch 6, global step 700: val_loss reached 0.50817 (best 0.50817), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.51-epoch=6.ckpt" as top 3
--Epoch 7, global step 800: val_loss reached 0.50056 (best 0.50056), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.50-epoch=7.ckpt" as top 3
--Epoch 8, global step 900: val_loss reached 0.49674 (best 0.49674), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.50-epoch=8.ckpt" as top 3
--Epoch 9, global step 1000: val_loss reached 0.48774 (best 0.48774), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.49-epoch=9.ckpt" as top 3
--Global seed set to 42
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 257   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--825 K     Trainable params
--12.7 M    Non-trainable params
--13.5 M    Total params
--LR finder stopped early due to diverging loss.
--Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/lr_find_temp_model.ckpt
--Global seed set to 42
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 257   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--825 K     Trainable params
--12.7 M    Non-trainable params
--13.5 M    Total params
--Epoch 0, global step 1101: val_loss reached 0.39019 (best 0.39019), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.39-epoch=0.ckpt" as top 3
--Epoch 1, global step 1201: val_loss reached 0.39089 (best 0.39019), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.39-epoch=1.ckpt" as top 3
--Epoch 2, global step 1301: val_loss reached 0.39575 (best 0.39019), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.40-epoch=2.ckpt" as top 3
--Epoch 3, global step 1401: val_loss reached 0.36015 (best 0.36015), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.36-epoch=3.ckpt" as top 3
--Epoch 4, global step 1501: val_loss reached 0.37551 (best 0.36015), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.38-epoch=4.ckpt" as top 3
--Epoch 5, global step 1601: val_loss reached 0.37441 (best 0.36015), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.37-epoch=5.ckpt" as top 3
--Epoch 6, global step 1701: val_loss reached 0.37034 (best 0.36015), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.37-epoch=6.ckpt" as top 3
--Epoch 7, global step 1801: val_loss reached 0.37041 (best 0.36015), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.37-epoch=7.ckpt" as top 3
--Epoch 8, global step 1901: val_loss reached 0.36448 (best 0.36015), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.36-epoch=8.ckpt" as top 3
--Epoch 9, global step 2001: val_loss reached 0.36510 (best 0.36015), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.37-epoch=9.ckpt" as top 3
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--Using environment variable NODE_RANK for node rank (0).
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/nemo_error_log.txt
-deleted file mode 100644
-index b8ca551..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/nemo_error_log.txt
-+++ /dev/null
-@@ -1,43 +0,0 @@
--[NeMo W 2021-02-08 08:37:54 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-08 08:38:03 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 08:38:03 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 08:38:05 nemo_logging:349] /home/nxingyu/project/experiment/core/classification_report.py:116: UserWarning: This overload of nonzero is deprecated:
--    	nonzero(Tensor input, *, Tensor out)
--    Consider using one of the following signatures instead:
--    	nonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629411241/work/torch/csrc/utils/python_arg_parser.cpp:766.)
--      num_non_empty_classes = torch.nonzero(self.num_examples_per_class).size(0)
--    
--[NeMo W 2021-02-08 08:38:05 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 08:41:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 08:41:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 08:41:42 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 08:41:46 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:118: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-08 08:47:06 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f4fd08d6910> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-08 08:47:29 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f4fd8f0b4c0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-08 09:39:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 10:32:21 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 10:32:46 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f4fd8f0b5e0> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
--      warnings.warn(warn_msg)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/nemo_log_globalrank-0_localrank-0.txt
-deleted file mode 100644
-index 1b8f68d..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54/nemo_log_globalrank-0_localrank-0.txt
-+++ /dev/null
-@@ -1,45 +0,0 @@
--[NeMo I 2021-02-08 08:37:54 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_08-37-54
--[NeMo I 2021-02-08 08:37:54 exp_manager:519] TensorboardLogger has been set up
--[NeMo W 2021-02-08 08:37:54 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-08 08:38:03 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 08:38:03 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 08:38:05 nemo_logging:349] /home/nxingyu/project/experiment/core/classification_report.py:116: UserWarning: This overload of nonzero is deprecated:
--    	nonzero(Tensor input, *, Tensor out)
--    Consider using one of the following signatures instead:
--    	nonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629411241/work/torch/csrc/utils/python_arg_parser.cpp:766.)
--      num_non_empty_classes = torch.nonzero(self.num_examples_per_class).size(0)
--    
--[NeMo W 2021-02-08 08:38:05 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 08:41:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 08:41:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 08:41:42 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 08:41:46 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:118: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-08 08:47:06 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f4fd08d6910> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-08 08:47:29 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f4fd8f0b4c0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-08 09:39:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 10:32:21 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 10:32:46 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f4fd8f0b5e0> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
--      warnings.warn(warn_msg)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/cmd-args.log
-deleted file mode 100644
-index 11a5d8e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/cmd-args.log
-+++ /dev/null
-@@ -1 +0,0 @@
--main.py
-\ No newline at end of file
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/events.out.tfevents.1612756904.Titan.23886.0 b/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/events.out.tfevents.1612756904.Titan.23886.0
-deleted file mode 100644
-index 26929bf..0000000
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/events.out.tfevents.1612756904.Titan.23886.0 and /dev/null differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/git-info.log
-deleted file mode 100644
-index 5a0e322..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/git-info.log
-+++ /dev/null
-@@ -1,1992 +0,0 @@
--commit hash: 66d59fddd871d29e1ceab6de212c3812501bf786
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/cmd-args.log
--deleted file mode 100644
--index 11a5d8e..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/cmd-args.log
--+++ /dev/null
--@@ -1 +0,0 @@
---main.py
--\ No newline at end of file
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/events.out.tfevents.1612753856.Titan.19011.0 b/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/events.out.tfevents.1612753856.Titan.19011.0
--deleted file mode 100644
--index 481eef2..0000000
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/events.out.tfevents.1612753856.Titan.19011.0 and /dev/null differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/git-info.log
--deleted file mode 100644
--index daa2b33..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/git-info.log
--+++ /dev/null
--@@ -1,627 +0,0 @@
---commit hash: 939a671c8c117db6975316767ced5d95449e2b27
---diff --git a/README.md b/README.md
---index d52dba5..0b403d3 100644
------ a/README.md
---+++ b/README.md
---@@ -333,4 +333,71 @@ label                                                precision    recall       f
---  'punct_f1': 0.5858508944511414,
---  'punct_precision': 0.30176490545272827,
---  'punct_recall': 10.0,
---- 'test_loss': 0.8140875697135925}
---\ No newline at end of file
---+ 'test_loss': 0.8140875697135925}
---+
---+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
---+
---+ (label_id: 0)                                          62.15     100.00      76.66       5154
---+! (label_id: 1)                                          0.00       0.00       0.00        108
---+, (label_id: 2)                                          0.00       0.00       0.00      18022
---+- (label_id: 3)                                          0.00       0.00       0.00       1557
---+. (label_id: 4)                                         41.74      94.01      57.81      15164
---+: (label_id: 5)                                          0.00       0.00       0.00        319
---+; (label_id: 6)                                          0.00       0.00       0.00         88
---+? (label_id: 7)                                          0.00       0.00       0.00       1217
---+ (label_id: 8)                                          0.00       0.00       0.00        752
---+… (label_id: 9)                                          0.00       0.00       0.00         67
---+-------------------
---+micro avg                                               45.72      45.72      45.72      42448
---+macro avg                                               10.39      19.40      13.45      42448
---+weighted avg                                            22.46      45.72      29.96      42448
---+{ 'punct_f1': 13.446383476257324,
---+ 'punct_precision': 10.388500213623047,
---+ 'punct_recall': 19.400554656982422,
---+ 'test_loss': 0.44148480892181396}
---+
---+
---+ ## Log for 8/2/2021
---+
---+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
---+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
---+
---+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
---+End frozen 
---+micro avg                                               41.42      41.42      41.42      33406
---+macro avg                                               11.01      13.54      11.07      33406
---+weighted avg                                            34.88      41.42      34.20      33406
---+
---+1st layer best lr 1e-10, set to 0.007943282347242822
---+micro avg                                               36.65      36.65      36.65      33463
---+macro avg                                               10.71       9.91       8.26      33463
---+weighted avg                                            34.32      36.65      31.08      33463
---+
---+2nd layer best lr 1e-10, set to 0.007943282347242822
---+micro avg                                               35.72      35.72      35.72      42448
---+macro avg                                                3.57      10.00       5.26      42448
---+weighted avg                                            12.76      35.72      18.81      42448
---+
---+{'punct_f1': 5.264181137084961,
---+ 'punct_precision': 3.572371006011963,
---+ 'punct_recall': 10.0,
---+ 'test_loss': 18.49854850769043}
---+
---+
---+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
---+alpha from 3->4 seems to reduce convergence rate.
---+
---+micro avg                                               50.98      50.98      50.98      33463
---+macro avg                                               25.99      25.38      23.38      33463
---+weighted avg                                            50.31      50.98      48.27      33463
---+
---+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
---+micro avg                                               58.55      58.55      58.55      39340
---+macro avg                                               30.02      29.74      29.52      39340
---+weighted avg                                            57.91      58.55      57.51      39340
---+
---+still increasing?!
---+{'punct_f1': 29.523975372314453,
---+ 'punct_precision': 30.015613555908203,
---+ 'punct_recall': 29.738296508789062,
---+ 'test_loss': 0.3690211772918701}
---diff --git a/experiment/config.yaml b/experiment/config.yaml
---index fe58670..a4a012a 100644
------ a/experiment/config.yaml
---+++ b/experiment/config.yaml
---@@ -1,49 +1,49 @@
--- seed: 42
--- trainer:
----    # gpus: 1 # the number of gpus, 0 for CPU
----    # num_nodes: 1
----    # max_epochs: 2
----    # max_steps: null # precedence over max_epochs
----    # accumulate_grad_batches: 4 # accumulates grads every k batches
----    # gradient_clip_val: 0
----    # amp_level: O1 # O1/O2 for mixed precision
----    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----    # accelerator: ddp
----    # checkpoint_callback: false  # Provided by exp_manager
----    # logger: false #false  # Provided by exp_manager
----    # log_every_n_steps: 1  # Interval of logging.
----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----    # resume_from_checkpoint: null
----
----    gpus: 0 # the number of gpus, 0 for CPU
---+    gpus: 1 # the number of gpus, 0 for CPU
---     num_nodes: 1
----    max_epochs: 8
---+    max_epochs: 10
---     max_steps: null # precedence over max_epochs
----    accumulate_grad_batches: 1 # accumulates grads every k batches
---+    accumulate_grad_batches: 4 # accumulates grads every k batches
---     gradient_clip_val: 0
----    amp_level: O0 # O1/O2 for mixed precision
----    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----    # accelerator: ddp
---+    amp_level: O1 # O1/O2 for mixed precision
---+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
---+    accelerator: ddp
---     checkpoint_callback: false  # Provided by exp_manager
---     logger: false #false  # Provided by exp_manager
---     log_every_n_steps: 1  # Interval of logging.
---     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----    reload_dataloaders_every_epoch: true
---     resume_from_checkpoint: null
--- 
---+    # gpus: 0 # the number of gpus, 0 for CPU
---+    # num_nodes: 1
---+    # max_epochs: 8
---+    # max_steps: null # precedence over max_epochs
---+    # accumulate_grad_batches: 1 # accumulates grads every k batches
---+    # gradient_clip_val: 0
---+    # amp_level: O0 # O1/O2 for mixed precision
---+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
---+    # # accelerator: ddp
---+    # checkpoint_callback: false  # Provided by exp_manager
---+    # logger: false #false  # Provided by exp_manager
---+    # log_every_n_steps: 1  # Interval of logging.
---+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
---+    # reload_dataloaders_every_epoch: true
---+    # resume_from_checkpoint: null
---+
--- exp_manager:
----    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
---+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
---     name: Punctuation_with_Domain_discriminator  # The name of your model
---     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
---     create_checkpoint_callback: true 
----base_path: /home/nxingyu2/data # /root/data # 
----tmp_path: /home/nxingyu2/data/tmp # /tmp # 
---+base_path: /home/nxingyu/data # /root/data # 
---+tmp_path: /home/nxingyu/data/tmp # /tmp # 
--- 
--- model:
---     nemo_path: null
----    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
---+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
---     unfrozen: 0
----    maximum_unfrozen: 2
---+    maximum_unfrozen: 1
---     unfreeze_step: 1
---     # unfreeze_every: 3
---     punct_label_ids:
---@@ -58,10 +58,10 @@ model:
---         - "—"
---         - "…"
--- 
----    punct_class_weights: true
---+    punct_class_weights: false
---     
---     dataset:
----        data_dir: /home/nxingyu2/data # /root/data # 
---+        data_dir: /home/nxingyu/data # /root/data # 
---         labelled:
---             - ${base_path}/ted_talks_processed #
---         unlabelled:
---@@ -106,6 +106,12 @@ model:
---         config: null
---         # unfrozen_layers: 1
--- 
---+    mlp:
---+        num_fc_layers: 2
---+        fc_dropout: 0.1
---+        log_softmax: false
---+        activation: 'relu'
---+        
---     punct_head:
---         punct_num_fc_layers: 1
---         fc_dropout: 0.1
---@@ -122,17 +128,19 @@ model:
---         use_transformer_init: true
---         loss: 'cel'
---         gamma: 0.1 # coefficient of gradient reversal
---+        pooling: 'mean'
---+        idx_conditioned_on: 0
---     
---     dice_loss:
---         epsilon: 0.01
----        alpha: 3
---+        alpha: 4
---         macro_average: true
--- 
---     focal_loss: 
---         gamma: 5
--- 
---     optim:
----        name: novograd
---+        name: adamw
---         lr: 1e-3
---         weight_decay: 0.00
---         sched:
---diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
---index d4ff927..f9927ac 100644
------ a/experiment/core/layers/sequence_classifier.py
---+++ b/experiment/core/layers/sequence_classifier.py
---@@ -1,3 +1,4 @@
---+import torch
--- from torch import nn
--- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
--- from core.utils import transformer_weights_init
---@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
---         log_softmax: bool = True,
---         dropout: float = 0.0,
---         use_transformer_init: bool = True,
----        idx_conditioned_on: int = 0,
---+        pooling: str = 'mean', # mean, max, mean_max, token
---+        idx_conditioned_on: int = None,
---     ):
---         """
---         Initializes the SequenceClassifier module.
---@@ -37,8 +39,9 @@ class SequenceClassifier(nn.Module):
---         super().__init__()
---         self.log_softmax = log_softmax
---         self._idx_conditioned_on = idx_conditioned_on
---+        self.pooling = pooling
---         self.mlp = MultiLayerPerceptron(
----            hidden_size=hidden_size,
---+            hidden_size=(hidden_size*2 if pooling=='mean_max' else hidden_size),
---             num_classes=num_classes,
---             num_layers=num_layers,
---             activation=activation,
---@@ -48,7 +51,23 @@ class SequenceClassifier(nn.Module):
---         if use_transformer_init:
---             self.apply(lambda module: transformer_weights_init(module, xavier=False))
--- 
----    def forward(self, hidden_states):
---+    def forward(self, hidden_states, subtoken_mask=None):
---         hidden_states = self.dropout(hidden_states)
----        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
---+        if self.pooling=='token':
---+            pooled = hidden_states[:, self._idx_conditioned_on]
---+        else:
---+            if subtoken_mask==None:
---+                ct=hidden_states.shape[1] # Seq len
---+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
---+            else:
---+                ct = torch.sum(subtoken_mask,axis=1).unsqueeze(1)
---+            pooled_sum = torch.sum(hidden_states,axis=1)            
---+            if self.pooling=='mean' or self.pooling == 'mean_max':
---+                pooled_mean = torch.div(pooled_sum,ct)
---+            if self.pooling=='max' or self.pooling=='mean_max':
---+                pooled_max = torch.max(hidden_states,axis=1)[0]
---+            pooled=pooled_mean if self.pooling=='mean' else \
---+                pooled_max if self.pooling=='max' else \
---+                    torch.cat([pooled_mean,pooled_max],axis=-1)
---+        logits = self.mlp(pooled)
---         return logits
---diff --git a/experiment/info.log b/experiment/info.log
---index 2471fe9..e69de29 100644
------ a/experiment/info.log
---+++ b/experiment/info.log
---@@ -1,300 +0,0 @@
----[INFO] - GPU available: True, used: False
----[INFO] - TPU available: None, using: 0 TPU cores
----[INFO] - shuffling train set
----[INFO] - Optimizer config = Novograd (
----Parameter Group 0
----    amsgrad: False
----    betas: (0.95, 0.98)
----    eps: 1e-08
----    grad_averaging: False
----    lr: 0.001
----    weight_decay: 0.0
----)
----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
----will be used during training (effective maximum steps = 80) - 
----Parameters : 
----(warmup_steps: null
----warmup_ratio: 0.1
----last_epoch: -1
----max_steps: 80
----)
----[INFO] - 
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 108 M 
----1 | punct_classifier    | TokenClassifier      | 7.7 K 
----2 | domain_classifier   | SequenceClassifier   | 769   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----8.5 K     Trainable params
----108 M     Non-trainable params
----108 M     Total params
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.00       0.00       0.00        184
----! (label_id: 1)                                          0.00       0.00       0.00          4
----, (label_id: 2)                                          1.23       0.34       0.53        594
----- (label_id: 3)                                          3.06      25.42       5.46         59
----. (label_id: 4)                                         47.22      12.98      20.36        524
----: (label_id: 5)                                          0.00       0.00       0.00         18
----; (label_id: 6)                                          0.00       0.00       0.00         13
----? (label_id: 7)                                          8.45       6.32       7.23         95
----— (label_id: 8)                                          0.00       0.00       0.00         12
----… (label_id: 9)                                          0.00       0.00       0.00          0
-----------------------
----micro avg                                                6.05       6.05       6.05       1503
----macro avg                                                6.66       5.01       3.73       1503
----weighted avg                                            17.61       6.05       7.98       1503
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00         92
-----------------------
----micro avg                                              100.00     100.00     100.00         92
----macro avg                                              100.00     100.00     100.00         92
----weighted avg                                           100.00     100.00     100.00         92
----
----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
----[INFO] - Optimizer config = Novograd (
----Parameter Group 0
----    amsgrad: False
----    betas: (0.95, 0.98)
----    eps: 1e-08
----    grad_averaging: False
----    lr: 1.5848931924611143e-08
----    weight_decay: 0.0
----)
----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
----will be used during training (effective maximum steps = 3192) - 
----Parameters : 
----(warmup_steps: null
----warmup_ratio: 0.1
----last_epoch: -1
----max_steps: 3192
----)
----[INFO] - 
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 108 M 
----1 | punct_classifier    | TokenClassifier      | 7.7 K 
----2 | domain_classifier   | SequenceClassifier   | 769   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----8.5 K     Trainable params
----108 M     Non-trainable params
----108 M     Total params
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.00       0.00       0.00        202
----! (label_id: 1)                                          0.00       0.00       0.00          4
----, (label_id: 2)                                          1.62       0.45       0.70        669
----- (label_id: 3)                                          3.48      27.27       6.17         66
----. (label_id: 4)                                         45.06      13.01      20.19        561
----: (label_id: 5)                                          1.52       6.67       2.47         15
----; (label_id: 6)                                          0.00       0.00       0.00         15
----? (label_id: 7)                                          8.70       7.32       7.95         82
----— (label_id: 8)                                          0.00       0.00       0.00         13
----… (label_id: 9)                                          0.00       0.00       0.00          1
-----------------------
----micro avg                                                6.20       6.20       6.20       1628
----macro avg                                                6.04       5.47       3.75       1628
----weighted avg                                            16.79       6.20       7.92       1628
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00        101
-----------------------
----micro avg                                              100.00     100.00     100.00        101
----macro avg                                              100.00     100.00     100.00        101
----weighted avg                                           100.00     100.00     100.00        101
----
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.34       0.73       0.46       4402
----! (label_id: 1)                                          0.42      13.95       0.82        129
----, (label_id: 2)                                          2.53       0.64       1.03      15243
----- (label_id: 3)                                          2.45      21.03       4.38       1322
----. (label_id: 4)                                         44.00      11.40      18.11      12542
----: (label_id: 5)                                          0.43       1.41       0.65        354
----; (label_id: 6)                                          0.00       0.00       0.00        163
----? (label_id: 7)                                          4.16       6.27       5.00       1117
----— (label_id: 8)                                          3.00       0.61       1.02        488
----… (label_id: 9)                                          0.97       6.17       1.68         81
-----------------------
----micro avg                                                5.41       5.41       5.41      35841
----macro avg                                                5.83       6.22       3.32      35841
----weighted avg                                            16.78       5.41       7.18      35841
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2201
-----------------------
----micro avg                                              100.00     100.00     100.00       2201
----macro avg                                              100.00     100.00     100.00       2201
----weighted avg                                           100.00     100.00     100.00       2201
----
----[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.20       0.43       0.27       4226
----! (label_id: 1)                                          0.44      14.17       0.86        127
----, (label_id: 2)                                          1.93       0.49       0.78      14611
----- (label_id: 3)                                          2.23      19.56       4.01       1237
----. (label_id: 4)                                         43.37      11.25      17.86      11977
----: (label_id: 5)                                          0.68       2.34       1.05        342
----; (label_id: 6)                                          0.00       0.00       0.00        129
----? (label_id: 7)                                          5.16       7.47       6.10       1058
----— (label_id: 8)                                          2.15       0.49       0.80        409
----… (label_id: 9)                                          0.69       4.23       1.19         71
-----------------------
----micro avg                                                5.23       5.23       5.23      34187
----macro avg                                                5.68       6.04       3.29      34187
----weighted avg                                            16.32       5.23       6.98      34187
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2113
-----------------------
----micro avg                                              100.00     100.00     100.00       2113
----macro avg                                              100.00     100.00     100.00       2113
----weighted avg                                           100.00     100.00     100.00       2113
----
----[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.28       0.61       0.39       4228
----! (label_id: 1)                                          0.30       8.28       0.58        145
----, (label_id: 2)                                          2.27       0.58       0.92      14495
----- (label_id: 3)                                          2.64      21.78       4.70       1327
----. (label_id: 4)                                         44.87      11.66      18.51      12193
----: (label_id: 5)                                          0.60       1.93       0.91        362
----; (label_id: 6)                                          0.00       0.00       0.00        164
----? (label_id: 7)                                          4.19       6.40       5.07       1078
----— (label_id: 8)                                          1.16       0.22       0.37        459
----… (label_id: 9)                                          0.85       4.17       1.41         96
-----------------------
----micro avg                                                5.54       5.54       5.54      34547
----macro avg                                                5.72       5.56       3.29      34547
----weighted avg                                            17.08       5.54       7.33      34547
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2114
-----------------------
----micro avg                                              100.00     100.00     100.00       2114
----macro avg                                              100.00     100.00     100.00       2114
----weighted avg                                           100.00     100.00     100.00       2114
----
----[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.29       0.63       0.40       4444
----! (label_id: 1)                                          0.38      10.67       0.74        150
----, (label_id: 2)                                          2.32       0.59       0.94      15290
----- (label_id: 3)                                          2.34      20.28       4.19       1292
----. (label_id: 4)                                         43.85      11.68      18.44      12599
----: (label_id: 5)                                          0.41       1.28       0.62        392
----; (label_id: 6)                                          0.00       0.00       0.00        164
----? (label_id: 7)                                          4.24       6.30       5.07       1111
----— (label_id: 8)                                          0.00       0.00       0.00        456
----… (label_id: 9)                                          0.38       2.41       0.65         83
-----------------------
----micro avg                                                5.40       5.40       5.40      35981
----macro avg                                                5.42       5.38       3.11      35981
----weighted avg                                            16.59       5.40       7.22      35981
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2222
-----------------------
----micro avg                                              100.00     100.00     100.00       2222
----macro avg                                              100.00     100.00     100.00       2222
----weighted avg                                           100.00     100.00     100.00       2222
----
----[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.35       0.73       0.48       3844
----! (label_id: 1)                                          0.54      14.62       1.04        130
----, (label_id: 2)                                          2.32       0.59       0.94      13056
----- (label_id: 3)                                          2.67      22.28       4.77       1194
----. (label_id: 4)                                         44.45      11.95      18.84      10791
----: (label_id: 5)                                          0.84       3.21       1.33        280
----; (label_id: 6)                                          0.00       0.00       0.00        140
----? (label_id: 7)                                          4.17       6.56       5.10        914
----— (label_id: 8)                                          0.00       0.00       0.00        401
----… (label_id: 9)                                          0.48       2.63       0.81         76
-----------------------
----micro avg                                                5.68       5.68       5.68      30826
----macro avg                                                5.58       6.26       3.33      30826
----weighted avg                                            16.82       5.68       7.41      30826
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       1922
-----------------------
----micro avg                                              100.00     100.00     100.00       1922
----macro avg                                              100.00     100.00     100.00       1922
----weighted avg                                           100.00     100.00     100.00       1922
----
----[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.28       0.60       0.39       3970
----! (label_id: 1)                                          0.35      10.66       0.68        122
----, (label_id: 2)                                          2.09       0.53       0.85      13469
----- (label_id: 3)                                          2.29      19.32       4.10       1201
----. (label_id: 4)                                         43.43      11.24      17.86      11227
----: (label_id: 5)                                          0.63       2.30       0.99        304
----; (label_id: 6)                                          0.00       0.00       0.00        141
----? (label_id: 7)                                          4.52       6.86       5.45       1006
----— (label_id: 8)                                          1.15       0.23       0.38        444
----… (label_id: 9)                                          0.45       2.67       0.78         75
-----------------------
----micro avg                                                5.26       5.26       5.26      31959
----macro avg                                                5.52       5.44       3.15      31959
----weighted avg                                            16.42       5.26       7.02      31959
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       1985
-----------------------
----micro avg                                              100.00     100.00     100.00       1985
----macro avg                                              100.00     100.00     100.00       1985
----weighted avg                                           100.00     100.00     100.00       1985
----
----[INFO] - Epoch 5, step 2394: val_loss was not in top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.23       0.48       0.31       4126
----! (label_id: 1)                                          0.29       9.40       0.56        117
----, (label_id: 2)                                          1.91       0.49       0.77      14019
----- (label_id: 3)                                          2.52      22.59       4.53       1164
----. (label_id: 4)                                         44.15      11.65      18.44      11789
----: (label_id: 5)                                          0.72       2.41       1.11        332
----; (label_id: 6)                                          0.56       0.61       0.58        165
----? (label_id: 7)                                          3.89       6.53       4.88        980
----— (label_id: 8)                                          2.30       0.47       0.77        430
----… (label_id: 9)                                          1.18       8.33       2.07         60
-----------------------
----micro avg                                                5.47       5.47       5.47      33182
----macro avg                                                5.77       6.30       3.40      33182
----weighted avg                                            16.77       5.47       7.25      33182
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2063
-----------------------
----micro avg                                              100.00     100.00     100.00       2063
----macro avg                                              100.00     100.00     100.00       2063
----weighted avg                                           100.00     100.00     100.00       2063
----
----[INFO] - Epoch 6, step 2793: val_loss was not in top 3
---diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
---index fa37b4c..43fc93d 100644
------ a/experiment/models/punctuation_domain_model.py
---+++ b/experiment/models/punctuation_domain_model.py
---@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---         else:
---             self.hparams.model.punct_class_weights=None
--- 
---+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
---+        self.mlp = MultiLayerPerceptron(
---+            self.transformer.config.hidden_size,
---+            self.transformer.config.hidden_size,
---+            num_layers=self.hparams.model.mlp.num_fc_layers, 
---+            activation=self.hparams.model.mlp.activation, 
---+            log_softmax=self.hparams.model.mlp.log_softmax
---+        )
---+
---         self.punct_classifier = TokenClassifier(
---             hidden_size=self.transformer.config.hidden_size,
---             num_classes=len(self.labels_to_ids),
---@@ -80,6 +89,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---             log_softmax=self.hparams.model.domain_head.log_softmax,
---             dropout=self.hparams.model.domain_head.fc_dropout,
---             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
---+            pooling=self.hparams.model.domain_head.pooling,
---+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
---         )
--- 
---         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
---@@ -119,14 +130,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
---         self.freeze()
--- 
----    def forward(self, input_ids, attention_mask, domain_ids=None):
---+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
---         hidden_states = self.transformer(
---             input_ids=input_ids, attention_mask=attention_mask
---         )[0]
---+        hidden_states = self.dropout(hidden_states)
---+        hidden_states = self.mlp(hidden_states)
---         punct_logits = self.punct_classifier(hidden_states=hidden_states)
---         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
---         domain_logits = self.domain_classifier(
----            hidden_states=reverse_grad_hidden_states)
---+            hidden_states=reverse_grad_hidden_states,
---+            subtoken_mask=subtoken_mask)
---         return punct_logits, domain_logits
--- 
---     def _make_step(self, batch):
---@@ -136,7 +150,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---         punct_labels = batch['labels']
---         domain_labels = batch['domain']
---         punct_logits, domain_logits = self(
----            input_ids=input_ids, attention_mask=attention_mask
---+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
---         )
---         punctuation_loss = self.punctuation_loss(
---             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/hparams.yaml
--deleted file mode 100644
--index ebcb726..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/hparams.yaml
--+++ /dev/null
--@@ -1,114 +0,0 @@
---seed: 42
---trainer:
---  gpus: 1
---  num_nodes: 1
---  max_epochs: 10
---  max_steps: null
---  accumulate_grad_batches: 4
---  gradient_clip_val: 0
---  amp_level: O1
---  precision: 16
---  accelerator: ddp
---  checkpoint_callback: false
---  logger: false
---  log_every_n_steps: 1
---  val_check_interval: 1.0
---  resume_from_checkpoint: null
---exp_manager:
---  exp_dir: /home/nxingyu/project/
---  name: Punctuation_with_Domain_discriminator
---  create_tensorboard_logger: true
---  create_checkpoint_callback: true
---base_path: /home/nxingyu/data
---tmp_path: /home/nxingyu/data/tmp
---model:
---  nemo_path: null
---  transformer_path: google/electra-small-discriminator
---  unfrozen: 0
---  maximum_unfrozen: 1
---  unfreeze_step: 1
---  punct_label_ids:
---  - ''
---  - '!'
---  - ','
---  - '-'
---  - .
---  - ':'
---  - ;
---  - '?'
---  - —
---  - …
---  punct_class_weights: false
---  dataset:
---    data_dir: /home/nxingyu/data
---    labelled:
---    - /home/nxingyu/data/ted_talks_processed
---    unlabelled: null
---    max_seq_length: 128
---    pad_label: ''
---    ignore_extra_tokens: false
---    ignore_start_end: false
---    use_cache: false
---    num_workers: 0
---    pin_memory: true
---    drop_last: false
---    num_labels: 10
---    num_domains: 1
---    test_unlabelled: true
---    train_ds:
---      shuffle: true
---      num_samples: -1
---      batch_size: 8
---    validation_ds:
---      shuffle: true
---      num_samples: -1
---      batch_size: 8
---  tokenizer:
---    tokenizer_name: google/electra-small-discriminator
---    vocab_file: null
---    tokenizer_model: null
---    special_tokens: null
---  language_model:
---    pretrained_model_name: google/electra-small-discriminator
---    lm_checkpoint: null
---    config_file: null
---    config: null
---  mlp:
---    num_fc_layers: 2
---    fc_dropout: 0.1
---    log_softmax: false
---    activation: relu
---  punct_head:
---    punct_num_fc_layers: 1
---    fc_dropout: 0.1
---    activation: relu
---    log_softmax: false
---    use_transformer_init: true
---    loss: dice
---  domain_head:
---    domain_num_fc_layers: 1
---    fc_dropout: 0.1
---    activation: relu
---    log_softmax: false
---    use_transformer_init: true
---    loss: cel
---    gamma: 0.1
---    pooling: mean
---    idx_conditioned_on: 0
---  dice_loss:
---    epsilon: 0.01
---    alpha: 4
---    macro_average: true
---  focal_loss:
---    gamma: 5
---  optim:
---    name: adamw
---    lr: 0.001
---    weight_decay: 0.0
---    sched:
---      name: WarmupAnnealing
---      warmup_steps: null
---      warmup_ratio: 0.1
---      last_epoch: -1
---      monitor: val_loss
---      reduce_on_plateau: false
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/lightning_logs.txt
--deleted file mode 100644
--index d3676e6..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/lightning_logs.txt
--+++ /dev/null
--@@ -1,46 +0,0 @@
---Global seed set to 42
---GPU available: True, used: True
---TPU available: None, using: 0 TPU cores
---LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
---Using native 16bit precision.
---Global seed set to 42
---initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | dropout             | Dropout              | 0     
---2 | mlp                 | MultiLayerPerceptron | 131 K 
---3 | punct_classifier    | TokenClassifier      | 2.6 K 
---4 | domain_classifier   | SequenceClassifier   | 257   
---5 | punctuation_loss    | FocalDiceLoss        | 0     
---6 | domain_loss         | CrossEntropyLoss     | 0     
---7 | agg_loss            | AggregatorLoss       | 0     
---8 | punct_class_report  | ClassificationReport | 0     
---9 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---167 K     Trainable params
---13.4 M    Non-trainable params
---13.6 M    Total params
---Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/lr_find_temp_model.ckpt
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | dropout             | Dropout              | 0     
---2 | mlp                 | MultiLayerPerceptron | 131 K 
---3 | punct_classifier    | TokenClassifier      | 2.6 K 
---4 | domain_classifier   | SequenceClassifier   | 257   
---5 | punctuation_loss    | FocalDiceLoss        | 0     
---6 | domain_loss         | CrossEntropyLoss     | 0     
---7 | agg_loss            | AggregatorLoss       | 0     
---8 | punct_class_report  | ClassificationReport | 0     
---9 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---167 K     Trainable params
---13.4 M    Non-trainable params
---13.6 M    Total params
---Epoch 0, global step 100: val_loss reached 0.87292 (best 0.87292), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=0.ckpt" as top 3
---Epoch 1, global step 200: val_loss reached 0.80471 (best 0.80471), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.80-epoch=1.ckpt" as top 3
---Epoch 2, global step 300: val_loss reached 0.80542 (best 0.80471), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.81-epoch=2.ckpt" as top 3
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/nemo_error_log.txt
--deleted file mode 100644
--index 5b76cfb..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/nemo_error_log.txt
--+++ /dev/null
--@@ -1,28 +0,0 @@
---[NeMo W 2021-02-08 11:07:07 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-08 11:07:16 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:07:16 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:07:18 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:10:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:10:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:10:57 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:11:01 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
---[NeMo W 2021-02-08 11:16:19 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7fe326ec1700> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-08 11:16:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7fe326ea8040> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/nemo_log_globalrank-0_localrank-0.txt
--deleted file mode 100644
--index 45ada6c..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/nemo_log_globalrank-0_localrank-0.txt
--+++ /dev/null
--@@ -1,30 +0,0 @@
---[NeMo I 2021-02-08 11:07:07 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07
---[NeMo I 2021-02-08 11:07:07 exp_manager:519] TensorboardLogger has been set up
---[NeMo W 2021-02-08 11:07:07 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-08 11:07:16 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:07:16 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:07:18 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:10:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:10:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:10:57 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:11:01 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
---[NeMo W 2021-02-08 11:16:19 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7fe326ec1700> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-08 11:16:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7fe326ea8040> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/cmd-args.log
--deleted file mode 100644
--index 11a5d8e..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/cmd-args.log
--+++ /dev/null
--@@ -1 +0,0 @@
---main.py
--\ No newline at end of file
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/events.out.tfevents.1612754918.Titan.21528.0 b/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/events.out.tfevents.1612754918.Titan.21528.0
--deleted file mode 100644
--index 9596b1a..0000000
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/events.out.tfevents.1612754918.Titan.21528.0 and /dev/null differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/git-info.log
--deleted file mode 100644
--index e04e39e..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/git-info.log
--+++ /dev/null
--@@ -1,699 +0,0 @@
---commit hash: 939a671c8c117db6975316767ced5d95449e2b27
---diff --git a/README.md b/README.md
---index d52dba5..51ab30b 100644
------ a/README.md
---+++ b/README.md
---@@ -291,21 +291,22 @@ weighted avg         |  43.14   | 33.52 |  29.43  |  67486
---  0 layer not too much improvement, 1 layer pretty decent.
---  alpha 5 seems too high. to try full run 4 next.
--- layer 0 * 8 + layer 1 * 3
----
----  (label_id: 0)                                           0.00       0.00       0.00       5704
----! (label_id: 1)                                          0.00       0.00       0.00        110
----, (label_id: 2)                                          0.00       0.00       0.00      19711
----- (label_id: 3)                                          6.82      29.32      11.07       1702
----. (label_id: 4)                                         37.30      83.82      51.62      18406
----: (label_id: 5)                                          0.00       0.00       0.00        379
----; (label_id: 6)                                          0.00       0.00       0.00        190
----? (label_id: 7)                                          6.71       1.31       2.20       1446
----— (label_id: 8)                                          0.00       0.00       0.00       1227
----… (label_id: 9)                                          0.00       0.00       0.00         86
-----------------------
----micro avg                                               32.57      32.57      32.57      48961
----macro avg                                                5.08      11.44       6.49      48961
----weighted avg                                            14.46      32.57      19.86      48961
---+label                |  precision | recall |   f1   |    support
---+---|---|---|---|---
---+  (label_id: 0)        | 0.00     | 0.00    | 0.00  | 5704
---+! (label_id: 1)        | 0.00     | 0.00    | 0.00  | 110
---+, (label_id: 2)        | 0.00     | 0.00    | 0.00  | 19711
---+- (label_id: 3)        | 6.82     | 29.32   | 11.07 | 1702
---+. (label_id: 4)        | 37.30    | 83.82   | 51.62 | 18406
---+: (label_id: 5)        | 0.00     | 0.00    | 0.00  | 379
---+; (label_id: 6)        | 0.00     | 0.00    | 0.00  | 190
---+? (label_id: 7)        | 6.71     | 1.31    | 2.20  | 1446
---+— (label_id: 8)        | 0.00     | 0.00    | 0.00  | 1227
---+… (label_id: 9)        | 0.00     | 0.00    | 0.00  | 86
---+-------------------||||
---+micro avg              | 32.57    | 32.57   | 32.57 | 48961
---+macro avg              | 5.08     | 11.44   | 6.49  | 48961
---+weighted avg           | 14.46    | 32.57   | 19.86 | 48961
--- 
---  {'punct_f1': 6.104840278625488,
---  'punct_precision': 4.423948764801025,
---@@ -318,19 +319,96 @@ lr 0 : 0.008413951416451957
--- 1: 0.00031622776601683794 ** too high. to adjust the min to 1e-10?
--- 2: 0.00031622776601683794
--- 
----label                                                precision    recall       f1           support
---- (label_id: 0)                                           0.00       0.00       0.00       7470
----! (label_id: 1)                                          0.00       0.00       0.00        148
----, (label_id: 2)                                          0.00       0.00       0.00      28513
----- (label_id: 3)                                          3.02     100.00       5.86       2074
----. (label_id: 4)                                          0.00       0.00       0.00      25120
----: (label_id: 5)                                          0.00       0.00       0.00        570
----; (label_id: 6)                                          0.00       0.00       0.00        534
----? (label_id: 7)                                          0.00       0.00       0.00       2085
----— (label_id: 8)                                          0.00       0.00       0.00       2073
----… (label_id: 9)                                          0.00       0.00       0.00        142
----
---- 'punct_f1': 0.5858508944511414,
---+label                |  precision | recall |   f1   |    support
---+---|---|---|---|---
---+ (label_id: 0)       |   0.00  | 0.00     | 0.00  | 7470
---+! (label_id: 1)      |   0.00  | 0.00     | 0.00  | 148
---+, (label_id: 2)      |   0.00  | 0.00     | 0.00  | 28513
---+- (label_id: 3)      |   3.02  | 100.00   | 5.86  | 2074
---+. (label_id: 4)      |   0.00  | 0.00     | 0.00  | 25120
---+: (label_id: 5)      |   0.00  | 0.00     | 0.00  | 570
---+; (label_id: 6)      |   0.00  | 0.00     | 0.00  | 534
---+? (label_id: 7)      |   0.00  | 0.00     | 0.00  | 2085
---+— (label_id: 8)      |   0.00  | 0.00     | 0.00  | 2073
---+… (label_id: 9)      |   0.00  | 0.00     | 0.00  | 142
---+
---+ {'punct_f1': 0.5858508944511414,
---  'punct_precision': 0.30176490545272827,
---  'punct_recall': 10.0,
---- 'test_loss': 0.8140875697135925}
---\ No newline at end of file
---+ 'test_loss': 0.8140875697135925}
---+
---+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
---+
---+label                |  precision | recall |   f1   |    support
---+---|---|---|---|---
---+ (label_id: 0)      | 62.15  | 100.00   | 76.66   | 5154
---+! (label_id: 1)     | 0.00   | 0.00     | 0.00    | 108
---+, (label_id: 2)     | 0.00   | 0.00     | 0.00    | 18022
---+- (label_id: 3)     | 0.00   | 0.00     | 0.00    | 1557
---+. (label_id: 4)     | 41.74  | 94.01    | 57.81   | 15164
---+: (label_id: 5)     | 0.00   | 0.00     | 0.00    | 319
---+; (label_id: 6)     | 0.00   | 0.00     | 0.00    | 88
---+? (label_id: 7)     | 0.00   | 0.00     | 0.00    | 1217
---+ (label_id: 8)      | 0.00   | 0.00     | 0.00    | 752
---+… (label_id: 9)     | 0.00   | 0.00     | 0.00    | 67
---+-------------------||||
---+micro avg           | 45.72 | 45.72 | 45.72 | 42448
---+macro avg           | 10.39 | 19.40 | 13.45 | 42448
---+weighted avg        | 22.46 | 45.72 | 29.96 | 42448
---+
---+{ 'punct_f1': 13.446383476257324,
---+ 'punct_precision': 10.388500213623047,
---+ 'punct_recall': 19.400554656982422,
---+ 'test_loss': 0.44148480892181396}
---+
---+
---+ ## Log for 8/2/2021
---+
---+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
---+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
---+
---+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
---+End frozen 
---+micro avg      |  41.42 |   41.42  |    41.42   |   33406
---+macro avg      |  11.01 |   13.54  |    11.07   |   33406
---+weighted avg   |  34.88 |   41.42  |    34.20   |   33406
---+
---+1st layer best lr 1e-10, set to 0.007943282347242822
---+micro avg            |       36.65  |    36.65  |    36.65  |    33463
---+macro avg            |       10.71  |     9.91  |     8.26  |    33463
---+weighted avg         |       34.32  |    36.65  |    31.08  |    33463
---+
---+2nd layer best lr 1e-10, set to 0.007943282347242822
---+micro avg        |   35.72  |    35.72  |    35.72  |    42448
---+macro avg        |    3.57  |    10.00  |     5.26  |    42448
---+weighted avg     |   12.76  |    35.72  |    18.81  |    42448
---+
---+{'punct_f1': 5.264181137084961,
---+ 'punct_precision': 3.572371006011963,
---+ 'punct_recall': 10.0,
---+ 'test_loss': 18.49854850769043}
---+
---+
---+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
---+alpha from 3->4 seems to reduce convergence rate.
---+
---+micro avg        |   50.98  | 50.98  |  50.98    |  33463
---+macro avg        |   25.99  | 25.38  |  23.38    |  33463
---+weighted avg     |   50.31  | 50.98  |  48.27    |  33463
---+
---+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
---+micro avg     |  58.55  |  58.55 |  58.55 | 39340
---+macro avg     |  30.02  |  29.74 |  29.52 | 39340
---+weighted avg  |  57.91  |  58.55 |  57.51 | 39340
---+
---+still increasing?!
---+{'punct_f1': 29.523975372314453,
---+ 'punct_precision': 30.015613555908203,
---+ 'punct_recall': 29.738296508789062,
---+ 'test_loss': 0.3690211772918701}
---+
---+
---+### Implemented mlp 2 layer before classifier, 
---+
---+adamw mean 2 layer domain, dice, alpha 4 10 batch, accgrad 4 2021-02-08_11-07-07/
---+frozen lr 0.0025118864315095825 best: 0.01,
---diff --git a/experiment/config.yaml b/experiment/config.yaml
---index fe58670..b137ae8 100644
------ a/experiment/config.yaml
---+++ b/experiment/config.yaml
---@@ -1,49 +1,49 @@
--- seed: 42
--- trainer:
----    # gpus: 1 # the number of gpus, 0 for CPU
----    # num_nodes: 1
----    # max_epochs: 2
----    # max_steps: null # precedence over max_epochs
----    # accumulate_grad_batches: 4 # accumulates grads every k batches
----    # gradient_clip_val: 0
----    # amp_level: O1 # O1/O2 for mixed precision
----    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----    # accelerator: ddp
----    # checkpoint_callback: false  # Provided by exp_manager
----    # logger: false #false  # Provided by exp_manager
----    # log_every_n_steps: 1  # Interval of logging.
----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----    # resume_from_checkpoint: null
----
----    gpus: 0 # the number of gpus, 0 for CPU
---+    gpus: 1 # the number of gpus, 0 for CPU
---     num_nodes: 1
----    max_epochs: 8
---+    max_epochs: 10
---     max_steps: null # precedence over max_epochs
----    accumulate_grad_batches: 1 # accumulates grads every k batches
---+    accumulate_grad_batches: 4 # accumulates grads every k batches
---     gradient_clip_val: 0
----    amp_level: O0 # O1/O2 for mixed precision
----    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----    # accelerator: ddp
---+    amp_level: O1 # O1/O2 for mixed precision
---+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
---+    accelerator: ddp
---     checkpoint_callback: false  # Provided by exp_manager
---     logger: false #false  # Provided by exp_manager
---     log_every_n_steps: 1  # Interval of logging.
---     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----    reload_dataloaders_every_epoch: true
---     resume_from_checkpoint: null
--- 
---+    # gpus: 0 # the number of gpus, 0 for CPU
---+    # num_nodes: 1
---+    # max_epochs: 8
---+    # max_steps: null # precedence over max_epochs
---+    # accumulate_grad_batches: 1 # accumulates grads every k batches
---+    # gradient_clip_val: 0
---+    # amp_level: O0 # O1/O2 for mixed precision
---+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
---+    # # accelerator: ddp
---+    # checkpoint_callback: false  # Provided by exp_manager
---+    # logger: false #false  # Provided by exp_manager
---+    # log_every_n_steps: 1  # Interval of logging.
---+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
---+    # reload_dataloaders_every_epoch: true
---+    # resume_from_checkpoint: null
---+
--- exp_manager:
----    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
---+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
---     name: Punctuation_with_Domain_discriminator  # The name of your model
---     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
---     create_checkpoint_callback: true 
----base_path: /home/nxingyu2/data # /root/data # 
----tmp_path: /home/nxingyu2/data/tmp # /tmp # 
---+base_path: /home/nxingyu/data # /root/data # 
---+tmp_path: /home/nxingyu/data/tmp # /tmp # 
--- 
--- model:
---     nemo_path: null
----    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
---+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
---     unfrozen: 0
----    maximum_unfrozen: 2
---+    maximum_unfrozen: 1
---     unfreeze_step: 1
---     # unfreeze_every: 3
---     punct_label_ids:
---@@ -58,10 +58,10 @@ model:
---         - "—"
---         - "…"
--- 
----    punct_class_weights: true
---+    punct_class_weights: false
---     
---     dataset:
----        data_dir: /home/nxingyu2/data # /root/data # 
---+        data_dir: /home/nxingyu/data # /root/data # 
---         labelled:
---             - ${base_path}/ted_talks_processed #
---         unlabelled:
---@@ -106,6 +106,12 @@ model:
---         config: null
---         # unfrozen_layers: 1
--- 
---+    mlp:
---+        num_fc_layers: 1
---+        fc_dropout: 0.1
---+        log_softmax: false
---+        activation: 'relu'
---+        
---     punct_head:
---         punct_num_fc_layers: 1
---         fc_dropout: 0.1
---@@ -122,6 +128,8 @@ model:
---         use_transformer_init: true
---         loss: 'cel'
---         gamma: 0.1 # coefficient of gradient reversal
---+        pooling: 'mean'
---+        idx_conditioned_on: 0
---     
---     dice_loss:
---         epsilon: 0.01
---@@ -132,7 +140,7 @@ model:
---         gamma: 5
--- 
---     optim:
----        name: novograd
---+        name: adamw
---         lr: 1e-3
---         weight_decay: 0.00
---         sched:
---diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
---index d4ff927..f9927ac 100644
------ a/experiment/core/layers/sequence_classifier.py
---+++ b/experiment/core/layers/sequence_classifier.py
---@@ -1,3 +1,4 @@
---+import torch
--- from torch import nn
--- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
--- from core.utils import transformer_weights_init
---@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
---         log_softmax: bool = True,
---         dropout: float = 0.0,
---         use_transformer_init: bool = True,
----        idx_conditioned_on: int = 0,
---+        pooling: str = 'mean', # mean, max, mean_max, token
---+        idx_conditioned_on: int = None,
---     ):
---         """
---         Initializes the SequenceClassifier module.
---@@ -37,8 +39,9 @@ class SequenceClassifier(nn.Module):
---         super().__init__()
---         self.log_softmax = log_softmax
---         self._idx_conditioned_on = idx_conditioned_on
---+        self.pooling = pooling
---         self.mlp = MultiLayerPerceptron(
----            hidden_size=hidden_size,
---+            hidden_size=(hidden_size*2 if pooling=='mean_max' else hidden_size),
---             num_classes=num_classes,
---             num_layers=num_layers,
---             activation=activation,
---@@ -48,7 +51,23 @@ class SequenceClassifier(nn.Module):
---         if use_transformer_init:
---             self.apply(lambda module: transformer_weights_init(module, xavier=False))
--- 
----    def forward(self, hidden_states):
---+    def forward(self, hidden_states, subtoken_mask=None):
---         hidden_states = self.dropout(hidden_states)
----        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
---+        if self.pooling=='token':
---+            pooled = hidden_states[:, self._idx_conditioned_on]
---+        else:
---+            if subtoken_mask==None:
---+                ct=hidden_states.shape[1] # Seq len
---+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
---+            else:
---+                ct = torch.sum(subtoken_mask,axis=1).unsqueeze(1)
---+            pooled_sum = torch.sum(hidden_states,axis=1)            
---+            if self.pooling=='mean' or self.pooling == 'mean_max':
---+                pooled_mean = torch.div(pooled_sum,ct)
---+            if self.pooling=='max' or self.pooling=='mean_max':
---+                pooled_max = torch.max(hidden_states,axis=1)[0]
---+            pooled=pooled_mean if self.pooling=='mean' else \
---+                pooled_max if self.pooling=='max' else \
---+                    torch.cat([pooled_mean,pooled_max],axis=-1)
---+        logits = self.mlp(pooled)
---         return logits
---diff --git a/experiment/info.log b/experiment/info.log
---index 2471fe9..e69de29 100644
------ a/experiment/info.log
---+++ b/experiment/info.log
---@@ -1,300 +0,0 @@
----[INFO] - GPU available: True, used: False
----[INFO] - TPU available: None, using: 0 TPU cores
----[INFO] - shuffling train set
----[INFO] - Optimizer config = Novograd (
----Parameter Group 0
----    amsgrad: False
----    betas: (0.95, 0.98)
----    eps: 1e-08
----    grad_averaging: False
----    lr: 0.001
----    weight_decay: 0.0
----)
----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
----will be used during training (effective maximum steps = 80) - 
----Parameters : 
----(warmup_steps: null
----warmup_ratio: 0.1
----last_epoch: -1
----max_steps: 80
----)
----[INFO] - 
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 108 M 
----1 | punct_classifier    | TokenClassifier      | 7.7 K 
----2 | domain_classifier   | SequenceClassifier   | 769   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----8.5 K     Trainable params
----108 M     Non-trainable params
----108 M     Total params
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.00       0.00       0.00        184
----! (label_id: 1)                                          0.00       0.00       0.00          4
----, (label_id: 2)                                          1.23       0.34       0.53        594
----- (label_id: 3)                                          3.06      25.42       5.46         59
----. (label_id: 4)                                         47.22      12.98      20.36        524
----: (label_id: 5)                                          0.00       0.00       0.00         18
----; (label_id: 6)                                          0.00       0.00       0.00         13
----? (label_id: 7)                                          8.45       6.32       7.23         95
----— (label_id: 8)                                          0.00       0.00       0.00         12
----… (label_id: 9)                                          0.00       0.00       0.00          0
-----------------------
----micro avg                                                6.05       6.05       6.05       1503
----macro avg                                                6.66       5.01       3.73       1503
----weighted avg                                            17.61       6.05       7.98       1503
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00         92
-----------------------
----micro avg                                              100.00     100.00     100.00         92
----macro avg                                              100.00     100.00     100.00         92
----weighted avg                                           100.00     100.00     100.00         92
----
----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
----[INFO] - Optimizer config = Novograd (
----Parameter Group 0
----    amsgrad: False
----    betas: (0.95, 0.98)
----    eps: 1e-08
----    grad_averaging: False
----    lr: 1.5848931924611143e-08
----    weight_decay: 0.0
----)
----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
----will be used during training (effective maximum steps = 3192) - 
----Parameters : 
----(warmup_steps: null
----warmup_ratio: 0.1
----last_epoch: -1
----max_steps: 3192
----)
----[INFO] - 
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 108 M 
----1 | punct_classifier    | TokenClassifier      | 7.7 K 
----2 | domain_classifier   | SequenceClassifier   | 769   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----8.5 K     Trainable params
----108 M     Non-trainable params
----108 M     Total params
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.00       0.00       0.00        202
----! (label_id: 1)                                          0.00       0.00       0.00          4
----, (label_id: 2)                                          1.62       0.45       0.70        669
----- (label_id: 3)                                          3.48      27.27       6.17         66
----. (label_id: 4)                                         45.06      13.01      20.19        561
----: (label_id: 5)                                          1.52       6.67       2.47         15
----; (label_id: 6)                                          0.00       0.00       0.00         15
----? (label_id: 7)                                          8.70       7.32       7.95         82
----— (label_id: 8)                                          0.00       0.00       0.00         13
----… (label_id: 9)                                          0.00       0.00       0.00          1
-----------------------
----micro avg                                                6.20       6.20       6.20       1628
----macro avg                                                6.04       5.47       3.75       1628
----weighted avg                                            16.79       6.20       7.92       1628
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00        101
-----------------------
----micro avg                                              100.00     100.00     100.00        101
----macro avg                                              100.00     100.00     100.00        101
----weighted avg                                           100.00     100.00     100.00        101
----
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.34       0.73       0.46       4402
----! (label_id: 1)                                          0.42      13.95       0.82        129
----, (label_id: 2)                                          2.53       0.64       1.03      15243
----- (label_id: 3)                                          2.45      21.03       4.38       1322
----. (label_id: 4)                                         44.00      11.40      18.11      12542
----: (label_id: 5)                                          0.43       1.41       0.65        354
----; (label_id: 6)                                          0.00       0.00       0.00        163
----? (label_id: 7)                                          4.16       6.27       5.00       1117
----— (label_id: 8)                                          3.00       0.61       1.02        488
----… (label_id: 9)                                          0.97       6.17       1.68         81
-----------------------
----micro avg                                                5.41       5.41       5.41      35841
----macro avg                                                5.83       6.22       3.32      35841
----weighted avg                                            16.78       5.41       7.18      35841
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2201
-----------------------
----micro avg                                              100.00     100.00     100.00       2201
----macro avg                                              100.00     100.00     100.00       2201
----weighted avg                                           100.00     100.00     100.00       2201
----
----[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.20       0.43       0.27       4226
----! (label_id: 1)                                          0.44      14.17       0.86        127
----, (label_id: 2)                                          1.93       0.49       0.78      14611
----- (label_id: 3)                                          2.23      19.56       4.01       1237
----. (label_id: 4)                                         43.37      11.25      17.86      11977
----: (label_id: 5)                                          0.68       2.34       1.05        342
----; (label_id: 6)                                          0.00       0.00       0.00        129
----? (label_id: 7)                                          5.16       7.47       6.10       1058
----— (label_id: 8)                                          2.15       0.49       0.80        409
----… (label_id: 9)                                          0.69       4.23       1.19         71
-----------------------
----micro avg                                                5.23       5.23       5.23      34187
----macro avg                                                5.68       6.04       3.29      34187
----weighted avg                                            16.32       5.23       6.98      34187
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2113
-----------------------
----micro avg                                              100.00     100.00     100.00       2113
----macro avg                                              100.00     100.00     100.00       2113
----weighted avg                                           100.00     100.00     100.00       2113
----
----[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.28       0.61       0.39       4228
----! (label_id: 1)                                          0.30       8.28       0.58        145
----, (label_id: 2)                                          2.27       0.58       0.92      14495
----- (label_id: 3)                                          2.64      21.78       4.70       1327
----. (label_id: 4)                                         44.87      11.66      18.51      12193
----: (label_id: 5)                                          0.60       1.93       0.91        362
----; (label_id: 6)                                          0.00       0.00       0.00        164
----? (label_id: 7)                                          4.19       6.40       5.07       1078
----— (label_id: 8)                                          1.16       0.22       0.37        459
----… (label_id: 9)                                          0.85       4.17       1.41         96
-----------------------
----micro avg                                                5.54       5.54       5.54      34547
----macro avg                                                5.72       5.56       3.29      34547
----weighted avg                                            17.08       5.54       7.33      34547
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2114
-----------------------
----micro avg                                              100.00     100.00     100.00       2114
----macro avg                                              100.00     100.00     100.00       2114
----weighted avg                                           100.00     100.00     100.00       2114
----
----[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.29       0.63       0.40       4444
----! (label_id: 1)                                          0.38      10.67       0.74        150
----, (label_id: 2)                                          2.32       0.59       0.94      15290
----- (label_id: 3)                                          2.34      20.28       4.19       1292
----. (label_id: 4)                                         43.85      11.68      18.44      12599
----: (label_id: 5)                                          0.41       1.28       0.62        392
----; (label_id: 6)                                          0.00       0.00       0.00        164
----? (label_id: 7)                                          4.24       6.30       5.07       1111
----— (label_id: 8)                                          0.00       0.00       0.00        456
----… (label_id: 9)                                          0.38       2.41       0.65         83
-----------------------
----micro avg                                                5.40       5.40       5.40      35981
----macro avg                                                5.42       5.38       3.11      35981
----weighted avg                                            16.59       5.40       7.22      35981
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2222
-----------------------
----micro avg                                              100.00     100.00     100.00       2222
----macro avg                                              100.00     100.00     100.00       2222
----weighted avg                                           100.00     100.00     100.00       2222
----
----[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.35       0.73       0.48       3844
----! (label_id: 1)                                          0.54      14.62       1.04        130
----, (label_id: 2)                                          2.32       0.59       0.94      13056
----- (label_id: 3)                                          2.67      22.28       4.77       1194
----. (label_id: 4)                                         44.45      11.95      18.84      10791
----: (label_id: 5)                                          0.84       3.21       1.33        280
----; (label_id: 6)                                          0.00       0.00       0.00        140
----? (label_id: 7)                                          4.17       6.56       5.10        914
----— (label_id: 8)                                          0.00       0.00       0.00        401
----… (label_id: 9)                                          0.48       2.63       0.81         76
-----------------------
----micro avg                                                5.68       5.68       5.68      30826
----macro avg                                                5.58       6.26       3.33      30826
----weighted avg                                            16.82       5.68       7.41      30826
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       1922
-----------------------
----micro avg                                              100.00     100.00     100.00       1922
----macro avg                                              100.00     100.00     100.00       1922
----weighted avg                                           100.00     100.00     100.00       1922
----
----[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.28       0.60       0.39       3970
----! (label_id: 1)                                          0.35      10.66       0.68        122
----, (label_id: 2)                                          2.09       0.53       0.85      13469
----- (label_id: 3)                                          2.29      19.32       4.10       1201
----. (label_id: 4)                                         43.43      11.24      17.86      11227
----: (label_id: 5)                                          0.63       2.30       0.99        304
----; (label_id: 6)                                          0.00       0.00       0.00        141
----? (label_id: 7)                                          4.52       6.86       5.45       1006
----— (label_id: 8)                                          1.15       0.23       0.38        444
----… (label_id: 9)                                          0.45       2.67       0.78         75
-----------------------
----micro avg                                                5.26       5.26       5.26      31959
----macro avg                                                5.52       5.44       3.15      31959
----weighted avg                                            16.42       5.26       7.02      31959
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       1985
-----------------------
----micro avg                                              100.00     100.00     100.00       1985
----macro avg                                              100.00     100.00     100.00       1985
----weighted avg                                           100.00     100.00     100.00       1985
----
----[INFO] - Epoch 5, step 2394: val_loss was not in top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.23       0.48       0.31       4126
----! (label_id: 1)                                          0.29       9.40       0.56        117
----, (label_id: 2)                                          1.91       0.49       0.77      14019
----- (label_id: 3)                                          2.52      22.59       4.53       1164
----. (label_id: 4)                                         44.15      11.65      18.44      11789
----: (label_id: 5)                                          0.72       2.41       1.11        332
----; (label_id: 6)                                          0.56       0.61       0.58        165
----? (label_id: 7)                                          3.89       6.53       4.88        980
----— (label_id: 8)                                          2.30       0.47       0.77        430
----… (label_id: 9)                                          1.18       8.33       2.07         60
-----------------------
----micro avg                                                5.47       5.47       5.47      33182
----macro avg                                                5.77       6.30       3.40      33182
----weighted avg                                            16.77       5.47       7.25      33182
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2063
-----------------------
----micro avg                                              100.00     100.00     100.00       2063
----macro avg                                              100.00     100.00     100.00       2063
----weighted avg                                           100.00     100.00     100.00       2063
----
----[INFO] - Epoch 6, step 2793: val_loss was not in top 3
---diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
---index fa37b4c..43fc93d 100644
------ a/experiment/models/punctuation_domain_model.py
---+++ b/experiment/models/punctuation_domain_model.py
---@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---         else:
---             self.hparams.model.punct_class_weights=None
--- 
---+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
---+        self.mlp = MultiLayerPerceptron(
---+            self.transformer.config.hidden_size,
---+            self.transformer.config.hidden_size,
---+            num_layers=self.hparams.model.mlp.num_fc_layers, 
---+            activation=self.hparams.model.mlp.activation, 
---+            log_softmax=self.hparams.model.mlp.log_softmax
---+        )
---+
---         self.punct_classifier = TokenClassifier(
---             hidden_size=self.transformer.config.hidden_size,
---             num_classes=len(self.labels_to_ids),
---@@ -80,6 +89,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---             log_softmax=self.hparams.model.domain_head.log_softmax,
---             dropout=self.hparams.model.domain_head.fc_dropout,
---             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
---+            pooling=self.hparams.model.domain_head.pooling,
---+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
---         )
--- 
---         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
---@@ -119,14 +130,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
---         self.freeze()
--- 
----    def forward(self, input_ids, attention_mask, domain_ids=None):
---+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
---         hidden_states = self.transformer(
---             input_ids=input_ids, attention_mask=attention_mask
---         )[0]
---+        hidden_states = self.dropout(hidden_states)
---+        hidden_states = self.mlp(hidden_states)
---         punct_logits = self.punct_classifier(hidden_states=hidden_states)
---         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
---         domain_logits = self.domain_classifier(
----            hidden_states=reverse_grad_hidden_states)
---+            hidden_states=reverse_grad_hidden_states,
---+            subtoken_mask=subtoken_mask)
---         return punct_logits, domain_logits
--- 
---     def _make_step(self, batch):
---@@ -136,7 +150,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---         punct_labels = batch['labels']
---         domain_labels = batch['domain']
---         punct_logits, domain_logits = self(
----            input_ids=input_ids, attention_mask=attention_mask
---+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
---         )
---         punctuation_loss = self.punctuation_loss(
---             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/hparams.yaml
--deleted file mode 100644
--index 8fe49f4..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/hparams.yaml
--+++ /dev/null
--@@ -1,114 +0,0 @@
---seed: 42
---trainer:
---  gpus: 1
---  num_nodes: 1
---  max_epochs: 10
---  max_steps: null
---  accumulate_grad_batches: 4
---  gradient_clip_val: 0
---  amp_level: O1
---  precision: 16
---  accelerator: ddp
---  checkpoint_callback: false
---  logger: false
---  log_every_n_steps: 1
---  val_check_interval: 1.0
---  resume_from_checkpoint: null
---exp_manager:
---  exp_dir: /home/nxingyu/project/
---  name: Punctuation_with_Domain_discriminator
---  create_tensorboard_logger: true
---  create_checkpoint_callback: true
---base_path: /home/nxingyu/data
---tmp_path: /home/nxingyu/data/tmp
---model:
---  nemo_path: null
---  transformer_path: google/electra-small-discriminator
---  unfrozen: 0
---  maximum_unfrozen: 1
---  unfreeze_step: 1
---  punct_label_ids:
---  - ''
---  - '!'
---  - ','
---  - '-'
---  - .
---  - ':'
---  - ;
---  - '?'
---  - —
---  - …
---  punct_class_weights: false
---  dataset:
---    data_dir: /home/nxingyu/data
---    labelled:
---    - /home/nxingyu/data/ted_talks_processed
---    unlabelled: null
---    max_seq_length: 128
---    pad_label: ''
---    ignore_extra_tokens: false
---    ignore_start_end: false
---    use_cache: false
---    num_workers: 0
---    pin_memory: true
---    drop_last: false
---    num_labels: 10
---    num_domains: 1
---    test_unlabelled: true
---    train_ds:
---      shuffle: true
---      num_samples: -1
---      batch_size: 8
---    validation_ds:
---      shuffle: true
---      num_samples: -1
---      batch_size: 8
---  tokenizer:
---    tokenizer_name: google/electra-small-discriminator
---    vocab_file: null
---    tokenizer_model: null
---    special_tokens: null
---  language_model:
---    pretrained_model_name: google/electra-small-discriminator
---    lm_checkpoint: null
---    config_file: null
---    config: null
---  mlp:
---    num_fc_layers: 1
---    fc_dropout: 0.1
---    log_softmax: false
---    activation: relu
---  punct_head:
---    punct_num_fc_layers: 1
---    fc_dropout: 0.1
---    activation: relu
---    log_softmax: false
---    use_transformer_init: true
---    loss: dice
---  domain_head:
---    domain_num_fc_layers: 1
---    fc_dropout: 0.1
---    activation: relu
---    log_softmax: false
---    use_transformer_init: true
---    loss: cel
---    gamma: 0.1
---    pooling: mean
---    idx_conditioned_on: 0
---  dice_loss:
---    epsilon: 0.01
---    alpha: 3
---    macro_average: true
---  focal_loss:
---    gamma: 5
---  optim:
---    name: adamw
---    lr: 0.001
---    weight_decay: 0.0
---    sched:
---      name: WarmupAnnealing
---      warmup_steps: null
---      warmup_ratio: 0.1
---      last_epoch: -1
---      monitor: val_loss
---      reduce_on_plateau: false
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/lightning_logs.txt
--deleted file mode 100644
--index 40113aa..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/lightning_logs.txt
--+++ /dev/null
--@@ -1,43 +0,0 @@
---Global seed set to 42
---GPU available: True, used: True
---TPU available: None, using: 0 TPU cores
---LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
---Using native 16bit precision.
---Global seed set to 42
---initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | dropout             | Dropout              | 0     
---2 | mlp                 | MultiLayerPerceptron | 65.8 K
---3 | punct_classifier    | TokenClassifier      | 2.6 K 
---4 | domain_classifier   | SequenceClassifier   | 257   
---5 | punctuation_loss    | FocalDiceLoss        | 0     
---6 | domain_loss         | CrossEntropyLoss     | 0     
---7 | agg_loss            | AggregatorLoss       | 0     
---8 | punct_class_report  | ClassificationReport | 0     
---9 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---101 K     Trainable params
---13.4 M    Non-trainable params
---13.6 M    Total params
---Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/lr_find_temp_model.ckpt
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | dropout             | Dropout              | 0     
---2 | mlp                 | MultiLayerPerceptron | 65.8 K
---3 | punct_classifier    | TokenClassifier      | 2.6 K 
---4 | domain_classifier   | SequenceClassifier   | 257   
---5 | punctuation_loss    | FocalDiceLoss        | 0     
---6 | domain_loss         | CrossEntropyLoss     | 0     
---7 | agg_loss            | AggregatorLoss       | 0     
---8 | punct_class_report  | ClassificationReport | 0     
---9 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---101 K     Trainable params
---13.4 M    Non-trainable params
---13.6 M    Total params
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/nemo_error_log.txt
--deleted file mode 100644
--index e026dd1..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/nemo_error_log.txt
--+++ /dev/null
--@@ -1,22 +0,0 @@
---[NeMo W 2021-02-08 11:24:45 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-08 11:24:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:24:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:24:58 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:28:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:28:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:28:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:28:44 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/nemo_log_globalrank-0_localrank-0.txt
--deleted file mode 100644
--index b012129..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/nemo_log_globalrank-0_localrank-0.txt
--+++ /dev/null
--@@ -1,24 +0,0 @@
---[NeMo I 2021-02-08 11:24:45 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45
---[NeMo I 2021-02-08 11:24:45 exp_manager:519] TensorboardLogger has been set up
---[NeMo W 2021-02-08 11:24:45 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-08 11:24:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:24:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:24:58 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:28:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:28:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:28:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:28:44 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
--diff --git a/README.md b/README.md
--index 51ab30b..ccce635 100644
----- a/README.md
--+++ b/README.md
--@@ -167,17 +167,18 @@ label            | precision    | recall   | f1     | support
-- … (label_id: 9)  | 0.00         | 0.00     | 0.00   | 66
-- 
-- ###Focal DistilBERT gamma 3 0 unfrozen ted
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                         100.00      51.29      67.80       4118
---! (label_id: 1)                                          0.00       0.00       0.00         91
---, (label_id: 2)                                          0.00       0.00       0.00      13953
---- (label_id: 3)                                         94.27      46.49      62.27       1310
---. (label_id: 4)                                         39.51      99.94      56.63      12142
---: (label_id: 5)                                          0.00       0.00       0.00        254
---; (label_id: 6)                                          0.00       0.00       0.00         79
---? (label_id: 7)                                          0.00       0.00       0.00        905
---— (label_id: 8)                                          0.00       0.00       0.00        566
---… (label_id: 9)                                          0.00       0.00       0.00         52
--+label               | precision   | recall | f1    | support   
--+---|---|---|---|---
--+ (label_id: 0)      | 100.00      | 51.29  | 67.80 | 4118
--+! (label_id: 1)     | 0.00        | 0.00   | 0.00  | 91
--+, (label_id: 2)     | 0.00        | 0.00   | 0.00  | 13953
--+\- (label_id: 3)    | 94.27       | 46.49  | 62.27 | 1310
--+. (label_id: 4)     | 39.51       | 99.94  | 56.63 | 12142
--+: (label_id: 5)     | 0.00        | 0.00   | 0.00  | 254
--+; (label_id: 6)     | 0.00        | 0.00   | 0.00  | 79
--+? (label_id: 7)     | 0.00        | 0.00   | 0.00  | 905
--+— (label_id: 8)     | 0.00        | 0.00   | 0.00  | 566
--+… (label_id: 9)     | 0.00        | 0.00   | 0.00  | 52
-- 
-- ## Observations
-- - CRF tends to perform better on higher proportion classes like blank, comma and period without class weights.
--@@ -195,7 +196,7 @@ label                 |   precision  |  recall |    f1    |      support
--  (label_id: 0)        |      79.50   |   29.94 |   43.50  |    5026
-- ! (label_id: 1)       |       6.84   |   20.59 |   10.27  |     102
-- , (label_id: 2)       |      50.70   |   60.09 |   55.00  |   17571
---- (label_id: 3)       |      64.45   |   82.11 |   72.22  |    1526
--+\- (label_id: 3)       |      64.45   |   82.11 |   72.22  |    1526
-- . (label_id: 4)       |      57.40   |   49.43 |   53.12  |   14767
-- : (label_id: 5)       |      17.86   |   31.83 |   22.89  |     289
-- ; (label_id: 6)       |       1.50   |    5.88 |    2.39  |      85
--@@ -220,7 +221,7 @@ label                 |   precision  |  recall |    f1    |      support
--  (label_id: 0)             |     0.00  |  0.00 |   0.00  |  5026
-- ! (label_id: 1)            |     0.00  |  0.00 |   0.00  |   102
-- , (label_id: 2)            |    42.79  | 47.54 |  45.04  | 17571
---- (label_id: 3)            |    73.63  | 80.87 |  77.08  |  1526
--+\- (label_id: 3)            |    73.63  | 80.87 |  77.08  |  1526
-- . (label_id: 4)            |    47.36  | 55.16 |  50.96  | 14767
-- : (label_id: 5)            |    10.88  | 27.68 |  15.62  |   289
-- ; (label_id: 6)            |     0.00  |  0.00 |   0.00  |    85
--@@ -245,7 +246,7 @@ label                  |  precision | recall |   f1   |     support
--  (label_id: 0)         |     59.35  |  52.35 |  55.63 |   7314
-- ! (label_id: 1)        |      0.00  |   0.00 |   0.00 |    154
-- , (label_id: 2)        |     44.15  |  82.80 |  57.59 |  28180
---- (label_id: 3)        |      3.91  |   2.02 |   2.66 |   1933
--+\- (label_id: 3)        |      3.91  |   2.02 |   2.66 |   1933
-- . (label_id: 4)        |     39.91  |  11.64 |  18.02 |  24624
-- : (label_id: 5)        |      0.00  |   0.00 |   0.00 |    522
-- ; (label_id: 6)        |      0.00  |   0.00 |   0.00 |    485
--@@ -268,7 +269,7 @@ label                |  precision | recall |   f1   |    support
--  (label_id: 0)       |  62.32   | 99.78 |  76.72  |   7314
-- ! (label_id: 1)      |   0.00   |  0.00 |   0.00  |    154
-- , (label_id: 2)      |  49.81   |  4.72 |   8.62  |  28180
---- (label_id: 3)      |   5.91   | 28.35 |   9.78  |   1933
--+\- (label_id: 3)      |   5.91   | 28.35 |   9.78  |   1933
-- . (label_id: 4)      |  41.80   | 52.40 |  46.50  |  24624
-- : (label_id: 5)      |   0.94   |  4.02 |   1.53  |    522
-- ; (label_id: 6)      |   0.00   |  0.00 |   0.00  |    485
--@@ -296,7 +297,7 @@ label                |  precision | recall |   f1   |    support
--   (label_id: 0)        | 0.00     | 0.00    | 0.00  | 5704
-- ! (label_id: 1)        | 0.00     | 0.00    | 0.00  | 110
-- , (label_id: 2)        | 0.00     | 0.00    | 0.00  | 19711
---- (label_id: 3)        | 6.82     | 29.32   | 11.07 | 1702
--+\- (label_id: 3)        | 6.82     | 29.32   | 11.07 | 1702
-- . (label_id: 4)        | 37.30    | 83.82   | 51.62 | 18406
-- : (label_id: 5)        | 0.00     | 0.00    | 0.00  | 379
-- ; (label_id: 6)        | 0.00     | 0.00    | 0.00  | 190
--@@ -324,7 +325,7 @@ label                |  precision | recall |   f1   |    support
--  (label_id: 0)       |   0.00  | 0.00     | 0.00  | 7470
-- ! (label_id: 1)      |   0.00  | 0.00     | 0.00  | 148
-- , (label_id: 2)      |   0.00  | 0.00     | 0.00  | 28513
---- (label_id: 3)      |   3.02  | 100.00   | 5.86  | 2074
--+\- (label_id: 3)      |   3.02  | 100.00   | 5.86  | 2074
-- . (label_id: 4)      |   0.00  | 0.00     | 0.00  | 25120
-- : (label_id: 5)      |   0.00  | 0.00     | 0.00  | 570
-- ; (label_id: 6)      |   0.00  | 0.00     | 0.00  | 534
--@@ -344,7 +345,7 @@ label                |  precision | recall |   f1   |    support
--  (label_id: 0)      | 62.15  | 100.00   | 76.66   | 5154
-- ! (label_id: 1)     | 0.00   | 0.00     | 0.00    | 108
-- , (label_id: 2)     | 0.00   | 0.00     | 0.00    | 18022
---- (label_id: 3)     | 0.00   | 0.00     | 0.00    | 1557
--+\- (label_id: 3)     | 0.00   | 0.00     | 0.00    | 1557
-- . (label_id: 4)     | 41.74  | 94.01    | 57.81   | 15164
-- : (label_id: 5)     | 0.00   | 0.00     | 0.00    | 319
-- ; (label_id: 6)     | 0.00   | 0.00     | 0.00    | 88
--diff --git a/experiment/config.yaml b/experiment/config.yaml
--index b137ae8..1672082 100644
----- a/experiment/config.yaml
--+++ b/experiment/config.yaml
--@@ -105,12 +105,6 @@ model:
--         config_file: null # json file, precedence over config
--         config: null
--         # unfrozen_layers: 1
---
---    mlp:
---        num_fc_layers: 1
---        fc_dropout: 0.1
---        log_softmax: false
---        activation: 'relu'
--         
--     punct_head:
--         punct_num_fc_layers: 1
--diff --git a/experiment/info.log b/experiment/info.log
--index bd124fd..e69de29 100644
--Binary files a/experiment/info.log and b/experiment/info.log differ
--diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
--index 43fc93d..2f33d3c 100644
----- a/experiment/models/punctuation_domain_model.py
--+++ b/experiment/models/punctuation_domain_model.py
--@@ -62,15 +62,6 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
--         else:
--             self.hparams.model.punct_class_weights=None
-- 
---        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
---        self.mlp = MultiLayerPerceptron(
---            self.transformer.config.hidden_size,
---            self.transformer.config.hidden_size,
---            num_layers=self.hparams.model.mlp.num_fc_layers, 
---            activation=self.hparams.model.mlp.activation, 
---            log_softmax=self.hparams.model.mlp.log_softmax
---        )
---
--         self.punct_classifier = TokenClassifier(
--             hidden_size=self.transformer.config.hidden_size,
--             num_classes=len(self.labels_to_ids),
--@@ -134,8 +125,6 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
--         hidden_states = self.transformer(
--             input_ids=input_ids, attention_mask=attention_mask
--         )[0]
---        hidden_states = self.dropout(hidden_states)
---        hidden_states = self.mlp(hidden_states)
--         punct_logits = self.punct_classifier(hidden_states=hidden_states)
--         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
--         domain_logits = self.domain_classifier(
--@@ -669,6 +658,13 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-- 
--         for layer in list(encoder.layer)[n:]:
--             set_requires_grad_for_module(layer, True)
--+        
--+        # Set output layer to true.
--+        last_iter=iter(encoder.layer[-1].children())
--+        last = next(last_iter)
--+        for last in last_iter:
--+            continue
--+        set_requires_grad_for_module(last, True)
-- 
--     def freeze(self) -> None:
--         try:
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/hparams.yaml
-deleted file mode 100644
-index e18b518..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/hparams.yaml
-+++ /dev/null
-@@ -1,109 +0,0 @@
--seed: 42
--trainer:
--  gpus: 1
--  num_nodes: 1
--  max_epochs: 10
--  max_steps: null
--  accumulate_grad_batches: 4
--  gradient_clip_val: 0
--  amp_level: O1
--  precision: 16
--  accelerator: ddp
--  checkpoint_callback: false
--  logger: false
--  log_every_n_steps: 1
--  val_check_interval: 1.0
--  resume_from_checkpoint: null
--exp_manager:
--  exp_dir: /home/nxingyu/project/
--  name: Punctuation_with_Domain_discriminator
--  create_tensorboard_logger: true
--  create_checkpoint_callback: true
--base_path: /home/nxingyu/data
--tmp_path: /home/nxingyu/data/tmp
--model:
--  nemo_path: null
--  transformer_path: google/electra-small-discriminator
--  unfrozen: 0
--  maximum_unfrozen: 1
--  unfreeze_step: 1
--  punct_label_ids:
--  - ''
--  - '!'
--  - ','
--  - '-'
--  - .
--  - ':'
--  - ;
--  - '?'
--  - —
--  - …
--  punct_class_weights: false
--  dataset:
--    data_dir: /home/nxingyu/data
--    labelled:
--    - /home/nxingyu/data/ted_talks_processed
--    unlabelled: null
--    max_seq_length: 128
--    pad_label: ''
--    ignore_extra_tokens: false
--    ignore_start_end: false
--    use_cache: false
--    num_workers: 0
--    pin_memory: true
--    drop_last: false
--    num_labels: 10
--    num_domains: 1
--    test_unlabelled: true
--    train_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 8
--    validation_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 8
--  tokenizer:
--    tokenizer_name: google/electra-small-discriminator
--    vocab_file: null
--    tokenizer_model: null
--    special_tokens: null
--  language_model:
--    pretrained_model_name: google/electra-small-discriminator
--    lm_checkpoint: null
--    config_file: null
--    config: null
--  punct_head:
--    punct_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: dice
--  domain_head:
--    domain_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: cel
--    gamma: 0.1
--    pooling: mean
--    idx_conditioned_on: 0
--  dice_loss:
--    epsilon: 0.01
--    alpha: 3
--    macro_average: true
--  focal_loss:
--    gamma: 5
--  optim:
--    name: adamw
--    lr: 0.001
--    weight_decay: 0.0
--    sched:
--      name: WarmupAnnealing
--      warmup_steps: null
--      warmup_ratio: 0.1
--      last_epoch: -1
--      monitor: val_loss
--      reduce_on_plateau: false
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/lightning_logs.txt
-deleted file mode 100644
-index 935a28a..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/lightning_logs.txt
-+++ /dev/null
-@@ -1,97 +0,0 @@
--Global seed set to 42
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
--Using native 16bit precision.
--Global seed set to 42
--initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 257   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--298 K     Trainable params
--13.2 M    Non-trainable params
--13.5 M    Total params
--Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/lr_find_temp_model.ckpt
--Global seed set to 42
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 257   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--298 K     Trainable params
--13.2 M    Non-trainable params
--13.5 M    Total params
--Epoch 0, global step 100: val_loss reached 0.58924 (best 0.58924), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.59-epoch=0.ckpt" as top 3
--Epoch 1, global step 200: val_loss reached 0.58555 (best 0.58555), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.59-epoch=1.ckpt" as top 3
--Epoch 2, global step 300: val_loss reached 0.55571 (best 0.55571), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.56-epoch=2.ckpt" as top 3
--Epoch 3, global step 400: val_loss reached 0.55282 (best 0.55282), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.55-epoch=3.ckpt" as top 3
--Epoch 4, global step 500: val_loss reached 0.54534 (best 0.54534), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.55-epoch=4.ckpt" as top 3
--Epoch 5, global step 600: val_loss reached 0.55121 (best 0.54534), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.55-epoch=5.ckpt" as top 3
--Epoch 6, global step 700: val_loss reached 0.51013 (best 0.51013), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.51-epoch=6.ckpt" as top 3
--Epoch 7, global step 800: val_loss reached 0.51113 (best 0.51013), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.51-epoch=7.ckpt" as top 3
--Epoch 8, global step 900: val_loss reached 0.50036 (best 0.50036), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.50-epoch=8.ckpt" as top 3
--Epoch 9, global step 1000: val_loss reached 0.47973 (best 0.47973), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.48-epoch=9.ckpt" as top 3
--Global seed set to 42
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 257   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--825 K     Trainable params
--12.7 M    Non-trainable params
--13.5 M    Total params
--LR finder stopped early due to diverging loss.
--Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/lr_find_temp_model.ckpt
--Global seed set to 42
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 257   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--825 K     Trainable params
--12.7 M    Non-trainable params
--13.5 M    Total params
--Epoch 0, global step 1101: val_loss reached 0.35265 (best 0.35265), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.35-epoch=0.ckpt" as top 3
--Epoch 1, global step 1201: val_loss reached 0.33861 (best 0.33861), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.34-epoch=1.ckpt" as top 3
--Epoch 2, global step 1301: val_loss reached 0.28917 (best 0.28917), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.29-epoch=2.ckpt" as top 3
--Epoch 3, global step 1401: val_loss reached 0.28519 (best 0.28519), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.29-epoch=3.ckpt" as top 3
--Epoch 4, global step 1501: val_loss reached 0.29089 (best 0.28519), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.29-epoch=4.ckpt" as top 3
--Epoch 5, step 1601: val_loss was not in top 3
--Epoch 6, global step 1701: val_loss reached 0.26082 (best 0.26082), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=6.ckpt" as top 3
--Epoch 7, global step 1801: val_loss reached 0.25499 (best 0.25499), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.25-epoch=7.ckpt" as top 3
--Epoch 8, global step 1901: val_loss reached 0.26339 (best 0.25499), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=8.ckpt" as top 3
--Epoch 9, global step 2001: val_loss reached 0.24793 (best 0.24793), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.25-epoch=9.ckpt" as top 3
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--Using environment variable NODE_RANK for node rank (0).
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/nemo_error_log.txt
-deleted file mode 100644
-index 0b8eee4..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/nemo_error_log.txt
-+++ /dev/null
-@@ -1,37 +0,0 @@
--[NeMo W 2021-02-08 11:57:58 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-08 11:58:06 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 11:58:06 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 11:58:08 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 12:01:44 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 12:01:44 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 12:01:45 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 12:01:49 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-08 12:06:25 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f72b07adb20> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-08 12:06:48 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f72baba88b0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-08 12:57:49 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 13:48:48 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 13:49:11 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f72baba89d0> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
--      warnings.warn(warn_msg)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/nemo_log_globalrank-0_localrank-0.txt
-deleted file mode 100644
-index bcd5542..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/nemo_log_globalrank-0_localrank-0.txt
-+++ /dev/null
-@@ -1,39 +0,0 @@
--[NeMo I 2021-02-08 11:57:58 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58
--[NeMo I 2021-02-08 11:57:58 exp_manager:519] TensorboardLogger has been set up
--[NeMo W 2021-02-08 11:57:58 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-08 11:58:06 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 11:58:06 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 11:58:08 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 12:01:44 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 12:01:44 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 12:01:45 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 12:01:49 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-08 12:06:25 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f72b07adb20> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-08 12:06:48 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f72baba88b0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-08 12:57:49 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 13:48:48 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 13:49:11 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f72baba89d0> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
--      warnings.warn(warn_msg)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/cmd-args.log
-deleted file mode 100644
-index 11a5d8e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/cmd-args.log
-+++ /dev/null
-@@ -1 +0,0 @@
--main.py
-\ No newline at end of file
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/events.out.tfevents.1612767597.Titan.4388.0 b/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/events.out.tfevents.1612767597.Titan.4388.0
-deleted file mode 100644
-index 3e35b7d..0000000
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/events.out.tfevents.1612767597.Titan.4388.0 and /dev/null differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/git-info.log
-deleted file mode 100644
-index 1b02705..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/git-info.log
-+++ /dev/null
-@@ -1,2075 +0,0 @@
--commit hash: 66d59fddd871d29e1ceab6de212c3812501bf786
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/cmd-args.log
--deleted file mode 100644
--index 11a5d8e..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/cmd-args.log
--+++ /dev/null
--@@ -1 +0,0 @@
---main.py
--\ No newline at end of file
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/events.out.tfevents.1612753856.Titan.19011.0 b/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/events.out.tfevents.1612753856.Titan.19011.0
--deleted file mode 100644
--index 481eef2..0000000
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/events.out.tfevents.1612753856.Titan.19011.0 and /dev/null differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/git-info.log
--deleted file mode 100644
--index daa2b33..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/git-info.log
--+++ /dev/null
--@@ -1,627 +0,0 @@
---commit hash: 939a671c8c117db6975316767ced5d95449e2b27
---diff --git a/README.md b/README.md
---index d52dba5..0b403d3 100644
------ a/README.md
---+++ b/README.md
---@@ -333,4 +333,71 @@ label                                                precision    recall       f
---  'punct_f1': 0.5858508944511414,
---  'punct_precision': 0.30176490545272827,
---  'punct_recall': 10.0,
---- 'test_loss': 0.8140875697135925}
---\ No newline at end of file
---+ 'test_loss': 0.8140875697135925}
---+
---+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
---+
---+ (label_id: 0)                                          62.15     100.00      76.66       5154
---+! (label_id: 1)                                          0.00       0.00       0.00        108
---+, (label_id: 2)                                          0.00       0.00       0.00      18022
---+- (label_id: 3)                                          0.00       0.00       0.00       1557
---+. (label_id: 4)                                         41.74      94.01      57.81      15164
---+: (label_id: 5)                                          0.00       0.00       0.00        319
---+; (label_id: 6)                                          0.00       0.00       0.00         88
---+? (label_id: 7)                                          0.00       0.00       0.00       1217
---+ (label_id: 8)                                          0.00       0.00       0.00        752
---+… (label_id: 9)                                          0.00       0.00       0.00         67
---+-------------------
---+micro avg                                               45.72      45.72      45.72      42448
---+macro avg                                               10.39      19.40      13.45      42448
---+weighted avg                                            22.46      45.72      29.96      42448
---+{ 'punct_f1': 13.446383476257324,
---+ 'punct_precision': 10.388500213623047,
---+ 'punct_recall': 19.400554656982422,
---+ 'test_loss': 0.44148480892181396}
---+
---+
---+ ## Log for 8/2/2021
---+
---+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
---+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
---+
---+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
---+End frozen 
---+micro avg                                               41.42      41.42      41.42      33406
---+macro avg                                               11.01      13.54      11.07      33406
---+weighted avg                                            34.88      41.42      34.20      33406
---+
---+1st layer best lr 1e-10, set to 0.007943282347242822
---+micro avg                                               36.65      36.65      36.65      33463
---+macro avg                                               10.71       9.91       8.26      33463
---+weighted avg                                            34.32      36.65      31.08      33463
---+
---+2nd layer best lr 1e-10, set to 0.007943282347242822
---+micro avg                                               35.72      35.72      35.72      42448
---+macro avg                                                3.57      10.00       5.26      42448
---+weighted avg                                            12.76      35.72      18.81      42448
---+
---+{'punct_f1': 5.264181137084961,
---+ 'punct_precision': 3.572371006011963,
---+ 'punct_recall': 10.0,
---+ 'test_loss': 18.49854850769043}
---+
---+
---+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
---+alpha from 3->4 seems to reduce convergence rate.
---+
---+micro avg                                               50.98      50.98      50.98      33463
---+macro avg                                               25.99      25.38      23.38      33463
---+weighted avg                                            50.31      50.98      48.27      33463
---+
---+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
---+micro avg                                               58.55      58.55      58.55      39340
---+macro avg                                               30.02      29.74      29.52      39340
---+weighted avg                                            57.91      58.55      57.51      39340
---+
---+still increasing?!
---+{'punct_f1': 29.523975372314453,
---+ 'punct_precision': 30.015613555908203,
---+ 'punct_recall': 29.738296508789062,
---+ 'test_loss': 0.3690211772918701}
---diff --git a/experiment/config.yaml b/experiment/config.yaml
---index fe58670..a4a012a 100644
------ a/experiment/config.yaml
---+++ b/experiment/config.yaml
---@@ -1,49 +1,49 @@
--- seed: 42
--- trainer:
----    # gpus: 1 # the number of gpus, 0 for CPU
----    # num_nodes: 1
----    # max_epochs: 2
----    # max_steps: null # precedence over max_epochs
----    # accumulate_grad_batches: 4 # accumulates grads every k batches
----    # gradient_clip_val: 0
----    # amp_level: O1 # O1/O2 for mixed precision
----    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----    # accelerator: ddp
----    # checkpoint_callback: false  # Provided by exp_manager
----    # logger: false #false  # Provided by exp_manager
----    # log_every_n_steps: 1  # Interval of logging.
----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----    # resume_from_checkpoint: null
----
----    gpus: 0 # the number of gpus, 0 for CPU
---+    gpus: 1 # the number of gpus, 0 for CPU
---     num_nodes: 1
----    max_epochs: 8
---+    max_epochs: 10
---     max_steps: null # precedence over max_epochs
----    accumulate_grad_batches: 1 # accumulates grads every k batches
---+    accumulate_grad_batches: 4 # accumulates grads every k batches
---     gradient_clip_val: 0
----    amp_level: O0 # O1/O2 for mixed precision
----    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----    # accelerator: ddp
---+    amp_level: O1 # O1/O2 for mixed precision
---+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
---+    accelerator: ddp
---     checkpoint_callback: false  # Provided by exp_manager
---     logger: false #false  # Provided by exp_manager
---     log_every_n_steps: 1  # Interval of logging.
---     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----    reload_dataloaders_every_epoch: true
---     resume_from_checkpoint: null
--- 
---+    # gpus: 0 # the number of gpus, 0 for CPU
---+    # num_nodes: 1
---+    # max_epochs: 8
---+    # max_steps: null # precedence over max_epochs
---+    # accumulate_grad_batches: 1 # accumulates grads every k batches
---+    # gradient_clip_val: 0
---+    # amp_level: O0 # O1/O2 for mixed precision
---+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
---+    # # accelerator: ddp
---+    # checkpoint_callback: false  # Provided by exp_manager
---+    # logger: false #false  # Provided by exp_manager
---+    # log_every_n_steps: 1  # Interval of logging.
---+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
---+    # reload_dataloaders_every_epoch: true
---+    # resume_from_checkpoint: null
---+
--- exp_manager:
----    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
---+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
---     name: Punctuation_with_Domain_discriminator  # The name of your model
---     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
---     create_checkpoint_callback: true 
----base_path: /home/nxingyu2/data # /root/data # 
----tmp_path: /home/nxingyu2/data/tmp # /tmp # 
---+base_path: /home/nxingyu/data # /root/data # 
---+tmp_path: /home/nxingyu/data/tmp # /tmp # 
--- 
--- model:
---     nemo_path: null
----    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
---+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
---     unfrozen: 0
----    maximum_unfrozen: 2
---+    maximum_unfrozen: 1
---     unfreeze_step: 1
---     # unfreeze_every: 3
---     punct_label_ids:
---@@ -58,10 +58,10 @@ model:
---         - "—"
---         - "…"
--- 
----    punct_class_weights: true
---+    punct_class_weights: false
---     
---     dataset:
----        data_dir: /home/nxingyu2/data # /root/data # 
---+        data_dir: /home/nxingyu/data # /root/data # 
---         labelled:
---             - ${base_path}/ted_talks_processed #
---         unlabelled:
---@@ -106,6 +106,12 @@ model:
---         config: null
---         # unfrozen_layers: 1
--- 
---+    mlp:
---+        num_fc_layers: 2
---+        fc_dropout: 0.1
---+        log_softmax: false
---+        activation: 'relu'
---+        
---     punct_head:
---         punct_num_fc_layers: 1
---         fc_dropout: 0.1
---@@ -122,17 +128,19 @@ model:
---         use_transformer_init: true
---         loss: 'cel'
---         gamma: 0.1 # coefficient of gradient reversal
---+        pooling: 'mean'
---+        idx_conditioned_on: 0
---     
---     dice_loss:
---         epsilon: 0.01
----        alpha: 3
---+        alpha: 4
---         macro_average: true
--- 
---     focal_loss: 
---         gamma: 5
--- 
---     optim:
----        name: novograd
---+        name: adamw
---         lr: 1e-3
---         weight_decay: 0.00
---         sched:
---diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
---index d4ff927..f9927ac 100644
------ a/experiment/core/layers/sequence_classifier.py
---+++ b/experiment/core/layers/sequence_classifier.py
---@@ -1,3 +1,4 @@
---+import torch
--- from torch import nn
--- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
--- from core.utils import transformer_weights_init
---@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
---         log_softmax: bool = True,
---         dropout: float = 0.0,
---         use_transformer_init: bool = True,
----        idx_conditioned_on: int = 0,
---+        pooling: str = 'mean', # mean, max, mean_max, token
---+        idx_conditioned_on: int = None,
---     ):
---         """
---         Initializes the SequenceClassifier module.
---@@ -37,8 +39,9 @@ class SequenceClassifier(nn.Module):
---         super().__init__()
---         self.log_softmax = log_softmax
---         self._idx_conditioned_on = idx_conditioned_on
---+        self.pooling = pooling
---         self.mlp = MultiLayerPerceptron(
----            hidden_size=hidden_size,
---+            hidden_size=(hidden_size*2 if pooling=='mean_max' else hidden_size),
---             num_classes=num_classes,
---             num_layers=num_layers,
---             activation=activation,
---@@ -48,7 +51,23 @@ class SequenceClassifier(nn.Module):
---         if use_transformer_init:
---             self.apply(lambda module: transformer_weights_init(module, xavier=False))
--- 
----    def forward(self, hidden_states):
---+    def forward(self, hidden_states, subtoken_mask=None):
---         hidden_states = self.dropout(hidden_states)
----        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
---+        if self.pooling=='token':
---+            pooled = hidden_states[:, self._idx_conditioned_on]
---+        else:
---+            if subtoken_mask==None:
---+                ct=hidden_states.shape[1] # Seq len
---+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
---+            else:
---+                ct = torch.sum(subtoken_mask,axis=1).unsqueeze(1)
---+            pooled_sum = torch.sum(hidden_states,axis=1)            
---+            if self.pooling=='mean' or self.pooling == 'mean_max':
---+                pooled_mean = torch.div(pooled_sum,ct)
---+            if self.pooling=='max' or self.pooling=='mean_max':
---+                pooled_max = torch.max(hidden_states,axis=1)[0]
---+            pooled=pooled_mean if self.pooling=='mean' else \
---+                pooled_max if self.pooling=='max' else \
---+                    torch.cat([pooled_mean,pooled_max],axis=-1)
---+        logits = self.mlp(pooled)
---         return logits
---diff --git a/experiment/info.log b/experiment/info.log
---index 2471fe9..e69de29 100644
------ a/experiment/info.log
---+++ b/experiment/info.log
---@@ -1,300 +0,0 @@
----[INFO] - GPU available: True, used: False
----[INFO] - TPU available: None, using: 0 TPU cores
----[INFO] - shuffling train set
----[INFO] - Optimizer config = Novograd (
----Parameter Group 0
----    amsgrad: False
----    betas: (0.95, 0.98)
----    eps: 1e-08
----    grad_averaging: False
----    lr: 0.001
----    weight_decay: 0.0
----)
----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
----will be used during training (effective maximum steps = 80) - 
----Parameters : 
----(warmup_steps: null
----warmup_ratio: 0.1
----last_epoch: -1
----max_steps: 80
----)
----[INFO] - 
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 108 M 
----1 | punct_classifier    | TokenClassifier      | 7.7 K 
----2 | domain_classifier   | SequenceClassifier   | 769   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----8.5 K     Trainable params
----108 M     Non-trainable params
----108 M     Total params
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.00       0.00       0.00        184
----! (label_id: 1)                                          0.00       0.00       0.00          4
----, (label_id: 2)                                          1.23       0.34       0.53        594
----- (label_id: 3)                                          3.06      25.42       5.46         59
----. (label_id: 4)                                         47.22      12.98      20.36        524
----: (label_id: 5)                                          0.00       0.00       0.00         18
----; (label_id: 6)                                          0.00       0.00       0.00         13
----? (label_id: 7)                                          8.45       6.32       7.23         95
----— (label_id: 8)                                          0.00       0.00       0.00         12
----… (label_id: 9)                                          0.00       0.00       0.00          0
-----------------------
----micro avg                                                6.05       6.05       6.05       1503
----macro avg                                                6.66       5.01       3.73       1503
----weighted avg                                            17.61       6.05       7.98       1503
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00         92
-----------------------
----micro avg                                              100.00     100.00     100.00         92
----macro avg                                              100.00     100.00     100.00         92
----weighted avg                                           100.00     100.00     100.00         92
----
----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
----[INFO] - Optimizer config = Novograd (
----Parameter Group 0
----    amsgrad: False
----    betas: (0.95, 0.98)
----    eps: 1e-08
----    grad_averaging: False
----    lr: 1.5848931924611143e-08
----    weight_decay: 0.0
----)
----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
----will be used during training (effective maximum steps = 3192) - 
----Parameters : 
----(warmup_steps: null
----warmup_ratio: 0.1
----last_epoch: -1
----max_steps: 3192
----)
----[INFO] - 
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 108 M 
----1 | punct_classifier    | TokenClassifier      | 7.7 K 
----2 | domain_classifier   | SequenceClassifier   | 769   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----8.5 K     Trainable params
----108 M     Non-trainable params
----108 M     Total params
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.00       0.00       0.00        202
----! (label_id: 1)                                          0.00       0.00       0.00          4
----, (label_id: 2)                                          1.62       0.45       0.70        669
----- (label_id: 3)                                          3.48      27.27       6.17         66
----. (label_id: 4)                                         45.06      13.01      20.19        561
----: (label_id: 5)                                          1.52       6.67       2.47         15
----; (label_id: 6)                                          0.00       0.00       0.00         15
----? (label_id: 7)                                          8.70       7.32       7.95         82
----— (label_id: 8)                                          0.00       0.00       0.00         13
----… (label_id: 9)                                          0.00       0.00       0.00          1
-----------------------
----micro avg                                                6.20       6.20       6.20       1628
----macro avg                                                6.04       5.47       3.75       1628
----weighted avg                                            16.79       6.20       7.92       1628
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00        101
-----------------------
----micro avg                                              100.00     100.00     100.00        101
----macro avg                                              100.00     100.00     100.00        101
----weighted avg                                           100.00     100.00     100.00        101
----
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.34       0.73       0.46       4402
----! (label_id: 1)                                          0.42      13.95       0.82        129
----, (label_id: 2)                                          2.53       0.64       1.03      15243
----- (label_id: 3)                                          2.45      21.03       4.38       1322
----. (label_id: 4)                                         44.00      11.40      18.11      12542
----: (label_id: 5)                                          0.43       1.41       0.65        354
----; (label_id: 6)                                          0.00       0.00       0.00        163
----? (label_id: 7)                                          4.16       6.27       5.00       1117
----— (label_id: 8)                                          3.00       0.61       1.02        488
----… (label_id: 9)                                          0.97       6.17       1.68         81
-----------------------
----micro avg                                                5.41       5.41       5.41      35841
----macro avg                                                5.83       6.22       3.32      35841
----weighted avg                                            16.78       5.41       7.18      35841
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2201
-----------------------
----micro avg                                              100.00     100.00     100.00       2201
----macro avg                                              100.00     100.00     100.00       2201
----weighted avg                                           100.00     100.00     100.00       2201
----
----[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.20       0.43       0.27       4226
----! (label_id: 1)                                          0.44      14.17       0.86        127
----, (label_id: 2)                                          1.93       0.49       0.78      14611
----- (label_id: 3)                                          2.23      19.56       4.01       1237
----. (label_id: 4)                                         43.37      11.25      17.86      11977
----: (label_id: 5)                                          0.68       2.34       1.05        342
----; (label_id: 6)                                          0.00       0.00       0.00        129
----? (label_id: 7)                                          5.16       7.47       6.10       1058
----— (label_id: 8)                                          2.15       0.49       0.80        409
----… (label_id: 9)                                          0.69       4.23       1.19         71
-----------------------
----micro avg                                                5.23       5.23       5.23      34187
----macro avg                                                5.68       6.04       3.29      34187
----weighted avg                                            16.32       5.23       6.98      34187
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2113
-----------------------
----micro avg                                              100.00     100.00     100.00       2113
----macro avg                                              100.00     100.00     100.00       2113
----weighted avg                                           100.00     100.00     100.00       2113
----
----[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.28       0.61       0.39       4228
----! (label_id: 1)                                          0.30       8.28       0.58        145
----, (label_id: 2)                                          2.27       0.58       0.92      14495
----- (label_id: 3)                                          2.64      21.78       4.70       1327
----. (label_id: 4)                                         44.87      11.66      18.51      12193
----: (label_id: 5)                                          0.60       1.93       0.91        362
----; (label_id: 6)                                          0.00       0.00       0.00        164
----? (label_id: 7)                                          4.19       6.40       5.07       1078
----— (label_id: 8)                                          1.16       0.22       0.37        459
----… (label_id: 9)                                          0.85       4.17       1.41         96
-----------------------
----micro avg                                                5.54       5.54       5.54      34547
----macro avg                                                5.72       5.56       3.29      34547
----weighted avg                                            17.08       5.54       7.33      34547
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2114
-----------------------
----micro avg                                              100.00     100.00     100.00       2114
----macro avg                                              100.00     100.00     100.00       2114
----weighted avg                                           100.00     100.00     100.00       2114
----
----[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.29       0.63       0.40       4444
----! (label_id: 1)                                          0.38      10.67       0.74        150
----, (label_id: 2)                                          2.32       0.59       0.94      15290
----- (label_id: 3)                                          2.34      20.28       4.19       1292
----. (label_id: 4)                                         43.85      11.68      18.44      12599
----: (label_id: 5)                                          0.41       1.28       0.62        392
----; (label_id: 6)                                          0.00       0.00       0.00        164
----? (label_id: 7)                                          4.24       6.30       5.07       1111
----— (label_id: 8)                                          0.00       0.00       0.00        456
----… (label_id: 9)                                          0.38       2.41       0.65         83
-----------------------
----micro avg                                                5.40       5.40       5.40      35981
----macro avg                                                5.42       5.38       3.11      35981
----weighted avg                                            16.59       5.40       7.22      35981
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2222
-----------------------
----micro avg                                              100.00     100.00     100.00       2222
----macro avg                                              100.00     100.00     100.00       2222
----weighted avg                                           100.00     100.00     100.00       2222
----
----[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.35       0.73       0.48       3844
----! (label_id: 1)                                          0.54      14.62       1.04        130
----, (label_id: 2)                                          2.32       0.59       0.94      13056
----- (label_id: 3)                                          2.67      22.28       4.77       1194
----. (label_id: 4)                                         44.45      11.95      18.84      10791
----: (label_id: 5)                                          0.84       3.21       1.33        280
----; (label_id: 6)                                          0.00       0.00       0.00        140
----? (label_id: 7)                                          4.17       6.56       5.10        914
----— (label_id: 8)                                          0.00       0.00       0.00        401
----… (label_id: 9)                                          0.48       2.63       0.81         76
-----------------------
----micro avg                                                5.68       5.68       5.68      30826
----macro avg                                                5.58       6.26       3.33      30826
----weighted avg                                            16.82       5.68       7.41      30826
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       1922
-----------------------
----micro avg                                              100.00     100.00     100.00       1922
----macro avg                                              100.00     100.00     100.00       1922
----weighted avg                                           100.00     100.00     100.00       1922
----
----[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.28       0.60       0.39       3970
----! (label_id: 1)                                          0.35      10.66       0.68        122
----, (label_id: 2)                                          2.09       0.53       0.85      13469
----- (label_id: 3)                                          2.29      19.32       4.10       1201
----. (label_id: 4)                                         43.43      11.24      17.86      11227
----: (label_id: 5)                                          0.63       2.30       0.99        304
----; (label_id: 6)                                          0.00       0.00       0.00        141
----? (label_id: 7)                                          4.52       6.86       5.45       1006
----— (label_id: 8)                                          1.15       0.23       0.38        444
----… (label_id: 9)                                          0.45       2.67       0.78         75
-----------------------
----micro avg                                                5.26       5.26       5.26      31959
----macro avg                                                5.52       5.44       3.15      31959
----weighted avg                                            16.42       5.26       7.02      31959
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       1985
-----------------------
----micro avg                                              100.00     100.00     100.00       1985
----macro avg                                              100.00     100.00     100.00       1985
----weighted avg                                           100.00     100.00     100.00       1985
----
----[INFO] - Epoch 5, step 2394: val_loss was not in top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.23       0.48       0.31       4126
----! (label_id: 1)                                          0.29       9.40       0.56        117
----, (label_id: 2)                                          1.91       0.49       0.77      14019
----- (label_id: 3)                                          2.52      22.59       4.53       1164
----. (label_id: 4)                                         44.15      11.65      18.44      11789
----: (label_id: 5)                                          0.72       2.41       1.11        332
----; (label_id: 6)                                          0.56       0.61       0.58        165
----? (label_id: 7)                                          3.89       6.53       4.88        980
----— (label_id: 8)                                          2.30       0.47       0.77        430
----… (label_id: 9)                                          1.18       8.33       2.07         60
-----------------------
----micro avg                                                5.47       5.47       5.47      33182
----macro avg                                                5.77       6.30       3.40      33182
----weighted avg                                            16.77       5.47       7.25      33182
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2063
-----------------------
----micro avg                                              100.00     100.00     100.00       2063
----macro avg                                              100.00     100.00     100.00       2063
----weighted avg                                           100.00     100.00     100.00       2063
----
----[INFO] - Epoch 6, step 2793: val_loss was not in top 3
---diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
---index fa37b4c..43fc93d 100644
------ a/experiment/models/punctuation_domain_model.py
---+++ b/experiment/models/punctuation_domain_model.py
---@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---         else:
---             self.hparams.model.punct_class_weights=None
--- 
---+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
---+        self.mlp = MultiLayerPerceptron(
---+            self.transformer.config.hidden_size,
---+            self.transformer.config.hidden_size,
---+            num_layers=self.hparams.model.mlp.num_fc_layers, 
---+            activation=self.hparams.model.mlp.activation, 
---+            log_softmax=self.hparams.model.mlp.log_softmax
---+        )
---+
---         self.punct_classifier = TokenClassifier(
---             hidden_size=self.transformer.config.hidden_size,
---             num_classes=len(self.labels_to_ids),
---@@ -80,6 +89,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---             log_softmax=self.hparams.model.domain_head.log_softmax,
---             dropout=self.hparams.model.domain_head.fc_dropout,
---             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
---+            pooling=self.hparams.model.domain_head.pooling,
---+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
---         )
--- 
---         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
---@@ -119,14 +130,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
---         self.freeze()
--- 
----    def forward(self, input_ids, attention_mask, domain_ids=None):
---+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
---         hidden_states = self.transformer(
---             input_ids=input_ids, attention_mask=attention_mask
---         )[0]
---+        hidden_states = self.dropout(hidden_states)
---+        hidden_states = self.mlp(hidden_states)
---         punct_logits = self.punct_classifier(hidden_states=hidden_states)
---         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
---         domain_logits = self.domain_classifier(
----            hidden_states=reverse_grad_hidden_states)
---+            hidden_states=reverse_grad_hidden_states,
---+            subtoken_mask=subtoken_mask)
---         return punct_logits, domain_logits
--- 
---     def _make_step(self, batch):
---@@ -136,7 +150,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---         punct_labels = batch['labels']
---         domain_labels = batch['domain']
---         punct_logits, domain_logits = self(
----            input_ids=input_ids, attention_mask=attention_mask
---+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
---         )
---         punctuation_loss = self.punctuation_loss(
---             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/hparams.yaml
--deleted file mode 100644
--index ebcb726..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/hparams.yaml
--+++ /dev/null
--@@ -1,114 +0,0 @@
---seed: 42
---trainer:
---  gpus: 1
---  num_nodes: 1
---  max_epochs: 10
---  max_steps: null
---  accumulate_grad_batches: 4
---  gradient_clip_val: 0
---  amp_level: O1
---  precision: 16
---  accelerator: ddp
---  checkpoint_callback: false
---  logger: false
---  log_every_n_steps: 1
---  val_check_interval: 1.0
---  resume_from_checkpoint: null
---exp_manager:
---  exp_dir: /home/nxingyu/project/
---  name: Punctuation_with_Domain_discriminator
---  create_tensorboard_logger: true
---  create_checkpoint_callback: true
---base_path: /home/nxingyu/data
---tmp_path: /home/nxingyu/data/tmp
---model:
---  nemo_path: null
---  transformer_path: google/electra-small-discriminator
---  unfrozen: 0
---  maximum_unfrozen: 1
---  unfreeze_step: 1
---  punct_label_ids:
---  - ''
---  - '!'
---  - ','
---  - '-'
---  - .
---  - ':'
---  - ;
---  - '?'
---  - —
---  - …
---  punct_class_weights: false
---  dataset:
---    data_dir: /home/nxingyu/data
---    labelled:
---    - /home/nxingyu/data/ted_talks_processed
---    unlabelled: null
---    max_seq_length: 128
---    pad_label: ''
---    ignore_extra_tokens: false
---    ignore_start_end: false
---    use_cache: false
---    num_workers: 0
---    pin_memory: true
---    drop_last: false
---    num_labels: 10
---    num_domains: 1
---    test_unlabelled: true
---    train_ds:
---      shuffle: true
---      num_samples: -1
---      batch_size: 8
---    validation_ds:
---      shuffle: true
---      num_samples: -1
---      batch_size: 8
---  tokenizer:
---    tokenizer_name: google/electra-small-discriminator
---    vocab_file: null
---    tokenizer_model: null
---    special_tokens: null
---  language_model:
---    pretrained_model_name: google/electra-small-discriminator
---    lm_checkpoint: null
---    config_file: null
---    config: null
---  mlp:
---    num_fc_layers: 2
---    fc_dropout: 0.1
---    log_softmax: false
---    activation: relu
---  punct_head:
---    punct_num_fc_layers: 1
---    fc_dropout: 0.1
---    activation: relu
---    log_softmax: false
---    use_transformer_init: true
---    loss: dice
---  domain_head:
---    domain_num_fc_layers: 1
---    fc_dropout: 0.1
---    activation: relu
---    log_softmax: false
---    use_transformer_init: true
---    loss: cel
---    gamma: 0.1
---    pooling: mean
---    idx_conditioned_on: 0
---  dice_loss:
---    epsilon: 0.01
---    alpha: 4
---    macro_average: true
---  focal_loss:
---    gamma: 5
---  optim:
---    name: adamw
---    lr: 0.001
---    weight_decay: 0.0
---    sched:
---      name: WarmupAnnealing
---      warmup_steps: null
---      warmup_ratio: 0.1
---      last_epoch: -1
---      monitor: val_loss
---      reduce_on_plateau: false
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/lightning_logs.txt
--deleted file mode 100644
--index d3676e6..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/lightning_logs.txt
--+++ /dev/null
--@@ -1,46 +0,0 @@
---Global seed set to 42
---GPU available: True, used: True
---TPU available: None, using: 0 TPU cores
---LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
---Using native 16bit precision.
---Global seed set to 42
---initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | dropout             | Dropout              | 0     
---2 | mlp                 | MultiLayerPerceptron | 131 K 
---3 | punct_classifier    | TokenClassifier      | 2.6 K 
---4 | domain_classifier   | SequenceClassifier   | 257   
---5 | punctuation_loss    | FocalDiceLoss        | 0     
---6 | domain_loss         | CrossEntropyLoss     | 0     
---7 | agg_loss            | AggregatorLoss       | 0     
---8 | punct_class_report  | ClassificationReport | 0     
---9 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---167 K     Trainable params
---13.4 M    Non-trainable params
---13.6 M    Total params
---Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/lr_find_temp_model.ckpt
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | dropout             | Dropout              | 0     
---2 | mlp                 | MultiLayerPerceptron | 131 K 
---3 | punct_classifier    | TokenClassifier      | 2.6 K 
---4 | domain_classifier   | SequenceClassifier   | 257   
---5 | punctuation_loss    | FocalDiceLoss        | 0     
---6 | domain_loss         | CrossEntropyLoss     | 0     
---7 | agg_loss            | AggregatorLoss       | 0     
---8 | punct_class_report  | ClassificationReport | 0     
---9 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---167 K     Trainable params
---13.4 M    Non-trainable params
---13.6 M    Total params
---Epoch 0, global step 100: val_loss reached 0.87292 (best 0.87292), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=0.ckpt" as top 3
---Epoch 1, global step 200: val_loss reached 0.80471 (best 0.80471), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.80-epoch=1.ckpt" as top 3
---Epoch 2, global step 300: val_loss reached 0.80542 (best 0.80471), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.81-epoch=2.ckpt" as top 3
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/nemo_error_log.txt
--deleted file mode 100644
--index 5b76cfb..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/nemo_error_log.txt
--+++ /dev/null
--@@ -1,28 +0,0 @@
---[NeMo W 2021-02-08 11:07:07 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-08 11:07:16 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:07:16 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:07:18 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:10:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:10:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:10:57 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:11:01 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
---[NeMo W 2021-02-08 11:16:19 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7fe326ec1700> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-08 11:16:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7fe326ea8040> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/nemo_log_globalrank-0_localrank-0.txt
--deleted file mode 100644
--index 45ada6c..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/nemo_log_globalrank-0_localrank-0.txt
--+++ /dev/null
--@@ -1,30 +0,0 @@
---[NeMo I 2021-02-08 11:07:07 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07
---[NeMo I 2021-02-08 11:07:07 exp_manager:519] TensorboardLogger has been set up
---[NeMo W 2021-02-08 11:07:07 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-08 11:07:16 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:07:16 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:07:18 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:10:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:10:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:10:57 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:11:01 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
---[NeMo W 2021-02-08 11:16:19 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7fe326ec1700> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
---[NeMo W 2021-02-08 11:16:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7fe326ea8040> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
---      warnings.warn(warn_msg)
---    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/cmd-args.log
--deleted file mode 100644
--index 11a5d8e..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/cmd-args.log
--+++ /dev/null
--@@ -1 +0,0 @@
---main.py
--\ No newline at end of file
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/events.out.tfevents.1612754918.Titan.21528.0 b/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/events.out.tfevents.1612754918.Titan.21528.0
--deleted file mode 100644
--index 9596b1a..0000000
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/events.out.tfevents.1612754918.Titan.21528.0 and /dev/null differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/git-info.log
--deleted file mode 100644
--index e04e39e..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/git-info.log
--+++ /dev/null
--@@ -1,699 +0,0 @@
---commit hash: 939a671c8c117db6975316767ced5d95449e2b27
---diff --git a/README.md b/README.md
---index d52dba5..51ab30b 100644
------ a/README.md
---+++ b/README.md
---@@ -291,21 +291,22 @@ weighted avg         |  43.14   | 33.52 |  29.43  |  67486
---  0 layer not too much improvement, 1 layer pretty decent.
---  alpha 5 seems too high. to try full run 4 next.
--- layer 0 * 8 + layer 1 * 3
----
----  (label_id: 0)                                           0.00       0.00       0.00       5704
----! (label_id: 1)                                          0.00       0.00       0.00        110
----, (label_id: 2)                                          0.00       0.00       0.00      19711
----- (label_id: 3)                                          6.82      29.32      11.07       1702
----. (label_id: 4)                                         37.30      83.82      51.62      18406
----: (label_id: 5)                                          0.00       0.00       0.00        379
----; (label_id: 6)                                          0.00       0.00       0.00        190
----? (label_id: 7)                                          6.71       1.31       2.20       1446
----— (label_id: 8)                                          0.00       0.00       0.00       1227
----… (label_id: 9)                                          0.00       0.00       0.00         86
-----------------------
----micro avg                                               32.57      32.57      32.57      48961
----macro avg                                                5.08      11.44       6.49      48961
----weighted avg                                            14.46      32.57      19.86      48961
---+label                |  precision | recall |   f1   |    support
---+---|---|---|---|---
---+  (label_id: 0)        | 0.00     | 0.00    | 0.00  | 5704
---+! (label_id: 1)        | 0.00     | 0.00    | 0.00  | 110
---+, (label_id: 2)        | 0.00     | 0.00    | 0.00  | 19711
---+- (label_id: 3)        | 6.82     | 29.32   | 11.07 | 1702
---+. (label_id: 4)        | 37.30    | 83.82   | 51.62 | 18406
---+: (label_id: 5)        | 0.00     | 0.00    | 0.00  | 379
---+; (label_id: 6)        | 0.00     | 0.00    | 0.00  | 190
---+? (label_id: 7)        | 6.71     | 1.31    | 2.20  | 1446
---+— (label_id: 8)        | 0.00     | 0.00    | 0.00  | 1227
---+… (label_id: 9)        | 0.00     | 0.00    | 0.00  | 86
---+-------------------||||
---+micro avg              | 32.57    | 32.57   | 32.57 | 48961
---+macro avg              | 5.08     | 11.44   | 6.49  | 48961
---+weighted avg           | 14.46    | 32.57   | 19.86 | 48961
--- 
---  {'punct_f1': 6.104840278625488,
---  'punct_precision': 4.423948764801025,
---@@ -318,19 +319,96 @@ lr 0 : 0.008413951416451957
--- 1: 0.00031622776601683794 ** too high. to adjust the min to 1e-10?
--- 2: 0.00031622776601683794
--- 
----label                                                precision    recall       f1           support
---- (label_id: 0)                                           0.00       0.00       0.00       7470
----! (label_id: 1)                                          0.00       0.00       0.00        148
----, (label_id: 2)                                          0.00       0.00       0.00      28513
----- (label_id: 3)                                          3.02     100.00       5.86       2074
----. (label_id: 4)                                          0.00       0.00       0.00      25120
----: (label_id: 5)                                          0.00       0.00       0.00        570
----; (label_id: 6)                                          0.00       0.00       0.00        534
----? (label_id: 7)                                          0.00       0.00       0.00       2085
----— (label_id: 8)                                          0.00       0.00       0.00       2073
----… (label_id: 9)                                          0.00       0.00       0.00        142
----
---- 'punct_f1': 0.5858508944511414,
---+label                |  precision | recall |   f1   |    support
---+---|---|---|---|---
---+ (label_id: 0)       |   0.00  | 0.00     | 0.00  | 7470
---+! (label_id: 1)      |   0.00  | 0.00     | 0.00  | 148
---+, (label_id: 2)      |   0.00  | 0.00     | 0.00  | 28513
---+- (label_id: 3)      |   3.02  | 100.00   | 5.86  | 2074
---+. (label_id: 4)      |   0.00  | 0.00     | 0.00  | 25120
---+: (label_id: 5)      |   0.00  | 0.00     | 0.00  | 570
---+; (label_id: 6)      |   0.00  | 0.00     | 0.00  | 534
---+? (label_id: 7)      |   0.00  | 0.00     | 0.00  | 2085
---+— (label_id: 8)      |   0.00  | 0.00     | 0.00  | 2073
---+… (label_id: 9)      |   0.00  | 0.00     | 0.00  | 142
---+
---+ {'punct_f1': 0.5858508944511414,
---  'punct_precision': 0.30176490545272827,
---  'punct_recall': 10.0,
---- 'test_loss': 0.8140875697135925}
---\ No newline at end of file
---+ 'test_loss': 0.8140875697135925}
---+
---+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
---+
---+label                |  precision | recall |   f1   |    support
---+---|---|---|---|---
---+ (label_id: 0)      | 62.15  | 100.00   | 76.66   | 5154
---+! (label_id: 1)     | 0.00   | 0.00     | 0.00    | 108
---+, (label_id: 2)     | 0.00   | 0.00     | 0.00    | 18022
---+- (label_id: 3)     | 0.00   | 0.00     | 0.00    | 1557
---+. (label_id: 4)     | 41.74  | 94.01    | 57.81   | 15164
---+: (label_id: 5)     | 0.00   | 0.00     | 0.00    | 319
---+; (label_id: 6)     | 0.00   | 0.00     | 0.00    | 88
---+? (label_id: 7)     | 0.00   | 0.00     | 0.00    | 1217
---+ (label_id: 8)      | 0.00   | 0.00     | 0.00    | 752
---+… (label_id: 9)     | 0.00   | 0.00     | 0.00    | 67
---+-------------------||||
---+micro avg           | 45.72 | 45.72 | 45.72 | 42448
---+macro avg           | 10.39 | 19.40 | 13.45 | 42448
---+weighted avg        | 22.46 | 45.72 | 29.96 | 42448
---+
---+{ 'punct_f1': 13.446383476257324,
---+ 'punct_precision': 10.388500213623047,
---+ 'punct_recall': 19.400554656982422,
---+ 'test_loss': 0.44148480892181396}
---+
---+
---+ ## Log for 8/2/2021
---+
---+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
---+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
---+
---+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
---+End frozen 
---+micro avg      |  41.42 |   41.42  |    41.42   |   33406
---+macro avg      |  11.01 |   13.54  |    11.07   |   33406
---+weighted avg   |  34.88 |   41.42  |    34.20   |   33406
---+
---+1st layer best lr 1e-10, set to 0.007943282347242822
---+micro avg            |       36.65  |    36.65  |    36.65  |    33463
---+macro avg            |       10.71  |     9.91  |     8.26  |    33463
---+weighted avg         |       34.32  |    36.65  |    31.08  |    33463
---+
---+2nd layer best lr 1e-10, set to 0.007943282347242822
---+micro avg        |   35.72  |    35.72  |    35.72  |    42448
---+macro avg        |    3.57  |    10.00  |     5.26  |    42448
---+weighted avg     |   12.76  |    35.72  |    18.81  |    42448
---+
---+{'punct_f1': 5.264181137084961,
---+ 'punct_precision': 3.572371006011963,
---+ 'punct_recall': 10.0,
---+ 'test_loss': 18.49854850769043}
---+
---+
---+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
---+alpha from 3->4 seems to reduce convergence rate.
---+
---+micro avg        |   50.98  | 50.98  |  50.98    |  33463
---+macro avg        |   25.99  | 25.38  |  23.38    |  33463
---+weighted avg     |   50.31  | 50.98  |  48.27    |  33463
---+
---+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
---+micro avg     |  58.55  |  58.55 |  58.55 | 39340
---+macro avg     |  30.02  |  29.74 |  29.52 | 39340
---+weighted avg  |  57.91  |  58.55 |  57.51 | 39340
---+
---+still increasing?!
---+{'punct_f1': 29.523975372314453,
---+ 'punct_precision': 30.015613555908203,
---+ 'punct_recall': 29.738296508789062,
---+ 'test_loss': 0.3690211772918701}
---+
---+
---+### Implemented mlp 2 layer before classifier, 
---+
---+adamw mean 2 layer domain, dice, alpha 4 10 batch, accgrad 4 2021-02-08_11-07-07/
---+frozen lr 0.0025118864315095825 best: 0.01,
---diff --git a/experiment/config.yaml b/experiment/config.yaml
---index fe58670..b137ae8 100644
------ a/experiment/config.yaml
---+++ b/experiment/config.yaml
---@@ -1,49 +1,49 @@
--- seed: 42
--- trainer:
----    # gpus: 1 # the number of gpus, 0 for CPU
----    # num_nodes: 1
----    # max_epochs: 2
----    # max_steps: null # precedence over max_epochs
----    # accumulate_grad_batches: 4 # accumulates grads every k batches
----    # gradient_clip_val: 0
----    # amp_level: O1 # O1/O2 for mixed precision
----    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----    # accelerator: ddp
----    # checkpoint_callback: false  # Provided by exp_manager
----    # logger: false #false  # Provided by exp_manager
----    # log_every_n_steps: 1  # Interval of logging.
----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----    # resume_from_checkpoint: null
----
----    gpus: 0 # the number of gpus, 0 for CPU
---+    gpus: 1 # the number of gpus, 0 for CPU
---     num_nodes: 1
----    max_epochs: 8
---+    max_epochs: 10
---     max_steps: null # precedence over max_epochs
----    accumulate_grad_batches: 1 # accumulates grads every k batches
---+    accumulate_grad_batches: 4 # accumulates grads every k batches
---     gradient_clip_val: 0
----    amp_level: O0 # O1/O2 for mixed precision
----    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----    # accelerator: ddp
---+    amp_level: O1 # O1/O2 for mixed precision
---+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
---+    accelerator: ddp
---     checkpoint_callback: false  # Provided by exp_manager
---     logger: false #false  # Provided by exp_manager
---     log_every_n_steps: 1  # Interval of logging.
---     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----    reload_dataloaders_every_epoch: true
---     resume_from_checkpoint: null
--- 
---+    # gpus: 0 # the number of gpus, 0 for CPU
---+    # num_nodes: 1
---+    # max_epochs: 8
---+    # max_steps: null # precedence over max_epochs
---+    # accumulate_grad_batches: 1 # accumulates grads every k batches
---+    # gradient_clip_val: 0
---+    # amp_level: O0 # O1/O2 for mixed precision
---+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
---+    # # accelerator: ddp
---+    # checkpoint_callback: false  # Provided by exp_manager
---+    # logger: false #false  # Provided by exp_manager
---+    # log_every_n_steps: 1  # Interval of logging.
---+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
---+    # reload_dataloaders_every_epoch: true
---+    # resume_from_checkpoint: null
---+
--- exp_manager:
----    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
---+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
---     name: Punctuation_with_Domain_discriminator  # The name of your model
---     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
---     create_checkpoint_callback: true 
----base_path: /home/nxingyu2/data # /root/data # 
----tmp_path: /home/nxingyu2/data/tmp # /tmp # 
---+base_path: /home/nxingyu/data # /root/data # 
---+tmp_path: /home/nxingyu/data/tmp # /tmp # 
--- 
--- model:
---     nemo_path: null
----    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
---+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
---     unfrozen: 0
----    maximum_unfrozen: 2
---+    maximum_unfrozen: 1
---     unfreeze_step: 1
---     # unfreeze_every: 3
---     punct_label_ids:
---@@ -58,10 +58,10 @@ model:
---         - "—"
---         - "…"
--- 
----    punct_class_weights: true
---+    punct_class_weights: false
---     
---     dataset:
----        data_dir: /home/nxingyu2/data # /root/data # 
---+        data_dir: /home/nxingyu/data # /root/data # 
---         labelled:
---             - ${base_path}/ted_talks_processed #
---         unlabelled:
---@@ -106,6 +106,12 @@ model:
---         config: null
---         # unfrozen_layers: 1
--- 
---+    mlp:
---+        num_fc_layers: 1
---+        fc_dropout: 0.1
---+        log_softmax: false
---+        activation: 'relu'
---+        
---     punct_head:
---         punct_num_fc_layers: 1
---         fc_dropout: 0.1
---@@ -122,6 +128,8 @@ model:
---         use_transformer_init: true
---         loss: 'cel'
---         gamma: 0.1 # coefficient of gradient reversal
---+        pooling: 'mean'
---+        idx_conditioned_on: 0
---     
---     dice_loss:
---         epsilon: 0.01
---@@ -132,7 +140,7 @@ model:
---         gamma: 5
--- 
---     optim:
----        name: novograd
---+        name: adamw
---         lr: 1e-3
---         weight_decay: 0.00
---         sched:
---diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
---index d4ff927..f9927ac 100644
------ a/experiment/core/layers/sequence_classifier.py
---+++ b/experiment/core/layers/sequence_classifier.py
---@@ -1,3 +1,4 @@
---+import torch
--- from torch import nn
--- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
--- from core.utils import transformer_weights_init
---@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
---         log_softmax: bool = True,
---         dropout: float = 0.0,
---         use_transformer_init: bool = True,
----        idx_conditioned_on: int = 0,
---+        pooling: str = 'mean', # mean, max, mean_max, token
---+        idx_conditioned_on: int = None,
---     ):
---         """
---         Initializes the SequenceClassifier module.
---@@ -37,8 +39,9 @@ class SequenceClassifier(nn.Module):
---         super().__init__()
---         self.log_softmax = log_softmax
---         self._idx_conditioned_on = idx_conditioned_on
---+        self.pooling = pooling
---         self.mlp = MultiLayerPerceptron(
----            hidden_size=hidden_size,
---+            hidden_size=(hidden_size*2 if pooling=='mean_max' else hidden_size),
---             num_classes=num_classes,
---             num_layers=num_layers,
---             activation=activation,
---@@ -48,7 +51,23 @@ class SequenceClassifier(nn.Module):
---         if use_transformer_init:
---             self.apply(lambda module: transformer_weights_init(module, xavier=False))
--- 
----    def forward(self, hidden_states):
---+    def forward(self, hidden_states, subtoken_mask=None):
---         hidden_states = self.dropout(hidden_states)
----        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
---+        if self.pooling=='token':
---+            pooled = hidden_states[:, self._idx_conditioned_on]
---+        else:
---+            if subtoken_mask==None:
---+                ct=hidden_states.shape[1] # Seq len
---+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
---+            else:
---+                ct = torch.sum(subtoken_mask,axis=1).unsqueeze(1)
---+            pooled_sum = torch.sum(hidden_states,axis=1)            
---+            if self.pooling=='mean' or self.pooling == 'mean_max':
---+                pooled_mean = torch.div(pooled_sum,ct)
---+            if self.pooling=='max' or self.pooling=='mean_max':
---+                pooled_max = torch.max(hidden_states,axis=1)[0]
---+            pooled=pooled_mean if self.pooling=='mean' else \
---+                pooled_max if self.pooling=='max' else \
---+                    torch.cat([pooled_mean,pooled_max],axis=-1)
---+        logits = self.mlp(pooled)
---         return logits
---diff --git a/experiment/info.log b/experiment/info.log
---index 2471fe9..e69de29 100644
------ a/experiment/info.log
---+++ b/experiment/info.log
---@@ -1,300 +0,0 @@
----[INFO] - GPU available: True, used: False
----[INFO] - TPU available: None, using: 0 TPU cores
----[INFO] - shuffling train set
----[INFO] - Optimizer config = Novograd (
----Parameter Group 0
----    amsgrad: False
----    betas: (0.95, 0.98)
----    eps: 1e-08
----    grad_averaging: False
----    lr: 0.001
----    weight_decay: 0.0
----)
----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
----will be used during training (effective maximum steps = 80) - 
----Parameters : 
----(warmup_steps: null
----warmup_ratio: 0.1
----last_epoch: -1
----max_steps: 80
----)
----[INFO] - 
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 108 M 
----1 | punct_classifier    | TokenClassifier      | 7.7 K 
----2 | domain_classifier   | SequenceClassifier   | 769   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----8.5 K     Trainable params
----108 M     Non-trainable params
----108 M     Total params
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.00       0.00       0.00        184
----! (label_id: 1)                                          0.00       0.00       0.00          4
----, (label_id: 2)                                          1.23       0.34       0.53        594
----- (label_id: 3)                                          3.06      25.42       5.46         59
----. (label_id: 4)                                         47.22      12.98      20.36        524
----: (label_id: 5)                                          0.00       0.00       0.00         18
----; (label_id: 6)                                          0.00       0.00       0.00         13
----? (label_id: 7)                                          8.45       6.32       7.23         95
----— (label_id: 8)                                          0.00       0.00       0.00         12
----… (label_id: 9)                                          0.00       0.00       0.00          0
-----------------------
----micro avg                                                6.05       6.05       6.05       1503
----macro avg                                                6.66       5.01       3.73       1503
----weighted avg                                            17.61       6.05       7.98       1503
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00         92
-----------------------
----micro avg                                              100.00     100.00     100.00         92
----macro avg                                              100.00     100.00     100.00         92
----weighted avg                                           100.00     100.00     100.00         92
----
----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
----[INFO] - Optimizer config = Novograd (
----Parameter Group 0
----    amsgrad: False
----    betas: (0.95, 0.98)
----    eps: 1e-08
----    grad_averaging: False
----    lr: 1.5848931924611143e-08
----    weight_decay: 0.0
----)
----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
----will be used during training (effective maximum steps = 3192) - 
----Parameters : 
----(warmup_steps: null
----warmup_ratio: 0.1
----last_epoch: -1
----max_steps: 3192
----)
----[INFO] - 
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 108 M 
----1 | punct_classifier    | TokenClassifier      | 7.7 K 
----2 | domain_classifier   | SequenceClassifier   | 769   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----8.5 K     Trainable params
----108 M     Non-trainable params
----108 M     Total params
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.00       0.00       0.00        202
----! (label_id: 1)                                          0.00       0.00       0.00          4
----, (label_id: 2)                                          1.62       0.45       0.70        669
----- (label_id: 3)                                          3.48      27.27       6.17         66
----. (label_id: 4)                                         45.06      13.01      20.19        561
----: (label_id: 5)                                          1.52       6.67       2.47         15
----; (label_id: 6)                                          0.00       0.00       0.00         15
----? (label_id: 7)                                          8.70       7.32       7.95         82
----— (label_id: 8)                                          0.00       0.00       0.00         13
----… (label_id: 9)                                          0.00       0.00       0.00          1
-----------------------
----micro avg                                                6.20       6.20       6.20       1628
----macro avg                                                6.04       5.47       3.75       1628
----weighted avg                                            16.79       6.20       7.92       1628
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00        101
-----------------------
----micro avg                                              100.00     100.00     100.00        101
----macro avg                                              100.00     100.00     100.00        101
----weighted avg                                           100.00     100.00     100.00        101
----
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.34       0.73       0.46       4402
----! (label_id: 1)                                          0.42      13.95       0.82        129
----, (label_id: 2)                                          2.53       0.64       1.03      15243
----- (label_id: 3)                                          2.45      21.03       4.38       1322
----. (label_id: 4)                                         44.00      11.40      18.11      12542
----: (label_id: 5)                                          0.43       1.41       0.65        354
----; (label_id: 6)                                          0.00       0.00       0.00        163
----? (label_id: 7)                                          4.16       6.27       5.00       1117
----— (label_id: 8)                                          3.00       0.61       1.02        488
----… (label_id: 9)                                          0.97       6.17       1.68         81
-----------------------
----micro avg                                                5.41       5.41       5.41      35841
----macro avg                                                5.83       6.22       3.32      35841
----weighted avg                                            16.78       5.41       7.18      35841
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2201
-----------------------
----micro avg                                              100.00     100.00     100.00       2201
----macro avg                                              100.00     100.00     100.00       2201
----weighted avg                                           100.00     100.00     100.00       2201
----
----[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.20       0.43       0.27       4226
----! (label_id: 1)                                          0.44      14.17       0.86        127
----, (label_id: 2)                                          1.93       0.49       0.78      14611
----- (label_id: 3)                                          2.23      19.56       4.01       1237
----. (label_id: 4)                                         43.37      11.25      17.86      11977
----: (label_id: 5)                                          0.68       2.34       1.05        342
----; (label_id: 6)                                          0.00       0.00       0.00        129
----? (label_id: 7)                                          5.16       7.47       6.10       1058
----— (label_id: 8)                                          2.15       0.49       0.80        409
----… (label_id: 9)                                          0.69       4.23       1.19         71
-----------------------
----micro avg                                                5.23       5.23       5.23      34187
----macro avg                                                5.68       6.04       3.29      34187
----weighted avg                                            16.32       5.23       6.98      34187
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2113
-----------------------
----micro avg                                              100.00     100.00     100.00       2113
----macro avg                                              100.00     100.00     100.00       2113
----weighted avg                                           100.00     100.00     100.00       2113
----
----[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.28       0.61       0.39       4228
----! (label_id: 1)                                          0.30       8.28       0.58        145
----, (label_id: 2)                                          2.27       0.58       0.92      14495
----- (label_id: 3)                                          2.64      21.78       4.70       1327
----. (label_id: 4)                                         44.87      11.66      18.51      12193
----: (label_id: 5)                                          0.60       1.93       0.91        362
----; (label_id: 6)                                          0.00       0.00       0.00        164
----? (label_id: 7)                                          4.19       6.40       5.07       1078
----— (label_id: 8)                                          1.16       0.22       0.37        459
----… (label_id: 9)                                          0.85       4.17       1.41         96
-----------------------
----micro avg                                                5.54       5.54       5.54      34547
----macro avg                                                5.72       5.56       3.29      34547
----weighted avg                                            17.08       5.54       7.33      34547
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2114
-----------------------
----micro avg                                              100.00     100.00     100.00       2114
----macro avg                                              100.00     100.00     100.00       2114
----weighted avg                                           100.00     100.00     100.00       2114
----
----[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.29       0.63       0.40       4444
----! (label_id: 1)                                          0.38      10.67       0.74        150
----, (label_id: 2)                                          2.32       0.59       0.94      15290
----- (label_id: 3)                                          2.34      20.28       4.19       1292
----. (label_id: 4)                                         43.85      11.68      18.44      12599
----: (label_id: 5)                                          0.41       1.28       0.62        392
----; (label_id: 6)                                          0.00       0.00       0.00        164
----? (label_id: 7)                                          4.24       6.30       5.07       1111
----— (label_id: 8)                                          0.00       0.00       0.00        456
----… (label_id: 9)                                          0.38       2.41       0.65         83
-----------------------
----micro avg                                                5.40       5.40       5.40      35981
----macro avg                                                5.42       5.38       3.11      35981
----weighted avg                                            16.59       5.40       7.22      35981
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2222
-----------------------
----micro avg                                              100.00     100.00     100.00       2222
----macro avg                                              100.00     100.00     100.00       2222
----weighted avg                                           100.00     100.00     100.00       2222
----
----[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.35       0.73       0.48       3844
----! (label_id: 1)                                          0.54      14.62       1.04        130
----, (label_id: 2)                                          2.32       0.59       0.94      13056
----- (label_id: 3)                                          2.67      22.28       4.77       1194
----. (label_id: 4)                                         44.45      11.95      18.84      10791
----: (label_id: 5)                                          0.84       3.21       1.33        280
----; (label_id: 6)                                          0.00       0.00       0.00        140
----? (label_id: 7)                                          4.17       6.56       5.10        914
----— (label_id: 8)                                          0.00       0.00       0.00        401
----… (label_id: 9)                                          0.48       2.63       0.81         76
-----------------------
----micro avg                                                5.68       5.68       5.68      30826
----macro avg                                                5.58       6.26       3.33      30826
----weighted avg                                            16.82       5.68       7.41      30826
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       1922
-----------------------
----micro avg                                              100.00     100.00     100.00       1922
----macro avg                                              100.00     100.00     100.00       1922
----weighted avg                                           100.00     100.00     100.00       1922
----
----[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.28       0.60       0.39       3970
----! (label_id: 1)                                          0.35      10.66       0.68        122
----, (label_id: 2)                                          2.09       0.53       0.85      13469
----- (label_id: 3)                                          2.29      19.32       4.10       1201
----. (label_id: 4)                                         43.43      11.24      17.86      11227
----: (label_id: 5)                                          0.63       2.30       0.99        304
----; (label_id: 6)                                          0.00       0.00       0.00        141
----? (label_id: 7)                                          4.52       6.86       5.45       1006
----— (label_id: 8)                                          1.15       0.23       0.38        444
----… (label_id: 9)                                          0.45       2.67       0.78         75
-----------------------
----micro avg                                                5.26       5.26       5.26      31959
----macro avg                                                5.52       5.44       3.15      31959
----weighted avg                                            16.42       5.26       7.02      31959
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       1985
-----------------------
----micro avg                                              100.00     100.00     100.00       1985
----macro avg                                              100.00     100.00     100.00       1985
----weighted avg                                           100.00     100.00     100.00       1985
----
----[INFO] - Epoch 5, step 2394: val_loss was not in top 3
----[INFO] - Punctuation report: 
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                           0.23       0.48       0.31       4126
----! (label_id: 1)                                          0.29       9.40       0.56        117
----, (label_id: 2)                                          1.91       0.49       0.77      14019
----- (label_id: 3)                                          2.52      22.59       4.53       1164
----. (label_id: 4)                                         44.15      11.65      18.44      11789
----: (label_id: 5)                                          0.72       2.41       1.11        332
----; (label_id: 6)                                          0.56       0.61       0.58        165
----? (label_id: 7)                                          3.89       6.53       4.88        980
----— (label_id: 8)                                          2.30       0.47       0.77        430
----… (label_id: 9)                                          1.18       8.33       2.07         60
-----------------------
----micro avg                                                5.47       5.47       5.47      33182
----macro avg                                                5.77       6.30       3.40      33182
----weighted avg                                            16.77       5.47       7.25      33182
----
----[INFO] - Domain report: 
----label                                                precision    recall       f1           support   
----0 (label_id: 0)                                        100.00     100.00     100.00       2063
-----------------------
----micro avg                                              100.00     100.00     100.00       2063
----macro avg                                              100.00     100.00     100.00       2063
----weighted avg                                           100.00     100.00     100.00       2063
----
----[INFO] - Epoch 6, step 2793: val_loss was not in top 3
---diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
---index fa37b4c..43fc93d 100644
------ a/experiment/models/punctuation_domain_model.py
---+++ b/experiment/models/punctuation_domain_model.py
---@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---         else:
---             self.hparams.model.punct_class_weights=None
--- 
---+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
---+        self.mlp = MultiLayerPerceptron(
---+            self.transformer.config.hidden_size,
---+            self.transformer.config.hidden_size,
---+            num_layers=self.hparams.model.mlp.num_fc_layers, 
---+            activation=self.hparams.model.mlp.activation, 
---+            log_softmax=self.hparams.model.mlp.log_softmax
---+        )
---+
---         self.punct_classifier = TokenClassifier(
---             hidden_size=self.transformer.config.hidden_size,
---             num_classes=len(self.labels_to_ids),
---@@ -80,6 +89,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---             log_softmax=self.hparams.model.domain_head.log_softmax,
---             dropout=self.hparams.model.domain_head.fc_dropout,
---             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
---+            pooling=self.hparams.model.domain_head.pooling,
---+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
---         )
--- 
---         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
---@@ -119,14 +130,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
---         self.freeze()
--- 
----    def forward(self, input_ids, attention_mask, domain_ids=None):
---+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
---         hidden_states = self.transformer(
---             input_ids=input_ids, attention_mask=attention_mask
---         )[0]
---+        hidden_states = self.dropout(hidden_states)
---+        hidden_states = self.mlp(hidden_states)
---         punct_logits = self.punct_classifier(hidden_states=hidden_states)
---         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
---         domain_logits = self.domain_classifier(
----            hidden_states=reverse_grad_hidden_states)
---+            hidden_states=reverse_grad_hidden_states,
---+            subtoken_mask=subtoken_mask)
---         return punct_logits, domain_logits
--- 
---     def _make_step(self, batch):
---@@ -136,7 +150,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---         punct_labels = batch['labels']
---         domain_labels = batch['domain']
---         punct_logits, domain_logits = self(
----            input_ids=input_ids, attention_mask=attention_mask
---+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
---         )
---         punctuation_loss = self.punctuation_loss(
---             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/hparams.yaml
--deleted file mode 100644
--index 8fe49f4..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/hparams.yaml
--+++ /dev/null
--@@ -1,114 +0,0 @@
---seed: 42
---trainer:
---  gpus: 1
---  num_nodes: 1
---  max_epochs: 10
---  max_steps: null
---  accumulate_grad_batches: 4
---  gradient_clip_val: 0
---  amp_level: O1
---  precision: 16
---  accelerator: ddp
---  checkpoint_callback: false
---  logger: false
---  log_every_n_steps: 1
---  val_check_interval: 1.0
---  resume_from_checkpoint: null
---exp_manager:
---  exp_dir: /home/nxingyu/project/
---  name: Punctuation_with_Domain_discriminator
---  create_tensorboard_logger: true
---  create_checkpoint_callback: true
---base_path: /home/nxingyu/data
---tmp_path: /home/nxingyu/data/tmp
---model:
---  nemo_path: null
---  transformer_path: google/electra-small-discriminator
---  unfrozen: 0
---  maximum_unfrozen: 1
---  unfreeze_step: 1
---  punct_label_ids:
---  - ''
---  - '!'
---  - ','
---  - '-'
---  - .
---  - ':'
---  - ;
---  - '?'
---  - —
---  - …
---  punct_class_weights: false
---  dataset:
---    data_dir: /home/nxingyu/data
---    labelled:
---    - /home/nxingyu/data/ted_talks_processed
---    unlabelled: null
---    max_seq_length: 128
---    pad_label: ''
---    ignore_extra_tokens: false
---    ignore_start_end: false
---    use_cache: false
---    num_workers: 0
---    pin_memory: true
---    drop_last: false
---    num_labels: 10
---    num_domains: 1
---    test_unlabelled: true
---    train_ds:
---      shuffle: true
---      num_samples: -1
---      batch_size: 8
---    validation_ds:
---      shuffle: true
---      num_samples: -1
---      batch_size: 8
---  tokenizer:
---    tokenizer_name: google/electra-small-discriminator
---    vocab_file: null
---    tokenizer_model: null
---    special_tokens: null
---  language_model:
---    pretrained_model_name: google/electra-small-discriminator
---    lm_checkpoint: null
---    config_file: null
---    config: null
---  mlp:
---    num_fc_layers: 1
---    fc_dropout: 0.1
---    log_softmax: false
---    activation: relu
---  punct_head:
---    punct_num_fc_layers: 1
---    fc_dropout: 0.1
---    activation: relu
---    log_softmax: false
---    use_transformer_init: true
---    loss: dice
---  domain_head:
---    domain_num_fc_layers: 1
---    fc_dropout: 0.1
---    activation: relu
---    log_softmax: false
---    use_transformer_init: true
---    loss: cel
---    gamma: 0.1
---    pooling: mean
---    idx_conditioned_on: 0
---  dice_loss:
---    epsilon: 0.01
---    alpha: 3
---    macro_average: true
---  focal_loss:
---    gamma: 5
---  optim:
---    name: adamw
---    lr: 0.001
---    weight_decay: 0.0
---    sched:
---      name: WarmupAnnealing
---      warmup_steps: null
---      warmup_ratio: 0.1
---      last_epoch: -1
---      monitor: val_loss
---      reduce_on_plateau: false
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/lightning_logs.txt
--deleted file mode 100644
--index 40113aa..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/lightning_logs.txt
--+++ /dev/null
--@@ -1,43 +0,0 @@
---Global seed set to 42
---GPU available: True, used: True
---TPU available: None, using: 0 TPU cores
---LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
---Using native 16bit precision.
---Global seed set to 42
---initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | dropout             | Dropout              | 0     
---2 | mlp                 | MultiLayerPerceptron | 65.8 K
---3 | punct_classifier    | TokenClassifier      | 2.6 K 
---4 | domain_classifier   | SequenceClassifier   | 257   
---5 | punctuation_loss    | FocalDiceLoss        | 0     
---6 | domain_loss         | CrossEntropyLoss     | 0     
---7 | agg_loss            | AggregatorLoss       | 0     
---8 | punct_class_report  | ClassificationReport | 0     
---9 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---101 K     Trainable params
---13.4 M    Non-trainable params
---13.6 M    Total params
---Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/lr_find_temp_model.ckpt
---Global seed set to 42
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | dropout             | Dropout              | 0     
---2 | mlp                 | MultiLayerPerceptron | 65.8 K
---3 | punct_classifier    | TokenClassifier      | 2.6 K 
---4 | domain_classifier   | SequenceClassifier   | 257   
---5 | punctuation_loss    | FocalDiceLoss        | 0     
---6 | domain_loss         | CrossEntropyLoss     | 0     
---7 | agg_loss            | AggregatorLoss       | 0     
---8 | punct_class_report  | ClassificationReport | 0     
---9 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---101 K     Trainable params
---13.4 M    Non-trainable params
---13.6 M    Total params
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/nemo_error_log.txt
--deleted file mode 100644
--index e026dd1..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/nemo_error_log.txt
--+++ /dev/null
--@@ -1,22 +0,0 @@
---[NeMo W 2021-02-08 11:24:45 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-08 11:24:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:24:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:24:58 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:28:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:28:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:28:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:28:44 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/nemo_log_globalrank-0_localrank-0.txt
--deleted file mode 100644
--index b012129..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/nemo_log_globalrank-0_localrank-0.txt
--+++ /dev/null
--@@ -1,24 +0,0 @@
---[NeMo I 2021-02-08 11:24:45 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45
---[NeMo I 2021-02-08 11:24:45 exp_manager:519] TensorboardLogger has been set up
---[NeMo W 2021-02-08 11:24:45 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-08 11:24:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:24:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:24:58 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:28:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:28:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:28:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
---      warnings.warn(*args, **kwargs)
---    
---[NeMo W 2021-02-08 11:28:44 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
---      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
---    
--diff --git a/README.md b/README.md
--index 51ab30b..aa93b64 100644
----- a/README.md
--+++ b/README.md
--@@ -167,17 +167,18 @@ label            | precision    | recall   | f1     | support
-- … (label_id: 9)  | 0.00         | 0.00     | 0.00   | 66
-- 
-- ###Focal DistilBERT gamma 3 0 unfrozen ted
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                         100.00      51.29      67.80       4118
---! (label_id: 1)                                          0.00       0.00       0.00         91
---, (label_id: 2)                                          0.00       0.00       0.00      13953
---- (label_id: 3)                                         94.27      46.49      62.27       1310
---. (label_id: 4)                                         39.51      99.94      56.63      12142
---: (label_id: 5)                                          0.00       0.00       0.00        254
---; (label_id: 6)                                          0.00       0.00       0.00         79
---? (label_id: 7)                                          0.00       0.00       0.00        905
---— (label_id: 8)                                          0.00       0.00       0.00        566
---… (label_id: 9)                                          0.00       0.00       0.00         52
--+label               | precision   | recall | f1    | support   
--+---|---|---|---|---
--+ (label_id: 0)      | 100.00      | 51.29  | 67.80 | 4118
--+! (label_id: 1)     | 0.00        | 0.00   | 0.00  | 91
--+, (label_id: 2)     | 0.00        | 0.00   | 0.00  | 13953
--+\- (label_id: 3)    | 94.27       | 46.49  | 62.27 | 1310
--+. (label_id: 4)     | 39.51       | 99.94  | 56.63 | 12142
--+: (label_id: 5)     | 0.00        | 0.00   | 0.00  | 254
--+; (label_id: 6)     | 0.00        | 0.00   | 0.00  | 79
--+? (label_id: 7)     | 0.00        | 0.00   | 0.00  | 905
--+— (label_id: 8)     | 0.00        | 0.00   | 0.00  | 566
--+… (label_id: 9)     | 0.00        | 0.00   | 0.00  | 52
-- 
-- ## Observations
-- - CRF tends to perform better on higher proportion classes like blank, comma and period without class weights.
--@@ -195,7 +196,7 @@ label                 |   precision  |  recall |    f1    |      support
--  (label_id: 0)        |      79.50   |   29.94 |   43.50  |    5026
-- ! (label_id: 1)       |       6.84   |   20.59 |   10.27  |     102
-- , (label_id: 2)       |      50.70   |   60.09 |   55.00  |   17571
---- (label_id: 3)       |      64.45   |   82.11 |   72.22  |    1526
--+\- (label_id: 3)       |      64.45   |   82.11 |   72.22  |    1526
-- . (label_id: 4)       |      57.40   |   49.43 |   53.12  |   14767
-- : (label_id: 5)       |      17.86   |   31.83 |   22.89  |     289
-- ; (label_id: 6)       |       1.50   |    5.88 |    2.39  |      85
--@@ -220,7 +221,7 @@ label                 |   precision  |  recall |    f1    |      support
--  (label_id: 0)             |     0.00  |  0.00 |   0.00  |  5026
-- ! (label_id: 1)            |     0.00  |  0.00 |   0.00  |   102
-- , (label_id: 2)            |    42.79  | 47.54 |  45.04  | 17571
---- (label_id: 3)            |    73.63  | 80.87 |  77.08  |  1526
--+\- (label_id: 3)            |    73.63  | 80.87 |  77.08  |  1526
-- . (label_id: 4)            |    47.36  | 55.16 |  50.96  | 14767
-- : (label_id: 5)            |    10.88  | 27.68 |  15.62  |   289
-- ; (label_id: 6)            |     0.00  |  0.00 |   0.00  |    85
--@@ -245,7 +246,7 @@ label                  |  precision | recall |   f1   |     support
--  (label_id: 0)         |     59.35  |  52.35 |  55.63 |   7314
-- ! (label_id: 1)        |      0.00  |   0.00 |   0.00 |    154
-- , (label_id: 2)        |     44.15  |  82.80 |  57.59 |  28180
---- (label_id: 3)        |      3.91  |   2.02 |   2.66 |   1933
--+\- (label_id: 3)        |      3.91  |   2.02 |   2.66 |   1933
-- . (label_id: 4)        |     39.91  |  11.64 |  18.02 |  24624
-- : (label_id: 5)        |      0.00  |   0.00 |   0.00 |    522
-- ; (label_id: 6)        |      0.00  |   0.00 |   0.00 |    485
--@@ -268,7 +269,7 @@ label                |  precision | recall |   f1   |    support
--  (label_id: 0)       |  62.32   | 99.78 |  76.72  |   7314
-- ! (label_id: 1)      |   0.00   |  0.00 |   0.00  |    154
-- , (label_id: 2)      |  49.81   |  4.72 |   8.62  |  28180
---- (label_id: 3)      |   5.91   | 28.35 |   9.78  |   1933
--+\- (label_id: 3)      |   5.91   | 28.35 |   9.78  |   1933
-- . (label_id: 4)      |  41.80   | 52.40 |  46.50  |  24624
-- : (label_id: 5)      |   0.94   |  4.02 |   1.53  |    522
-- ; (label_id: 6)      |   0.00   |  0.00 |   0.00  |    485
--@@ -296,7 +297,7 @@ label                |  precision | recall |   f1   |    support
--   (label_id: 0)        | 0.00     | 0.00    | 0.00  | 5704
-- ! (label_id: 1)        | 0.00     | 0.00    | 0.00  | 110
-- , (label_id: 2)        | 0.00     | 0.00    | 0.00  | 19711
---- (label_id: 3)        | 6.82     | 29.32   | 11.07 | 1702
--+\- (label_id: 3)        | 6.82     | 29.32   | 11.07 | 1702
-- . (label_id: 4)        | 37.30    | 83.82   | 51.62 | 18406
-- : (label_id: 5)        | 0.00     | 0.00    | 0.00  | 379
-- ; (label_id: 6)        | 0.00     | 0.00    | 0.00  | 190
--@@ -324,7 +325,7 @@ label                |  precision | recall |   f1   |    support
--  (label_id: 0)       |   0.00  | 0.00     | 0.00  | 7470
-- ! (label_id: 1)      |   0.00  | 0.00     | 0.00  | 148
-- , (label_id: 2)      |   0.00  | 0.00     | 0.00  | 28513
---- (label_id: 3)      |   3.02  | 100.00   | 5.86  | 2074
--+\- (label_id: 3)      |   3.02  | 100.00   | 5.86  | 2074
-- . (label_id: 4)      |   0.00  | 0.00     | 0.00  | 25120
-- : (label_id: 5)      |   0.00  | 0.00     | 0.00  | 570
-- ; (label_id: 6)      |   0.00  | 0.00     | 0.00  | 534
--@@ -344,7 +345,7 @@ label                |  precision | recall |   f1   |    support
--  (label_id: 0)      | 62.15  | 100.00   | 76.66   | 5154
-- ! (label_id: 1)     | 0.00   | 0.00     | 0.00    | 108
-- , (label_id: 2)     | 0.00   | 0.00     | 0.00    | 18022
---- (label_id: 3)     | 0.00   | 0.00     | 0.00    | 1557
--+\- (label_id: 3)     | 0.00   | 0.00     | 0.00    | 1557
-- . (label_id: 4)     | 41.74  | 94.01    | 57.81   | 15164
-- : (label_id: 5)     | 0.00   | 0.00     | 0.00    | 319
-- ; (label_id: 6)     | 0.00   | 0.00     | 0.00    | 88
--@@ -412,3 +413,11 @@ still increasing?!
-- 
-- adamw mean 2 layer domain, dice, alpha 4 10 batch, accgrad 4 2021-02-08_11-07-07/
-- frozen lr 0.0025118864315095825 best: 0.01,
--+
--+unfreeze 0.07943282347242822 best lr 1e-10
--+
--+ep 6 
--+micro avg    | 64.21 | 64.21 | 64.21 | 33835
--+macro avg    | 36.55 | 37.56 | 36.71 | 33835
--+weighted avg | 63.77 | 64.21 | 63.91 | 33835
--+
--diff --git a/experiment/config.yaml b/experiment/config.yaml
--index b137ae8..c3fd1d4 100644
----- a/experiment/config.yaml
--+++ b/experiment/config.yaml
--@@ -73,7 +73,7 @@ model:
--         ignore_start_end: false
--         use_cache: false
--         # shared among dataloaders
---        num_workers:  0
--+        num_workers:  4
--         pin_memory: true
--         drop_last: false
--         num_labels: 10
--@@ -105,12 +105,6 @@ model:
--         config_file: null # json file, precedence over config
--         config: null
--         # unfrozen_layers: 1
---
---    mlp:
---        num_fc_layers: 1
---        fc_dropout: 0.1
---        log_softmax: false
---        activation: 'relu'
--         
--     punct_head:
--         punct_num_fc_layers: 1
--@@ -118,7 +112,7 @@ model:
--         activation: 'relu'
--         log_softmax: false
--         use_transformer_init: true
---        loss: 'dice'
--+        loss: 'focal'
-- 
--     domain_head:
--         domain_num_fc_layers: 1
--@@ -127,7 +121,7 @@ model:
--         log_softmax: false
--         use_transformer_init: true
--         loss: 'cel'
---        gamma: 0.1 # coefficient of gradient reversal
--+        gamma: 0 #0.1 # coefficient of gradient reversal
--         pooling: 'mean'
--         idx_conditioned_on: 0
--     
--@@ -137,7 +131,7 @@ model:
--         macro_average: true
-- 
--     focal_loss: 
---        gamma: 5
--+        gamma: 1
-- 
--     optim:
--         name: adamw
--diff --git a/experiment/data/__init__.py b/experiment/data/__init__.py
--index 9b0816f..818d8e3 100644
----- a/experiment/data/__init__.py
--+++ b/experiment/data/__init__.py
--@@ -1,3 +1,3 @@
---from data.punctuation_dataset import PunctuationDomainDataset, PunctuationDomainDatasets, PunctuationInferenceDataset
--+from data.punctuation_dataset_multi import PunctuationDomainDataset, PunctuationDomainDatasets, PunctuationInferenceDataset
-- from data.punctuation_datamodule import PunctuationDataModule
-- 
--diff --git a/experiment/data/punctuation_dataset.py b/experiment/data/punctuation_dataset.py
--index 8374d29..bfd015c 100644
----- a/experiment/data/punctuation_dataset.py
--+++ b/experiment/data/punctuation_dataset.py
--@@ -36,7 +36,9 @@ class PunctuationDomainDataset(IterableDataset):
--         labelled=True,
--         randomize=True,
--         target_file='',
---        tmp_path='~/data/tmp'
--+        tmp_path='~/data/tmp',
--+        start=0,
--+        end=-1,
--     ):
--         if not (os.path.exists(csv_file)):
--             raise FileNotFoundError(
--@@ -148,7 +150,9 @@ class PunctuationDomainDatasets(IterableDataset):
--                  tokenizer,
--                  randomize:bool=True,
--                  data_id='',
---                 tmp_path='~/data/tmp'):
--+                 tmp_path='~/data/tmp',
--+                 start=0,
--+                 end=-1,):
--         self.num_labelled=len(labelled)
--         self.datasets = []
--         self.iterables=[]
--diff --git a/experiment/info.log b/experiment/info.log
--index bd124fd..e69de29 100644
--Binary files a/experiment/info.log and b/experiment/info.log differ
--diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
--index 43fc93d..2f33d3c 100644
----- a/experiment/models/punctuation_domain_model.py
--+++ b/experiment/models/punctuation_domain_model.py
--@@ -62,15 +62,6 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
--         else:
--             self.hparams.model.punct_class_weights=None
-- 
---        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
---        self.mlp = MultiLayerPerceptron(
---            self.transformer.config.hidden_size,
---            self.transformer.config.hidden_size,
---            num_layers=self.hparams.model.mlp.num_fc_layers, 
---            activation=self.hparams.model.mlp.activation, 
---            log_softmax=self.hparams.model.mlp.log_softmax
---        )
---
--         self.punct_classifier = TokenClassifier(
--             hidden_size=self.transformer.config.hidden_size,
--             num_classes=len(self.labels_to_ids),
--@@ -134,8 +125,6 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
--         hidden_states = self.transformer(
--             input_ids=input_ids, attention_mask=attention_mask
--         )[0]
---        hidden_states = self.dropout(hidden_states)
---        hidden_states = self.mlp(hidden_states)
--         punct_logits = self.punct_classifier(hidden_states=hidden_states)
--         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
--         domain_logits = self.domain_classifier(
--@@ -669,6 +658,13 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-- 
--         for layer in list(encoder.layer)[n:]:
--             set_requires_grad_for_module(layer, True)
--+        
--+        # Set output layer to true.
--+        last_iter=iter(encoder.layer[-1].children())
--+        last = next(last_iter)
--+        for last in last_iter:
--+            continue
--+        set_requires_grad_for_module(last, True)
-- 
--     def freeze(self) -> None:
--         try:
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/hparams.yaml
-deleted file mode 100644
-index 8ec59c3..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/hparams.yaml
-+++ /dev/null
-@@ -1,109 +0,0 @@
--seed: 42
--trainer:
--  gpus: 1
--  num_nodes: 1
--  max_epochs: 10
--  max_steps: null
--  accumulate_grad_batches: 4
--  gradient_clip_val: 0
--  amp_level: O1
--  precision: 16
--  accelerator: ddp
--  checkpoint_callback: false
--  logger: false
--  log_every_n_steps: 1
--  val_check_interval: 1.0
--  resume_from_checkpoint: null
--exp_manager:
--  exp_dir: /home/nxingyu/project/
--  name: Punctuation_with_Domain_discriminator
--  create_tensorboard_logger: true
--  create_checkpoint_callback: true
--base_path: /home/nxingyu/data
--tmp_path: /home/nxingyu/data/tmp
--model:
--  nemo_path: null
--  transformer_path: google/electra-small-discriminator
--  unfrozen: 0
--  maximum_unfrozen: 1
--  unfreeze_step: 1
--  punct_label_ids:
--  - ''
--  - '!'
--  - ','
--  - '-'
--  - .
--  - ':'
--  - ;
--  - '?'
--  - —
--  - …
--  punct_class_weights: false
--  dataset:
--    data_dir: /home/nxingyu/data
--    labelled:
--    - /home/nxingyu/data/ted_talks_processed
--    unlabelled: null
--    max_seq_length: 128
--    pad_label: ''
--    ignore_extra_tokens: false
--    ignore_start_end: false
--    use_cache: false
--    num_workers: 4
--    pin_memory: true
--    drop_last: false
--    num_labels: 10
--    num_domains: 1
--    test_unlabelled: true
--    train_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 8
--    validation_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 8
--  tokenizer:
--    tokenizer_name: google/electra-small-discriminator
--    vocab_file: null
--    tokenizer_model: null
--    special_tokens: null
--  language_model:
--    pretrained_model_name: google/electra-small-discriminator
--    lm_checkpoint: null
--    config_file: null
--    config: null
--  punct_head:
--    punct_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: focal
--  domain_head:
--    domain_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: cel
--    gamma: 0
--    pooling: mean
--    idx_conditioned_on: 0
--  dice_loss:
--    epsilon: 0.01
--    alpha: 3
--    macro_average: true
--  focal_loss:
--    gamma: 1
--  optim:
--    name: adamw
--    lr: 0.001
--    weight_decay: 0.0
--    sched:
--      name: WarmupAnnealing
--      warmup_steps: null
--      warmup_ratio: 0.1
--      last_epoch: -1
--      monitor: val_loss
--      reduce_on_plateau: false
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/lightning_logs.txt
-deleted file mode 100644
-index 6fb3529..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/lightning_logs.txt
-+++ /dev/null
-@@ -1,97 +0,0 @@
--Global seed set to 42
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
--Using native 16bit precision.
--Global seed set to 42
--initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 257   
--3 | punctuation_loss    | FocalLoss            | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--298 K     Trainable params
--13.2 M    Non-trainable params
--13.5 M    Total params
--Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/lr_find_temp_model.ckpt
--Global seed set to 42
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 257   
--3 | punctuation_loss    | FocalLoss            | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--298 K     Trainable params
--13.2 M    Non-trainable params
--13.5 M    Total params
--Epoch 0, global step 100: val_loss reached 3.78377 (best 3.78377), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=3.78-epoch=0.ckpt" as top 3
--Epoch 1, global step 200: val_loss reached 3.18810 (best 3.18810), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=3.19-epoch=1.ckpt" as top 3
--Epoch 2, global step 300: val_loss reached 2.57271 (best 2.57271), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=2.57-epoch=2.ckpt" as top 3
--Epoch 3, global step 400: val_loss reached 1.99144 (best 1.99144), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=1.99-epoch=3.ckpt" as top 3
--Epoch 4, global step 500: val_loss reached 1.53183 (best 1.53183), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=1.53-epoch=4.ckpt" as top 3
--Epoch 5, global step 600: val_loss reached 1.23454 (best 1.23454), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=1.23-epoch=5.ckpt" as top 3
--Epoch 6, global step 700: val_loss reached 1.04915 (best 1.04915), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=1.05-epoch=6.ckpt" as top 3
--Epoch 7, global step 800: val_loss reached 0.87923 (best 0.87923), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.88-epoch=7.ckpt" as top 3
--Epoch 8, global step 900: val_loss reached 0.73565 (best 0.73565), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.74-epoch=8.ckpt" as top 3
--Epoch 9, global step 1000: val_loss reached 0.64555 (best 0.64555), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.65-epoch=9.ckpt" as top 3
--Global seed set to 42
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 257   
--3 | punctuation_loss    | FocalLoss            | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--825 K     Trainable params
--12.7 M    Non-trainable params
--13.5 M    Total params
--LR finder stopped early due to diverging loss.
--Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/lr_find_temp_model.ckpt
--Global seed set to 42
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 257   
--3 | punctuation_loss    | FocalLoss            | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--825 K     Trainable params
--12.7 M    Non-trainable params
--13.5 M    Total params
--Epoch 0, global step 1101: val_loss reached 0.39432 (best 0.39432), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.39-epoch=0.ckpt" as top 3
--Epoch 1, global step 1201: val_loss reached 0.34903 (best 0.34903), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.35-epoch=1.ckpt" as top 3
--Epoch 2, global step 1301: val_loss reached 0.31863 (best 0.31863), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.32-epoch=2.ckpt" as top 3
--Epoch 3, global step 1401: val_loss reached 0.29965 (best 0.29965), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.30-epoch=3.ckpt" as top 3
--Epoch 4, global step 1501: val_loss reached 0.28616 (best 0.28616), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.29-epoch=4.ckpt" as top 3
--Epoch 5, global step 1601: val_loss reached 0.27245 (best 0.27245), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=5.ckpt" as top 3
--Epoch 6, global step 1701: val_loss reached 0.25894 (best 0.25894), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=6.ckpt" as top 3
--Epoch 7, global step 1801: val_loss reached 0.24971 (best 0.24971), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.25-epoch=7.ckpt" as top 3
--Epoch 8, global step 1901: val_loss reached 0.24226 (best 0.24226), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=8.ckpt" as top 3
--Epoch 9, global step 2001: val_loss reached 0.22899 (best 0.22899), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.23-epoch=9.ckpt" as top 3
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--Using environment variable NODE_RANK for node rank (0).
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/nemo_error_log.txt
-deleted file mode 100644
-index bf5b2d0..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/nemo_error_log.txt
-+++ /dev/null
-@@ -1,22 +0,0 @@
--[NeMo W 2021-02-08 14:58:58 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-08 14:59:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 14:59:57 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 15:00:01 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-08 15:01:00 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f78f069baf0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-08 15:01:06 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f78f8cc5880> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-08 15:13:49 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 15:26:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f78f8cc5610> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/nemo_log_globalrank-0_localrank-0.txt
-deleted file mode 100644
-index 4503717..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58/nemo_log_globalrank-0_localrank-0.txt
-+++ /dev/null
-@@ -1,24 +0,0 @@
--[NeMo I 2021-02-08 14:58:58 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_14-58-58
--[NeMo I 2021-02-08 14:58:58 exp_manager:519] TensorboardLogger has been set up
--[NeMo W 2021-02-08 14:58:58 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-08 14:59:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 14:59:57 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 15:00:01 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-08 15:01:00 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f78f069baf0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-08 15:01:06 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f78f8cc5880> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-08 15:13:49 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 15:26:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f78f8cc5610> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/cmd-args.log
-deleted file mode 100644
-index 11a5d8e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/cmd-args.log
-+++ /dev/null
-@@ -1 +0,0 @@
--main.py
-\ No newline at end of file
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/events.out.tfevents.1612772700.Titan.28938.0 b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/events.out.tfevents.1612772700.Titan.28938.0
-deleted file mode 100644
-index 04e8367..0000000
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/events.out.tfevents.1612772700.Titan.28938.0 and /dev/null differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/git-info.log
-deleted file mode 100644
-index 3beaa18..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/git-info.log
-+++ /dev/null
-@@ -1,28666 +0,0 @@
--commit hash: d9cdb13829a1dfa2d74afb03fde5acec0f85d2cc
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/cmd-args.log
--deleted file mode 100644
--index 11a5d8e..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/cmd-args.log
--+++ /dev/null
--@@ -1 +0,0 @@
---main.py
--\ No newline at end of file
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/git-info.log
--deleted file mode 100644
--index 1cbaaa2..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/git-info.log
--+++ /dev/null
--@@ -1,28480 +0,0 @@
---commit hash: 66d59fddd871d29e1ceab6de212c3812501bf786
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/events.out.tfevents.1612493730.intern-instance.20106.0 b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/events.out.tfevents.1612493730.intern-instance.20106.0
---deleted file mode 100644
---index 25acd93..0000000
---Binary files a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/events.out.tfevents.1612493730.intern-instance.20106.0 and /dev/null differ
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/git-info.log
---deleted file mode 100644
---index 2bf17c1..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/git-info.log
---+++ /dev/null
---@@ -1,521 +0,0 @@
----commit hash: 5b55597d5aa99268a9f2ee637608d94abc88934b
----diff --git a/experiment/Nemo2Lightning.ipynb b/experiment/Nemo2Lightning.ipynb
----index ebb0800..d2ec988 100644
------- a/experiment/Nemo2Lightning.ipynb
----+++ b/experiment/Nemo2Lightning.ipynb
----@@ -74,79 +74,79 @@
----   },
----   {
----    "cell_type": "code",
-----   "execution_count": 11,
----+   "execution_count": 13,
----    "metadata": {},
----    "outputs": [
----     {
----      "name": "stderr",
----      "output_type": "stream",
----      "text": [
-----      "09:00:02.50 LOG:\n",
-----      "09:00:02.52 .... 'cel none' = 'cel none'\n",
-----      "09:00:02.53 .... output = tensor([1.5514, 1.5514, 0.5514, 0.5514], grad_fn=<NllLossBackward>)\n",
-----      "09:00:02.53 LOG:\n",
-----      "09:00:02.53 .... 'cel mean' = 'cel mean'\n",
-----      "09:00:02.53 .... output = tensor(1.0514, grad_fn=<NllLossBackward>)\n",
-----      "09:00:02.53 LOG:\n",
-----      "09:00:02.53 .... 'cel sum' = 'cel sum'\n",
-----      "09:00:02.53 .... output = tensor(4.2058, grad_fn=<NllLossBackward>)\n",
-----      "09:00:02.53 LOG:\n",
-----      "09:00:02.54 .... 'focal sum' = 'focal sum'\n",
-----      "09:00:02.54 .... loss(inp, tar) = tensor(6.7352, grad_fn=<SumBackward0>)\n",
-----      "09:00:02.54 LOG:\n",
-----      "09:00:02.54 .... 'focal mean' = 'focal mean'\n",
-----      "09:00:02.54 .... loss(inp, tar) = tensor(0.4210, grad_fn=<MeanBackward0>)\n",
-----      "09:00:02.54 LOG:\n",
-----      "09:00:02.54 .... 'focal none' = 'focal none'\n",
-----      "09:00:02.55 .... loss(inp, tar) = tensor([0.9635, 0.9635, 0.0991, 0.0991], grad_fn=<MulBackward0>)\n",
-----      "09:00:02.55 LOG:\n",
-----      "09:00:02.55 .... 'focal none' = 'focal none'\n",
-----      "09:00:02.55 .... loss(inp, tar) = tensor([0.4716, 0.4716, 0.0075, 0.0075], grad_fn=<MulBackward0>)\n",
-----      "09:00:02.55 LOG:\n",
-----      "09:00:02.55 .... 'crf,none' = 'crf,none'\n",
-----      "09:00:02.56 .... output = tensor([4.2377], grad_fn=<NegBackward>)\n",
-----      "09:00:02.56 LOG:\n",
-----      "09:00:02.56 .... 'crf,mean' = 'crf,mean'\n",
-----      "09:00:02.56 .... output = tensor(4.3934, grad_fn=<NegBackward>)\n",
-----      "09:00:02.56 LOG:\n",
-----      "09:00:02.56 .... 'crf,sum' = 'crf,sum'\n",
-----      "09:00:02.57 .... output = tensor(4.3557, grad_fn=<NegBackward>)\n",
-----      "09:00:02.57 LOG:\n",
-----      "09:00:02.57 .... 'crf,token_mean' = 'crf,token_mean'\n",
-----      "09:00:02.57 .... output = tensor(1.0820, grad_fn=<DivBackward0>)\n",
-----      "09:00:02.57 LOG:\n",
-----      "09:00:02.57 .... 'dice none,micro' = 'dice none,micro'\n",
-----      "09:00:02.57 .... output = tensor(0.1876, grad_fn=<PowBackward0>)\n",
-----      "09:00:02.58 LOG:\n",
-----      "09:00:02.58 .... 'dice mean,micro' = 'dice mean,micro'\n",
-----      "09:00:02.58 .... output = tensor(0.0625, grad_fn=<DivBackward0>)\n",
-----      "09:00:02.58 LOG:\n",
-----      "09:00:02.58 .... 'dice sum,micro' = 'dice sum,micro'\n",
-----      "09:00:02.58 .... output = tensor(0.1876, grad_fn=<SumBackward0>)\n",
-----      "09:00:02.58 LOG:\n",
-----      "09:00:02.58 .... 'dice sum,micro' = 'dice sum,micro'\n",
-----      "09:00:02.59 .... output = tensor(0.0812, grad_fn=<SumBackward0>)\n",
-----      "09:00:02.59 LOG:\n",
-----      "09:00:02.59 .... 'dice none,macro' = 'dice none,macro'\n",
-----      "09:00:02.59 .... loss(inp, tar) = tensor([5.9547, 4.4872, 2.0551], grad_fn=<MulBackward0>)\n",
-----      "09:00:02.59 LOG:\n",
-----      "09:00:02.59 .... 'dice none,macro' = 'dice none,macro'\n",
-----      "09:00:02.60 .... loss(inp, tar) = tensor([0.0042, 0.1202, 0.0027], grad_fn=<MulBackward0>)\n",
-----      "09:00:02.60 LOG:\n",
-----      "09:00:02.60 .... 'dice none,macro' = 'dice none,macro'\n",
-----      "09:00:02.60 .... loss(inp, tar) = tensor([0.1116, 0.4285, 0.0935], grad_fn=<MulBackward0>)\n",
-----      "09:00:02.60 LOG:\n",
-----      "09:00:02.60 .... 'dice mean,macro' = 'dice mean,macro'\n",
-----      "09:00:02.60 .... loss(inp, tar) = tensor(0.2112, grad_fn=<DivBackward0>)\n",
-----      "09:00:02.61 LOG:\n",
-----      "09:00:02.61 .... 'dice sum,macro' = 'dice sum,macro'\n",
-----      "09:00:02.61 .... loss(inp, tar) = tensor(0.6335, grad_fn=<SumBackward0>)\n",
-----      "09:00:02.61 LOG:\n",
-----      "09:00:02.61 .... 'dice sum,macro' = 'dice sum,macro'\n",
-----      "09:00:02.61 .... output = tensor([0.5989, 0.7696, 0.2411], grad_fn=<MulBackward0>)\n",
-----      "09:00:02.61 LOG:\n",
-----      "09:00:02.62 .... 'dice sum,macro' = 'dice sum,macro'\n",
-----      "09:00:02.62 .... output = tensor([0.2148, 0.4559, 0.0140], grad_fn=<MulBackward0>)\n"
----+      "09:01:12.28 LOG:\n",
----+      "09:01:12.30 .... 'cel none' = 'cel none'\n",
----+      "09:01:12.31 .... output = tensor([1.5514, 1.5514, 0.5514, 0.5514], grad_fn=<NllLossBackward>)\n",
----+      "09:01:12.31 LOG:\n",
----+      "09:01:12.31 .... 'cel mean' = 'cel mean'\n",
----+      "09:01:12.31 .... output = tensor(1.0514, grad_fn=<NllLossBackward>)\n",
----+      "09:01:12.31 LOG:\n",
----+      "09:01:12.31 .... 'cel sum' = 'cel sum'\n",
----+      "09:01:12.31 .... output = tensor(4.2058, grad_fn=<NllLossBackward>)\n",
----+      "09:01:12.31 LOG:\n",
----+      "09:01:12.32 .... 'focal sum' = 'focal sum'\n",
----+      "09:01:12.32 .... loss(inp, tar) = tensor(6.7352, grad_fn=<SumBackward0>)\n",
----+      "09:01:12.32 LOG:\n",
----+      "09:01:12.32 .... 'focal mean' = 'focal mean'\n",
----+      "09:01:12.32 .... loss(inp, tar) = tensor(0.4210, grad_fn=<MeanBackward0>)\n",
----+      "09:01:12.32 LOG:\n",
----+      "09:01:12.33 .... 'focal none' = 'focal none'\n",
----+      "09:01:12.33 .... loss(inp, tar) = tensor([0.9635, 0.9635, 0.0991, 0.0991], grad_fn=<MulBackward0>)\n",
----+      "09:01:12.33 LOG:\n",
----+      "09:01:12.33 .... 'focal none' = 'focal none'\n",
----+      "09:01:12.33 .... loss(inp, tar) = tensor([0.4716, 0.4716, 0.0075, 0.0075], grad_fn=<MulBackward0>)\n",
----+      "09:01:12.33 LOG:\n",
----+      "09:01:12.34 .... 'crf,none' = 'crf,none'\n",
----+      "09:01:12.34 .... output = tensor([4.2927], grad_fn=<NegBackward>)\n",
----+      "09:01:12.34 LOG:\n",
----+      "09:01:12.34 .... 'crf,mean' = 'crf,mean'\n",
----+      "09:01:12.34 .... output = tensor(4.3138, grad_fn=<NegBackward>)\n",
----+      "09:01:12.34 LOG:\n",
----+      "09:01:12.35 .... 'crf,sum' = 'crf,sum'\n",
----+      "09:01:12.35 .... output = tensor(4.2588, grad_fn=<NegBackward>)\n",
----+      "09:01:12.35 LOG:\n",
----+      "09:01:12.35 .... 'crf,token_mean' = 'crf,token_mean'\n",
----+      "09:01:12.35 .... output = tensor(1.0429, grad_fn=<DivBackward0>)\n",
----+      "09:01:12.35 LOG:\n",
----+      "09:01:12.35 .... 'dice none,micro' = 'dice none,micro'\n",
----+      "09:01:12.36 .... output = tensor(0.1876, grad_fn=<PowBackward0>)\n",
----+      "09:01:12.36 LOG:\n",
----+      "09:01:12.36 .... 'dice mean,micro' = 'dice mean,micro'\n",
----+      "09:01:12.36 .... output = tensor(0.0625, grad_fn=<DivBackward0>)\n",
----+      "09:01:12.36 LOG:\n",
----+      "09:01:12.36 .... 'dice sum,micro' = 'dice sum,micro'\n",
----+      "09:01:12.36 .... output = tensor(0.1876, grad_fn=<SumBackward0>)\n",
----+      "09:01:12.37 LOG:\n",
----+      "09:01:12.37 .... 'dice sum,micro' = 'dice sum,micro'\n",
----+      "09:01:12.37 .... output = tensor(0.0812, grad_fn=<SumBackward0>)\n",
----+      "09:01:12.37 LOG:\n",
----+      "09:01:12.37 .... 'dice none,macro' = 'dice none,macro'\n",
----+      "09:01:12.37 .... loss(inp, tar) = tensor([5.9547, 4.4872, 2.0551], grad_fn=<MulBackward0>)\n",
----+      "09:01:12.37 LOG:\n",
----+      "09:01:12.38 .... 'dice mean,macro' = 'dice mean,macro'\n",
----+      "09:01:12.38 .... loss(inp, tar) = tensor(0.2112, grad_fn=<DivBackward0>)\n",
----+      "09:01:12.38 LOG:\n",
----+      "09:01:12.38 .... 'dice sum,macro' = 'dice sum,macro'\n",
----+      "09:01:12.38 .... loss(inp, tar) = tensor(0.6335, grad_fn=<SumBackward0>)\n",
----+      "09:01:12.38 LOG:\n",
----+      "09:01:12.38 .... 'dice none,macro' = 'dice none,macro'\n",
----+      "09:01:12.39 .... loss(inp, tar) = tensor([0.3340, 0.6546, 0.3057], grad_fn=<MulBackward0>)\n",
----+      "09:01:12.39 LOG:\n",
----+      "09:01:12.39 .... 'dice none,macro' = 'dice none,macro'\n",
----+      "09:01:12.39 .... loss(inp, tar) = tensor([0.0373, 0.2805, 0.0286], grad_fn=<MulBackward0>)\n",
----+      "09:01:12.39 LOG:\n",
----+      "09:01:12.39 .... 'dice sum,macro' = 'dice sum,macro'\n",
----+      "09:01:12.40 .... output = tensor([0.5989, 0.7696, 0.2411], grad_fn=<MulBackward0>)\n",
----+      "09:01:12.40 LOG:\n",
----+      "09:01:12.40 .... 'dice sum,macro' = 'dice sum,macro'\n",
----+      "09:01:12.40 .... output = tensor([0.2148, 0.4559, 0.0140], grad_fn=<MulBackward0>)\n"
----      ]
----     },
----     {
----@@ -155,7 +155,7 @@
----        "('dice sum,macro', tensor([0.2148, 0.4559, 0.0140], grad_fn=<MulBackward0>))"
----       ]
----      },
-----     "execution_count": 11,
----+     "execution_count": 13,
----      "metadata": {},
----      "output_type": "execute_result"
----     }
----@@ -249,25 +249,25 @@
----     "# output.backward()\n",
----     "pp('dice none,macro',loss(inp, tar))\n",
----     "\n",
-----    "loss = FocalDiceLoss(reduction='none',macro_average=True,alpha=5.0)\n",
----+    "loss = FocalDiceLoss(reduction='mean',macro_average=True)\n",
----     "output = loss(inp, tar)\n",
----     "# output.backward()\n",
-----    "pp('dice none,macro',loss(inp, tar))\n",
----+    "pp('dice mean,macro',loss(inp, tar))\n",
----     "\n",
-----    "loss = FocalDiceLoss(reduction='none',macro_average=True)\n",
----+    "loss = FocalDiceLoss(reduction='sum',macro_average=True)\n",
----     "output = loss(inp, tar)\n",
----     "# output.backward()\n",
-----    "pp('dice none,macro',loss(inp, tar))\n",
----+    "pp('dice sum,macro',loss(inp, tar))\n",
----     "\n",
-----    "loss = FocalDiceLoss(reduction='mean',macro_average=True)\n",
----+    "loss = FocalDiceLoss(reduction='none',macro_average=True,alpha=1.0)\n",
----     "output = loss(inp, tar)\n",
----     "# output.backward()\n",
-----    "pp('dice mean,macro',loss(inp, tar))\n",
----+    "pp('dice none,macro',loss(inp, tar))\n",
----     "\n",
-----    "loss = FocalDiceLoss(reduction='sum',macro_average=True)\n",
----+    "loss = FocalDiceLoss(reduction='none',macro_average=True, alpha=3)\n",
----     "output = loss(inp, tar)\n",
----     "# output.backward()\n",
-----    "pp('dice sum,macro',loss(inp, tar))\n",
----+    "pp('dice none,macro',loss(inp, tar))\n",
----     "\n",
----     "inp = torch.tensor([[[0,1,0],[1,0,1],[0,0,1],[0,1,0]]],dtype=torch.float, requires_grad=True)\n",
----     "tar = torch.tensor([[0,1,2,0]],dtype=torch.long)\n",
----@@ -284,78 +284,6 @@
----     "pp('dice sum,macro',output)"
----    ]
----   },
-----  {
-----   "cell_type": "code",
-----   "execution_count": null,
-----   "metadata": {},
-----   "outputs": [],
-----   "source": [
-----    "# loss = LinearChainCRF(num_labels=5,reduction='none')\n",
-----    "# output = loss(inp, tar,mask)\n",
-----    "# # output.backward()\n",
-----    "# ic('crf,none',output)\n",
-----    "\n",
-----    "# loss = LinearChainCRF(num_labels=5,reduction='mean')\n",
-----    "# output = loss(inp, tar,mask)\n",
-----    "# # output.backward()\n",
-----    "# ic('crf,mean',output)\n",
-----    "\n",
-----    "# loss = LinearChainCRF(num_labels=5,reduction='sum')\n",
-----    "# output = loss(inp, tar,mask)\n",
-----    "# # output.backward()\n",
-----    "# ic('crf,sum',output)\n",
-----    "\n",
-----    "# loss = LinearChainCRF(num_labels=5,reduction='token_mean')\n",
-----    "# output = loss(inp, tar,mask)\n",
-----    "# # output.backward()\n",
-----    "# ic('crf,token_mean',output)\n",
-----    "\n",
-----    "# loss = FocalDiceLoss(reduction='none',macro_average=True)\n",
-----    "# output = loss(inp, tar)\n",
-----    "# # output.backward()\n",
-----    "# pp('focal none,macro',loss(inp, tar))\n",
-----    "\n",
-----    "# loss = FocalDiceLoss(reduction='mean',macro_average=True)\n",
-----    "# output = loss(inp, tar)\n",
-----    "# # output.backward()\n",
-----    "# pp('focal mean,macro',loss(inp, tar))\n",
-----    "\n",
-----    "# loss = FocalDiceLoss(reduction='sum',macro_average=True)\n",
-----    "# output = loss(inp, tar)\n",
-----    "# # output.backward()\n",
-----    "# pp('focal sum,macro',loss(inp, tar))\n",
-----    "\n",
-----    "# loss = FocalDiceLoss(reduction='none', macro_average=False)\n",
-----    "# output = loss(inp, tar)\n",
-----    "# # output.backward()\n",
-----    "# pp('focal none,micro',output)\n",
-----    "\n",
-----    "# loss = FocalDiceLoss(reduction='mean', macro_average=False)\n",
-----    "# output = loss(inp, tar)\n",
-----    "# # output.backward()\n",
-----    "# pp('focal mean,micro',output)\n",
-----    "\n",
-----    "# loss = FocalDiceLoss(reduction='sum', macro_average=False)\n",
-----    "# output = loss(inp, tar)\n",
-----    "# # output.backward()\n",
-----    "# pp('focal sum,micro',output)\n",
-----    "\n",
-----    "# loss = CrossEntropyLoss(reduction='none')\n",
-----    "# output = loss(inp, tar)\n",
-----    "# # output.backward()\n",
-----    "# pp('cel none',output)\n",
-----    "\n",
-----    "# loss = CrossEntropyLoss(reduction='mean')\n",
-----    "# output = loss(inp, tar)\n",
-----    "# # output.backward()\n",
-----    "# pp('cel mean',output)\n",
-----    "\n",
-----    "# loss = CrossEntropyLoss(reduction='sum')\n",
-----    "# output = loss(inp, tar)\n",
-----    "# # output.backward()\n",
-----    "# pp('cel sum',output)"
-----   ]
-----  },
----   {
----    "cell_type": "code",
----    "execution_count": 2,
----@@ -365,15 +293,20 @@
----      "name": "stderr",
----      "output_type": "stream",
----      "text": [
-----      "ic| os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M')): 0\n",
-----      "ic| os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M')): 0\n",
-----      "ic| max(len(d) for d in self.datasets): 12\n"
----+      "10:05:46.40 LOG:\n",
----+      "10:05:46.46 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
----+      "10:05:46.66 LOG:\n",
----+      "10:05:46.66 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
----+      "10:06:04.19 LOG:\n",
----+      "10:06:04.20 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
----+      "10:06:04.34 LOG:\n",
----+      "10:06:04.34 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n"
----      ]
----     },
----     {
----      "data": {
----       "text/plain": [
-----       "12"
----+       "10609"
----       ]
----      },
----      "execution_count": 2,
----@@ -408,6 +341,35 @@
----     "len(dm.train_dataset)"
----    ]
----   },
----+  {
----+   "cell_type": "code",
----+   "execution_count": 4,
----+   "metadata": {},
----+   "outputs": [
----+    {
----+     "data": {
----+      "text/plain": [
----+       "10609"
----+      ]
----+     },
----+     "execution_count": 4,
----+     "metadata": {},
----+     "output_type": "execute_result"
----+    }
----+   ],
----+   "source": [
----+    "# it=dm.train_dataset\n",
----+    "# ni=next(it)\n",
----+    "# it=dm.train_dataset.datasets[0]\n",
----+    "dm.train_dataset.__len__()#determine_class_weights()\n",
----+    "# ct=torch.zeros(10)\n",
----+    "# for _ in range(64):\n",
----+    "#     print('.',end='')\n",
----+    "#     ni=next(it)\n",
----+    "#     ct+=torch.bincount(ni['labels'].view(-1))\n",
----+    "# return ct/sum(ct)"
----+   ]
----+  },
----   {
----    "cell_type": "code",
----    "execution_count": 19,
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index 9d99993..7417f08 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,42 +1,42 @@
---- seed: 42
---- trainer:
-----    gpus: 1 # the number of gpus, 0 for CPU
----+    # gpus: 1 # the number of gpus, 0 for CPU
----+    # num_nodes: 1
----+    # max_epochs: 8
----+    # max_steps: null # precedence over max_epochs
----+    # accumulate_grad_batches: 4 # accumulates grads every k batches
----+    # gradient_clip_val: 0
----+    # amp_level: O1 # O1/O2 for mixed precision
----+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----+    # checkpoint_callback: false  # Provided by exp_manager
----+    # logger: false #false  # Provided by exp_manager
----+    # log_every_n_steps: 1  # Interval of logging.
----+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    # resume_from_checkpoint: null
----+
----+    gpus: 0 # the number of gpus, 0 for CPU
----     num_nodes: 1
----     max_epochs: 8
----     max_steps: null # precedence over max_epochs
----     accumulate_grad_batches: 4 # accumulates grads every k batches
----     gradient_clip_val: 0
-----    amp_level: O1 # O1/O2 for mixed precision
-----    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    accelerator: ddp
----+    amp_level: O0 # O1/O2 for mixed precision
----+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----     checkpoint_callback: false  # Provided by exp_manager
----     logger: false #false  # Provided by exp_manager
----     log_every_n_steps: 1  # Interval of logging.
----     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----     resume_from_checkpoint: null
---- 
-----    # gpus: 0 # the number of gpus, 0 for CPU
-----    # num_nodes: 1
-----    # max_epochs: 3
-----    # max_steps: null # precedence over max_epochs
-----    # accumulate_grad_batches: 8 # accumulates grads every k batches
-----    # gradient_clip_val: 0.5
-----    # amp_level: O0 # O1/O2 for mixed precision
-----    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # # accelerator: ddp
-----    # checkpoint_callback: false  # Provided by exp_manager
-----    # logger: false #false  # Provided by exp_manager
-----    # log_every_n_steps: 1  # Interval of logging.
-----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # resume_from_checkpoint: null
-----
---- exp_manager:
-----    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /root/data # /home/nxingyu2/data # 
-----tmp_path: /tmp # /home/nxingyu2/data/tmp # 
----+base_path: /home/nxingyu2/data # /root/data # 
----+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
----@@ -57,7 +57,7 @@ model:
----     punct_class_weights: true
----     
----     dataset:
-----        data_dir: /root/data # /home/nxingyu2/data # 
----+        data_dir: /home/nxingyu2/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -108,7 +108,7 @@ model:
----         activation: 'relu'
----         log_softmax: false
----         use_transformer_init: true
-----        loss: 'cel'
----+        loss: 'dice'
---- 
----     domain_head:
----         domain_num_fc_layers: 1
----@@ -121,7 +121,7 @@ model:
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 3
----+        alpha: 4
----         macro_average: true
---- 
----     focal_loss: 
----diff --git a/experiment/info.log b/experiment/info.log
----index 6f9e06b..5024c4f 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,83 +1,2 @@
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f13d3dd1400>" 
-----will be used during training (effective maximum steps = 60) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 60
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.003162277660168378
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f13c79baa30>" 
-----will be used during training (effective maximum steps = 800) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 800
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
----+[INFO] - GPU available: True, used: False
----+[INFO] - TPU available: None, using: 0 TPU cores
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/hparams.yaml
---deleted file mode 100644
---index 650b6cc..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/hparams.yaml
---+++ /dev/null
---@@ -1,104 +0,0 @@
----seed: 42
----trainer:
----  gpus: 0
----  num_nodes: 1
----  max_epochs: 8
----  max_steps: null
----  accumulate_grad_batches: 4
----  gradient_clip_val: 0
----  amp_level: O0
----  precision: 32
----  checkpoint_callback: false
----  logger: false
----  log_every_n_steps: 1
----  val_check_interval: 1.0
----  resume_from_checkpoint: null
----exp_manager:
----  exp_dir: /home/nxingyu2/project/
----  name: Punctuation_with_Domain_discriminator
----  create_tensorboard_logger: true
----  create_checkpoint_callback: true
----base_path: /home/nxingyu2/data
----tmp_path: /home/nxingyu2/data/tmp
----model:
----  nemo_path: null
----  transformer_path: google/electra-small-discriminator
----  initial_unfrozen: 1
----  punct_label_ids:
----  - ''
----  - '!'
----  - ','
----  - '-'
----  - .
----  - ':'
----  - ;
----  - '?'
----  - —
----  - …
----  punct_class_weights: true
----  dataset:
----    data_dir: /home/nxingyu2/data
----    labelled:
----    - /home/nxingyu2/data/ted_talks_processed
----    unlabelled: null
----    max_seq_length: 128
----    pad_label: ''
----    ignore_extra_tokens: false
----    ignore_start_end: false
----    use_cache: false
----    num_workers: 0
----    pin_memory: true
----    drop_last: false
----    num_labels: 10
----    num_domains: 1
----    test_unlabelled: true
----    train_ds:
----      shuffle: true
----      num_samples: -1
----      batch_size: 8
----    validation_ds:
----      shuffle: true
----      num_samples: -1
----      batch_size: 8
----  tokenizer:
----    tokenizer_name: google/electra-small-discriminator
----    vocab_file: null
----    tokenizer_model: null
----    special_tokens: null
----  language_model:
----    pretrained_model_name: google/electra-small-discriminator
----    lm_checkpoint: null
----    config_file: null
----    config: null
----  punct_head:
----    punct_num_fc_layers: 1
----    fc_dropout: 0.1
----    activation: relu
----    log_softmax: false
----    use_transformer_init: true
----    loss: dice
----  domain_head:
----    domain_num_fc_layers: 1
----    fc_dropout: 0.1
----    activation: relu
----    log_softmax: false
----    use_transformer_init: true
----    loss: cel
----    gamma: 0.1
----  dice_loss:
----    epsilon: 0.01
----    alpha: 4
----    macro_average: true
----  focal_loss:
----    gamma: 5
----  optim:
----    name: adamw
----    lr: 0.001
----    weight_decay: 0.0
----    sched:
----      name: WarmupAnnealing
----      warmup_steps: null
----      warmup_ratio: 0.1
----      last_epoch: -1
----      monitor: val_loss
----      reduce_on_plateau: false
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/lightning_logs.txt
---deleted file mode 100644
---index 756258b..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/lightning_logs.txt
---+++ /dev/null
---@@ -1,42 +0,0 @@
----GPU available: True, used: False
----TPU available: None, using: 0 TPU cores
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | punct_classifier    | TokenClassifier      | 2.6 K 
----2 | domain_classifier   | SequenceClassifier   | 257   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----825 K     Trainable params
----12.7 M    Non-trainable params
----13.5 M    Total params
----Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/lr_find_temp_model.ckpt
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | punct_classifier    | TokenClassifier      | 2.6 K 
----2 | domain_classifier   | SequenceClassifier   | 257   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----825 K     Trainable params
----12.7 M    Non-trainable params
----13.5 M    Total params
----Epoch 1, global step 100: val_loss reached 0.71243 (best 0.71243), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.71-epoch=1.ckpt" as top 3
----Epoch 2, global step 200: val_loss reached 0.60420 (best 0.60420), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.60-epoch=2.ckpt" as top 3
----Epoch 3, global step 300: val_loss reached 0.54977 (best 0.54977), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.55-epoch=3.ckpt" as top 3
----Epoch 4, global step 400: val_loss reached 0.58447 (best 0.54977), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.58-epoch=4.ckpt" as top 3
----Epoch 5, global step 500: val_loss reached 0.58260 (best 0.54977), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.58-epoch=5.ckpt" as top 3
----Epoch 6, global step 600: val_loss reached 0.53657 (best 0.53657), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.54-epoch=6.ckpt" as top 3
----Epoch 7, global step 700: val_loss reached 0.56844 (best 0.53657), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.57-epoch=7.ckpt" as top 3
----GPU available: True, used: False
----TPU available: None, using: 0 TPU cores
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_error_log.txt
---deleted file mode 100644
---index e6499a0..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_error_log.txt
---+++ /dev/null
---@@ -1,28 +0,0 @@
----[NeMo W 2021-02-05 10:48:12 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 10:48:12 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-05 10:48:24 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 10:48:24 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 10:48:29 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 11:08:02 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f1b0f34dac0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-05 11:08:35 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f1b0f34d5b0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-05 12:25:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 12:25:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 12:26:20 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f1b0f34d130> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index e5bb9e8..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,30 +0,0 @@
----[NeMo W 2021-02-05 10:48:12 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo I 2021-02-05 10:48:12 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12
----[NeMo I 2021-02-05 10:48:12 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-05 10:48:12 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-05 10:48:24 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 10:48:24 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 10:48:29 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 11:08:02 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f1b0f34dac0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-05 11:08:35 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f1b0f34d5b0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-05 12:25:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 12:25:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 12:26:20 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f1b0f34d130> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/events.out.tfevents.1612499897.intern-instance.31879.0 b/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/events.out.tfevents.1612499897.intern-instance.31879.0
---deleted file mode 100644
---index 658293c..0000000
---Binary files a/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/events.out.tfevents.1612499897.intern-instance.31879.0 and /dev/null differ
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/git-info.log
---deleted file mode 100644
---index 1104b94..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/git-info.log
---+++ /dev/null
---@@ -1,321 +0,0 @@
----commit hash: ed7303d1e763178a0ec2d13f917d7de6c8553488
----diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/events.out.tfevents.1612493730.intern-instance.20106.0 b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/events.out.tfevents.1612493730.intern-instance.20106.0
----index af71914..25acd93 100644
----Binary files a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/events.out.tfevents.1612493730.intern-instance.20106.0 and b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/events.out.tfevents.1612493730.intern-instance.20106.0 differ
----diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/lightning_logs.txt
----index 95f4866..756258b 100644
------- a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/lightning_logs.txt
----+++ b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/lightning_logs.txt
----@@ -35,3 +35,8 @@ Epoch 1, global step 100: val_loss reached 0.71243 (best 0.71243), saving model
---- Epoch 2, global step 200: val_loss reached 0.60420 (best 0.60420), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.60-epoch=2.ckpt" as top 3
---- Epoch 3, global step 300: val_loss reached 0.54977 (best 0.54977), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.55-epoch=3.ckpt" as top 3
---- Epoch 4, global step 400: val_loss reached 0.58447 (best 0.54977), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.58-epoch=4.ckpt" as top 3
----+Epoch 5, global step 500: val_loss reached 0.58260 (best 0.54977), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.58-epoch=5.ckpt" as top 3
----+Epoch 6, global step 600: val_loss reached 0.53657 (best 0.53657), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.54-epoch=6.ckpt" as top 3
----+Epoch 7, global step 700: val_loss reached 0.56844 (best 0.53657), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.57-epoch=7.ckpt" as top 3
----+GPU available: True, used: False
----+TPU available: None, using: 0 TPU cores
----diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_error_log.txt
----index 7732b8a..e6499a0 100644
------- a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_error_log.txt
----+++ b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_error_log.txt
----@@ -17,3 +17,12 @@
---- [NeMo W 2021-02-05 11:08:35 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f1b0f34d5b0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
----       warnings.warn(warn_msg)
----     
----+[NeMo W 2021-02-05 12:25:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----+      warnings.warn(*args, **kwargs)
----+    
----+[NeMo W 2021-02-05 12:25:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----+      warnings.warn(*args, **kwargs)
----+    
----+[NeMo W 2021-02-05 12:26:20 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f1b0f34d130> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
----+      warnings.warn(warn_msg)
----+    
----diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_log_globalrank-0_localrank-0.txt
----index cb6d529..e5bb9e8 100644
------- a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_log_globalrank-0_localrank-0.txt
----+++ b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_log_globalrank-0_localrank-0.txt
----@@ -19,3 +19,12 @@
---- [NeMo W 2021-02-05 11:08:35 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f1b0f34d5b0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
----       warnings.warn(warn_msg)
----     
----+[NeMo W 2021-02-05 12:25:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----+      warnings.warn(*args, **kwargs)
----+    
----+[NeMo W 2021-02-05 12:25:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----+      warnings.warn(*args, **kwargs)
----+    
----+[NeMo W 2021-02-05 12:26:20 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f1b0f34d130> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
----+      warnings.warn(warn_msg)
----+    
----diff --git a/README.md b/README.md
----index ca7075a..ae03334 100644
------- a/README.md
----+++ b/README.md
----@@ -182,4 +182,26 @@ label                                                precision    recall       f
---- Electra small cel weighted ted l 1 unfrozen 0.0031622776 lr adamw accgrad4 bs8
---- 
---- 
-----
----+### elsmall dice alpha 4 weighted ted-l unfrozen 0.003162277660 lr adamw accgrad4 bbs8
----+
----+label                 |   precision  |  recall |    f1    |      support
----+---|---|---|---|---
----+ (label_id: 0)        |      79.50   |   29.94 |   43.50  |    5026
----+! (label_id: 1)       |       6.84   |   20.59 |   10.27  |     102
----+, (label_id: 2)       |      50.70   |   60.09 |   55.00  |   17571
----+- (label_id: 3)       |      64.45   |   82.11 |   72.22  |    1526
----+. (label_id: 4)       |      57.40   |   49.43 |   53.12  |   14767
----+: (label_id: 5)       |      17.86   |   31.83 |   22.89  |     289
----+; (label_id: 6)       |       1.50   |    5.88 |    2.39  |      85
----+? (label_id: 7)       |      37.02   |   61.32 |   46.17  |    1228
----+— (label_id: 8)       |       6.44   |    7.34 |    6.86  |     763
----+… (label_id: 9)       |       0.00   |    0.00 |    0.00  |      80
----+-------------------||||
----+micro avg             |      51.99   |   51.99 |   51.99  |   41437
----+macro avg             |      32.17   |   34.85 |   31.24  |   41437
----+weighted avg          |      55.33   |   51.99 |   51.87  |   41437
----+
----+{'punct_f1': tensor(31.2411),
----+ 'punct_precision': tensor(32.1728),
----+ 'punct_recall': tensor(34.8539),
----+ 'test_loss': tensor(0.6303)}
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index 7417f08..ace920a 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -121,7 +121,7 @@ model:
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 4
----+        alpha: 1
----         macro_average: true
---- 
----     focal_loss: 
----diff --git a/experiment/info.log b/experiment/info.log
----index ccfd4ae..5024c4f 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,220 +1,2 @@
---- [INFO] - GPU available: True, used: False
---- [INFO] - TPU available: None, using: 0 TPU cores
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f1b0f3334c0>" 
-----will be used during training (effective maximum steps = 60) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 60
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 13.5 M
-----1 | punct_classifier    | TokenClassifier      | 2.6 K 
-----2 | domain_classifier   | SequenceClassifier   | 257   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----825 K     Trainable params
-----12.7 M    Non-trainable params
-----13.5 M    Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          17.26      96.74      29.30        184
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                         58.62       2.86       5.46        594
------ (label_id: 3)                                          7.14       8.47       7.75         59
-----. (label_id: 4)                                         48.80      15.46      23.48        524
-----: (label_id: 5)                                          1.12       5.56       1.87         18
-----; (label_id: 6)                                          0.00       0.00       0.00         13
-----? (label_id: 7)                                         14.89       7.37       9.86         95
-----— (label_id: 8)                                          0.00       0.00       0.00         12
-----… (label_id: 9)                                          0.00       0.00       0.00          0
------------------------
-----micro avg                                               19.23      19.23      19.23       1503
-----macro avg                                               16.43      15.16       8.64       1503
-----weighted avg                                            43.53      19.23      14.88       1503
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00         92
------------------------
-----micro avg                                              100.00     100.00     100.00         92
-----macro avg                                              100.00     100.00     100.00         92
-----weighted avg                                           100.00     100.00     100.00         92
-----
-----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/lr_find_temp_model.ckpt
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.003162277660168378
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f1b00306f10>" 
-----will be used during training (effective maximum steps = 800) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 800
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 13.5 M
-----1 | punct_classifier    | TokenClassifier      | 2.6 K 
-----2 | domain_classifier   | SequenceClassifier   | 257   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----825 K     Trainable params
-----12.7 M    Non-trainable params
-----13.5 M    Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          17.82      95.16      30.03        124
-----! (label_id: 1)                                          0.00       0.00       0.00          3
-----, (label_id: 2)                                         70.59       3.01       5.77        399
------ (label_id: 3)                                          2.56       3.23       2.86         31
-----. (label_id: 4)                                         48.76      17.40      25.65        339
-----: (label_id: 5)                                          1.79       9.09       2.99         11
-----; (label_id: 6)                                          0.00       0.00       0.00          1
-----? (label_id: 7)                                         15.62       7.58      10.20         66
-----— (label_id: 8)                                          0.00       0.00       0.00         11
-----… (label_id: 9)                                          0.00       0.00       0.00          0
------------------------
-----micro avg                                               19.90      19.90      19.90        985
-----macro avg                                               17.46      15.05       8.61        985
-----weighted avg                                            48.77      19.90      15.75        985
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00         62
------------------------
-----micro avg                                              100.00     100.00     100.00         62
-----macro avg                                              100.00     100.00     100.00         62
-----weighted avg                                           100.00     100.00     100.00         62
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                         100.00       0.05       0.09       4284
-----! (label_id: 1)                                          0.00       0.00       0.00        151
-----, (label_id: 2)                                         44.08      77.25      56.13      14795
------ (label_id: 3)                                         65.97      71.00      68.39       1286
-----. (label_id: 4)                                         55.76       5.49       9.99      12169
-----: (label_id: 5)                                         16.64      34.86      22.53        350
-----; (label_id: 6)                                          3.61       8.48       5.06        165
-----? (label_id: 7)                                         18.27      69.05      28.90       1021
-----— (label_id: 8)                                          2.51       7.36       3.74        421
-----… (label_id: 9)                                          0.00       0.00       0.00         85
------------------------
-----micro avg                                               39.98      39.98      39.98      34727
-----macro avg                                               30.68      27.35      19.48      34727
-----weighted avg                                            53.85      39.98      31.11      34727
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2142
------------------------
-----micro avg                                              100.00     100.00     100.00       2142
-----macro avg                                              100.00     100.00     100.00       2142
-----weighted avg                                           100.00     100.00     100.00       2142
-----
-----[INFO] - Epoch 1, global step 100: val_loss reached 0.71243 (best 0.71243), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.71-epoch=1.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       4044
-----! (label_id: 1)                                          0.00       0.00       0.00        145
-----, (label_id: 2)                                         43.32      43.32      43.32      13833
------ (label_id: 3)                                         59.53      75.81      66.69       1174
-----. (label_id: 4)                                         51.13      53.22      52.15      11588
-----: (label_id: 5)                                         32.11      24.69      27.92        320
-----; (label_id: 6)                                          3.27      10.76       5.01        158
-----? (label_id: 7)                                         22.43      67.01      33.61       1067
-----— (label_id: 8)                                          3.45      11.46       5.30        445
-----… (label_id: 9)                                          0.00       0.00       0.00         74
------------------------
-----micro avg                                               42.35      42.35      42.35      32848
-----macro avg                                               21.52      28.63      23.40      32848
-----weighted avg                                            39.51      42.35      40.48      32848
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2022
------------------------
-----micro avg                                              100.00     100.00     100.00       2022
-----macro avg                                              100.00     100.00     100.00       2022
-----weighted avg                                           100.00     100.00     100.00       2022
-----
-----[INFO] - Epoch 2, global step 200: val_loss reached 0.60420 (best 0.60420), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.60-epoch=2.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          84.44       0.89       1.75       4292
-----! (label_id: 1)                                          7.41       9.02       8.14        133
-----, (label_id: 2)                                         43.74      44.01      43.88      14694
------ (label_id: 3)                                         52.78      80.86      63.87       1327
-----. (label_id: 4)                                         52.94      50.45      51.66      12213
-----: (label_id: 5)                                         18.73      39.23      25.36        339
-----; (label_id: 6)                                          3.05      21.52       5.35        158
-----? (label_id: 7)                                         27.21      55.73      36.57       1055
-----— (label_id: 8)                                          3.02      13.20       4.92        485
-----… (label_id: 9)                                          0.00       0.00       0.00         68
------------------------
-----micro avg                                               41.91      41.91      41.91      34764
-----macro avg                                               29.33      31.49      24.15      34764
-----weighted avg                                            50.62      41.91      40.83      34764
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2146
------------------------
-----micro avg                                              100.00     100.00     100.00       2146
-----macro avg                                              100.00     100.00     100.00       2146
-----weighted avg                                           100.00     100.00     100.00       2146
-----
-----[INFO] - Epoch 3, global step 300: val_loss reached 0.54977 (best 0.54977), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.55-epoch=3.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          53.66       2.71       5.15       4066
-----! (label_id: 1)                                         10.99       6.90       8.47        145
-----, (label_id: 2)                                         43.36      35.48      39.02      14051
------ (label_id: 3)                                         63.28      71.71      67.23       1276
-----. (label_id: 4)                                         50.84      73.36      60.06      11507
-----: (label_id: 5)                                         23.79      39.88      29.80        321
-----; (label_id: 6)                                          3.17       2.67       2.90        150
-----? (label_id: 7)                                         44.77      47.00      45.86        983
-----— (label_id: 8)                                          2.80      10.36       4.41        386
-----… (label_id: 9)                                          0.00       0.00       0.00         83
------------------------
-----micro avg                                               45.79      45.79      45.79      32968
-----macro avg                                               29.67      29.01      26.29      32968
-----weighted avg                                            46.95      45.79      42.59      32968
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2033
------------------------
-----micro avg                                              100.00     100.00     100.00       2033
-----macro avg                                              100.00     100.00     100.00       2033
-----weighted avg                                           100.00     100.00     100.00       2033
-----
-----[INFO] - Epoch 4, global step 400: val_loss reached 0.58447 (best 0.54977), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.58-epoch=4.ckpt" as top 3
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/hparams.yaml
---deleted file mode 100644
---index a7e1e19..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/hparams.yaml
---+++ /dev/null
---@@ -1,104 +0,0 @@
----seed: 42
----trainer:
----  gpus: 0
----  num_nodes: 1
----  max_epochs: 8
----  max_steps: null
----  accumulate_grad_batches: 4
----  gradient_clip_val: 0
----  amp_level: O0
----  precision: 32
----  checkpoint_callback: false
----  logger: false
----  log_every_n_steps: 1
----  val_check_interval: 1.0
----  resume_from_checkpoint: null
----exp_manager:
----  exp_dir: /home/nxingyu2/project/
----  name: Punctuation_with_Domain_discriminator
----  create_tensorboard_logger: true
----  create_checkpoint_callback: true
----base_path: /home/nxingyu2/data
----tmp_path: /home/nxingyu2/data/tmp
----model:
----  nemo_path: null
----  transformer_path: google/electra-small-discriminator
----  initial_unfrozen: 1
----  punct_label_ids:
----  - ''
----  - '!'
----  - ','
----  - '-'
----  - .
----  - ':'
----  - ;
----  - '?'
----  - —
----  - …
----  punct_class_weights: true
----  dataset:
----    data_dir: /home/nxingyu2/data
----    labelled:
----    - /home/nxingyu2/data/ted_talks_processed
----    unlabelled: null
----    max_seq_length: 128
----    pad_label: ''
----    ignore_extra_tokens: false
----    ignore_start_end: false
----    use_cache: false
----    num_workers: 0
----    pin_memory: true
----    drop_last: false
----    num_labels: 10
----    num_domains: 1
----    test_unlabelled: true
----    train_ds:
----      shuffle: true
----      num_samples: -1
----      batch_size: 8
----    validation_ds:
----      shuffle: true
----      num_samples: -1
----      batch_size: 8
----  tokenizer:
----    tokenizer_name: google/electra-small-discriminator
----    vocab_file: null
----    tokenizer_model: null
----    special_tokens: null
----  language_model:
----    pretrained_model_name: google/electra-small-discriminator
----    lm_checkpoint: null
----    config_file: null
----    config: null
----  punct_head:
----    punct_num_fc_layers: 1
----    fc_dropout: 0.1
----    activation: relu
----    log_softmax: false
----    use_transformer_init: true
----    loss: dice
----  domain_head:
----    domain_num_fc_layers: 1
----    fc_dropout: 0.1
----    activation: relu
----    log_softmax: false
----    use_transformer_init: true
----    loss: cel
----    gamma: 0.1
----  dice_loss:
----    epsilon: 0.01
----    alpha: 1
----    macro_average: true
----  focal_loss:
----    gamma: 5
----  optim:
----    name: adamw
----    lr: 0.001
----    weight_decay: 0.0
----    sched:
----      name: WarmupAnnealing
----      warmup_steps: null
----      warmup_ratio: 0.1
----      last_epoch: -1
----      monitor: val_loss
----      reduce_on_plateau: false
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/lightning_logs.txt
---deleted file mode 100644
---index 6c6e9b9..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/lightning_logs.txt
---+++ /dev/null
---@@ -1,42 +0,0 @@
----GPU available: True, used: False
----TPU available: None, using: 0 TPU cores
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | punct_classifier    | TokenClassifier      | 2.6 K 
----2 | domain_classifier   | SequenceClassifier   | 257   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----825 K     Trainable params
----12.7 M    Non-trainable params
----13.5 M    Total params
----Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/lr_find_temp_model.ckpt
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | punct_classifier    | TokenClassifier      | 2.6 K 
----2 | domain_classifier   | SequenceClassifier   | 257   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----825 K     Trainable params
----12.7 M    Non-trainable params
----13.5 M    Total params
----Epoch 1, global step 100: val_loss reached 0.37278 (best 0.37278), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.37-epoch=1.ckpt" as top 3
----Epoch 2, global step 200: val_loss reached 0.26551 (best 0.26551), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=2.ckpt" as top 3
----Epoch 3, global step 300: val_loss reached 0.37096 (best 0.26551), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.37-epoch=3.ckpt" as top 3
----Epoch 4, global step 400: val_loss reached 0.25577 (best 0.25577), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=4.ckpt" as top 3
----Epoch 5, global step 500: val_loss reached 0.35675 (best 0.25577), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.36-epoch=5.ckpt" as top 3
----Epoch 6, global step 600: val_loss reached 0.30581 (best 0.25577), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.31-epoch=6.ckpt" as top 3
----Epoch 7, global step 700: val_loss reached 0.24315 (best 0.24315), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=7.ckpt" as top 3
----GPU available: True, used: False
----TPU available: None, using: 0 TPU cores
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/nemo_error_log.txt
---deleted file mode 100644
---index e628500..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/nemo_error_log.txt
---+++ /dev/null
---@@ -1,28 +0,0 @@
----[NeMo W 2021-02-05 12:30:41 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 12:30:41 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-05 12:30:53 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 12:30:53 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 12:30:58 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 12:50:21 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f65f74ef100> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-05 12:50:55 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f65f7515220> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-05 14:06:12 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 14:06:12 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 14:06:48 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f65f7515040> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index f8349d0..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,30 +0,0 @@
----[NeMo W 2021-02-05 12:30:41 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo I 2021-02-05 12:30:41 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_12-30-41
----[NeMo I 2021-02-05 12:30:41 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-05 12:30:41 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-05 12:30:53 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 12:30:53 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 12:30:58 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 12:50:21 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f65f74ef100> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-05 12:50:55 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f65f7515220> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-05 14:06:12 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 14:06:12 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 14:06:48 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f65f7515040> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/events.out.tfevents.1612505959.intern-instance.12714.0 b/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/events.out.tfevents.1612505959.intern-instance.12714.0
---deleted file mode 100644
---index 0303750..0000000
---Binary files a/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/events.out.tfevents.1612505959.intern-instance.12714.0 and /dev/null differ
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/git-info.log
---deleted file mode 100644
---index ae1343a..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/git-info.log
---+++ /dev/null
---@@ -1,412 +0,0 @@
----commit hash: ed7303d1e763178a0ec2d13f917d7de6c8553488
----diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/events.out.tfevents.1612493730.intern-instance.20106.0 b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/events.out.tfevents.1612493730.intern-instance.20106.0
----index af71914..25acd93 100644
----Binary files a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/events.out.tfevents.1612493730.intern-instance.20106.0 and b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/events.out.tfevents.1612493730.intern-instance.20106.0 differ
----diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/lightning_logs.txt
----index 95f4866..756258b 100644
------- a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/lightning_logs.txt
----+++ b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/lightning_logs.txt
----@@ -35,3 +35,8 @@ Epoch 1, global step 100: val_loss reached 0.71243 (best 0.71243), saving model
---- Epoch 2, global step 200: val_loss reached 0.60420 (best 0.60420), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.60-epoch=2.ckpt" as top 3
---- Epoch 3, global step 300: val_loss reached 0.54977 (best 0.54977), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.55-epoch=3.ckpt" as top 3
---- Epoch 4, global step 400: val_loss reached 0.58447 (best 0.54977), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.58-epoch=4.ckpt" as top 3
----+Epoch 5, global step 500: val_loss reached 0.58260 (best 0.54977), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.58-epoch=5.ckpt" as top 3
----+Epoch 6, global step 600: val_loss reached 0.53657 (best 0.53657), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.54-epoch=6.ckpt" as top 3
----+Epoch 7, global step 700: val_loss reached 0.56844 (best 0.53657), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.57-epoch=7.ckpt" as top 3
----+GPU available: True, used: False
----+TPU available: None, using: 0 TPU cores
----diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_error_log.txt
----index 7732b8a..e6499a0 100644
------- a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_error_log.txt
----+++ b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_error_log.txt
----@@ -17,3 +17,12 @@
---- [NeMo W 2021-02-05 11:08:35 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f1b0f34d5b0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
----       warnings.warn(warn_msg)
----     
----+[NeMo W 2021-02-05 12:25:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----+      warnings.warn(*args, **kwargs)
----+    
----+[NeMo W 2021-02-05 12:25:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----+      warnings.warn(*args, **kwargs)
----+    
----+[NeMo W 2021-02-05 12:26:20 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f1b0f34d130> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
----+      warnings.warn(warn_msg)
----+    
----diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_log_globalrank-0_localrank-0.txt
----index cb6d529..e5bb9e8 100644
------- a/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_log_globalrank-0_localrank-0.txt
----+++ b/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/nemo_log_globalrank-0_localrank-0.txt
----@@ -19,3 +19,12 @@
---- [NeMo W 2021-02-05 11:08:35 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f1b0f34d5b0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
----       warnings.warn(warn_msg)
----     
----+[NeMo W 2021-02-05 12:25:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----+      warnings.warn(*args, **kwargs)
----+    
----+[NeMo W 2021-02-05 12:25:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----+      warnings.warn(*args, **kwargs)
----+    
----+[NeMo W 2021-02-05 12:26:20 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f1b0f34d130> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
----+      warnings.warn(warn_msg)
----+    
----diff --git a/README.md b/README.md
----index ca7075a..1f40c55 100644
------- a/README.md
----+++ b/README.md
----@@ -182,4 +182,100 @@ label                                                precision    recall       f
---- Electra small cel weighted ted l 1 unfrozen 0.0031622776 lr adamw accgrad4 bs8
---- 
---- 
----+### elsmall dice alpha 4 weighted ted-l unfrozen 0.003162277660 lr adamw accgrad4 bbs8
----+
----+label                 |   precision  |  recall |    f1    |      support
----+---|---|---|---|---
----+ (label_id: 0)        |      79.50   |   29.94 |   43.50  |    5026
----+! (label_id: 1)       |       6.84   |   20.59 |   10.27  |     102
----+, (label_id: 2)       |      50.70   |   60.09 |   55.00  |   17571
----+- (label_id: 3)       |      64.45   |   82.11 |   72.22  |    1526
----+. (label_id: 4)       |      57.40   |   49.43 |   53.12  |   14767
----+: (label_id: 5)       |      17.86   |   31.83 |   22.89  |     289
----+; (label_id: 6)       |       1.50   |    5.88 |    2.39  |      85
----+? (label_id: 7)       |      37.02   |   61.32 |   46.17  |    1228
----+— (label_id: 8)       |       6.44   |    7.34 |    6.86  |     763
----+… (label_id: 9)       |       0.00   |    0.00 |    0.00  |      80
----+-------------------||||
----+micro avg             |      51.99   |   51.99 |   51.99  |   41437
----+macro avg             |      32.17   |   34.85 |   31.24  |   41437
----+weighted avg          |      55.33   |   51.99 |   51.87  |   41437
----+
----+{'punct_f1': tensor(31.2411),
----+ 'punct_precision': tensor(32.1728),
----+ 'punct_recall': tensor(34.8539),
----+ 'test_loss': tensor(0.6303)}
----+
----+
----+### elsmall dice alpha 1 weighted ted-l unfrozen 0.007943282347 lr adamw accgrad4 bbs7
----+
----+label                 |   precision  |  recall |    f1    |      support
----+---|---|---|---|---
----+ (label_id: 0)             |     0.00  |  0.00 |   0.00  |  5026
----+! (label_id: 1)            |     0.00  |  0.00 |   0.00  |   102
----+, (label_id: 2)            |    42.79  | 47.54 |  45.04  | 17571
----+- (label_id: 3)            |    73.63  | 80.87 |  77.08  |  1526
----+. (label_id: 4)            |    47.36  | 55.16 |  50.96  | 14767
----+: (label_id: 5)            |    10.88  | 27.68 |  15.62  |   289
----+; (label_id: 6)            |     0.00  |  0.00 |   0.00  |    85
----+? (label_id: 7)            |    43.18  | 60.10 |  50.26  |  1228
----+— (label_id: 8)            |     3.03  |  2.36 |   2.65  |   763
----+… (label_id: 9)            |     0.00  |  0.00 |   0.00  |    80
----+-------------------||||
----+micro avg                  |    44.81  | 44.81 |  44.81  | 41437
----+macro avg                  |    22.09  | 27.37 |  24.16  | 41437
----+weighted avg               |    39.14  | 44.81 |  41.75  | 41437
----+
----+{'punct_f1': tensor(24.1611),
----+ 'punct_precision': tensor(22.0869),
----+ 'punct_recall': tensor(27.3705),
----+ 'test_loss': tensor(0.4047)}
----+
----+
----+### elsmall crf ted-l unfrozen 0.005011872336272719 lr adamw accgrad4 bbs8
----+
----+label                  |  precision | recall |   f1   |     support
----+---|---|---|---|---
----+ (label_id: 0)         |     59.35  |  52.35 |  55.63 |   7314
----+! (label_id: 1)        |      0.00  |   0.00 |   0.00 |    154
----+, (label_id: 2)        |     44.15  |  82.80 |  57.59 |  28180
----+- (label_id: 3)        |      3.91  |   2.02 |   2.66 |   1933
----+. (label_id: 4)        |     39.91  |  11.64 |  18.02 |  24624
----+: (label_id: 5)        |      0.00  |   0.00 |   0.00 |    522
----+; (label_id: 6)        |      0.00  |   0.00 |   0.00 |    485
----+? (label_id: 7)        |      0.00  |   0.00 |   0.00 |   2096
----+— (label_id: 8)        |      0.00  |   0.00 |   0.00 |   2055
----+… (label_id: 9)        |      0.00  |   0.00 |   0.00 |    123
----+-------------------||||
----+micro avg              |     44.55  |  44.55 |  44.55 |  67486
----+macro avg              |     14.73  |  14.88 |  13.39 |  67486
----+weighted avg           |     39.54  |  44.55 |  36.73 |  67486
----+
----+{'punct_f1': 13.390362739562988,
----+ 'punct_precision': 14.73101806640625,
----+ 'punct_recall': 14.881169319152832,
----+ 'test_loss': 11.328206062316895}
----+
----+### elsmall dice alpha 3 no weight ted-l unfrozen 0.005011872336272719 lr adamw accgrad4 bbs8
----+label                |  precision | recall |   f1   |    support
----+---|---|---|---|---
----+ (label_id: 0)       |  62.32   | 99.78 |  76.72  |   7314
----+! (label_id: 1)      |   0.00   |  0.00 |   0.00  |    154
----+, (label_id: 2)      |  49.81   |  4.72 |   8.62  |  28180
----+- (label_id: 3)      |   5.91   | 28.35 |   9.78  |   1933
----+. (label_id: 4)      |  41.80   | 52.40 |  46.50  |  24624
----+: (label_id: 5)      |   0.94   |  4.02 |   1.53  |    522
----+; (label_id: 6)      |   0.00   |  0.00 |   0.00  |    485
----+? (label_id: 7)      |   4.92   | 24.86 |   8.22  |   2096
----+— (label_id: 8)      |   0.00   |  0.00 |   0.00  |   2055
----+… (label_id: 9)      |   0.00   |  0.00 |   0.00  |    123
----+-------------------||||
----+micro avg            |  33.52   | 33.52 |  33.52  |  67486
----+macro avg            |  16.57   | 21.41 |  15.14  |  67486
----+weighted avg         |  43.14   | 33.52 |  29.43  |  67486
----+
----+'punct_f1': 15.136445999145508,
----+ 'punct_precision': 16.57059097290039,
----+ 'punct_recall': 21.41229820251465,
----+ 'test_loss': 0.6608337163925171}
---- 
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index 7417f08..4e38aa0 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -41,7 +41,7 @@ tmp_path: /home/nxingyu2/data/tmp # /tmp #
---- model:
----     nemo_path: null
----     transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-----    initial_unfrozen: 1
----+    initial_unfrozen: 0
----     punct_label_ids:
----         - ""
----         - "!"
----@@ -108,7 +108,7 @@ model:
----         activation: 'relu'
----         log_softmax: false
----         use_transformer_init: true
-----        loss: 'dice'
----+        loss: 'crf'
---- 
----     domain_head:
----         domain_num_fc_layers: 1
----@@ -121,7 +121,7 @@ model:
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 4
----+        alpha: 1
----         macro_average: true
---- 
----     focal_loss: 
----diff --git a/experiment/info.log b/experiment/info.log
----index ccfd4ae..5024c4f 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,220 +1,2 @@
---- [INFO] - GPU available: True, used: False
---- [INFO] - TPU available: None, using: 0 TPU cores
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f1b0f3334c0>" 
-----will be used during training (effective maximum steps = 60) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 60
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 13.5 M
-----1 | punct_classifier    | TokenClassifier      | 2.6 K 
-----2 | domain_classifier   | SequenceClassifier   | 257   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----825 K     Trainable params
-----12.7 M    Non-trainable params
-----13.5 M    Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          17.26      96.74      29.30        184
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                         58.62       2.86       5.46        594
------ (label_id: 3)                                          7.14       8.47       7.75         59
-----. (label_id: 4)                                         48.80      15.46      23.48        524
-----: (label_id: 5)                                          1.12       5.56       1.87         18
-----; (label_id: 6)                                          0.00       0.00       0.00         13
-----? (label_id: 7)                                         14.89       7.37       9.86         95
-----— (label_id: 8)                                          0.00       0.00       0.00         12
-----… (label_id: 9)                                          0.00       0.00       0.00          0
------------------------
-----micro avg                                               19.23      19.23      19.23       1503
-----macro avg                                               16.43      15.16       8.64       1503
-----weighted avg                                            43.53      19.23      14.88       1503
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00         92
------------------------
-----micro avg                                              100.00     100.00     100.00         92
-----macro avg                                              100.00     100.00     100.00         92
-----weighted avg                                           100.00     100.00     100.00         92
-----
-----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/lr_find_temp_model.ckpt
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.003162277660168378
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f1b00306f10>" 
-----will be used during training (effective maximum steps = 800) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 800
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 13.5 M
-----1 | punct_classifier    | TokenClassifier      | 2.6 K 
-----2 | domain_classifier   | SequenceClassifier   | 257   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----825 K     Trainable params
-----12.7 M    Non-trainable params
-----13.5 M    Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          17.82      95.16      30.03        124
-----! (label_id: 1)                                          0.00       0.00       0.00          3
-----, (label_id: 2)                                         70.59       3.01       5.77        399
------ (label_id: 3)                                          2.56       3.23       2.86         31
-----. (label_id: 4)                                         48.76      17.40      25.65        339
-----: (label_id: 5)                                          1.79       9.09       2.99         11
-----; (label_id: 6)                                          0.00       0.00       0.00          1
-----? (label_id: 7)                                         15.62       7.58      10.20         66
-----— (label_id: 8)                                          0.00       0.00       0.00         11
-----… (label_id: 9)                                          0.00       0.00       0.00          0
------------------------
-----micro avg                                               19.90      19.90      19.90        985
-----macro avg                                               17.46      15.05       8.61        985
-----weighted avg                                            48.77      19.90      15.75        985
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00         62
------------------------
-----micro avg                                              100.00     100.00     100.00         62
-----macro avg                                              100.00     100.00     100.00         62
-----weighted avg                                           100.00     100.00     100.00         62
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                         100.00       0.05       0.09       4284
-----! (label_id: 1)                                          0.00       0.00       0.00        151
-----, (label_id: 2)                                         44.08      77.25      56.13      14795
------ (label_id: 3)                                         65.97      71.00      68.39       1286
-----. (label_id: 4)                                         55.76       5.49       9.99      12169
-----: (label_id: 5)                                         16.64      34.86      22.53        350
-----; (label_id: 6)                                          3.61       8.48       5.06        165
-----? (label_id: 7)                                         18.27      69.05      28.90       1021
-----— (label_id: 8)                                          2.51       7.36       3.74        421
-----… (label_id: 9)                                          0.00       0.00       0.00         85
------------------------
-----micro avg                                               39.98      39.98      39.98      34727
-----macro avg                                               30.68      27.35      19.48      34727
-----weighted avg                                            53.85      39.98      31.11      34727
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2142
------------------------
-----micro avg                                              100.00     100.00     100.00       2142
-----macro avg                                              100.00     100.00     100.00       2142
-----weighted avg                                           100.00     100.00     100.00       2142
-----
-----[INFO] - Epoch 1, global step 100: val_loss reached 0.71243 (best 0.71243), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.71-epoch=1.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       4044
-----! (label_id: 1)                                          0.00       0.00       0.00        145
-----, (label_id: 2)                                         43.32      43.32      43.32      13833
------ (label_id: 3)                                         59.53      75.81      66.69       1174
-----. (label_id: 4)                                         51.13      53.22      52.15      11588
-----: (label_id: 5)                                         32.11      24.69      27.92        320
-----; (label_id: 6)                                          3.27      10.76       5.01        158
-----? (label_id: 7)                                         22.43      67.01      33.61       1067
-----— (label_id: 8)                                          3.45      11.46       5.30        445
-----… (label_id: 9)                                          0.00       0.00       0.00         74
------------------------
-----micro avg                                               42.35      42.35      42.35      32848
-----macro avg                                               21.52      28.63      23.40      32848
-----weighted avg                                            39.51      42.35      40.48      32848
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2022
------------------------
-----micro avg                                              100.00     100.00     100.00       2022
-----macro avg                                              100.00     100.00     100.00       2022
-----weighted avg                                           100.00     100.00     100.00       2022
-----
-----[INFO] - Epoch 2, global step 200: val_loss reached 0.60420 (best 0.60420), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.60-epoch=2.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          84.44       0.89       1.75       4292
-----! (label_id: 1)                                          7.41       9.02       8.14        133
-----, (label_id: 2)                                         43.74      44.01      43.88      14694
------ (label_id: 3)                                         52.78      80.86      63.87       1327
-----. (label_id: 4)                                         52.94      50.45      51.66      12213
-----: (label_id: 5)                                         18.73      39.23      25.36        339
-----; (label_id: 6)                                          3.05      21.52       5.35        158
-----? (label_id: 7)                                         27.21      55.73      36.57       1055
-----— (label_id: 8)                                          3.02      13.20       4.92        485
-----… (label_id: 9)                                          0.00       0.00       0.00         68
------------------------
-----micro avg                                               41.91      41.91      41.91      34764
-----macro avg                                               29.33      31.49      24.15      34764
-----weighted avg                                            50.62      41.91      40.83      34764
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2146
------------------------
-----micro avg                                              100.00     100.00     100.00       2146
-----macro avg                                              100.00     100.00     100.00       2146
-----weighted avg                                           100.00     100.00     100.00       2146
-----
-----[INFO] - Epoch 3, global step 300: val_loss reached 0.54977 (best 0.54977), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.55-epoch=3.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          53.66       2.71       5.15       4066
-----! (label_id: 1)                                         10.99       6.90       8.47        145
-----, (label_id: 2)                                         43.36      35.48      39.02      14051
------ (label_id: 3)                                         63.28      71.71      67.23       1276
-----. (label_id: 4)                                         50.84      73.36      60.06      11507
-----: (label_id: 5)                                         23.79      39.88      29.80        321
-----; (label_id: 6)                                          3.17       2.67       2.90        150
-----? (label_id: 7)                                         44.77      47.00      45.86        983
-----— (label_id: 8)                                          2.80      10.36       4.41        386
-----… (label_id: 9)                                          0.00       0.00       0.00         83
------------------------
-----micro avg                                               45.79      45.79      45.79      32968
-----macro avg                                               29.67      29.01      26.29      32968
-----weighted avg                                            46.95      45.79      42.59      32968
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2033
------------------------
-----micro avg                                              100.00     100.00     100.00       2033
-----macro avg                                              100.00     100.00     100.00       2033
-----weighted avg                                           100.00     100.00     100.00       2033
-----
-----[INFO] - Epoch 4, global step 400: val_loss reached 0.58447 (best 0.54977), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_10-48-12/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.58-epoch=4.ckpt" as top 3
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/hparams.yaml
---deleted file mode 100644
---index 28a5d44..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/hparams.yaml
---+++ /dev/null
---@@ -1,104 +0,0 @@
----seed: 42
----trainer:
----  gpus: 0
----  num_nodes: 1
----  max_epochs: 8
----  max_steps: null
----  accumulate_grad_batches: 4
----  gradient_clip_val: 0
----  amp_level: O0
----  precision: 32
----  checkpoint_callback: false
----  logger: false
----  log_every_n_steps: 1
----  val_check_interval: 1.0
----  resume_from_checkpoint: null
----exp_manager:
----  exp_dir: /home/nxingyu2/project/
----  name: Punctuation_with_Domain_discriminator
----  create_tensorboard_logger: true
----  create_checkpoint_callback: true
----base_path: /home/nxingyu2/data
----tmp_path: /home/nxingyu2/data/tmp
----model:
----  nemo_path: null
----  transformer_path: google/electra-small-discriminator
----  initial_unfrozen: 0
----  punct_label_ids:
----  - ''
----  - '!'
----  - ','
----  - '-'
----  - .
----  - ':'
----  - ;
----  - '?'
----  - —
----  - …
----  punct_class_weights: true
----  dataset:
----    data_dir: /home/nxingyu2/data
----    labelled:
----    - /home/nxingyu2/data/ted_talks_processed
----    unlabelled: null
----    max_seq_length: 128
----    pad_label: ''
----    ignore_extra_tokens: false
----    ignore_start_end: false
----    use_cache: false
----    num_workers: 0
----    pin_memory: true
----    drop_last: false
----    num_labels: 10
----    num_domains: 1
----    test_unlabelled: true
----    train_ds:
----      shuffle: true
----      num_samples: -1
----      batch_size: 8
----    validation_ds:
----      shuffle: true
----      num_samples: -1
----      batch_size: 8
----  tokenizer:
----    tokenizer_name: google/electra-small-discriminator
----    vocab_file: null
----    tokenizer_model: null
----    special_tokens: null
----  language_model:
----    pretrained_model_name: google/electra-small-discriminator
----    lm_checkpoint: null
----    config_file: null
----    config: null
----  punct_head:
----    punct_num_fc_layers: 1
----    fc_dropout: 0.1
----    activation: relu
----    log_softmax: false
----    use_transformer_init: true
----    loss: crf
----  domain_head:
----    domain_num_fc_layers: 1
----    fc_dropout: 0.1
----    activation: relu
----    log_softmax: false
----    use_transformer_init: true
----    loss: cel
----    gamma: 0.1
----  dice_loss:
----    epsilon: 0.01
----    alpha: 1
----    macro_average: true
----  focal_loss:
----    gamma: 5
----  optim:
----    name: adamw
----    lr: 0.001
----    weight_decay: 0.0
----    sched:
----      name: WarmupAnnealing
----      warmup_steps: null
----      warmup_ratio: 0.1
----      last_epoch: -1
----      monitor: val_loss
----      reduce_on_plateau: false
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/lightning_logs.txt
---deleted file mode 100644
---index 9bbaa3a..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/lightning_logs.txt
---+++ /dev/null
---@@ -1,42 +0,0 @@
----GPU available: True, used: False
----TPU available: None, using: 0 TPU cores
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | punct_classifier    | TokenClassifier      | 2.6 K 
----2 | domain_classifier   | SequenceClassifier   | 257   
----3 | punctuation_loss    | LinearChainCRF       | 120   
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----36.0 K    Trainable params
----13.4 M    Non-trainable params
----13.5 M    Total params
----Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/lr_find_temp_model.ckpt
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | punct_classifier    | TokenClassifier      | 2.6 K 
----2 | domain_classifier   | SequenceClassifier   | 257   
----3 | punctuation_loss    | LinearChainCRF       | 120   
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----36.0 K    Trainable params
----13.4 M    Non-trainable params
----13.5 M    Total params
----Epoch 1, global step 100: val_loss reached 4.50218 (best 4.50218), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/checkpoints/Punctuation_with_Domain_discriminator---val_loss=4.50-epoch=1.ckpt" as top 3
----Epoch 2, global step 200: val_loss reached -7.95181 (best -7.95181), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-7.95-epoch=2.ckpt" as top 3
----Epoch 3, global step 300: val_loss reached -17.86337 (best -17.86337), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-17.86-epoch=3.ckpt" as top 3
----Epoch 4, global step 400: val_loss reached -25.60198 (best -25.60198), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-25.60-epoch=4.ckpt" as top 3
----Epoch 5, global step 500: val_loss reached -31.27675 (best -31.27675), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-31.28-epoch=5.ckpt" as top 3
----Epoch 6, global step 600: val_loss reached -35.57878 (best -35.57878), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-35.58-epoch=6.ckpt" as top 3
----Epoch 7, global step 700: val_loss reached -38.22509 (best -38.22509), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-38.23-epoch=7.ckpt" as top 3
----GPU available: True, used: False
----TPU available: None, using: 0 TPU cores
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/nemo_error_log.txt
---deleted file mode 100644
---index 6cd04a9..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/nemo_error_log.txt
---+++ /dev/null
---@@ -1,28 +0,0 @@
----[NeMo W 2021-02-05 14:11:35 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 14:11:35 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-05 14:11:37 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 14:11:37 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 14:11:42 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 14:31:49 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f44c1de1ac0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-05 14:32:24 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f44c1de15b0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-05 15:49:26 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 15:49:26 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 15:50:05 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f44c1de1130> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index cad8ec2..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,30 +0,0 @@
----[NeMo W 2021-02-05 14:11:35 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo I 2021-02-05 14:11:35 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-05_14-11-35
----[NeMo I 2021-02-05 14:11:35 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-05 14:11:35 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-05 14:11:37 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 14:11:37 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 14:11:42 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 14:31:49 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f44c1de1ac0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-05 14:32:24 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f44c1de15b0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-05 15:49:26 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 15:49:26 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-05 15:50:05 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f44c1de1130> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/git-info.log
---deleted file mode 100644
---index d760651..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/git-info.log
---+++ /dev/null
---@@ -1,690 +0,0 @@
----commit hash: cce540d4c95ad9ed32010552a7aa1fb2486658c8
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index 3d27dd7..f13ae33 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,43 +1,43 @@
---- seed: 42
---- trainer:
-----    gpus: 1 # the number of gpus, 0 for CPU
-----    num_nodes: 1
-----    max_epochs: 2
-----    max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 4 # accumulates grads every k batches
-----    gradient_clip_val: 0
-----    amp_level: O1 # O1/O2 for mixed precision
-----    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    accelerator: ddp
-----    checkpoint_callback: false  # Provided by exp_manager
-----    logger: false #false  # Provided by exp_manager
-----    log_every_n_steps: 1  # Interval of logging.
-----    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    resume_from_checkpoint: null
-----
-----    # gpus: 0 # the number of gpus, 0 for CPU
----+    # gpus: 1 # the number of gpus, 0 for CPU
----     # num_nodes: 1
-----    # max_epochs: 10
----+    # max_epochs: 2
----     # max_steps: null # precedence over max_epochs
----     # accumulate_grad_batches: 4 # accumulates grads every k batches
----     # gradient_clip_val: 0
-----    # amp_level: O0 # O1/O2 for mixed precision
-----    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # # accelerator: ddp
----+    # amp_level: O1 # O1/O2 for mixed precision
----+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----     # checkpoint_callback: false  # Provided by exp_manager
----     # logger: false #false  # Provided by exp_manager
----     # log_every_n_steps: 1  # Interval of logging.
----     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # reload_dataloaders_every_epoch: true
----     # resume_from_checkpoint: null
---- 
----+    gpus: 0 # the number of gpus, 0 for CPU
----+    num_nodes: 1
----+    max_epochs: 8
----+    max_steps: null # precedence over max_epochs
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----+    gradient_clip_val: 0
----+    amp_level: O0 # O1/O2 for mixed precision
----+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----+    checkpoint_callback: false  # Provided by exp_manager
----+    logger: false #false  # Provided by exp_manager
----+    log_every_n_steps: 1  # Interval of logging.
----+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    reload_dataloaders_every_epoch: true
----+    resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /root/data # /home/nxingyu2/data # 
-----tmp_path: /tmp # /home/nxingyu2/data/tmp # 
----+base_path: /home/nxingyu2/data # /root/data # 
----+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
----@@ -61,7 +61,7 @@ model:
----     punct_class_weights: true
----     
----     dataset:
-----        data_dir: /root/data # /home/nxingyu2/data # 
----+        data_dir: /home/nxingyu2/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -125,14 +125,14 @@ model:
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 4
----+        alpha: 3
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: adamw
----+        name: ranger
----         lr: 1e-3
----         weight_decay: 0.00
---- 
----@@ -142,10 +142,12 @@ model:
----             warmup_steps: null
----             warmup_ratio: 0.1
----             last_epoch: -1
----+            delay_epochs: 4
----+
---- 
----             # pytorch lightning args
----             monitor: val_loss
-----            reduce_on_plateau: true
----+            reduce_on_plateau: false
---- hydra:
----     run:
----         dir: .
----diff --git a/experiment/core/optim/lr_scheduler.py b/experiment/core/optim/lr_scheduler.py
----index a17ebf8..a24b059 100644
------- a/experiment/core/optim/lr_scheduler.py
----+++ b/experiment/core/optim/lr_scheduler.py
----@@ -347,6 +347,39 @@ class PolynomialHoldDecayAnnealing(WarmupHoldPolicy):
----         return new_lrs
---- 
---- 
----+class DelayerScheduler(_LRScheduler):
----+	""" Starts with a flat lr schedule until it reaches N epochs the applies a scheduler 
----+	Args:
----+		optimizer (Optimizer): Wrapped optimizer.
----+		delay_epochs: number of epochs to keep the initial lr until starting aplying the scheduler
----+		after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)
----+	"""
----+
----+	def __init__(self, optimizer, delay_epochs, after_scheduler):
----+		self.delay_epochs = delay_epochs
----+		self.after_scheduler = after_scheduler
----+		self.finished = False
----+		super().__init__(optimizer)
----+
----+	def get_lr(self):
----+		if self.last_epoch >= self.delay_epochs:
----+			if not self.finished:
----+				self.after_scheduler.base_lrs = self.base_lrs
----+				self.finished = True
----+			return self.after_scheduler.get_lr()
----+
----+		return self.base_lrs
----+
----+	def step(self, epoch=None):
----+		if self.finished:
----+			if epoch is None:
----+				self.after_scheduler.step(None)
----+			else:
----+				self.after_scheduler.step(epoch - self.delay_epochs)
----+		else:
----+			return super(DelayerScheduler, self).step(epoch)
----+
----+
---- def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: SchedulerParams):
----     """
----     Checks if the scheduler name exists in the registry, and if it doesnt, adds it.
----@@ -365,7 +398,7 @@ def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: Sch
----     register_scheduler_params(name=sched_name, scheduler_params=scheduler_params)
---- 
---- 
-----def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
----+def get_scheduler(name: str, optimizer, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
----     """
----     Convenience method to obtain an _LRScheduler class and partially instantiate it with optimizer kwargs.
----     Args:
----@@ -380,7 +413,9 @@ def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler
----         )
---- 
----     scheduler_cls = AVAILABLE_SCHEDULERS[name]
-----    scheduler = partial(scheduler_cls, **kwargs)
----+    scheduler = scheduler_cls(optimizer=optimizer, **kwargs)
----+    if delay_epochs>0:
----+        scheduler = DelayerScheduler(optimizer=optimizer,delay_epochs=delay_epochs,after_scheduler=scheduler)
----     return scheduler
---- 
---- 
----@@ -584,10 +619,10 @@ def prepare_lr_scheduler(
----         scheduler_args['max_steps'] = max_steps
---- 
----     # Get the scheduler class from the config
-----    scheduler_cls = get_scheduler(scheduler_name, **scheduler_args)
----+    scheduler = get_scheduler(scheduler_name, optimizer **scheduler_args)
---- 
----     # Instantiate the LR schedule
-----    schedule = scheduler_cls(optimizer, **scheduler_args)
----+    # schedule = scheduler_cls(optimizer, **scheduler_args)
---- 
----     logging.info(
----         'Scheduler "%s" \nwill be used during training (effective maximum steps = %d) - \nParameters : \n(%s)',
----diff --git a/experiment/core/optim/optimizers.py b/experiment/core/optim/optimizers.py
----index 5afc4fd..f15686e 100644
------- a/experiment/core/optim/optimizers.py
----+++ b/experiment/core/optim/optimizers.py
----@@ -7,6 +7,7 @@ import torch.optim as optim
---- from omegaconf import DictConfig, OmegaConf
---- from torch.optim import adadelta, adagrad, adamax, rmsprop, rprop
---- from torch.optim.optimizer import Optimizer
----+from torchtools.optim import Ranger
---- 
---- from core.config import OptimizerParams, get_optimizer_config, register_optimizer_params
---- from core.optim.novograd import Novograd
----@@ -24,6 +25,7 @@ AVAILABLE_OPTIMIZERS = {
----     'rmsprop': rmsprop.RMSprop,
----     'rprop': rprop.Rprop,
----     'novograd': Novograd,
----+    'ranger': Ranger,
---- }
---- 
---- 
----diff --git a/experiment/info.log b/experiment/info.log
----index b211d42..5024c4f 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,473 +1,2 @@
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4da7f8e0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.008413951416451957
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e67aa00>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          22.42      11.12      14.86       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                         38.62       7.46      12.50      19530
------ (label_id: 3)                                          6.60      15.75       9.30       1746
-----. (label_id: 4)                                         33.33       0.01       0.01      17976
-----: (label_id: 5)                                          0.79      19.15       1.52        376
-----; (label_id: 6)                                          0.45       2.94       0.78        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.17      50.56       0.33         89
------------------------
-----micro avg                                                5.14       5.14       5.14      48233
-----macro avg                                               10.24      10.70       3.93      48233
-----weighted avg                                            30.92       5.14       7.15      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          22.09      11.27      14.93       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                         35.39       6.74      11.32      19813
------ (label_id: 3)                                          6.03      16.28       8.79       1708
-----. (label_id: 4)                                        100.00       0.01       0.02      18084
-----: (label_id: 5)                                          0.78      20.69       1.51        348
-----; (label_id: 6)                                          0.45       2.55       0.77        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.19      58.62       0.38         87
------------------------
-----micro avg                                                4.89       4.89       4.89      48633
-----macro avg                                               16.49      11.62       3.77      48633
-----weighted avg                                            54.38       4.89       6.67      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.008413951416451957
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e74d910>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          19.61      14.42      16.62        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         38.84       6.82      11.60        689
------ (label_id: 3)                                          4.50      15.52       6.98         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.59      20.00       1.14         10
-----; (label_id: 6)                                          2.56      16.67       4.44          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.25      50.00       0.51          4
------------------------
-----micro avg                                                5.55       5.55       5.55       1641
-----macro avg                                                6.64      12.34       4.13       1641
-----weighted avg                                            18.97       5.55       7.25       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.63      98.91       7.00       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          3.96       1.76       2.44       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.63       3.63       3.63      48233
-----macro avg                                                0.76      10.07       0.94      48233
-----weighted avg                                             0.25       3.63       0.33      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e68b8b0>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          19.61      14.42      16.62        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         38.84       6.82      11.60        689
------ (label_id: 3)                                          4.50      15.52       6.98         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.59      20.00       1.14         10
-----; (label_id: 6)                                          2.56      16.67       4.44          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.25      50.00       0.51          4
------------------------
-----micro avg                                                5.55       5.55       5.55       1641
-----macro avg                                                6.64      12.34       4.13       1641
-----weighted avg                                            18.97       5.55       7.25       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                          0.00       0.00       0.00      19813
------ (label_id: 3)                                          3.51     100.00       6.79       1708
-----. (label_id: 4)                                          0.00       0.00       0.00      18084
-----: (label_id: 5)                                          0.00       0.00       0.00        348
-----; (label_id: 6)                                          0.00       0.00       0.00        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.00       0.00       0.00         87
------------------------
-----micro avg                                                3.51       3.51       3.51      48633
-----macro avg                                                0.35      10.00       0.68      48633
-----weighted avg                                             0.12       3.51       0.24      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75eaf0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                          0.00       0.00       0.00        689
------ (label_id: 3)                                          3.53     100.00       6.83         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.00       0.00       0.00         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                3.53       3.53       3.53       1641
-----macro avg                                                0.35      10.00       0.68       1641
-----weighted avg                                             0.12       3.53       0.24       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75e160>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                          0.00       0.00       0.00        689
------ (label_id: 3)                                          3.53     100.00       6.83         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.00       0.00       0.00         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                3.53       3.53       3.53       1641
-----macro avg                                                0.35      10.00       0.68       1641
-----weighted avg                                             0.12       3.53       0.24       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                          0.00       0.00       0.00      19813
------ (label_id: 3)                                          3.51     100.00       6.79       1708
-----. (label_id: 4)                                          0.00       0.00       0.00      18084
-----: (label_id: 5)                                          0.00       0.00       0.00        348
-----; (label_id: 6)                                          0.00       0.00       0.00        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.00       0.00       0.00         87
------------------------
-----micro avg                                                3.51       3.51       3.51      48633
-----macro avg                                                0.35      10.00       0.68      48633
-----weighted avg                                             0.12       3.51       0.24      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       7470
-----! (label_id: 1)                                          0.00       0.00       0.00        148
-----, (label_id: 2)                                          0.00       0.00       0.00      28513
------ (label_id: 3)                                          3.02     100.00       5.86       2074
-----. (label_id: 4)                                          0.00       0.00       0.00      25120
-----: (label_id: 5)                                          0.00       0.00       0.00        570
-----; (label_id: 6)                                          0.00       0.00       0.00        534
-----? (label_id: 7)                                          0.00       0.00       0.00       2085
-----— (label_id: 8)                                          0.00       0.00       0.00       2073
-----… (label_id: 9)                                          0.00       0.00       0.00        142
------------------------
-----micro avg                                                3.02       3.02       3.02      68729
-----macro avg                                                0.30      10.00       0.59      68729
-----weighted avg                                             0.09       3.02       0.18      68729
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       3735
------------------------
-----micro avg                                              100.00     100.00     100.00       3735
-----macro avg                                              100.00     100.00     100.00       3735
-----weighted avg                                           100.00     100.00     100.00       3735
-----
-----[INFO] - Internal process exited
----+[INFO] - GPU available: True, used: False
----+[INFO] - TPU available: None, using: 0 TPU cores
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/lightning_logs.txt
---deleted file mode 100644
---index 01e1664..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/lightning_logs.txt
---+++ /dev/null
---@@ -1,2 +0,0 @@
----GPU available: True, used: False
----TPU available: None, using: 0 TPU cores
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/nemo_error_log.txt
---deleted file mode 100644
---index ec4f502..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/nemo_error_log.txt
---+++ /dev/null
---@@ -1,4 +0,0 @@
----[NeMo W 2021-02-06 09:03:49 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 09:03:49 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index 8b41f44..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,6 +0,0 @@
----[NeMo W 2021-02-06 09:03:49 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo I 2021-02-06 09:03:49 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_09-03-49
----[NeMo I 2021-02-06 09:03:49 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-06 09:03:49 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/git-info.log
---deleted file mode 100644
---index 43bd866..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/git-info.log
---+++ /dev/null
---@@ -1,690 +0,0 @@
----commit hash: cce540d4c95ad9ed32010552a7aa1fb2486658c8
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index 3d27dd7..f13ae33 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,43 +1,43 @@
---- seed: 42
---- trainer:
-----    gpus: 1 # the number of gpus, 0 for CPU
-----    num_nodes: 1
-----    max_epochs: 2
-----    max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 4 # accumulates grads every k batches
-----    gradient_clip_val: 0
-----    amp_level: O1 # O1/O2 for mixed precision
-----    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    accelerator: ddp
-----    checkpoint_callback: false  # Provided by exp_manager
-----    logger: false #false  # Provided by exp_manager
-----    log_every_n_steps: 1  # Interval of logging.
-----    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    resume_from_checkpoint: null
-----
-----    # gpus: 0 # the number of gpus, 0 for CPU
----+    # gpus: 1 # the number of gpus, 0 for CPU
----     # num_nodes: 1
-----    # max_epochs: 10
----+    # max_epochs: 2
----     # max_steps: null # precedence over max_epochs
----     # accumulate_grad_batches: 4 # accumulates grads every k batches
----     # gradient_clip_val: 0
-----    # amp_level: O0 # O1/O2 for mixed precision
-----    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # # accelerator: ddp
----+    # amp_level: O1 # O1/O2 for mixed precision
----+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----     # checkpoint_callback: false  # Provided by exp_manager
----     # logger: false #false  # Provided by exp_manager
----     # log_every_n_steps: 1  # Interval of logging.
----     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # reload_dataloaders_every_epoch: true
----     # resume_from_checkpoint: null
---- 
----+    gpus: 0 # the number of gpus, 0 for CPU
----+    num_nodes: 1
----+    max_epochs: 8
----+    max_steps: null # precedence over max_epochs
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----+    gradient_clip_val: 0
----+    amp_level: O0 # O1/O2 for mixed precision
----+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----+    checkpoint_callback: false  # Provided by exp_manager
----+    logger: false #false  # Provided by exp_manager
----+    log_every_n_steps: 1  # Interval of logging.
----+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    reload_dataloaders_every_epoch: true
----+    resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /root/data # /home/nxingyu2/data # 
-----tmp_path: /tmp # /home/nxingyu2/data/tmp # 
----+base_path: /home/nxingyu2/data # /root/data # 
----+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
----@@ -61,7 +61,7 @@ model:
----     punct_class_weights: true
----     
----     dataset:
-----        data_dir: /root/data # /home/nxingyu2/data # 
----+        data_dir: /home/nxingyu2/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -125,14 +125,14 @@ model:
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 4
----+        alpha: 3
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: adamw
----+        name: ranger
----         lr: 1e-3
----         weight_decay: 0.00
---- 
----@@ -142,10 +142,12 @@ model:
----             warmup_steps: null
----             warmup_ratio: 0.1
----             last_epoch: -1
----+            delay_epochs: 4
----+
---- 
----             # pytorch lightning args
----             monitor: val_loss
-----            reduce_on_plateau: true
----+            reduce_on_plateau: false
---- hydra:
----     run:
----         dir: .
----diff --git a/experiment/core/optim/lr_scheduler.py b/experiment/core/optim/lr_scheduler.py
----index a17ebf8..ed029f3 100644
------- a/experiment/core/optim/lr_scheduler.py
----+++ b/experiment/core/optim/lr_scheduler.py
----@@ -347,6 +347,39 @@ class PolynomialHoldDecayAnnealing(WarmupHoldPolicy):
----         return new_lrs
---- 
---- 
----+class DelayerScheduler(_LRScheduler):
----+	""" Starts with a flat lr schedule until it reaches N epochs the applies a scheduler 
----+	Args:
----+		optimizer (Optimizer): Wrapped optimizer.
----+		delay_epochs: number of epochs to keep the initial lr until starting aplying the scheduler
----+		after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)
----+	"""
----+
----+	def __init__(self, optimizer, delay_epochs, after_scheduler):
----+		self.delay_epochs = delay_epochs
----+		self.after_scheduler = after_scheduler
----+		self.finished = False
----+		super().__init__(optimizer)
----+
----+	def get_lr(self):
----+		if self.last_epoch >= self.delay_epochs:
----+			if not self.finished:
----+				self.after_scheduler.base_lrs = self.base_lrs
----+				self.finished = True
----+			return self.after_scheduler.get_lr()
----+
----+		return self.base_lrs
----+
----+	def step(self, epoch=None):
----+		if self.finished:
----+			if epoch is None:
----+				self.after_scheduler.step(None)
----+			else:
----+				self.after_scheduler.step(epoch - self.delay_epochs)
----+		else:
----+			return super(DelayerScheduler, self).step(epoch)
----+
----+
---- def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: SchedulerParams):
----     """
----     Checks if the scheduler name exists in the registry, and if it doesnt, adds it.
----@@ -365,7 +398,7 @@ def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: Sch
----     register_scheduler_params(name=sched_name, scheduler_params=scheduler_params)
---- 
---- 
-----def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
----+def get_scheduler(name: str, optimizer, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
----     """
----     Convenience method to obtain an _LRScheduler class and partially instantiate it with optimizer kwargs.
----     Args:
----@@ -380,7 +413,9 @@ def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler
----         )
---- 
----     scheduler_cls = AVAILABLE_SCHEDULERS[name]
-----    scheduler = partial(scheduler_cls, **kwargs)
----+    scheduler = scheduler_cls(optimizer=optimizer, **kwargs)
----+    if delay_epochs>0:
----+        scheduler = DelayerScheduler(optimizer=optimizer,delay_epochs=delay_epochs,after_scheduler=scheduler)
----     return scheduler
---- 
---- 
----@@ -584,10 +619,10 @@ def prepare_lr_scheduler(
----         scheduler_args['max_steps'] = max_steps
---- 
----     # Get the scheduler class from the config
-----    scheduler_cls = get_scheduler(scheduler_name, **scheduler_args)
----+    scheduler = get_scheduler(scheduler_name, optimizer, **scheduler_args)
---- 
----     # Instantiate the LR schedule
-----    schedule = scheduler_cls(optimizer, **scheduler_args)
----+    # schedule = scheduler_cls(optimizer, **scheduler_args)
---- 
----     logging.info(
----         'Scheduler "%s" \nwill be used during training (effective maximum steps = %d) - \nParameters : \n(%s)',
----diff --git a/experiment/core/optim/optimizers.py b/experiment/core/optim/optimizers.py
----index 5afc4fd..f15686e 100644
------- a/experiment/core/optim/optimizers.py
----+++ b/experiment/core/optim/optimizers.py
----@@ -7,6 +7,7 @@ import torch.optim as optim
---- from omegaconf import DictConfig, OmegaConf
---- from torch.optim import adadelta, adagrad, adamax, rmsprop, rprop
---- from torch.optim.optimizer import Optimizer
----+from torchtools.optim import Ranger
---- 
---- from core.config import OptimizerParams, get_optimizer_config, register_optimizer_params
---- from core.optim.novograd import Novograd
----@@ -24,6 +25,7 @@ AVAILABLE_OPTIMIZERS = {
----     'rmsprop': rmsprop.RMSprop,
----     'rprop': rprop.Rprop,
----     'novograd': Novograd,
----+    'ranger': Ranger,
---- }
---- 
---- 
----diff --git a/experiment/info.log b/experiment/info.log
----index b211d42..5024c4f 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,473 +1,2 @@
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4da7f8e0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.008413951416451957
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e67aa00>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          22.42      11.12      14.86       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                         38.62       7.46      12.50      19530
------ (label_id: 3)                                          6.60      15.75       9.30       1746
-----. (label_id: 4)                                         33.33       0.01       0.01      17976
-----: (label_id: 5)                                          0.79      19.15       1.52        376
-----; (label_id: 6)                                          0.45       2.94       0.78        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.17      50.56       0.33         89
------------------------
-----micro avg                                                5.14       5.14       5.14      48233
-----macro avg                                               10.24      10.70       3.93      48233
-----weighted avg                                            30.92       5.14       7.15      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          22.09      11.27      14.93       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                         35.39       6.74      11.32      19813
------ (label_id: 3)                                          6.03      16.28       8.79       1708
-----. (label_id: 4)                                        100.00       0.01       0.02      18084
-----: (label_id: 5)                                          0.78      20.69       1.51        348
-----; (label_id: 6)                                          0.45       2.55       0.77        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.19      58.62       0.38         87
------------------------
-----micro avg                                                4.89       4.89       4.89      48633
-----macro avg                                               16.49      11.62       3.77      48633
-----weighted avg                                            54.38       4.89       6.67      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.008413951416451957
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e74d910>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          19.61      14.42      16.62        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         38.84       6.82      11.60        689
------ (label_id: 3)                                          4.50      15.52       6.98         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.59      20.00       1.14         10
-----; (label_id: 6)                                          2.56      16.67       4.44          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.25      50.00       0.51          4
------------------------
-----micro avg                                                5.55       5.55       5.55       1641
-----macro avg                                                6.64      12.34       4.13       1641
-----weighted avg                                            18.97       5.55       7.25       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.63      98.91       7.00       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          3.96       1.76       2.44       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.63       3.63       3.63      48233
-----macro avg                                                0.76      10.07       0.94      48233
-----weighted avg                                             0.25       3.63       0.33      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e68b8b0>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          19.61      14.42      16.62        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         38.84       6.82      11.60        689
------ (label_id: 3)                                          4.50      15.52       6.98         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.59      20.00       1.14         10
-----; (label_id: 6)                                          2.56      16.67       4.44          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.25      50.00       0.51          4
------------------------
-----micro avg                                                5.55       5.55       5.55       1641
-----macro avg                                                6.64      12.34       4.13       1641
-----weighted avg                                            18.97       5.55       7.25       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                          0.00       0.00       0.00      19813
------ (label_id: 3)                                          3.51     100.00       6.79       1708
-----. (label_id: 4)                                          0.00       0.00       0.00      18084
-----: (label_id: 5)                                          0.00       0.00       0.00        348
-----; (label_id: 6)                                          0.00       0.00       0.00        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.00       0.00       0.00         87
------------------------
-----micro avg                                                3.51       3.51       3.51      48633
-----macro avg                                                0.35      10.00       0.68      48633
-----weighted avg                                             0.12       3.51       0.24      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75eaf0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                          0.00       0.00       0.00        689
------ (label_id: 3)                                          3.53     100.00       6.83         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.00       0.00       0.00         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                3.53       3.53       3.53       1641
-----macro avg                                                0.35      10.00       0.68       1641
-----weighted avg                                             0.12       3.53       0.24       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75e160>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                          0.00       0.00       0.00        689
------ (label_id: 3)                                          3.53     100.00       6.83         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.00       0.00       0.00         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                3.53       3.53       3.53       1641
-----macro avg                                                0.35      10.00       0.68       1641
-----weighted avg                                             0.12       3.53       0.24       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                          0.00       0.00       0.00      19813
------ (label_id: 3)                                          3.51     100.00       6.79       1708
-----. (label_id: 4)                                          0.00       0.00       0.00      18084
-----: (label_id: 5)                                          0.00       0.00       0.00        348
-----; (label_id: 6)                                          0.00       0.00       0.00        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.00       0.00       0.00         87
------------------------
-----micro avg                                                3.51       3.51       3.51      48633
-----macro avg                                                0.35      10.00       0.68      48633
-----weighted avg                                             0.12       3.51       0.24      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       7470
-----! (label_id: 1)                                          0.00       0.00       0.00        148
-----, (label_id: 2)                                          0.00       0.00       0.00      28513
------ (label_id: 3)                                          3.02     100.00       5.86       2074
-----. (label_id: 4)                                          0.00       0.00       0.00      25120
-----: (label_id: 5)                                          0.00       0.00       0.00        570
-----; (label_id: 6)                                          0.00       0.00       0.00        534
-----? (label_id: 7)                                          0.00       0.00       0.00       2085
-----— (label_id: 8)                                          0.00       0.00       0.00       2073
-----… (label_id: 9)                                          0.00       0.00       0.00        142
------------------------
-----micro avg                                                3.02       3.02       3.02      68729
-----macro avg                                                0.30      10.00       0.59      68729
-----weighted avg                                             0.09       3.02       0.18      68729
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       3735
------------------------
-----micro avg                                              100.00     100.00     100.00       3735
-----macro avg                                              100.00     100.00     100.00       3735
-----weighted avg                                           100.00     100.00     100.00       3735
-----
-----[INFO] - Internal process exited
----+[INFO] - GPU available: True, used: False
----+[INFO] - TPU available: None, using: 0 TPU cores
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/lightning_logs.txt
---deleted file mode 100644
---index 01e1664..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/lightning_logs.txt
---+++ /dev/null
---@@ -1,2 +0,0 @@
----GPU available: True, used: False
----TPU available: None, using: 0 TPU cores
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/nemo_error_log.txt
---deleted file mode 100644
---index 9c11000..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/nemo_error_log.txt
---+++ /dev/null
---@@ -1,4 +0,0 @@
----[NeMo W 2021-02-06 09:08:50 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 09:08:50 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index 75b41a8..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,6 +0,0 @@
----[NeMo W 2021-02-06 09:08:50 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo I 2021-02-06 09:08:50 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_09-08-50
----[NeMo I 2021-02-06 09:08:50 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-06 09:08:50 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/git-info.log
---deleted file mode 100644
---index 8c7a2cc..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/git-info.log
---+++ /dev/null
---@@ -1,720 +0,0 @@
----commit hash: cce540d4c95ad9ed32010552a7aa1fb2486658c8
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index 3d27dd7..6d84830 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,43 +1,43 @@
---- seed: 42
---- trainer:
-----    gpus: 1 # the number of gpus, 0 for CPU
-----    num_nodes: 1
-----    max_epochs: 2
-----    max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 4 # accumulates grads every k batches
-----    gradient_clip_val: 0
-----    amp_level: O1 # O1/O2 for mixed precision
-----    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    accelerator: ddp
-----    checkpoint_callback: false  # Provided by exp_manager
-----    logger: false #false  # Provided by exp_manager
-----    log_every_n_steps: 1  # Interval of logging.
-----    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    resume_from_checkpoint: null
-----
-----    # gpus: 0 # the number of gpus, 0 for CPU
----+    # gpus: 1 # the number of gpus, 0 for CPU
----     # num_nodes: 1
-----    # max_epochs: 10
----+    # max_epochs: 2
----     # max_steps: null # precedence over max_epochs
----     # accumulate_grad_batches: 4 # accumulates grads every k batches
----     # gradient_clip_val: 0
-----    # amp_level: O0 # O1/O2 for mixed precision
-----    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # # accelerator: ddp
----+    # amp_level: O1 # O1/O2 for mixed precision
----+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----     # checkpoint_callback: false  # Provided by exp_manager
----     # logger: false #false  # Provided by exp_manager
----     # log_every_n_steps: 1  # Interval of logging.
----     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # reload_dataloaders_every_epoch: true
----     # resume_from_checkpoint: null
---- 
----+    gpus: 0 # the number of gpus, 0 for CPU
----+    num_nodes: 1
----+    max_epochs: 8
----+    max_steps: null # precedence over max_epochs
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----+    gradient_clip_val: 0
----+    amp_level: O0 # O1/O2 for mixed precision
----+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----+    checkpoint_callback: false  # Provided by exp_manager
----+    logger: false #false  # Provided by exp_manager
----+    log_every_n_steps: 1  # Interval of logging.
----+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    reload_dataloaders_every_epoch: true
----+    resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /root/data # /home/nxingyu2/data # 
-----tmp_path: /tmp # /home/nxingyu2/data/tmp # 
----+base_path: /home/nxingyu2/data # /root/data # 
----+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
----@@ -61,7 +61,7 @@ model:
----     punct_class_weights: true
----     
----     dataset:
-----        data_dir: /root/data # /home/nxingyu2/data # 
----+        data_dir: /home/nxingyu2/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -125,17 +125,18 @@ model:
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 4
----+        alpha: 3
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: adamw
----+        name: ranger
----         lr: 1e-3
----         weight_decay: 0.00
---- 
----+        delay_epochs: 4
----         sched:
----             name: WarmupAnnealing #CyclicLR
----             # Scheduler params
----@@ -145,7 +146,7 @@ model:
---- 
----             # pytorch lightning args
----             monitor: val_loss
-----            reduce_on_plateau: true
----+            reduce_on_plateau: false
---- hydra:
----     run:
----         dir: .
----diff --git a/experiment/core/optim/lr_scheduler.py b/experiment/core/optim/lr_scheduler.py
----index a17ebf8..4944555 100644
------- a/experiment/core/optim/lr_scheduler.py
----+++ b/experiment/core/optim/lr_scheduler.py
----@@ -347,6 +347,39 @@ class PolynomialHoldDecayAnnealing(WarmupHoldPolicy):
----         return new_lrs
---- 
---- 
----+class DelayerScheduler(_LRScheduler):
----+	""" Starts with a flat lr schedule until it reaches N epochs the applies a scheduler 
----+	Args:
----+		optimizer (Optimizer): Wrapped optimizer.
----+		delay_epochs: number of epochs to keep the initial lr until starting aplying the scheduler
----+		after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)
----+	"""
----+
----+	def __init__(self, optimizer, delay_epochs, after_scheduler):
----+		self.delay_epochs = delay_epochs
----+		self.after_scheduler = after_scheduler
----+		self.finished = False
----+		super().__init__(optimizer)
----+
----+	def get_lr(self):
----+		if self.last_epoch >= self.delay_epochs:
----+			if not self.finished:
----+				self.after_scheduler.base_lrs = self.base_lrs
----+				self.finished = True
----+			return self.after_scheduler.get_lr()
----+
----+		return self.base_lrs
----+
----+	def step(self, epoch=None):
----+		if self.finished:
----+			if epoch is None:
----+				self.after_scheduler.step(None)
----+			else:
----+				self.after_scheduler.step(epoch - self.delay_epochs)
----+		else:
----+			return super(DelayerScheduler, self).step(epoch)
----+
----+
---- def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: SchedulerParams):
----     """
----     Checks if the scheduler name exists in the registry, and if it doesnt, adds it.
----@@ -365,7 +398,7 @@ def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: Sch
----     register_scheduler_params(name=sched_name, scheduler_params=scheduler_params)
---- 
---- 
-----def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
----+def get_scheduler(name: str, optimizer, delay_epochs, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
----     """
----     Convenience method to obtain an _LRScheduler class and partially instantiate it with optimizer kwargs.
----     Args:
----@@ -380,12 +413,15 @@ def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler
----         )
---- 
----     scheduler_cls = AVAILABLE_SCHEDULERS[name]
-----    scheduler = partial(scheduler_cls, **kwargs)
----+    scheduler = scheduler_cls(optimizer=optimizer, **kwargs)
----+    if delay_epochs>0:
----+        scheduler = DelayerScheduler(optimizer=optimizer,delay_epochs=delay_epochs,after_scheduler=scheduler)
----     return scheduler
---- 
---- 
---- def prepare_lr_scheduler(
----     optimizer: optim.Optimizer,
----+    delay_epochs: int,
----     scheduler_config: Union[Dict[str, Any], DictConfig],
----     train_dataloader: Optional[Dict[str,int]] = None,
---- 
----@@ -584,10 +620,10 @@ def prepare_lr_scheduler(
----         scheduler_args['max_steps'] = max_steps
---- 
----     # Get the scheduler class from the config
-----    scheduler_cls = get_scheduler(scheduler_name, **scheduler_args)
----+    scheduler = get_scheduler(scheduler_name, optimizer, delay_epochs, **scheduler_args)
---- 
----     # Instantiate the LR schedule
-----    schedule = scheduler_cls(optimizer, **scheduler_args)
----+    # schedule = scheduler_cls(optimizer, **scheduler_args)
---- 
----     logging.info(
----         'Scheduler "%s" \nwill be used during training (effective maximum steps = %d) - \nParameters : \n(%s)',
----diff --git a/experiment/core/optim/optimizers.py b/experiment/core/optim/optimizers.py
----index 5afc4fd..f15686e 100644
------- a/experiment/core/optim/optimizers.py
----+++ b/experiment/core/optim/optimizers.py
----@@ -7,6 +7,7 @@ import torch.optim as optim
---- from omegaconf import DictConfig, OmegaConf
---- from torch.optim import adadelta, adagrad, adamax, rmsprop, rprop
---- from torch.optim.optimizer import Optimizer
----+from torchtools.optim import Ranger
---- 
---- from core.config import OptimizerParams, get_optimizer_config, register_optimizer_params
---- from core.optim.novograd import Novograd
----@@ -24,6 +25,7 @@ AVAILABLE_OPTIMIZERS = {
----     'rmsprop': rmsprop.RMSprop,
----     'rprop': rprop.Rprop,
----     'novograd': Novograd,
----+    'ranger': Ranger,
---- }
---- 
---- 
----diff --git a/experiment/info.log b/experiment/info.log
----index b211d42..5024c4f 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,473 +1,2 @@
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4da7f8e0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.008413951416451957
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e67aa00>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          22.42      11.12      14.86       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                         38.62       7.46      12.50      19530
------ (label_id: 3)                                          6.60      15.75       9.30       1746
-----. (label_id: 4)                                         33.33       0.01       0.01      17976
-----: (label_id: 5)                                          0.79      19.15       1.52        376
-----; (label_id: 6)                                          0.45       2.94       0.78        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.17      50.56       0.33         89
------------------------
-----micro avg                                                5.14       5.14       5.14      48233
-----macro avg                                               10.24      10.70       3.93      48233
-----weighted avg                                            30.92       5.14       7.15      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          22.09      11.27      14.93       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                         35.39       6.74      11.32      19813
------ (label_id: 3)                                          6.03      16.28       8.79       1708
-----. (label_id: 4)                                        100.00       0.01       0.02      18084
-----: (label_id: 5)                                          0.78      20.69       1.51        348
-----; (label_id: 6)                                          0.45       2.55       0.77        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.19      58.62       0.38         87
------------------------
-----micro avg                                                4.89       4.89       4.89      48633
-----macro avg                                               16.49      11.62       3.77      48633
-----weighted avg                                            54.38       4.89       6.67      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.008413951416451957
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e74d910>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          19.61      14.42      16.62        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         38.84       6.82      11.60        689
------ (label_id: 3)                                          4.50      15.52       6.98         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.59      20.00       1.14         10
-----; (label_id: 6)                                          2.56      16.67       4.44          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.25      50.00       0.51          4
------------------------
-----micro avg                                                5.55       5.55       5.55       1641
-----macro avg                                                6.64      12.34       4.13       1641
-----weighted avg                                            18.97       5.55       7.25       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.63      98.91       7.00       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          3.96       1.76       2.44       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.63       3.63       3.63      48233
-----macro avg                                                0.76      10.07       0.94      48233
-----weighted avg                                             0.25       3.63       0.33      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e68b8b0>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          19.61      14.42      16.62        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         38.84       6.82      11.60        689
------ (label_id: 3)                                          4.50      15.52       6.98         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.59      20.00       1.14         10
-----; (label_id: 6)                                          2.56      16.67       4.44          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.25      50.00       0.51          4
------------------------
-----micro avg                                                5.55       5.55       5.55       1641
-----macro avg                                                6.64      12.34       4.13       1641
-----weighted avg                                            18.97       5.55       7.25       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                          0.00       0.00       0.00      19813
------ (label_id: 3)                                          3.51     100.00       6.79       1708
-----. (label_id: 4)                                          0.00       0.00       0.00      18084
-----: (label_id: 5)                                          0.00       0.00       0.00        348
-----; (label_id: 6)                                          0.00       0.00       0.00        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.00       0.00       0.00         87
------------------------
-----micro avg                                                3.51       3.51       3.51      48633
-----macro avg                                                0.35      10.00       0.68      48633
-----weighted avg                                             0.12       3.51       0.24      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75eaf0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                          0.00       0.00       0.00        689
------ (label_id: 3)                                          3.53     100.00       6.83         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.00       0.00       0.00         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                3.53       3.53       3.53       1641
-----macro avg                                                0.35      10.00       0.68       1641
-----weighted avg                                             0.12       3.53       0.24       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75e160>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                          0.00       0.00       0.00        689
------ (label_id: 3)                                          3.53     100.00       6.83         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.00       0.00       0.00         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                3.53       3.53       3.53       1641
-----macro avg                                                0.35      10.00       0.68       1641
-----weighted avg                                             0.12       3.53       0.24       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                          0.00       0.00       0.00      19813
------ (label_id: 3)                                          3.51     100.00       6.79       1708
-----. (label_id: 4)                                          0.00       0.00       0.00      18084
-----: (label_id: 5)                                          0.00       0.00       0.00        348
-----; (label_id: 6)                                          0.00       0.00       0.00        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.00       0.00       0.00         87
------------------------
-----micro avg                                                3.51       3.51       3.51      48633
-----macro avg                                                0.35      10.00       0.68      48633
-----weighted avg                                             0.12       3.51       0.24      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       7470
-----! (label_id: 1)                                          0.00       0.00       0.00        148
-----, (label_id: 2)                                          0.00       0.00       0.00      28513
------ (label_id: 3)                                          3.02     100.00       5.86       2074
-----. (label_id: 4)                                          0.00       0.00       0.00      25120
-----: (label_id: 5)                                          0.00       0.00       0.00        570
-----; (label_id: 6)                                          0.00       0.00       0.00        534
-----? (label_id: 7)                                          0.00       0.00       0.00       2085
-----— (label_id: 8)                                          0.00       0.00       0.00       2073
-----… (label_id: 9)                                          0.00       0.00       0.00        142
------------------------
-----micro avg                                                3.02       3.02       3.02      68729
-----macro avg                                                0.30      10.00       0.59      68729
-----weighted avg                                             0.09       3.02       0.18      68729
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       3735
------------------------
-----micro avg                                              100.00     100.00     100.00       3735
-----macro avg                                              100.00     100.00     100.00       3735
-----weighted avg                                           100.00     100.00     100.00       3735
-----
-----[INFO] - Internal process exited
----+[INFO] - GPU available: True, used: False
----+[INFO] - TPU available: None, using: 0 TPU cores
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index 782fbfa..c51b05b 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -374,6 +374,11 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             scheduler_config = None
---- 
----+        if 'delay_epochs' in optim_config:
----+            delay_epochs = optim_config.pop('delay_epochs')
----+        else:
----+            delay_epochs = 0
----+
----         # Check if caller provided optimizer name, default to Adam otherwise
----         optimizer_cls = optim_config.get('_target_', None)
---- 
----@@ -455,7 +460,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---- 
----         # Try to instantiate scheduler for optimizer
----         self._scheduler = prepare_lr_scheduler(
-----            optimizer=self._optimizer, scheduler_config=scheduler_config,
----+            optimizer=self._optimizer, delay_epoch=delay_epochs,scheduler_config=scheduler_config,
----             train_dataloader=pp({'num_samples' : self.train_size*self.hparams.model.dataset.train_ds.batch_size, 
----             'batch_size': self.hparams.model.dataset.train_ds.batch_size,
----             'drop_last' : self.hparams.model.dataset.drop_last})
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/lightning_logs.txt
---deleted file mode 100644
---index 01e1664..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/lightning_logs.txt
---+++ /dev/null
---@@ -1,2 +0,0 @@
----GPU available: True, used: False
----TPU available: None, using: 0 TPU cores
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/nemo_error_log.txt
---deleted file mode 100644
---index bef3c22..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/nemo_error_log.txt
---+++ /dev/null
---@@ -1,4 +0,0 @@
----[NeMo W 2021-02-06 09:14:08 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 09:14:08 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index ff808b1..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,6 +0,0 @@
----[NeMo W 2021-02-06 09:14:08 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo I 2021-02-06 09:14:08 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_09-14-08
----[NeMo I 2021-02-06 09:14:08 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-06 09:14:08 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/git-info.log
---deleted file mode 100644
---index 04842e2..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/git-info.log
---+++ /dev/null
---@@ -1,720 +0,0 @@
----commit hash: cce540d4c95ad9ed32010552a7aa1fb2486658c8
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index 3d27dd7..6d84830 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,43 +1,43 @@
---- seed: 42
---- trainer:
-----    gpus: 1 # the number of gpus, 0 for CPU
-----    num_nodes: 1
-----    max_epochs: 2
-----    max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 4 # accumulates grads every k batches
-----    gradient_clip_val: 0
-----    amp_level: O1 # O1/O2 for mixed precision
-----    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    accelerator: ddp
-----    checkpoint_callback: false  # Provided by exp_manager
-----    logger: false #false  # Provided by exp_manager
-----    log_every_n_steps: 1  # Interval of logging.
-----    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    resume_from_checkpoint: null
-----
-----    # gpus: 0 # the number of gpus, 0 for CPU
----+    # gpus: 1 # the number of gpus, 0 for CPU
----     # num_nodes: 1
-----    # max_epochs: 10
----+    # max_epochs: 2
----     # max_steps: null # precedence over max_epochs
----     # accumulate_grad_batches: 4 # accumulates grads every k batches
----     # gradient_clip_val: 0
-----    # amp_level: O0 # O1/O2 for mixed precision
-----    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # # accelerator: ddp
----+    # amp_level: O1 # O1/O2 for mixed precision
----+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----     # checkpoint_callback: false  # Provided by exp_manager
----     # logger: false #false  # Provided by exp_manager
----     # log_every_n_steps: 1  # Interval of logging.
----     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # reload_dataloaders_every_epoch: true
----     # resume_from_checkpoint: null
---- 
----+    gpus: 0 # the number of gpus, 0 for CPU
----+    num_nodes: 1
----+    max_epochs: 8
----+    max_steps: null # precedence over max_epochs
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----+    gradient_clip_val: 0
----+    amp_level: O0 # O1/O2 for mixed precision
----+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----+    checkpoint_callback: false  # Provided by exp_manager
----+    logger: false #false  # Provided by exp_manager
----+    log_every_n_steps: 1  # Interval of logging.
----+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    reload_dataloaders_every_epoch: true
----+    resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /root/data # /home/nxingyu2/data # 
-----tmp_path: /tmp # /home/nxingyu2/data/tmp # 
----+base_path: /home/nxingyu2/data # /root/data # 
----+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
----@@ -61,7 +61,7 @@ model:
----     punct_class_weights: true
----     
----     dataset:
-----        data_dir: /root/data # /home/nxingyu2/data # 
----+        data_dir: /home/nxingyu2/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -125,17 +125,18 @@ model:
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 4
----+        alpha: 3
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: adamw
----+        name: ranger
----         lr: 1e-3
----         weight_decay: 0.00
---- 
----+        delay_epochs: 4
----         sched:
----             name: WarmupAnnealing #CyclicLR
----             # Scheduler params
----@@ -145,7 +146,7 @@ model:
---- 
----             # pytorch lightning args
----             monitor: val_loss
-----            reduce_on_plateau: true
----+            reduce_on_plateau: false
---- hydra:
----     run:
----         dir: .
----diff --git a/experiment/core/optim/lr_scheduler.py b/experiment/core/optim/lr_scheduler.py
----index a17ebf8..4944555 100644
------- a/experiment/core/optim/lr_scheduler.py
----+++ b/experiment/core/optim/lr_scheduler.py
----@@ -347,6 +347,39 @@ class PolynomialHoldDecayAnnealing(WarmupHoldPolicy):
----         return new_lrs
---- 
---- 
----+class DelayerScheduler(_LRScheduler):
----+	""" Starts with a flat lr schedule until it reaches N epochs the applies a scheduler 
----+	Args:
----+		optimizer (Optimizer): Wrapped optimizer.
----+		delay_epochs: number of epochs to keep the initial lr until starting aplying the scheduler
----+		after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)
----+	"""
----+
----+	def __init__(self, optimizer, delay_epochs, after_scheduler):
----+		self.delay_epochs = delay_epochs
----+		self.after_scheduler = after_scheduler
----+		self.finished = False
----+		super().__init__(optimizer)
----+
----+	def get_lr(self):
----+		if self.last_epoch >= self.delay_epochs:
----+			if not self.finished:
----+				self.after_scheduler.base_lrs = self.base_lrs
----+				self.finished = True
----+			return self.after_scheduler.get_lr()
----+
----+		return self.base_lrs
----+
----+	def step(self, epoch=None):
----+		if self.finished:
----+			if epoch is None:
----+				self.after_scheduler.step(None)
----+			else:
----+				self.after_scheduler.step(epoch - self.delay_epochs)
----+		else:
----+			return super(DelayerScheduler, self).step(epoch)
----+
----+
---- def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: SchedulerParams):
----     """
----     Checks if the scheduler name exists in the registry, and if it doesnt, adds it.
----@@ -365,7 +398,7 @@ def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: Sch
----     register_scheduler_params(name=sched_name, scheduler_params=scheduler_params)
---- 
---- 
-----def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
----+def get_scheduler(name: str, optimizer, delay_epochs, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
----     """
----     Convenience method to obtain an _LRScheduler class and partially instantiate it with optimizer kwargs.
----     Args:
----@@ -380,12 +413,15 @@ def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler
----         )
---- 
----     scheduler_cls = AVAILABLE_SCHEDULERS[name]
-----    scheduler = partial(scheduler_cls, **kwargs)
----+    scheduler = scheduler_cls(optimizer=optimizer, **kwargs)
----+    if delay_epochs>0:
----+        scheduler = DelayerScheduler(optimizer=optimizer,delay_epochs=delay_epochs,after_scheduler=scheduler)
----     return scheduler
---- 
---- 
---- def prepare_lr_scheduler(
----     optimizer: optim.Optimizer,
----+    delay_epochs: int,
----     scheduler_config: Union[Dict[str, Any], DictConfig],
----     train_dataloader: Optional[Dict[str,int]] = None,
---- 
----@@ -584,10 +620,10 @@ def prepare_lr_scheduler(
----         scheduler_args['max_steps'] = max_steps
---- 
----     # Get the scheduler class from the config
-----    scheduler_cls = get_scheduler(scheduler_name, **scheduler_args)
----+    scheduler = get_scheduler(scheduler_name, optimizer, delay_epochs, **scheduler_args)
---- 
----     # Instantiate the LR schedule
-----    schedule = scheduler_cls(optimizer, **scheduler_args)
----+    # schedule = scheduler_cls(optimizer, **scheduler_args)
---- 
----     logging.info(
----         'Scheduler "%s" \nwill be used during training (effective maximum steps = %d) - \nParameters : \n(%s)',
----diff --git a/experiment/core/optim/optimizers.py b/experiment/core/optim/optimizers.py
----index 5afc4fd..f15686e 100644
------- a/experiment/core/optim/optimizers.py
----+++ b/experiment/core/optim/optimizers.py
----@@ -7,6 +7,7 @@ import torch.optim as optim
---- from omegaconf import DictConfig, OmegaConf
---- from torch.optim import adadelta, adagrad, adamax, rmsprop, rprop
---- from torch.optim.optimizer import Optimizer
----+from torchtools.optim import Ranger
---- 
---- from core.config import OptimizerParams, get_optimizer_config, register_optimizer_params
---- from core.optim.novograd import Novograd
----@@ -24,6 +25,7 @@ AVAILABLE_OPTIMIZERS = {
----     'rmsprop': rmsprop.RMSprop,
----     'rprop': rprop.Rprop,
----     'novograd': Novograd,
----+    'ranger': Ranger,
---- }
---- 
---- 
----diff --git a/experiment/info.log b/experiment/info.log
----index b211d42..5024c4f 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,473 +1,2 @@
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4da7f8e0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.008413951416451957
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e67aa00>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          22.42      11.12      14.86       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                         38.62       7.46      12.50      19530
------ (label_id: 3)                                          6.60      15.75       9.30       1746
-----. (label_id: 4)                                         33.33       0.01       0.01      17976
-----: (label_id: 5)                                          0.79      19.15       1.52        376
-----; (label_id: 6)                                          0.45       2.94       0.78        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.17      50.56       0.33         89
------------------------
-----micro avg                                                5.14       5.14       5.14      48233
-----macro avg                                               10.24      10.70       3.93      48233
-----weighted avg                                            30.92       5.14       7.15      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          22.09      11.27      14.93       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                         35.39       6.74      11.32      19813
------ (label_id: 3)                                          6.03      16.28       8.79       1708
-----. (label_id: 4)                                        100.00       0.01       0.02      18084
-----: (label_id: 5)                                          0.78      20.69       1.51        348
-----; (label_id: 6)                                          0.45       2.55       0.77        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.19      58.62       0.38         87
------------------------
-----micro avg                                                4.89       4.89       4.89      48633
-----macro avg                                               16.49      11.62       3.77      48633
-----weighted avg                                            54.38       4.89       6.67      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.008413951416451957
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e74d910>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          19.61      14.42      16.62        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         38.84       6.82      11.60        689
------ (label_id: 3)                                          4.50      15.52       6.98         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.59      20.00       1.14         10
-----; (label_id: 6)                                          2.56      16.67       4.44          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.25      50.00       0.51          4
------------------------
-----micro avg                                                5.55       5.55       5.55       1641
-----macro avg                                                6.64      12.34       4.13       1641
-----weighted avg                                            18.97       5.55       7.25       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.63      98.91       7.00       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          3.96       1.76       2.44       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.63       3.63       3.63      48233
-----macro avg                                                0.76      10.07       0.94      48233
-----weighted avg                                             0.25       3.63       0.33      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e68b8b0>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          19.61      14.42      16.62        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         38.84       6.82      11.60        689
------ (label_id: 3)                                          4.50      15.52       6.98         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.59      20.00       1.14         10
-----; (label_id: 6)                                          2.56      16.67       4.44          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.25      50.00       0.51          4
------------------------
-----micro avg                                                5.55       5.55       5.55       1641
-----macro avg                                                6.64      12.34       4.13       1641
-----weighted avg                                            18.97       5.55       7.25       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                          0.00       0.00       0.00      19813
------ (label_id: 3)                                          3.51     100.00       6.79       1708
-----. (label_id: 4)                                          0.00       0.00       0.00      18084
-----: (label_id: 5)                                          0.00       0.00       0.00        348
-----; (label_id: 6)                                          0.00       0.00       0.00        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.00       0.00       0.00         87
------------------------
-----micro avg                                                3.51       3.51       3.51      48633
-----macro avg                                                0.35      10.00       0.68      48633
-----weighted avg                                             0.12       3.51       0.24      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75eaf0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                          0.00       0.00       0.00        689
------ (label_id: 3)                                          3.53     100.00       6.83         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.00       0.00       0.00         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                3.53       3.53       3.53       1641
-----macro avg                                                0.35      10.00       0.68       1641
-----weighted avg                                             0.12       3.53       0.24       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75e160>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                          0.00       0.00       0.00        689
------ (label_id: 3)                                          3.53     100.00       6.83         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.00       0.00       0.00         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                3.53       3.53       3.53       1641
-----macro avg                                                0.35      10.00       0.68       1641
-----weighted avg                                             0.12       3.53       0.24       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                          0.00       0.00       0.00      19813
------ (label_id: 3)                                          3.51     100.00       6.79       1708
-----. (label_id: 4)                                          0.00       0.00       0.00      18084
-----: (label_id: 5)                                          0.00       0.00       0.00        348
-----; (label_id: 6)                                          0.00       0.00       0.00        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.00       0.00       0.00         87
------------------------
-----micro avg                                                3.51       3.51       3.51      48633
-----macro avg                                                0.35      10.00       0.68      48633
-----weighted avg                                             0.12       3.51       0.24      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       7470
-----! (label_id: 1)                                          0.00       0.00       0.00        148
-----, (label_id: 2)                                          0.00       0.00       0.00      28513
------ (label_id: 3)                                          3.02     100.00       5.86       2074
-----. (label_id: 4)                                          0.00       0.00       0.00      25120
-----: (label_id: 5)                                          0.00       0.00       0.00        570
-----; (label_id: 6)                                          0.00       0.00       0.00        534
-----? (label_id: 7)                                          0.00       0.00       0.00       2085
-----— (label_id: 8)                                          0.00       0.00       0.00       2073
-----… (label_id: 9)                                          0.00       0.00       0.00        142
------------------------
-----micro avg                                                3.02       3.02       3.02      68729
-----macro avg                                                0.30      10.00       0.59      68729
-----weighted avg                                             0.09       3.02       0.18      68729
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       3735
------------------------
-----micro avg                                              100.00     100.00     100.00       3735
-----macro avg                                              100.00     100.00     100.00       3735
-----weighted avg                                           100.00     100.00     100.00       3735
-----
-----[INFO] - Internal process exited
----+[INFO] - GPU available: True, used: False
----+[INFO] - TPU available: None, using: 0 TPU cores
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index 782fbfa..464595d 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -374,6 +374,11 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             scheduler_config = None
---- 
----+        if 'delay_epochs' in optim_config:
----+            delay_epochs = optim_config.pop('delay_epochs')
----+        else:
----+            delay_epochs = 0
----+
----         # Check if caller provided optimizer name, default to Adam otherwise
----         optimizer_cls = optim_config.get('_target_', None)
---- 
----@@ -455,7 +460,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---- 
----         # Try to instantiate scheduler for optimizer
----         self._scheduler = prepare_lr_scheduler(
-----            optimizer=self._optimizer, scheduler_config=scheduler_config,
----+            optimizer=self._optimizer, delay_epochs=delay_epochs,scheduler_config=scheduler_config,
----             train_dataloader=pp({'num_samples' : self.train_size*self.hparams.model.dataset.train_ds.batch_size, 
----             'batch_size': self.hparams.model.dataset.train_ds.batch_size,
----             'drop_last' : self.hparams.model.dataset.drop_last})
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/lightning_logs.txt
---deleted file mode 100644
---index 01e1664..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/lightning_logs.txt
---+++ /dev/null
---@@ -1,2 +0,0 @@
----GPU available: True, used: False
----TPU available: None, using: 0 TPU cores
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/nemo_error_log.txt
---deleted file mode 100644
---index 13f2117..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/nemo_error_log.txt
---+++ /dev/null
---@@ -1,4 +0,0 @@
----[NeMo W 2021-02-06 09:14:51 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 09:14:51 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index 6f21c5c..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,6 +0,0 @@
----[NeMo W 2021-02-06 09:14:51 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo I 2021-02-06 09:14:51 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_09-14-51
----[NeMo I 2021-02-06 09:14:51 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-06 09:14:51 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/events.out.tfevents.1612574759.intern-instance.19385.0 b/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/events.out.tfevents.1612574759.intern-instance.19385.0
---deleted file mode 100644
---index 2abef90..0000000
---Binary files a/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/events.out.tfevents.1612574759.intern-instance.19385.0 and /dev/null differ
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/git-info.log
---deleted file mode 100644
---index 0d43585..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/git-info.log
---+++ /dev/null
---@@ -1,720 +0,0 @@
----commit hash: cce540d4c95ad9ed32010552a7aa1fb2486658c8
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index 3d27dd7..6d84830 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,43 +1,43 @@
---- seed: 42
---- trainer:
-----    gpus: 1 # the number of gpus, 0 for CPU
-----    num_nodes: 1
-----    max_epochs: 2
-----    max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 4 # accumulates grads every k batches
-----    gradient_clip_val: 0
-----    amp_level: O1 # O1/O2 for mixed precision
-----    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    accelerator: ddp
-----    checkpoint_callback: false  # Provided by exp_manager
-----    logger: false #false  # Provided by exp_manager
-----    log_every_n_steps: 1  # Interval of logging.
-----    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    resume_from_checkpoint: null
-----
-----    # gpus: 0 # the number of gpus, 0 for CPU
----+    # gpus: 1 # the number of gpus, 0 for CPU
----     # num_nodes: 1
-----    # max_epochs: 10
----+    # max_epochs: 2
----     # max_steps: null # precedence over max_epochs
----     # accumulate_grad_batches: 4 # accumulates grads every k batches
----     # gradient_clip_val: 0
-----    # amp_level: O0 # O1/O2 for mixed precision
-----    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # # accelerator: ddp
----+    # amp_level: O1 # O1/O2 for mixed precision
----+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----     # checkpoint_callback: false  # Provided by exp_manager
----     # logger: false #false  # Provided by exp_manager
----     # log_every_n_steps: 1  # Interval of logging.
----     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # reload_dataloaders_every_epoch: true
----     # resume_from_checkpoint: null
---- 
----+    gpus: 0 # the number of gpus, 0 for CPU
----+    num_nodes: 1
----+    max_epochs: 8
----+    max_steps: null # precedence over max_epochs
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----+    gradient_clip_val: 0
----+    amp_level: O0 # O1/O2 for mixed precision
----+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----+    checkpoint_callback: false  # Provided by exp_manager
----+    logger: false #false  # Provided by exp_manager
----+    log_every_n_steps: 1  # Interval of logging.
----+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    reload_dataloaders_every_epoch: true
----+    resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /root/data # /home/nxingyu2/data # 
-----tmp_path: /tmp # /home/nxingyu2/data/tmp # 
----+base_path: /home/nxingyu2/data # /root/data # 
----+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
----@@ -61,7 +61,7 @@ model:
----     punct_class_weights: true
----     
----     dataset:
-----        data_dir: /root/data # /home/nxingyu2/data # 
----+        data_dir: /home/nxingyu2/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -125,17 +125,18 @@ model:
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 4
----+        alpha: 3
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: adamw
----+        name: ranger
----         lr: 1e-3
----         weight_decay: 0.00
---- 
----+        delay_epochs: 4
----         sched:
----             name: WarmupAnnealing #CyclicLR
----             # Scheduler params
----@@ -145,7 +146,7 @@ model:
---- 
----             # pytorch lightning args
----             monitor: val_loss
-----            reduce_on_plateau: true
----+            reduce_on_plateau: false
---- hydra:
----     run:
----         dir: .
----diff --git a/experiment/core/optim/lr_scheduler.py b/experiment/core/optim/lr_scheduler.py
----index a17ebf8..a1bae5e 100644
------- a/experiment/core/optim/lr_scheduler.py
----+++ b/experiment/core/optim/lr_scheduler.py
----@@ -347,6 +347,39 @@ class PolynomialHoldDecayAnnealing(WarmupHoldPolicy):
----         return new_lrs
---- 
---- 
----+class DelayerScheduler(_LRScheduler):
----+	""" Starts with a flat lr schedule until it reaches N epochs the applies a scheduler 
----+	Args:
----+		optimizer (Optimizer): Wrapped optimizer.
----+		delay_epochs: number of epochs to keep the initial lr until starting aplying the scheduler
----+		after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)
----+	"""
----+
----+	def __init__(self, optimizer, delay_epochs, after_scheduler):
----+		self.delay_epochs = delay_epochs
----+		self.after_scheduler = after_scheduler
----+		self.finished = False
----+		super().__init__(optimizer)
----+
----+	def get_lr(self):
----+		if self.last_epoch >= self.delay_epochs:
----+			if not self.finished:
----+				self.after_scheduler.base_lrs = self.base_lrs
----+				self.finished = True
----+			return self.after_scheduler.get_lr()
----+
----+		return self.base_lrs
----+
----+	def step(self, epoch=None):
----+		if self.finished:
----+			if epoch is None:
----+				self.after_scheduler.step(None)
----+			else:
----+				self.after_scheduler.step(epoch - self.delay_epochs)
----+		else:
----+			return super(DelayerScheduler, self).step(epoch)
----+
----+
---- def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: SchedulerParams):
----     """
----     Checks if the scheduler name exists in the registry, and if it doesnt, adds it.
----@@ -365,7 +398,7 @@ def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: Sch
----     register_scheduler_params(name=sched_name, scheduler_params=scheduler_params)
---- 
---- 
-----def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
----+def get_scheduler(name: str, optimizer, delay_epochs, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
----     """
----     Convenience method to obtain an _LRScheduler class and partially instantiate it with optimizer kwargs.
----     Args:
----@@ -380,12 +413,15 @@ def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler
----         )
---- 
----     scheduler_cls = AVAILABLE_SCHEDULERS[name]
-----    scheduler = partial(scheduler_cls, **kwargs)
----+    scheduler = scheduler_cls(optimizer=optimizer, **kwargs)
----+    if delay_epochs>0:
----+        scheduler = DelayerScheduler(optimizer=optimizer,delay_epochs=delay_epochs,after_scheduler=scheduler)
----     return scheduler
---- 
---- 
---- def prepare_lr_scheduler(
----     optimizer: optim.Optimizer,
----+    delay_epochs: int,
----     scheduler_config: Union[Dict[str, Any], DictConfig],
----     train_dataloader: Optional[Dict[str,int]] = None,
---- 
----@@ -584,10 +620,10 @@ def prepare_lr_scheduler(
----         scheduler_args['max_steps'] = max_steps
---- 
----     # Get the scheduler class from the config
-----    scheduler_cls = get_scheduler(scheduler_name, **scheduler_args)
----+    schedule = get_scheduler(scheduler_name, optimizer, delay_epochs, **scheduler_args)
---- 
----     # Instantiate the LR schedule
-----    schedule = scheduler_cls(optimizer, **scheduler_args)
----+    # schedule = scheduler_cls(optimizer, **scheduler_args)
---- 
----     logging.info(
----         'Scheduler "%s" \nwill be used during training (effective maximum steps = %d) - \nParameters : \n(%s)',
----diff --git a/experiment/core/optim/optimizers.py b/experiment/core/optim/optimizers.py
----index 5afc4fd..f15686e 100644
------- a/experiment/core/optim/optimizers.py
----+++ b/experiment/core/optim/optimizers.py
----@@ -7,6 +7,7 @@ import torch.optim as optim
---- from omegaconf import DictConfig, OmegaConf
---- from torch.optim import adadelta, adagrad, adamax, rmsprop, rprop
---- from torch.optim.optimizer import Optimizer
----+from torchtools.optim import Ranger
---- 
---- from core.config import OptimizerParams, get_optimizer_config, register_optimizer_params
---- from core.optim.novograd import Novograd
----@@ -24,6 +25,7 @@ AVAILABLE_OPTIMIZERS = {
----     'rmsprop': rmsprop.RMSprop,
----     'rprop': rprop.Rprop,
----     'novograd': Novograd,
----+    'ranger': Ranger,
---- }
---- 
---- 
----diff --git a/experiment/info.log b/experiment/info.log
----index b211d42..5024c4f 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,473 +1,2 @@
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4da7f8e0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.008413951416451957
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e67aa00>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          22.42      11.12      14.86       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                         38.62       7.46      12.50      19530
------ (label_id: 3)                                          6.60      15.75       9.30       1746
-----. (label_id: 4)                                         33.33       0.01       0.01      17976
-----: (label_id: 5)                                          0.79      19.15       1.52        376
-----; (label_id: 6)                                          0.45       2.94       0.78        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.17      50.56       0.33         89
------------------------
-----micro avg                                                5.14       5.14       5.14      48233
-----macro avg                                               10.24      10.70       3.93      48233
-----weighted avg                                            30.92       5.14       7.15      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          22.09      11.27      14.93       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                         35.39       6.74      11.32      19813
------ (label_id: 3)                                          6.03      16.28       8.79       1708
-----. (label_id: 4)                                        100.00       0.01       0.02      18084
-----: (label_id: 5)                                          0.78      20.69       1.51        348
-----; (label_id: 6)                                          0.45       2.55       0.77        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.19      58.62       0.38         87
------------------------
-----micro avg                                                4.89       4.89       4.89      48633
-----macro avg                                               16.49      11.62       3.77      48633
-----weighted avg                                            54.38       4.89       6.67      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.008413951416451957
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e74d910>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          19.61      14.42      16.62        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         38.84       6.82      11.60        689
------ (label_id: 3)                                          4.50      15.52       6.98         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.59      20.00       1.14         10
-----; (label_id: 6)                                          2.56      16.67       4.44          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.25      50.00       0.51          4
------------------------
-----micro avg                                                5.55       5.55       5.55       1641
-----macro avg                                                6.64      12.34       4.13       1641
-----weighted avg                                            18.97       5.55       7.25       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.63      98.91       7.00       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          3.96       1.76       2.44       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.63       3.63       3.63      48233
-----macro avg                                                0.76      10.07       0.94      48233
-----weighted avg                                             0.25       3.63       0.33      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e68b8b0>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          19.61      14.42      16.62        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         38.84       6.82      11.60        689
------ (label_id: 3)                                          4.50      15.52       6.98         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.59      20.00       1.14         10
-----; (label_id: 6)                                          2.56      16.67       4.44          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.25      50.00       0.51          4
------------------------
-----micro avg                                                5.55       5.55       5.55       1641
-----macro avg                                                6.64      12.34       4.13       1641
-----weighted avg                                            18.97       5.55       7.25       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                          0.00       0.00       0.00      19813
------ (label_id: 3)                                          3.51     100.00       6.79       1708
-----. (label_id: 4)                                          0.00       0.00       0.00      18084
-----: (label_id: 5)                                          0.00       0.00       0.00        348
-----; (label_id: 6)                                          0.00       0.00       0.00        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.00       0.00       0.00         87
------------------------
-----micro avg                                                3.51       3.51       3.51      48633
-----macro avg                                                0.35      10.00       0.68      48633
-----weighted avg                                             0.12       3.51       0.24      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75eaf0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                          0.00       0.00       0.00        689
------ (label_id: 3)                                          3.53     100.00       6.83         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.00       0.00       0.00         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                3.53       3.53       3.53       1641
-----macro avg                                                0.35      10.00       0.68       1641
-----weighted avg                                             0.12       3.53       0.24       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75e160>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                          0.00       0.00       0.00        689
------ (label_id: 3)                                          3.53     100.00       6.83         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.00       0.00       0.00         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                3.53       3.53       3.53       1641
-----macro avg                                                0.35      10.00       0.68       1641
-----weighted avg                                             0.12       3.53       0.24       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                          0.00       0.00       0.00      19813
------ (label_id: 3)                                          3.51     100.00       6.79       1708
-----. (label_id: 4)                                          0.00       0.00       0.00      18084
-----: (label_id: 5)                                          0.00       0.00       0.00        348
-----; (label_id: 6)                                          0.00       0.00       0.00        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.00       0.00       0.00         87
------------------------
-----micro avg                                                3.51       3.51       3.51      48633
-----macro avg                                                0.35      10.00       0.68      48633
-----weighted avg                                             0.12       3.51       0.24      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       7470
-----! (label_id: 1)                                          0.00       0.00       0.00        148
-----, (label_id: 2)                                          0.00       0.00       0.00      28513
------ (label_id: 3)                                          3.02     100.00       5.86       2074
-----. (label_id: 4)                                          0.00       0.00       0.00      25120
-----: (label_id: 5)                                          0.00       0.00       0.00        570
-----; (label_id: 6)                                          0.00       0.00       0.00        534
-----? (label_id: 7)                                          0.00       0.00       0.00       2085
-----— (label_id: 8)                                          0.00       0.00       0.00       2073
-----… (label_id: 9)                                          0.00       0.00       0.00        142
------------------------
-----micro avg                                                3.02       3.02       3.02      68729
-----macro avg                                                0.30      10.00       0.59      68729
-----weighted avg                                             0.09       3.02       0.18      68729
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       3735
------------------------
-----micro avg                                              100.00     100.00     100.00       3735
-----macro avg                                              100.00     100.00     100.00       3735
-----weighted avg                                           100.00     100.00     100.00       3735
-----
-----[INFO] - Internal process exited
----+[INFO] - GPU available: True, used: False
----+[INFO] - TPU available: None, using: 0 TPU cores
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index 782fbfa..464595d 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -374,6 +374,11 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             scheduler_config = None
---- 
----+        if 'delay_epochs' in optim_config:
----+            delay_epochs = optim_config.pop('delay_epochs')
----+        else:
----+            delay_epochs = 0
----+
----         # Check if caller provided optimizer name, default to Adam otherwise
----         optimizer_cls = optim_config.get('_target_', None)
---- 
----@@ -455,7 +460,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---- 
----         # Try to instantiate scheduler for optimizer
----         self._scheduler = prepare_lr_scheduler(
-----            optimizer=self._optimizer, scheduler_config=scheduler_config,
----+            optimizer=self._optimizer, delay_epochs=delay_epochs,scheduler_config=scheduler_config,
----             train_dataloader=pp({'num_samples' : self.train_size*self.hparams.model.dataset.train_ds.batch_size, 
----             'batch_size': self.hparams.model.dataset.train_ds.batch_size,
----             'drop_last' : self.hparams.model.dataset.drop_last})
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/hparams.yaml
---deleted file mode 100644
---index 34d5300..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/hparams.yaml
---+++ /dev/null
---@@ -1,108 +0,0 @@
----seed: 42
----trainer:
----  gpus: 0
----  num_nodes: 1
----  max_epochs: 8
----  max_steps: null
----  accumulate_grad_batches: 4
----  gradient_clip_val: 0
----  amp_level: O0
----  precision: 32
----  checkpoint_callback: false
----  logger: false
----  log_every_n_steps: 1
----  val_check_interval: 1.0
----  reload_dataloaders_every_epoch: true
----  resume_from_checkpoint: null
----exp_manager:
----  exp_dir: /home/nxingyu2/project/
----  name: Punctuation_with_Domain_discriminator
----  create_tensorboard_logger: true
----  create_checkpoint_callback: true
----base_path: /home/nxingyu2/data
----tmp_path: /home/nxingyu2/data/tmp
----model:
----  nemo_path: null
----  transformer_path: google/electra-small-discriminator
----  unfrozen: 0
----  maximum_unfrozen: 2
----  unfreeze_step: 1
----  punct_label_ids:
----  - ''
----  - '!'
----  - ','
----  - '-'
----  - .
----  - ':'
----  - ;
----  - '?'
----  - —
----  - …
----  punct_class_weights: true
----  dataset:
----    data_dir: /home/nxingyu2/data
----    labelled:
----    - /home/nxingyu2/data/ted_talks_processed
----    unlabelled: null
----    max_seq_length: 128
----    pad_label: ''
----    ignore_extra_tokens: false
----    ignore_start_end: false
----    use_cache: false
----    num_workers: 0
----    pin_memory: true
----    drop_last: false
----    num_labels: 10
----    num_domains: 1
----    test_unlabelled: true
----    train_ds:
----      shuffle: true
----      num_samples: -1
----      batch_size: 8
----    validation_ds:
----      shuffle: true
----      num_samples: -1
----      batch_size: 8
----  tokenizer:
----    tokenizer_name: google/electra-small-discriminator
----    vocab_file: null
----    tokenizer_model: null
----    special_tokens: null
----  language_model:
----    pretrained_model_name: google/electra-small-discriminator
----    lm_checkpoint: null
----    config_file: null
----    config: null
----  punct_head:
----    punct_num_fc_layers: 1
----    fc_dropout: 0.1
----    activation: relu
----    log_softmax: false
----    use_transformer_init: true
----    loss: dice
----  domain_head:
----    domain_num_fc_layers: 1
----    fc_dropout: 0.1
----    activation: relu
----    log_softmax: false
----    use_transformer_init: true
----    loss: cel
----    gamma: 0.1
----  dice_loss:
----    epsilon: 0.01
----    alpha: 3
----    macro_average: true
----  focal_loss:
----    gamma: 5
----  optim:
----    name: ranger
----    lr: 0.001
----    weight_decay: 0.0
----    delay_epochs: 4
----    sched:
----      name: WarmupAnnealing
----      warmup_steps: null
----      warmup_ratio: 0.1
----      last_epoch: -1
----      monitor: val_loss
----      reduce_on_plateau: false
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/lightning_logs.txt
---deleted file mode 100644
---index fd892f0..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/lightning_logs.txt
---+++ /dev/null
---@@ -1,74 +0,0 @@
----GPU available: True, used: False
----TPU available: None, using: 0 TPU cores
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | punct_classifier    | TokenClassifier      | 2.6 K 
----2 | domain_classifier   | SequenceClassifier   | 257   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----35.9 K    Trainable params
----13.4 M    Non-trainable params
----13.5 M    Total params
----Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/lr_find_temp_model.ckpt
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | punct_classifier    | TokenClassifier      | 2.6 K 
----2 | domain_classifier   | SequenceClassifier   | 257   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----35.9 K    Trainable params
----13.4 M    Non-trainable params
----13.5 M    Total params
----Epoch 0, global step 100: val_loss reached 0.85947 (best 0.85947), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | punct_classifier    | TokenClassifier      | 2.6 K 
----2 | domain_classifier   | SequenceClassifier   | 257   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----825 K     Trainable params
----12.7 M    Non-trainable params
----13.5 M    Total params
----LR finder stopped early due to diverging loss.
----Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/lr_find_temp_model.ckpt
----Failed to compute suggesting for `lr`. There might not be enough points.
----Traceback (most recent call last):
----  File "/home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 353, in suggestion
----    min_grad = np.gradient(loss).argmin()
----  File "<__array_function__ internals>", line 5, in gradient
----  File "/home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
----    raise ValueError(
----ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | punct_classifier    | TokenClassifier      | 2.6 K 
----2 | domain_classifier   | SequenceClassifier   | 257   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----825 K     Trainable params
----12.7 M    Non-trainable params
----13.5 M    Total params
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/nemo_error_log.txt
---deleted file mode 100644
---index 94214f5..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/nemo_error_log.txt
---+++ /dev/null
---@@ -1,42 +0,0 @@
----[NeMo W 2021-02-06 09:16:57 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 09:16:57 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-06 09:17:08 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 09:17:08 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 09:17:12 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 09:17:21 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtools/optim/radam.py:62: UserWarning: This overload of addcmul_ is deprecated:
----    	addcmul_(Number value, Tensor tensor1, Tensor tensor2)
----    Consider using one of the following signatures instead:
----    	addcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/torch/csrc/utils/python_arg_parser.cpp:882.)
----      exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
----    
----[NeMo W 2021-02-06 09:26:31 nemo_logging:349] /home/nxingyu2/project/experiment/core/optim/lr_scheduler.py:49: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
----      warnings.warn(
----    
----[NeMo W 2021-02-06 09:36:51 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f56bb2964c0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-06 09:37:19 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f56bb296220> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-06 09:38:03 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 09:38:05 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
----      warnings.warn(*args, **kwargs)
----    
----Failed to compute suggesting for `lr`. There might not be enough points.
----Traceback (most recent call last):
----  File "/home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 353, in suggestion
----    min_grad = np.gradient(loss).argmin()
----  File "<__array_function__ internals>", line 5, in gradient
----  File "/home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
----    raise ValueError(
----ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index 82d30d0..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,36 +0,0 @@
----[NeMo W 2021-02-06 09:16:57 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo I 2021-02-06 09:16:57 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_09-16-57
----[NeMo I 2021-02-06 09:16:57 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-06 09:16:57 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-06 09:17:08 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 09:17:08 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 09:17:12 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 09:17:21 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtools/optim/radam.py:62: UserWarning: This overload of addcmul_ is deprecated:
----    	addcmul_(Number value, Tensor tensor1, Tensor tensor2)
----    Consider using one of the following signatures instead:
----    	addcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/torch/csrc/utils/python_arg_parser.cpp:882.)
----      exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
----    
----[NeMo W 2021-02-06 09:26:31 nemo_logging:349] /home/nxingyu2/project/experiment/core/optim/lr_scheduler.py:49: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
----      warnings.warn(
----    
----[NeMo W 2021-02-06 09:36:51 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f56bb2964c0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-06 09:37:19 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f56bb296220> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-06 09:38:03 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 09:38:05 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
----      warnings.warn(*args, **kwargs)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/git-info.log
---deleted file mode 100644
---index 35bebc4..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/git-info.log
---+++ /dev/null
---@@ -1,725 +0,0 @@
----commit hash: cce540d4c95ad9ed32010552a7aa1fb2486658c8
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index 3d27dd7..a30ba33 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,43 +1,43 @@
---- seed: 42
---- trainer:
-----    gpus: 1 # the number of gpus, 0 for CPU
-----    num_nodes: 1
-----    max_epochs: 2
-----    max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 4 # accumulates grads every k batches
-----    gradient_clip_val: 0
-----    amp_level: O1 # O1/O2 for mixed precision
-----    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    accelerator: ddp
-----    checkpoint_callback: false  # Provided by exp_manager
-----    logger: false #false  # Provided by exp_manager
-----    log_every_n_steps: 1  # Interval of logging.
-----    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    resume_from_checkpoint: null
-----
-----    # gpus: 0 # the number of gpus, 0 for CPU
----+    # gpus: 1 # the number of gpus, 0 for CPU
----     # num_nodes: 1
-----    # max_epochs: 10
----+    # max_epochs: 2
----     # max_steps: null # precedence over max_epochs
----     # accumulate_grad_batches: 4 # accumulates grads every k batches
----     # gradient_clip_val: 0
-----    # amp_level: O0 # O1/O2 for mixed precision
-----    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # # accelerator: ddp
----+    # amp_level: O1 # O1/O2 for mixed precision
----+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----     # checkpoint_callback: false  # Provided by exp_manager
----     # logger: false #false  # Provided by exp_manager
----     # log_every_n_steps: 1  # Interval of logging.
----     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # reload_dataloaders_every_epoch: true
----     # resume_from_checkpoint: null
---- 
----+    gpus: 0 # the number of gpus, 0 for CPU
----+    num_nodes: 1
----+    max_epochs: 8
----+    max_steps: null # precedence over max_epochs
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----+    gradient_clip_val: 0
----+    amp_level: O0 # O1/O2 for mixed precision
----+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----+    checkpoint_callback: false  # Provided by exp_manager
----+    logger: false #false  # Provided by exp_manager
----+    log_every_n_steps: 1  # Interval of logging.
----+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    reload_dataloaders_every_epoch: true
----+    resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /root/data # /home/nxingyu2/data # 
-----tmp_path: /tmp # /home/nxingyu2/data/tmp # 
----+base_path: /home/nxingyu2/data # /root/data # 
----+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
----@@ -61,7 +61,7 @@ model:
----     punct_class_weights: true
----     
----     dataset:
-----        data_dir: /root/data # /home/nxingyu2/data # 
----+        data_dir: /home/nxingyu2/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -125,27 +125,28 @@ model:
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 4
----+        alpha: 3
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: adamw
----+        name: ranger
----         lr: 1e-3
----         weight_decay: 0.00
---- 
----+        delay_epochs: 6
----         sched:
-----            name: WarmupAnnealing #CyclicLR
----+            name: CosineAnnealing #CyclicLR
----             # Scheduler params
-----            warmup_steps: null
-----            warmup_ratio: 0.1
----+            # warmup_steps: null
----+            # warmup_ratio: 0.1
----             last_epoch: -1
---- 
----             # pytorch lightning args
----             monitor: val_loss
-----            reduce_on_plateau: true
----+            reduce_on_plateau: false
---- hydra:
----     run:
----         dir: .
----diff --git a/experiment/core/optim/lr_scheduler.py b/experiment/core/optim/lr_scheduler.py
----index a17ebf8..3def2ff 100644
------- a/experiment/core/optim/lr_scheduler.py
----+++ b/experiment/core/optim/lr_scheduler.py
----@@ -347,6 +347,39 @@ class PolynomialHoldDecayAnnealing(WarmupHoldPolicy):
----         return new_lrs
---- 
---- 
----+class DelayerScheduler(_LRScheduler):
----+	""" Starts with a flat lr schedule until it reaches N epochs the applies a scheduler 
----+	Args:
----+		optimizer (Optimizer): Wrapped optimizer.
----+		delay_epochs: number of epochs to keep the initial lr until starting aplying the scheduler
----+		after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)
----+	"""
----+
----+	def __init__(self, optimizer, delay_epochs, after_scheduler):
----+		self.delay_epochs = delay_epochs
----+		self.after_scheduler = after_scheduler
----+		self.finished = False
----+		super().__init__(optimizer, last_epoch)
----+
----+	def get_lr(self):
----+		if self.last_epoch >= self.delay_epochs:
----+			if not self.finished:
----+				self.after_scheduler.base_lrs = self.base_lrs
----+				self.finished = True
----+			return self.after_scheduler._get_lr()
----+
----+		return self.base_lrs
----+
----+	def step(self, epoch=None):
----+		if self.finished:
----+			if epoch is None:
----+				self.after_scheduler.step(None)
----+			else:
----+				self.after_scheduler.step(epoch - self.delay_epochs)
----+		else:
----+			return super(DelayerScheduler, self).step(epoch)
----+
----+
---- def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: SchedulerParams):
----     """
----     Checks if the scheduler name exists in the registry, and if it doesnt, adds it.
----@@ -365,7 +398,7 @@ def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: Sch
----     register_scheduler_params(name=sched_name, scheduler_params=scheduler_params)
---- 
---- 
-----def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
----+def get_scheduler(name: str, optimizer, delay_epochs, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
----     """
----     Convenience method to obtain an _LRScheduler class and partially instantiate it with optimizer kwargs.
----     Args:
----@@ -380,12 +413,15 @@ def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler
----         )
---- 
----     scheduler_cls = AVAILABLE_SCHEDULERS[name]
-----    scheduler = partial(scheduler_cls, **kwargs)
----+    scheduler = scheduler_cls(optimizer=optimizer, **kwargs)
----+    if delay_epochs>0:
----+        scheduler = DelayerScheduler(optimizer=optimizer,delay_epochs=delay_epochs,after_scheduler=scheduler)
----     return scheduler
---- 
---- 
---- def prepare_lr_scheduler(
----     optimizer: optim.Optimizer,
----+    delay_epochs: int,
----     scheduler_config: Union[Dict[str, Any], DictConfig],
----     train_dataloader: Optional[Dict[str,int]] = None,
---- 
----@@ -584,10 +620,10 @@ def prepare_lr_scheduler(
----         scheduler_args['max_steps'] = max_steps
---- 
----     # Get the scheduler class from the config
-----    scheduler_cls = get_scheduler(scheduler_name, **scheduler_args)
----+    schedule = get_scheduler(scheduler_name, optimizer, delay_epochs, **scheduler_args)
---- 
----     # Instantiate the LR schedule
-----    schedule = scheduler_cls(optimizer, **scheduler_args)
----+    # schedule = scheduler_cls(optimizer, **scheduler_args)
---- 
----     logging.info(
----         'Scheduler "%s" \nwill be used during training (effective maximum steps = %d) - \nParameters : \n(%s)',
----diff --git a/experiment/core/optim/optimizers.py b/experiment/core/optim/optimizers.py
----index 5afc4fd..f15686e 100644
------- a/experiment/core/optim/optimizers.py
----+++ b/experiment/core/optim/optimizers.py
----@@ -7,6 +7,7 @@ import torch.optim as optim
---- from omegaconf import DictConfig, OmegaConf
---- from torch.optim import adadelta, adagrad, adamax, rmsprop, rprop
---- from torch.optim.optimizer import Optimizer
----+from torchtools.optim import Ranger
---- 
---- from core.config import OptimizerParams, get_optimizer_config, register_optimizer_params
---- from core.optim.novograd import Novograd
----@@ -24,6 +25,7 @@ AVAILABLE_OPTIMIZERS = {
----     'rmsprop': rmsprop.RMSprop,
----     'rprop': rprop.Rprop,
----     'novograd': Novograd,
----+    'ranger': Ranger,
---- }
---- 
---- 
----diff --git a/experiment/info.log b/experiment/info.log
----index b211d42..5024c4f 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,473 +1,2 @@
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4da7f8e0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.008413951416451957
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e67aa00>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          22.42      11.12      14.86       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                         38.62       7.46      12.50      19530
------ (label_id: 3)                                          6.60      15.75       9.30       1746
-----. (label_id: 4)                                         33.33       0.01       0.01      17976
-----: (label_id: 5)                                          0.79      19.15       1.52        376
-----; (label_id: 6)                                          0.45       2.94       0.78        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.17      50.56       0.33         89
------------------------
-----micro avg                                                5.14       5.14       5.14      48233
-----macro avg                                               10.24      10.70       3.93      48233
-----weighted avg                                            30.92       5.14       7.15      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          22.09      11.27      14.93       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                         35.39       6.74      11.32      19813
------ (label_id: 3)                                          6.03      16.28       8.79       1708
-----. (label_id: 4)                                        100.00       0.01       0.02      18084
-----: (label_id: 5)                                          0.78      20.69       1.51        348
-----; (label_id: 6)                                          0.45       2.55       0.77        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.19      58.62       0.38         87
------------------------
-----micro avg                                                4.89       4.89       4.89      48633
-----macro avg                                               16.49      11.62       3.77      48633
-----weighted avg                                            54.38       4.89       6.67      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.008413951416451957
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e74d910>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          19.61      14.42      16.62        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         38.84       6.82      11.60        689
------ (label_id: 3)                                          4.50      15.52       6.98         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.59      20.00       1.14         10
-----; (label_id: 6)                                          2.56      16.67       4.44          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.25      50.00       0.51          4
------------------------
-----micro avg                                                5.55       5.55       5.55       1641
-----macro avg                                                6.64      12.34       4.13       1641
-----weighted avg                                            18.97       5.55       7.25       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.63      98.91       7.00       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          3.96       1.76       2.44       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.63       3.63       3.63      48233
-----macro avg                                                0.76      10.07       0.94      48233
-----weighted avg                                             0.25       3.63       0.33      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e68b8b0>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          19.61      14.42      16.62        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         38.84       6.82      11.60        689
------ (label_id: 3)                                          4.50      15.52       6.98         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.59      20.00       1.14         10
-----; (label_id: 6)                                          2.56      16.67       4.44          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.25      50.00       0.51          4
------------------------
-----micro avg                                                5.55       5.55       5.55       1641
-----macro avg                                                6.64      12.34       4.13       1641
-----weighted avg                                            18.97       5.55       7.25       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                          0.00       0.00       0.00      19813
------ (label_id: 3)                                          3.51     100.00       6.79       1708
-----. (label_id: 4)                                          0.00       0.00       0.00      18084
-----: (label_id: 5)                                          0.00       0.00       0.00        348
-----; (label_id: 6)                                          0.00       0.00       0.00        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.00       0.00       0.00         87
------------------------
-----micro avg                                                3.51       3.51       3.51      48633
-----macro avg                                                0.35      10.00       0.68      48633
-----weighted avg                                             0.12       3.51       0.24      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75eaf0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                          0.00       0.00       0.00        689
------ (label_id: 3)                                          3.53     100.00       6.83         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.00       0.00       0.00         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                3.53       3.53       3.53       1641
-----macro avg                                                0.35      10.00       0.68       1641
-----weighted avg                                             0.12       3.53       0.24       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75e160>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                          0.00       0.00       0.00        689
------ (label_id: 3)                                          3.53     100.00       6.83         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.00       0.00       0.00         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                3.53       3.53       3.53       1641
-----macro avg                                                0.35      10.00       0.68       1641
-----weighted avg                                             0.12       3.53       0.24       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                          0.00       0.00       0.00      19813
------ (label_id: 3)                                          3.51     100.00       6.79       1708
-----. (label_id: 4)                                          0.00       0.00       0.00      18084
-----: (label_id: 5)                                          0.00       0.00       0.00        348
-----; (label_id: 6)                                          0.00       0.00       0.00        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.00       0.00       0.00         87
------------------------
-----micro avg                                                3.51       3.51       3.51      48633
-----macro avg                                                0.35      10.00       0.68      48633
-----weighted avg                                             0.12       3.51       0.24      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       7470
-----! (label_id: 1)                                          0.00       0.00       0.00        148
-----, (label_id: 2)                                          0.00       0.00       0.00      28513
------ (label_id: 3)                                          3.02     100.00       5.86       2074
-----. (label_id: 4)                                          0.00       0.00       0.00      25120
-----: (label_id: 5)                                          0.00       0.00       0.00        570
-----; (label_id: 6)                                          0.00       0.00       0.00        534
-----? (label_id: 7)                                          0.00       0.00       0.00       2085
-----— (label_id: 8)                                          0.00       0.00       0.00       2073
-----… (label_id: 9)                                          0.00       0.00       0.00        142
------------------------
-----micro avg                                                3.02       3.02       3.02      68729
-----macro avg                                                0.30      10.00       0.59      68729
-----weighted avg                                             0.09       3.02       0.18      68729
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       3735
------------------------
-----micro avg                                              100.00     100.00     100.00       3735
-----macro avg                                              100.00     100.00     100.00       3735
-----weighted avg                                           100.00     100.00     100.00       3735
-----
-----[INFO] - Internal process exited
----+[INFO] - GPU available: True, used: False
----+[INFO] - TPU available: None, using: 0 TPU cores
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index 782fbfa..464595d 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -374,6 +374,11 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             scheduler_config = None
---- 
----+        if 'delay_epochs' in optim_config:
----+            delay_epochs = optim_config.pop('delay_epochs')
----+        else:
----+            delay_epochs = 0
----+
----         # Check if caller provided optimizer name, default to Adam otherwise
----         optimizer_cls = optim_config.get('_target_', None)
---- 
----@@ -455,7 +460,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---- 
----         # Try to instantiate scheduler for optimizer
----         self._scheduler = prepare_lr_scheduler(
-----            optimizer=self._optimizer, scheduler_config=scheduler_config,
----+            optimizer=self._optimizer, delay_epochs=delay_epochs,scheduler_config=scheduler_config,
----             train_dataloader=pp({'num_samples' : self.train_size*self.hparams.model.dataset.train_ds.batch_size, 
----             'batch_size': self.hparams.model.dataset.train_ds.batch_size,
----             'drop_last' : self.hparams.model.dataset.drop_last})
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/lightning_logs.txt
---deleted file mode 100644
---index 01e1664..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/lightning_logs.txt
---+++ /dev/null
---@@ -1,2 +0,0 @@
----GPU available: True, used: False
----TPU available: None, using: 0 TPU cores
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/nemo_error_log.txt
---deleted file mode 100644
---index 98e6fd2..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/nemo_error_log.txt
---+++ /dev/null
---@@ -1,4 +0,0 @@
----[NeMo W 2021-02-06 09:39:02 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 09:39:02 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index 99e03a4..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,6 +0,0 @@
----[NeMo W 2021-02-06 09:39:02 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo I 2021-02-06 09:39:02 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_09-39-02
----[NeMo I 2021-02-06 09:39:02 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-06 09:39:02 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/events.out.tfevents.1612577016.intern-instance.23889.0 b/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/events.out.tfevents.1612577016.intern-instance.23889.0
---deleted file mode 100644
---index 487e986..0000000
---Binary files a/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/events.out.tfevents.1612577016.intern-instance.23889.0 and /dev/null differ
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/git-info.log
---deleted file mode 100644
---index 421b5c6..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/git-info.log
---+++ /dev/null
---@@ -1,726 +0,0 @@
----commit hash: cce540d4c95ad9ed32010552a7aa1fb2486658c8
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index 3d27dd7..a30ba33 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,43 +1,43 @@
---- seed: 42
---- trainer:
-----    gpus: 1 # the number of gpus, 0 for CPU
-----    num_nodes: 1
-----    max_epochs: 2
-----    max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 4 # accumulates grads every k batches
-----    gradient_clip_val: 0
-----    amp_level: O1 # O1/O2 for mixed precision
-----    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    accelerator: ddp
-----    checkpoint_callback: false  # Provided by exp_manager
-----    logger: false #false  # Provided by exp_manager
-----    log_every_n_steps: 1  # Interval of logging.
-----    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    resume_from_checkpoint: null
-----
-----    # gpus: 0 # the number of gpus, 0 for CPU
----+    # gpus: 1 # the number of gpus, 0 for CPU
----     # num_nodes: 1
-----    # max_epochs: 10
----+    # max_epochs: 2
----     # max_steps: null # precedence over max_epochs
----     # accumulate_grad_batches: 4 # accumulates grads every k batches
----     # gradient_clip_val: 0
-----    # amp_level: O0 # O1/O2 for mixed precision
-----    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # # accelerator: ddp
----+    # amp_level: O1 # O1/O2 for mixed precision
----+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----     # checkpoint_callback: false  # Provided by exp_manager
----     # logger: false #false  # Provided by exp_manager
----     # log_every_n_steps: 1  # Interval of logging.
----     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # reload_dataloaders_every_epoch: true
----     # resume_from_checkpoint: null
---- 
----+    gpus: 0 # the number of gpus, 0 for CPU
----+    num_nodes: 1
----+    max_epochs: 8
----+    max_steps: null # precedence over max_epochs
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----+    gradient_clip_val: 0
----+    amp_level: O0 # O1/O2 for mixed precision
----+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----+    checkpoint_callback: false  # Provided by exp_manager
----+    logger: false #false  # Provided by exp_manager
----+    log_every_n_steps: 1  # Interval of logging.
----+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    reload_dataloaders_every_epoch: true
----+    resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /root/data # /home/nxingyu2/data # 
-----tmp_path: /tmp # /home/nxingyu2/data/tmp # 
----+base_path: /home/nxingyu2/data # /root/data # 
----+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
----@@ -61,7 +61,7 @@ model:
----     punct_class_weights: true
----     
----     dataset:
-----        data_dir: /root/data # /home/nxingyu2/data # 
----+        data_dir: /home/nxingyu2/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -125,27 +125,28 @@ model:
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 4
----+        alpha: 3
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: adamw
----+        name: ranger
----         lr: 1e-3
----         weight_decay: 0.00
---- 
----+        delay_epochs: 6
----         sched:
-----            name: WarmupAnnealing #CyclicLR
----+            name: CosineAnnealing #CyclicLR
----             # Scheduler params
-----            warmup_steps: null
-----            warmup_ratio: 0.1
----+            # warmup_steps: null
----+            # warmup_ratio: 0.1
----             last_epoch: -1
---- 
----             # pytorch lightning args
----             monitor: val_loss
-----            reduce_on_plateau: true
----+            reduce_on_plateau: false
---- hydra:
----     run:
----         dir: .
----diff --git a/experiment/core/optim/lr_scheduler.py b/experiment/core/optim/lr_scheduler.py
----index a17ebf8..f87bb23 100644
------- a/experiment/core/optim/lr_scheduler.py
----+++ b/experiment/core/optim/lr_scheduler.py
----@@ -347,6 +347,40 @@ class PolynomialHoldDecayAnnealing(WarmupHoldPolicy):
----         return new_lrs
---- 
---- 
----+class DelayerScheduler(_LRScheduler):
----+    """ Starts with a flat lr schedule until it reaches N epochs the applies a scheduler 
----+    Args:
----+        optimizer (Optimizer): Wrapped optimizer.
----+        delay_epochs: number of epochs to keep the initial lr until starting aplying the scheduler
----+        after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)
----+    """
----+
----+    def __init__(self, optimizer, delay_epochs, after_scheduler):
----+        self.delay_epochs = delay_epochs
----+        self.after_scheduler = after_scheduler
----+        self.finished = False
----+        self.last_epoch = after_scheduler.last_epoch
----+        super().__init__(optimizer, self.last_epoch)
----+
----+    def get_lr(self):
----+        if self.last_epoch >= self.delay_epochs:
----+            if not self.finished:
----+                self.after_scheduler.base_lrs = self.base_lrs
----+                self.finished = True
----+            return self.after_scheduler._get_lr()
----+
----+        return self.base_lrs
----+
----+    def step(self, epoch=None):
----+        if self.finished:
----+            if epoch is None:
----+                self.after_scheduler.step(None)
----+            else:
----+                self.after_scheduler.step(epoch - self.delay_epochs)
----+        else:
----+            return super(DelayerScheduler, self).step(epoch)
----+
----+
---- def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: SchedulerParams):
----     """
----     Checks if the scheduler name exists in the registry, and if it doesnt, adds it.
----@@ -365,7 +399,7 @@ def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: Sch
----     register_scheduler_params(name=sched_name, scheduler_params=scheduler_params)
---- 
---- 
-----def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
----+def get_scheduler(name: str, optimizer, delay_epochs, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
----     """
----     Convenience method to obtain an _LRScheduler class and partially instantiate it with optimizer kwargs.
----     Args:
----@@ -380,12 +414,15 @@ def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler
----         )
---- 
----     scheduler_cls = AVAILABLE_SCHEDULERS[name]
-----    scheduler = partial(scheduler_cls, **kwargs)
----+    scheduler = scheduler_cls(optimizer=optimizer, **kwargs)
----+    if delay_epochs>0:
----+        scheduler = DelayerScheduler(optimizer=optimizer,delay_epochs=delay_epochs,after_scheduler=scheduler)
----     return scheduler
---- 
---- 
---- def prepare_lr_scheduler(
----     optimizer: optim.Optimizer,
----+    delay_epochs: int,
----     scheduler_config: Union[Dict[str, Any], DictConfig],
----     train_dataloader: Optional[Dict[str,int]] = None,
---- 
----@@ -584,10 +621,10 @@ def prepare_lr_scheduler(
----         scheduler_args['max_steps'] = max_steps
---- 
----     # Get the scheduler class from the config
-----    scheduler_cls = get_scheduler(scheduler_name, **scheduler_args)
----+    schedule = get_scheduler(scheduler_name, optimizer, delay_epochs, **scheduler_args)
---- 
----     # Instantiate the LR schedule
-----    schedule = scheduler_cls(optimizer, **scheduler_args)
----+    # schedule = scheduler_cls(optimizer, **scheduler_args)
---- 
----     logging.info(
----         'Scheduler "%s" \nwill be used during training (effective maximum steps = %d) - \nParameters : \n(%s)',
----diff --git a/experiment/core/optim/optimizers.py b/experiment/core/optim/optimizers.py
----index 5afc4fd..f15686e 100644
------- a/experiment/core/optim/optimizers.py
----+++ b/experiment/core/optim/optimizers.py
----@@ -7,6 +7,7 @@ import torch.optim as optim
---- from omegaconf import DictConfig, OmegaConf
---- from torch.optim import adadelta, adagrad, adamax, rmsprop, rprop
---- from torch.optim.optimizer import Optimizer
----+from torchtools.optim import Ranger
---- 
---- from core.config import OptimizerParams, get_optimizer_config, register_optimizer_params
---- from core.optim.novograd import Novograd
----@@ -24,6 +25,7 @@ AVAILABLE_OPTIMIZERS = {
----     'rmsprop': rmsprop.RMSprop,
----     'rprop': rprop.Rprop,
----     'novograd': Novograd,
----+    'ranger': Ranger,
---- }
---- 
---- 
----diff --git a/experiment/info.log b/experiment/info.log
----index b211d42..5024c4f 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,473 +1,2 @@
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4da7f8e0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.008413951416451957
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e67aa00>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          22.42      11.12      14.86       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                         38.62       7.46      12.50      19530
------ (label_id: 3)                                          6.60      15.75       9.30       1746
-----. (label_id: 4)                                         33.33       0.01       0.01      17976
-----: (label_id: 5)                                          0.79      19.15       1.52        376
-----; (label_id: 6)                                          0.45       2.94       0.78        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.17      50.56       0.33         89
------------------------
-----micro avg                                                5.14       5.14       5.14      48233
-----macro avg                                               10.24      10.70       3.93      48233
-----weighted avg                                            30.92       5.14       7.15      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          22.09      11.27      14.93       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                         35.39       6.74      11.32      19813
------ (label_id: 3)                                          6.03      16.28       8.79       1708
-----. (label_id: 4)                                        100.00       0.01       0.02      18084
-----: (label_id: 5)                                          0.78      20.69       1.51        348
-----; (label_id: 6)                                          0.45       2.55       0.77        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.19      58.62       0.38         87
------------------------
-----micro avg                                                4.89       4.89       4.89      48633
-----macro avg                                               16.49      11.62       3.77      48633
-----weighted avg                                            54.38       4.89       6.67      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.008413951416451957
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e74d910>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          19.61      14.42      16.62        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         38.84       6.82      11.60        689
------ (label_id: 3)                                          4.50      15.52       6.98         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.59      20.00       1.14         10
-----; (label_id: 6)                                          2.56      16.67       4.44          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.25      50.00       0.51          4
------------------------
-----micro avg                                                5.55       5.55       5.55       1641
-----macro avg                                                6.64      12.34       4.13       1641
-----weighted avg                                            18.97       5.55       7.25       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.63      98.91       7.00       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          3.96       1.76       2.44       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.63       3.63       3.63      48233
-----macro avg                                                0.76      10.07       0.94      48233
-----weighted avg                                             0.25       3.63       0.33      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e68b8b0>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          19.61      14.42      16.62        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         38.84       6.82      11.60        689
------ (label_id: 3)                                          4.50      15.52       6.98         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.59      20.00       1.14         10
-----; (label_id: 6)                                          2.56      16.67       4.44          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.25      50.00       0.51          4
------------------------
-----micro avg                                                5.55       5.55       5.55       1641
-----macro avg                                                6.64      12.34       4.13       1641
-----weighted avg                                            18.97       5.55       7.25       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                          0.00       0.00       0.00      19813
------ (label_id: 3)                                          3.51     100.00       6.79       1708
-----. (label_id: 4)                                          0.00       0.00       0.00      18084
-----: (label_id: 5)                                          0.00       0.00       0.00        348
-----; (label_id: 6)                                          0.00       0.00       0.00        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.00       0.00       0.00         87
------------------------
-----micro avg                                                3.51       3.51       3.51      48633
-----macro avg                                                0.35      10.00       0.68      48633
-----weighted avg                                             0.12       3.51       0.24      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75eaf0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                          0.00       0.00       0.00        689
------ (label_id: 3)                                          3.53     100.00       6.83         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.00       0.00       0.00         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                3.53       3.53       3.53       1641
-----macro avg                                                0.35      10.00       0.68       1641
-----weighted avg                                             0.12       3.53       0.24       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75e160>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                          0.00       0.00       0.00        689
------ (label_id: 3)                                          3.53     100.00       6.83         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.00       0.00       0.00         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                3.53       3.53       3.53       1641
-----macro avg                                                0.35      10.00       0.68       1641
-----weighted avg                                             0.12       3.53       0.24       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                          0.00       0.00       0.00      19813
------ (label_id: 3)                                          3.51     100.00       6.79       1708
-----. (label_id: 4)                                          0.00       0.00       0.00      18084
-----: (label_id: 5)                                          0.00       0.00       0.00        348
-----; (label_id: 6)                                          0.00       0.00       0.00        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.00       0.00       0.00         87
------------------------
-----micro avg                                                3.51       3.51       3.51      48633
-----macro avg                                                0.35      10.00       0.68      48633
-----weighted avg                                             0.12       3.51       0.24      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       7470
-----! (label_id: 1)                                          0.00       0.00       0.00        148
-----, (label_id: 2)                                          0.00       0.00       0.00      28513
------ (label_id: 3)                                          3.02     100.00       5.86       2074
-----. (label_id: 4)                                          0.00       0.00       0.00      25120
-----: (label_id: 5)                                          0.00       0.00       0.00        570
-----; (label_id: 6)                                          0.00       0.00       0.00        534
-----? (label_id: 7)                                          0.00       0.00       0.00       2085
-----— (label_id: 8)                                          0.00       0.00       0.00       2073
-----… (label_id: 9)                                          0.00       0.00       0.00        142
------------------------
-----micro avg                                                3.02       3.02       3.02      68729
-----macro avg                                                0.30      10.00       0.59      68729
-----weighted avg                                             0.09       3.02       0.18      68729
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       3735
------------------------
-----micro avg                                              100.00     100.00     100.00       3735
-----macro avg                                              100.00     100.00     100.00       3735
-----weighted avg                                           100.00     100.00     100.00       3735
-----
-----[INFO] - Internal process exited
----+[INFO] - GPU available: True, used: False
----+[INFO] - TPU available: None, using: 0 TPU cores
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index 782fbfa..464595d 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -374,6 +374,11 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             scheduler_config = None
---- 
----+        if 'delay_epochs' in optim_config:
----+            delay_epochs = optim_config.pop('delay_epochs')
----+        else:
----+            delay_epochs = 0
----+
----         # Check if caller provided optimizer name, default to Adam otherwise
----         optimizer_cls = optim_config.get('_target_', None)
---- 
----@@ -455,7 +460,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---- 
----         # Try to instantiate scheduler for optimizer
----         self._scheduler = prepare_lr_scheduler(
-----            optimizer=self._optimizer, scheduler_config=scheduler_config,
----+            optimizer=self._optimizer, delay_epochs=delay_epochs,scheduler_config=scheduler_config,
----             train_dataloader=pp({'num_samples' : self.train_size*self.hparams.model.dataset.train_ds.batch_size, 
----             'batch_size': self.hparams.model.dataset.train_ds.batch_size,
----             'drop_last' : self.hparams.model.dataset.drop_last})
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/hparams.yaml
---deleted file mode 100644
---index 767cea3..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/hparams.yaml
---+++ /dev/null
---@@ -1,106 +0,0 @@
----seed: 42
----trainer:
----  gpus: 0
----  num_nodes: 1
----  max_epochs: 8
----  max_steps: null
----  accumulate_grad_batches: 4
----  gradient_clip_val: 0
----  amp_level: O0
----  precision: 32
----  checkpoint_callback: false
----  logger: false
----  log_every_n_steps: 1
----  val_check_interval: 1.0
----  reload_dataloaders_every_epoch: true
----  resume_from_checkpoint: null
----exp_manager:
----  exp_dir: /home/nxingyu2/project/
----  name: Punctuation_with_Domain_discriminator
----  create_tensorboard_logger: true
----  create_checkpoint_callback: true
----base_path: /home/nxingyu2/data
----tmp_path: /home/nxingyu2/data/tmp
----model:
----  nemo_path: null
----  transformer_path: google/electra-small-discriminator
----  unfrozen: 0
----  maximum_unfrozen: 2
----  unfreeze_step: 1
----  punct_label_ids:
----  - ''
----  - '!'
----  - ','
----  - '-'
----  - .
----  - ':'
----  - ;
----  - '?'
----  - —
----  - …
----  punct_class_weights: true
----  dataset:
----    data_dir: /home/nxingyu2/data
----    labelled:
----    - /home/nxingyu2/data/ted_talks_processed
----    unlabelled: null
----    max_seq_length: 128
----    pad_label: ''
----    ignore_extra_tokens: false
----    ignore_start_end: false
----    use_cache: false
----    num_workers: 0
----    pin_memory: true
----    drop_last: false
----    num_labels: 10
----    num_domains: 1
----    test_unlabelled: true
----    train_ds:
----      shuffle: true
----      num_samples: -1
----      batch_size: 8
----    validation_ds:
----      shuffle: true
----      num_samples: -1
----      batch_size: 8
----  tokenizer:
----    tokenizer_name: google/electra-small-discriminator
----    vocab_file: null
----    tokenizer_model: null
----    special_tokens: null
----  language_model:
----    pretrained_model_name: google/electra-small-discriminator
----    lm_checkpoint: null
----    config_file: null
----    config: null
----  punct_head:
----    punct_num_fc_layers: 1
----    fc_dropout: 0.1
----    activation: relu
----    log_softmax: false
----    use_transformer_init: true
----    loss: dice
----  domain_head:
----    domain_num_fc_layers: 1
----    fc_dropout: 0.1
----    activation: relu
----    log_softmax: false
----    use_transformer_init: true
----    loss: cel
----    gamma: 0.1
----  dice_loss:
----    epsilon: 0.01
----    alpha: 3
----    macro_average: true
----  focal_loss:
----    gamma: 5
----  optim:
----    name: ranger
----    lr: 0.001
----    weight_decay: 0.0
----    delay_epochs: 6
----    sched:
----      name: CosineAnnealing
----      last_epoch: -1
----      monitor: val_loss
----      reduce_on_plateau: false
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/lightning_logs.txt
---deleted file mode 100644
---index 368bc65..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/lightning_logs.txt
---+++ /dev/null
---@@ -1,33 +0,0 @@
----GPU available: True, used: False
----TPU available: None, using: 0 TPU cores
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | punct_classifier    | TokenClassifier      | 2.6 K 
----2 | domain_classifier   | SequenceClassifier   | 257   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----35.9 K    Trainable params
----13.4 M    Non-trainable params
----13.5 M    Total params
----Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/lr_find_temp_model.ckpt
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | punct_classifier    | TokenClassifier      | 2.6 K 
----2 | domain_classifier   | SequenceClassifier   | 257   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----35.9 K    Trainable params
----13.4 M    Non-trainable params
----13.5 M    Total params
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/nemo_error_log.txt
---deleted file mode 100644
---index 74b9905..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/nemo_error_log.txt
---+++ /dev/null
---@@ -1,19 +0,0 @@
----[NeMo W 2021-02-06 09:54:19 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 09:54:19 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-06 09:54:30 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 09:54:30 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 09:54:34 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 09:54:43 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtools/optim/radam.py:62: UserWarning: This overload of addcmul_ is deprecated:
----    	addcmul_(Number value, Tensor tensor1, Tensor tensor2)
----    Consider using one of the following signatures instead:
----    	addcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/torch/csrc/utils/python_arg_parser.cpp:882.)
----      exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index 465eb8c..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,21 +0,0 @@
----[NeMo W 2021-02-06 09:54:19 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo I 2021-02-06 09:54:19 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_09-54-19
----[NeMo I 2021-02-06 09:54:19 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-06 09:54:19 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-06 09:54:30 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 09:54:30 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 09:54:34 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 09:54:43 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtools/optim/radam.py:62: UserWarning: This overload of addcmul_ is deprecated:
----    	addcmul_(Number value, Tensor tensor1, Tensor tensor2)
----    Consider using one of the following signatures instead:
----    	addcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/torch/csrc/utils/python_arg_parser.cpp:882.)
----      exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/git-info.log
---deleted file mode 100644
---index a9491ab..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/git-info.log
---+++ /dev/null
---@@ -1,678 +0,0 @@
----commit hash: cce540d4c95ad9ed32010552a7aa1fb2486658c8
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index 3d27dd7..e314c45 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,43 +1,43 @@
---- seed: 42
---- trainer:
-----    gpus: 1 # the number of gpus, 0 for CPU
-----    num_nodes: 1
-----    max_epochs: 2
-----    max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 4 # accumulates grads every k batches
-----    gradient_clip_val: 0
-----    amp_level: O1 # O1/O2 for mixed precision
-----    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    accelerator: ddp
-----    checkpoint_callback: false  # Provided by exp_manager
-----    logger: false #false  # Provided by exp_manager
-----    log_every_n_steps: 1  # Interval of logging.
-----    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    resume_from_checkpoint: null
-----
-----    # gpus: 0 # the number of gpus, 0 for CPU
----+    # gpus: 1 # the number of gpus, 0 for CPU
----     # num_nodes: 1
-----    # max_epochs: 10
----+    # max_epochs: 2
----     # max_steps: null # precedence over max_epochs
----     # accumulate_grad_batches: 4 # accumulates grads every k batches
----     # gradient_clip_val: 0
-----    # amp_level: O0 # O1/O2 for mixed precision
-----    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # # accelerator: ddp
----+    # amp_level: O1 # O1/O2 for mixed precision
----+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----     # checkpoint_callback: false  # Provided by exp_manager
----     # logger: false #false  # Provided by exp_manager
----     # log_every_n_steps: 1  # Interval of logging.
----     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # reload_dataloaders_every_epoch: true
----     # resume_from_checkpoint: null
---- 
----+    gpus: 0 # the number of gpus, 0 for CPU
----+    num_nodes: 1
----+    max_epochs: 8
----+    max_steps: null # precedence over max_epochs
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----+    gradient_clip_val: 0
----+    amp_level: O0 # O1/O2 for mixed precision
----+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----+    checkpoint_callback: false  # Provided by exp_manager
----+    logger: false #false  # Provided by exp_manager
----+    log_every_n_steps: 1  # Interval of logging.
----+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    reload_dataloaders_every_epoch: true
----+    resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /root/data # /home/nxingyu2/data # 
-----tmp_path: /tmp # /home/nxingyu2/data/tmp # 
----+base_path: /home/nxingyu2/data # /root/data # 
----+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
----@@ -61,7 +61,7 @@ model:
----     punct_class_weights: true
----     
----     dataset:
-----        data_dir: /root/data # /home/nxingyu2/data # 
----+        data_dir: /home/nxingyu2/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -125,27 +125,27 @@ model:
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 4
----+        alpha: 3
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: adamw
----+        name: ranger
----         lr: 1e-3
----         weight_decay: 0.00
-----
----         sched:
-----            name: WarmupAnnealing #CyclicLR
----+            name: CosineAnnealing #CyclicLR
----             # Scheduler params
-----            warmup_steps: null
-----            warmup_ratio: 0.1
----+            # warmup_steps: 6
----+            # warmup_ratio: 0.1
----+            hold_steps: 6
----             last_epoch: -1
---- 
----             # pytorch lightning args
----             monitor: val_loss
-----            reduce_on_plateau: true
----+            reduce_on_plateau: false
---- hydra:
----     run:
----         dir: .
----diff --git a/experiment/core/optim/lr_scheduler.py b/experiment/core/optim/lr_scheduler.py
----index a17ebf8..4c5f4a7 100644
------- a/experiment/core/optim/lr_scheduler.py
----+++ b/experiment/core/optim/lr_scheduler.py
----@@ -347,6 +347,29 @@ class PolynomialHoldDecayAnnealing(WarmupHoldPolicy):
----         return new_lrs
---- 
---- 
----+class CosineHoldDecayAnnealing(WarmupHoldPolicy):
----+    def __init__(self, optimizer, *, max_steps, min_lr=0.0, last_epoch=-1, **kwargs):
----+        super().__init__(optimizer=optimizer, max_steps=max_steps, last_epoch=last_epoch, min_lr=min_lr, **kwargs)
----+
----+    def _get_lr(self, step):
----+        for initial_lr in self.base_lrs:
----+            if initial_lr < self.min_lr:
----+                raise ValueError(
----+                    f"{self} received an initial learning rate that was lower than the minimum learning rate."
----+                )
----+
----+        new_lrs = [
----+            _cosine_annealing(
----+                initial_lr=initial_lr,
----+                step=step - self.hold_steps,
----+                max_steps=self.max_steps - max(self.warmup_steps, self.hold_steps),
----+                min_lr=self.min_lr,
----+            )
----+            for initial_lr in self.base_lrs
----+        ]
----+        return new_lrs
----+
----+
---- def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: SchedulerParams):
----     """
----     Checks if the scheduler name exists in the registry, and if it doesnt, adds it.
----@@ -365,7 +388,7 @@ def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: Sch
----     register_scheduler_params(name=sched_name, scheduler_params=scheduler_params)
---- 
---- 
-----def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
----+def get_scheduler(name: str, optimizer, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
----     """
----     Convenience method to obtain an _LRScheduler class and partially instantiate it with optimizer kwargs.
----     Args:
----@@ -654,4 +677,5 @@ AVAILABLE_SCHEDULERS = {
----     'ExponentialLR': pt_scheduler.ExponentialLR,
----     'ReduceLROnPlateau': pt_scheduler.ReduceLROnPlateau,
----     'CyclicLR': pt_scheduler.CyclicLR,
----+    'CosineHoldDecayAnnealing': CosineHoldDecayAnnealing,
---- }
----diff --git a/experiment/core/optim/optimizers.py b/experiment/core/optim/optimizers.py
----index 5afc4fd..f15686e 100644
------- a/experiment/core/optim/optimizers.py
----+++ b/experiment/core/optim/optimizers.py
----@@ -7,6 +7,7 @@ import torch.optim as optim
---- from omegaconf import DictConfig, OmegaConf
---- from torch.optim import adadelta, adagrad, adamax, rmsprop, rprop
---- from torch.optim.optimizer import Optimizer
----+from torchtools.optim import Ranger
---- 
---- from core.config import OptimizerParams, get_optimizer_config, register_optimizer_params
---- from core.optim.novograd import Novograd
----@@ -24,6 +25,7 @@ AVAILABLE_OPTIMIZERS = {
----     'rmsprop': rmsprop.RMSprop,
----     'rprop': rprop.Rprop,
----     'novograd': Novograd,
----+    'ranger': Ranger,
---- }
---- 
---- 
----diff --git a/experiment/info.log b/experiment/info.log
----index b211d42..5024c4f 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,473 +1,2 @@
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4da7f8e0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.008413951416451957
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e67aa00>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          22.42      11.12      14.86       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                         38.62       7.46      12.50      19530
------ (label_id: 3)                                          6.60      15.75       9.30       1746
-----. (label_id: 4)                                         33.33       0.01       0.01      17976
-----: (label_id: 5)                                          0.79      19.15       1.52        376
-----; (label_id: 6)                                          0.45       2.94       0.78        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.17      50.56       0.33         89
------------------------
-----micro avg                                                5.14       5.14       5.14      48233
-----macro avg                                               10.24      10.70       3.93      48233
-----weighted avg                                            30.92       5.14       7.15      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          22.09      11.27      14.93       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                         35.39       6.74      11.32      19813
------ (label_id: 3)                                          6.03      16.28       8.79       1708
-----. (label_id: 4)                                        100.00       0.01       0.02      18084
-----: (label_id: 5)                                          0.78      20.69       1.51        348
-----; (label_id: 6)                                          0.45       2.55       0.77        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.19      58.62       0.38         87
------------------------
-----micro avg                                                4.89       4.89       4.89      48633
-----macro avg                                               16.49      11.62       3.77      48633
-----weighted avg                                            54.38       4.89       6.67      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.008413951416451957
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e74d910>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          19.61      14.42      16.62        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         38.84       6.82      11.60        689
------ (label_id: 3)                                          4.50      15.52       6.98         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.59      20.00       1.14         10
-----; (label_id: 6)                                          2.56      16.67       4.44          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.25      50.00       0.51          4
------------------------
-----micro avg                                                5.55       5.55       5.55       1641
-----macro avg                                                6.64      12.34       4.13       1641
-----weighted avg                                            18.97       5.55       7.25       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.63      98.91       7.00       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          3.96       1.76       2.44       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.63       3.63       3.63      48233
-----macro avg                                                0.76      10.07       0.94      48233
-----weighted avg                                             0.25       3.63       0.33      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e68b8b0>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          19.61      14.42      16.62        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         38.84       6.82      11.60        689
------ (label_id: 3)                                          4.50      15.52       6.98         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.59      20.00       1.14         10
-----; (label_id: 6)                                          2.56      16.67       4.44          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.25      50.00       0.51          4
------------------------
-----micro avg                                                5.55       5.55       5.55       1641
-----macro avg                                                6.64      12.34       4.13       1641
-----weighted avg                                            18.97       5.55       7.25       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                          0.00       0.00       0.00      19813
------ (label_id: 3)                                          3.51     100.00       6.79       1708
-----. (label_id: 4)                                          0.00       0.00       0.00      18084
-----: (label_id: 5)                                          0.00       0.00       0.00        348
-----; (label_id: 6)                                          0.00       0.00       0.00        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.00       0.00       0.00         87
------------------------
-----micro avg                                                3.51       3.51       3.51      48633
-----macro avg                                                0.35      10.00       0.68      48633
-----weighted avg                                             0.12       3.51       0.24      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75eaf0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                          0.00       0.00       0.00        689
------ (label_id: 3)                                          3.53     100.00       6.83         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.00       0.00       0.00         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                3.53       3.53       3.53       1641
-----macro avg                                                0.35      10.00       0.68       1641
-----weighted avg                                             0.12       3.53       0.24       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75e160>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                          0.00       0.00       0.00        689
------ (label_id: 3)                                          3.53     100.00       6.83         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.00       0.00       0.00         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                3.53       3.53       3.53       1641
-----macro avg                                                0.35      10.00       0.68       1641
-----weighted avg                                             0.12       3.53       0.24       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                          0.00       0.00       0.00      19813
------ (label_id: 3)                                          3.51     100.00       6.79       1708
-----. (label_id: 4)                                          0.00       0.00       0.00      18084
-----: (label_id: 5)                                          0.00       0.00       0.00        348
-----; (label_id: 6)                                          0.00       0.00       0.00        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.00       0.00       0.00         87
------------------------
-----micro avg                                                3.51       3.51       3.51      48633
-----macro avg                                                0.35      10.00       0.68      48633
-----weighted avg                                             0.12       3.51       0.24      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       7470
-----! (label_id: 1)                                          0.00       0.00       0.00        148
-----, (label_id: 2)                                          0.00       0.00       0.00      28513
------ (label_id: 3)                                          3.02     100.00       5.86       2074
-----. (label_id: 4)                                          0.00       0.00       0.00      25120
-----: (label_id: 5)                                          0.00       0.00       0.00        570
-----; (label_id: 6)                                          0.00       0.00       0.00        534
-----? (label_id: 7)                                          0.00       0.00       0.00       2085
-----— (label_id: 8)                                          0.00       0.00       0.00       2073
-----… (label_id: 9)                                          0.00       0.00       0.00        142
------------------------
-----micro avg                                                3.02       3.02       3.02      68729
-----macro avg                                                0.30      10.00       0.59      68729
-----weighted avg                                             0.09       3.02       0.18      68729
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       3735
------------------------
-----micro avg                                              100.00     100.00     100.00       3735
-----macro avg                                              100.00     100.00     100.00       3735
-----weighted avg                                           100.00     100.00     100.00       3735
-----
-----[INFO] - Internal process exited
----+[INFO] - GPU available: True, used: False
----+[INFO] - TPU available: None, using: 0 TPU cores
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index 782fbfa..fa37b4c 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -374,6 +374,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             scheduler_config = None
---- 
----+
----         # Check if caller provided optimizer name, default to Adam otherwise
----         optimizer_cls = optim_config.get('_target_', None)
---- 
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/lightning_logs.txt
---deleted file mode 100644
---index 01e1664..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/lightning_logs.txt
---+++ /dev/null
---@@ -1,2 +0,0 @@
----GPU available: True, used: False
----TPU available: None, using: 0 TPU cores
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/nemo_error_log.txt
---deleted file mode 100644
---index 2ed3a0d..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/nemo_error_log.txt
---+++ /dev/null
---@@ -1,4 +0,0 @@
----[NeMo W 2021-02-06 10:21:49 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 10:21:49 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index bba476b..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,6 +0,0 @@
----[NeMo W 2021-02-06 10:21:49 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo I 2021-02-06 10:21:49 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_10-21-49
----[NeMo I 2021-02-06 10:21:49 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-06 10:21:49 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/git-info.log
---deleted file mode 100644
---index 7e71a38..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/git-info.log
---+++ /dev/null
---@@ -1,678 +0,0 @@
----commit hash: cce540d4c95ad9ed32010552a7aa1fb2486658c8
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index 3d27dd7..652b9dc 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,43 +1,43 @@
---- seed: 42
---- trainer:
-----    gpus: 1 # the number of gpus, 0 for CPU
-----    num_nodes: 1
-----    max_epochs: 2
-----    max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 4 # accumulates grads every k batches
-----    gradient_clip_val: 0
-----    amp_level: O1 # O1/O2 for mixed precision
-----    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    accelerator: ddp
-----    checkpoint_callback: false  # Provided by exp_manager
-----    logger: false #false  # Provided by exp_manager
-----    log_every_n_steps: 1  # Interval of logging.
-----    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    resume_from_checkpoint: null
-----
-----    # gpus: 0 # the number of gpus, 0 for CPU
----+    # gpus: 1 # the number of gpus, 0 for CPU
----     # num_nodes: 1
-----    # max_epochs: 10
----+    # max_epochs: 2
----     # max_steps: null # precedence over max_epochs
----     # accumulate_grad_batches: 4 # accumulates grads every k batches
----     # gradient_clip_val: 0
-----    # amp_level: O0 # O1/O2 for mixed precision
-----    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # # accelerator: ddp
----+    # amp_level: O1 # O1/O2 for mixed precision
----+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----     # checkpoint_callback: false  # Provided by exp_manager
----     # logger: false #false  # Provided by exp_manager
----     # log_every_n_steps: 1  # Interval of logging.
----     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # reload_dataloaders_every_epoch: true
----     # resume_from_checkpoint: null
---- 
----+    gpus: 0 # the number of gpus, 0 for CPU
----+    num_nodes: 1
----+    max_epochs: 8
----+    max_steps: null # precedence over max_epochs
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----+    gradient_clip_val: 0
----+    amp_level: O0 # O1/O2 for mixed precision
----+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----+    checkpoint_callback: false  # Provided by exp_manager
----+    logger: false #false  # Provided by exp_manager
----+    log_every_n_steps: 1  # Interval of logging.
----+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    reload_dataloaders_every_epoch: true
----+    resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /root/data # /home/nxingyu2/data # 
-----tmp_path: /tmp # /home/nxingyu2/data/tmp # 
----+base_path: /home/nxingyu2/data # /root/data # 
----+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
----@@ -61,7 +61,7 @@ model:
----     punct_class_weights: true
----     
----     dataset:
-----        data_dir: /root/data # /home/nxingyu2/data # 
----+        data_dir: /home/nxingyu2/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -125,27 +125,27 @@ model:
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 4
----+        alpha: 3
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: adamw
----+        name: ranger
----         lr: 1e-3
----         weight_decay: 0.00
-----
----         sched:
-----            name: WarmupAnnealing #CyclicLR
----+            name: CosineHoldDecayAnnealing #CyclicLR
----             # Scheduler params
-----            warmup_steps: null
-----            warmup_ratio: 0.1
----+            # warmup_steps: 6
----+            # warmup_ratio: 0.1
----+            hold_steps: 6
----             last_epoch: -1
---- 
----             # pytorch lightning args
----             monitor: val_loss
-----            reduce_on_plateau: true
----+            reduce_on_plateau: false
---- hydra:
----     run:
----         dir: .
----diff --git a/experiment/core/optim/lr_scheduler.py b/experiment/core/optim/lr_scheduler.py
----index a17ebf8..4c5f4a7 100644
------- a/experiment/core/optim/lr_scheduler.py
----+++ b/experiment/core/optim/lr_scheduler.py
----@@ -347,6 +347,29 @@ class PolynomialHoldDecayAnnealing(WarmupHoldPolicy):
----         return new_lrs
---- 
---- 
----+class CosineHoldDecayAnnealing(WarmupHoldPolicy):
----+    def __init__(self, optimizer, *, max_steps, min_lr=0.0, last_epoch=-1, **kwargs):
----+        super().__init__(optimizer=optimizer, max_steps=max_steps, last_epoch=last_epoch, min_lr=min_lr, **kwargs)
----+
----+    def _get_lr(self, step):
----+        for initial_lr in self.base_lrs:
----+            if initial_lr < self.min_lr:
----+                raise ValueError(
----+                    f"{self} received an initial learning rate that was lower than the minimum learning rate."
----+                )
----+
----+        new_lrs = [
----+            _cosine_annealing(
----+                initial_lr=initial_lr,
----+                step=step - self.hold_steps,
----+                max_steps=self.max_steps - max(self.warmup_steps, self.hold_steps),
----+                min_lr=self.min_lr,
----+            )
----+            for initial_lr in self.base_lrs
----+        ]
----+        return new_lrs
----+
----+
---- def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: SchedulerParams):
----     """
----     Checks if the scheduler name exists in the registry, and if it doesnt, adds it.
----@@ -365,7 +388,7 @@ def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: Sch
----     register_scheduler_params(name=sched_name, scheduler_params=scheduler_params)
---- 
---- 
-----def get_scheduler(name: str, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
----+def get_scheduler(name: str, optimizer, **kwargs: Optional[Dict[str, Any]]) -> _LRScheduler:
----     """
----     Convenience method to obtain an _LRScheduler class and partially instantiate it with optimizer kwargs.
----     Args:
----@@ -654,4 +677,5 @@ AVAILABLE_SCHEDULERS = {
----     'ExponentialLR': pt_scheduler.ExponentialLR,
----     'ReduceLROnPlateau': pt_scheduler.ReduceLROnPlateau,
----     'CyclicLR': pt_scheduler.CyclicLR,
----+    'CosineHoldDecayAnnealing': CosineHoldDecayAnnealing,
---- }
----diff --git a/experiment/core/optim/optimizers.py b/experiment/core/optim/optimizers.py
----index 5afc4fd..f15686e 100644
------- a/experiment/core/optim/optimizers.py
----+++ b/experiment/core/optim/optimizers.py
----@@ -7,6 +7,7 @@ import torch.optim as optim
---- from omegaconf import DictConfig, OmegaConf
---- from torch.optim import adadelta, adagrad, adamax, rmsprop, rprop
---- from torch.optim.optimizer import Optimizer
----+from torchtools.optim import Ranger
---- 
---- from core.config import OptimizerParams, get_optimizer_config, register_optimizer_params
---- from core.optim.novograd import Novograd
----@@ -24,6 +25,7 @@ AVAILABLE_OPTIMIZERS = {
----     'rmsprop': rmsprop.RMSprop,
----     'rprop': rprop.Rprop,
----     'novograd': Novograd,
----+    'ranger': Ranger,
---- }
---- 
---- 
----diff --git a/experiment/info.log b/experiment/info.log
----index b211d42..5024c4f 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,473 +1,2 @@
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4da7f8e0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.008413951416451957
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e67aa00>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          22.42      11.12      14.86       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                         38.62       7.46      12.50      19530
------ (label_id: 3)                                          6.60      15.75       9.30       1746
-----. (label_id: 4)                                         33.33       0.01       0.01      17976
-----: (label_id: 5)                                          0.79      19.15       1.52        376
-----; (label_id: 6)                                          0.45       2.94       0.78        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.17      50.56       0.33         89
------------------------
-----micro avg                                                5.14       5.14       5.14      48233
-----macro avg                                               10.24      10.70       3.93      48233
-----weighted avg                                            30.92       5.14       7.15      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          22.09      11.27      14.93       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                         35.39       6.74      11.32      19813
------ (label_id: 3)                                          6.03      16.28       8.79       1708
-----. (label_id: 4)                                        100.00       0.01       0.02      18084
-----: (label_id: 5)                                          0.78      20.69       1.51        348
-----; (label_id: 6)                                          0.45       2.55       0.77        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.19      58.62       0.38         87
------------------------
-----micro avg                                                4.89       4.89       4.89      48633
-----macro avg                                               16.49      11.62       3.77      48633
-----weighted avg                                            54.38       4.89       6.67      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.008413951416451957
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e74d910>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          19.61      14.42      16.62        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         38.84       6.82      11.60        689
------ (label_id: 3)                                          4.50      15.52       6.98         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.59      20.00       1.14         10
-----; (label_id: 6)                                          2.56      16.67       4.44          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.25      50.00       0.51          4
------------------------
-----micro avg                                                5.55       5.55       5.55       1641
-----macro avg                                                6.64      12.34       4.13       1641
-----weighted avg                                            18.97       5.55       7.25       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.63      98.91       7.00       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          3.96       1.76       2.44       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.63       3.63       3.63      48233
-----macro avg                                                0.76      10.07       0.94      48233
-----weighted avg                                             0.25       3.63       0.33      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e68b8b0>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          19.61      14.42      16.62        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         38.84       6.82      11.60        689
------ (label_id: 3)                                          4.50      15.52       6.98         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.59      20.00       1.14         10
-----; (label_id: 6)                                          2.56      16.67       4.44          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.25      50.00       0.51          4
------------------------
-----micro avg                                                5.55       5.55       5.55       1641
-----macro avg                                                6.64      12.34       4.13       1641
-----weighted avg                                            18.97       5.55       7.25       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                          0.00       0.00       0.00      19813
------ (label_id: 3)                                          3.51     100.00       6.79       1708
-----. (label_id: 4)                                          0.00       0.00       0.00      18084
-----: (label_id: 5)                                          0.00       0.00       0.00        348
-----; (label_id: 6)                                          0.00       0.00       0.00        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.00       0.00       0.00         87
------------------------
-----micro avg                                                3.51       3.51       3.51      48633
-----macro avg                                                0.35      10.00       0.68      48633
-----weighted avg                                             0.12       3.51       0.24      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75eaf0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                          0.00       0.00       0.00        689
------ (label_id: 3)                                          3.53     100.00       6.83         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.00       0.00       0.00         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                3.53       3.53       3.53       1641
-----macro avg                                                0.35      10.00       0.68       1641
-----weighted avg                                             0.12       3.53       0.24       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75e160>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                          0.00       0.00       0.00        689
------ (label_id: 3)                                          3.53     100.00       6.83         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.00       0.00       0.00         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                3.53       3.53       3.53       1641
-----macro avg                                                0.35      10.00       0.68       1641
-----weighted avg                                             0.12       3.53       0.24       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                          0.00       0.00       0.00      19813
------ (label_id: 3)                                          3.51     100.00       6.79       1708
-----. (label_id: 4)                                          0.00       0.00       0.00      18084
-----: (label_id: 5)                                          0.00       0.00       0.00        348
-----; (label_id: 6)                                          0.00       0.00       0.00        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.00       0.00       0.00         87
------------------------
-----micro avg                                                3.51       3.51       3.51      48633
-----macro avg                                                0.35      10.00       0.68      48633
-----weighted avg                                             0.12       3.51       0.24      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       7470
-----! (label_id: 1)                                          0.00       0.00       0.00        148
-----, (label_id: 2)                                          0.00       0.00       0.00      28513
------ (label_id: 3)                                          3.02     100.00       5.86       2074
-----. (label_id: 4)                                          0.00       0.00       0.00      25120
-----: (label_id: 5)                                          0.00       0.00       0.00        570
-----; (label_id: 6)                                          0.00       0.00       0.00        534
-----? (label_id: 7)                                          0.00       0.00       0.00       2085
-----— (label_id: 8)                                          0.00       0.00       0.00       2073
-----… (label_id: 9)                                          0.00       0.00       0.00        142
------------------------
-----micro avg                                                3.02       3.02       3.02      68729
-----macro avg                                                0.30      10.00       0.59      68729
-----weighted avg                                             0.09       3.02       0.18      68729
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       3735
------------------------
-----micro avg                                              100.00     100.00     100.00       3735
-----macro avg                                              100.00     100.00     100.00       3735
-----weighted avg                                           100.00     100.00     100.00       3735
-----
-----[INFO] - Internal process exited
----+[INFO] - GPU available: True, used: False
----+[INFO] - TPU available: None, using: 0 TPU cores
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index 782fbfa..fa37b4c 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -374,6 +374,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             scheduler_config = None
---- 
----+
----         # Check if caller provided optimizer name, default to Adam otherwise
----         optimizer_cls = optim_config.get('_target_', None)
---- 
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/lightning_logs.txt
---deleted file mode 100644
---index 01e1664..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/lightning_logs.txt
---+++ /dev/null
---@@ -1,2 +0,0 @@
----GPU available: True, used: False
----TPU available: None, using: 0 TPU cores
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/nemo_error_log.txt
---deleted file mode 100644
---index b057ce1..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/nemo_error_log.txt
---+++ /dev/null
---@@ -1,4 +0,0 @@
----[NeMo W 2021-02-06 10:22:46 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 10:22:46 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index 02f3d52..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,6 +0,0 @@
----[NeMo W 2021-02-06 10:22:46 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo I 2021-02-06 10:22:46 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_10-22-46
----[NeMo I 2021-02-06 10:22:46 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-06 10:22:46 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/events.out.tfevents.1612579094.intern-instance.28215.0 b/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/events.out.tfevents.1612579094.intern-instance.28215.0
---deleted file mode 100644
---index 134df00..0000000
---Binary files a/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/events.out.tfevents.1612579094.intern-instance.28215.0 and /dev/null differ
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/git-info.log
---deleted file mode 100644
---index 8b49757..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/git-info.log
---+++ /dev/null
---@@ -1,669 +0,0 @@
----commit hash: cce540d4c95ad9ed32010552a7aa1fb2486658c8
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index 3d27dd7..652b9dc 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,43 +1,43 @@
---- seed: 42
---- trainer:
-----    gpus: 1 # the number of gpus, 0 for CPU
-----    num_nodes: 1
-----    max_epochs: 2
-----    max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 4 # accumulates grads every k batches
-----    gradient_clip_val: 0
-----    amp_level: O1 # O1/O2 for mixed precision
-----    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    accelerator: ddp
-----    checkpoint_callback: false  # Provided by exp_manager
-----    logger: false #false  # Provided by exp_manager
-----    log_every_n_steps: 1  # Interval of logging.
-----    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    resume_from_checkpoint: null
-----
-----    # gpus: 0 # the number of gpus, 0 for CPU
----+    # gpus: 1 # the number of gpus, 0 for CPU
----     # num_nodes: 1
-----    # max_epochs: 10
----+    # max_epochs: 2
----     # max_steps: null # precedence over max_epochs
----     # accumulate_grad_batches: 4 # accumulates grads every k batches
----     # gradient_clip_val: 0
-----    # amp_level: O0 # O1/O2 for mixed precision
-----    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # # accelerator: ddp
----+    # amp_level: O1 # O1/O2 for mixed precision
----+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----     # checkpoint_callback: false  # Provided by exp_manager
----     # logger: false #false  # Provided by exp_manager
----     # log_every_n_steps: 1  # Interval of logging.
----     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # reload_dataloaders_every_epoch: true
----     # resume_from_checkpoint: null
---- 
----+    gpus: 0 # the number of gpus, 0 for CPU
----+    num_nodes: 1
----+    max_epochs: 8
----+    max_steps: null # precedence over max_epochs
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----+    gradient_clip_val: 0
----+    amp_level: O0 # O1/O2 for mixed precision
----+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----+    checkpoint_callback: false  # Provided by exp_manager
----+    logger: false #false  # Provided by exp_manager
----+    log_every_n_steps: 1  # Interval of logging.
----+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    reload_dataloaders_every_epoch: true
----+    resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /root/data # /home/nxingyu2/data # 
-----tmp_path: /tmp # /home/nxingyu2/data/tmp # 
----+base_path: /home/nxingyu2/data # /root/data # 
----+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
----@@ -61,7 +61,7 @@ model:
----     punct_class_weights: true
----     
----     dataset:
-----        data_dir: /root/data # /home/nxingyu2/data # 
----+        data_dir: /home/nxingyu2/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -125,27 +125,27 @@ model:
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 4
----+        alpha: 3
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: adamw
----+        name: ranger
----         lr: 1e-3
----         weight_decay: 0.00
-----
----         sched:
-----            name: WarmupAnnealing #CyclicLR
----+            name: CosineHoldDecayAnnealing #CyclicLR
----             # Scheduler params
-----            warmup_steps: null
-----            warmup_ratio: 0.1
----+            # warmup_steps: 6
----+            # warmup_ratio: 0.1
----+            hold_steps: 6
----             last_epoch: -1
---- 
----             # pytorch lightning args
----             monitor: val_loss
-----            reduce_on_plateau: true
----+            reduce_on_plateau: false
---- hydra:
----     run:
----         dir: .
----diff --git a/experiment/core/optim/lr_scheduler.py b/experiment/core/optim/lr_scheduler.py
----index a17ebf8..0c7d449 100644
------- a/experiment/core/optim/lr_scheduler.py
----+++ b/experiment/core/optim/lr_scheduler.py
----@@ -347,6 +347,29 @@ class PolynomialHoldDecayAnnealing(WarmupHoldPolicy):
----         return new_lrs
---- 
---- 
----+class CosineHoldDecayAnnealing(WarmupHoldPolicy):
----+    def __init__(self, optimizer, *, max_steps, min_lr=0.0, last_epoch=-1, **kwargs):
----+        super().__init__(optimizer=optimizer, max_steps=max_steps, last_epoch=last_epoch, min_lr=min_lr, **kwargs)
----+
----+    def _get_lr(self, step):
----+        for initial_lr in self.base_lrs:
----+            if initial_lr < self.min_lr:
----+                raise ValueError(
----+                    f"{self} received an initial learning rate that was lower than the minimum learning rate."
----+                )
----+
----+        new_lrs = [
----+            _cosine_annealing(
----+                initial_lr=initial_lr,
----+                step=step - self.hold_steps,
----+                max_steps=self.max_steps - max(self.warmup_steps, self.hold_steps),
----+                min_lr=self.min_lr,
----+            )
----+            for initial_lr in self.base_lrs
----+        ]
----+        return new_lrs
----+
----+
---- def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: SchedulerParams):
----     """
----     Checks if the scheduler name exists in the registry, and if it doesnt, adds it.
----@@ -654,4 +677,5 @@ AVAILABLE_SCHEDULERS = {
----     'ExponentialLR': pt_scheduler.ExponentialLR,
----     'ReduceLROnPlateau': pt_scheduler.ReduceLROnPlateau,
----     'CyclicLR': pt_scheduler.CyclicLR,
----+    'CosineHoldDecayAnnealing': CosineHoldDecayAnnealing,
---- }
----diff --git a/experiment/core/optim/optimizers.py b/experiment/core/optim/optimizers.py
----index 5afc4fd..f15686e 100644
------- a/experiment/core/optim/optimizers.py
----+++ b/experiment/core/optim/optimizers.py
----@@ -7,6 +7,7 @@ import torch.optim as optim
---- from omegaconf import DictConfig, OmegaConf
---- from torch.optim import adadelta, adagrad, adamax, rmsprop, rprop
---- from torch.optim.optimizer import Optimizer
----+from torchtools.optim import Ranger
---- 
---- from core.config import OptimizerParams, get_optimizer_config, register_optimizer_params
---- from core.optim.novograd import Novograd
----@@ -24,6 +25,7 @@ AVAILABLE_OPTIMIZERS = {
----     'rmsprop': rmsprop.RMSprop,
----     'rprop': rprop.Rprop,
----     'novograd': Novograd,
----+    'ranger': Ranger,
---- }
---- 
---- 
----diff --git a/experiment/info.log b/experiment/info.log
----index b211d42..5024c4f 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,473 +1,2 @@
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4da7f8e0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.008413951416451957
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e67aa00>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          22.42      11.12      14.86       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                         38.62       7.46      12.50      19530
------ (label_id: 3)                                          6.60      15.75       9.30       1746
-----. (label_id: 4)                                         33.33       0.01       0.01      17976
-----: (label_id: 5)                                          0.79      19.15       1.52        376
-----; (label_id: 6)                                          0.45       2.94       0.78        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.17      50.56       0.33         89
------------------------
-----micro avg                                                5.14       5.14       5.14      48233
-----macro avg                                               10.24      10.70       3.93      48233
-----weighted avg                                            30.92       5.14       7.15      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          22.09      11.27      14.93       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                         35.39       6.74      11.32      19813
------ (label_id: 3)                                          6.03      16.28       8.79       1708
-----. (label_id: 4)                                        100.00       0.01       0.02      18084
-----: (label_id: 5)                                          0.78      20.69       1.51        348
-----; (label_id: 6)                                          0.45       2.55       0.77        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.19      58.62       0.38         87
------------------------
-----micro avg                                                4.89       4.89       4.89      48633
-----macro avg                                               16.49      11.62       3.77      48633
-----weighted avg                                            54.38       4.89       6.67      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.008413951416451957
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e74d910>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          19.61      14.42      16.62        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         38.84       6.82      11.60        689
------ (label_id: 3)                                          4.50      15.52       6.98         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.59      20.00       1.14         10
-----; (label_id: 6)                                          2.56      16.67       4.44          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.25      50.00       0.51          4
------------------------
-----micro avg                                                5.55       5.55       5.55       1641
-----macro avg                                                6.64      12.34       4.13       1641
-----weighted avg                                            18.97       5.55       7.25       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.63      98.91       7.00       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          3.96       1.76       2.44       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.63       3.63       3.63      48233
-----macro avg                                                0.76      10.07       0.94      48233
-----weighted avg                                             0.25       3.63       0.33      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e68b8b0>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          19.61      14.42      16.62        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         38.84       6.82      11.60        689
------ (label_id: 3)                                          4.50      15.52       6.98         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.59      20.00       1.14         10
-----; (label_id: 6)                                          2.56      16.67       4.44          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.25      50.00       0.51          4
------------------------
-----micro avg                                                5.55       5.55       5.55       1641
-----macro avg                                                6.64      12.34       4.13       1641
-----weighted avg                                            18.97       5.55       7.25       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                          0.00       0.00       0.00      19813
------ (label_id: 3)                                          3.51     100.00       6.79       1708
-----. (label_id: 4)                                          0.00       0.00       0.00      18084
-----: (label_id: 5)                                          0.00       0.00       0.00        348
-----; (label_id: 6)                                          0.00       0.00       0.00        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.00       0.00       0.00         87
------------------------
-----micro avg                                                3.51       3.51       3.51      48633
-----macro avg                                                0.35      10.00       0.68      48633
-----weighted avg                                             0.12       3.51       0.24      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75eaf0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                          0.00       0.00       0.00        689
------ (label_id: 3)                                          3.53     100.00       6.83         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.00       0.00       0.00         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                3.53       3.53       3.53       1641
-----macro avg                                                0.35      10.00       0.68       1641
-----weighted avg                                             0.12       3.53       0.24       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75e160>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                          0.00       0.00       0.00        689
------ (label_id: 3)                                          3.53     100.00       6.83         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.00       0.00       0.00         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                3.53       3.53       3.53       1641
-----macro avg                                                0.35      10.00       0.68       1641
-----weighted avg                                             0.12       3.53       0.24       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                          0.00       0.00       0.00      19813
------ (label_id: 3)                                          3.51     100.00       6.79       1708
-----. (label_id: 4)                                          0.00       0.00       0.00      18084
-----: (label_id: 5)                                          0.00       0.00       0.00        348
-----; (label_id: 6)                                          0.00       0.00       0.00        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.00       0.00       0.00         87
------------------------
-----micro avg                                                3.51       3.51       3.51      48633
-----macro avg                                                0.35      10.00       0.68      48633
-----weighted avg                                             0.12       3.51       0.24      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       7470
-----! (label_id: 1)                                          0.00       0.00       0.00        148
-----, (label_id: 2)                                          0.00       0.00       0.00      28513
------ (label_id: 3)                                          3.02     100.00       5.86       2074
-----. (label_id: 4)                                          0.00       0.00       0.00      25120
-----: (label_id: 5)                                          0.00       0.00       0.00        570
-----; (label_id: 6)                                          0.00       0.00       0.00        534
-----? (label_id: 7)                                          0.00       0.00       0.00       2085
-----— (label_id: 8)                                          0.00       0.00       0.00       2073
-----… (label_id: 9)                                          0.00       0.00       0.00        142
------------------------
-----micro avg                                                3.02       3.02       3.02      68729
-----macro avg                                                0.30      10.00       0.59      68729
-----weighted avg                                             0.09       3.02       0.18      68729
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       3735
------------------------
-----micro avg                                              100.00     100.00     100.00       3735
-----macro avg                                              100.00     100.00     100.00       3735
-----weighted avg                                           100.00     100.00     100.00       3735
-----
-----[INFO] - Internal process exited
----+[INFO] - GPU available: True, used: False
----+[INFO] - TPU available: None, using: 0 TPU cores
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index 782fbfa..fa37b4c 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -374,6 +374,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             scheduler_config = None
---- 
----+
----         # Check if caller provided optimizer name, default to Adam otherwise
----         optimizer_cls = optim_config.get('_target_', None)
---- 
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/hparams.yaml
---deleted file mode 100644
---index 3d76cb3..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/hparams.yaml
---+++ /dev/null
---@@ -1,106 +0,0 @@
----seed: 42
----trainer:
----  gpus: 0
----  num_nodes: 1
----  max_epochs: 8
----  max_steps: null
----  accumulate_grad_batches: 4
----  gradient_clip_val: 0
----  amp_level: O0
----  precision: 32
----  checkpoint_callback: false
----  logger: false
----  log_every_n_steps: 1
----  val_check_interval: 1.0
----  reload_dataloaders_every_epoch: true
----  resume_from_checkpoint: null
----exp_manager:
----  exp_dir: /home/nxingyu2/project/
----  name: Punctuation_with_Domain_discriminator
----  create_tensorboard_logger: true
----  create_checkpoint_callback: true
----base_path: /home/nxingyu2/data
----tmp_path: /home/nxingyu2/data/tmp
----model:
----  nemo_path: null
----  transformer_path: google/electra-small-discriminator
----  unfrozen: 0
----  maximum_unfrozen: 2
----  unfreeze_step: 1
----  punct_label_ids:
----  - ''
----  - '!'
----  - ','
----  - '-'
----  - .
----  - ':'
----  - ;
----  - '?'
----  - —
----  - …
----  punct_class_weights: true
----  dataset:
----    data_dir: /home/nxingyu2/data
----    labelled:
----    - /home/nxingyu2/data/ted_talks_processed
----    unlabelled: null
----    max_seq_length: 128
----    pad_label: ''
----    ignore_extra_tokens: false
----    ignore_start_end: false
----    use_cache: false
----    num_workers: 0
----    pin_memory: true
----    drop_last: false
----    num_labels: 10
----    num_domains: 1
----    test_unlabelled: true
----    train_ds:
----      shuffle: true
----      num_samples: -1
----      batch_size: 8
----    validation_ds:
----      shuffle: true
----      num_samples: -1
----      batch_size: 8
----  tokenizer:
----    tokenizer_name: google/electra-small-discriminator
----    vocab_file: null
----    tokenizer_model: null
----    special_tokens: null
----  language_model:
----    pretrained_model_name: google/electra-small-discriminator
----    lm_checkpoint: null
----    config_file: null
----    config: null
----  punct_head:
----    punct_num_fc_layers: 1
----    fc_dropout: 0.1
----    activation: relu
----    log_softmax: false
----    use_transformer_init: true
----    loss: dice
----  domain_head:
----    domain_num_fc_layers: 1
----    fc_dropout: 0.1
----    activation: relu
----    log_softmax: false
----    use_transformer_init: true
----    loss: cel
----    gamma: 0.1
----  dice_loss:
----    epsilon: 0.01
----    alpha: 3
----    macro_average: true
----  focal_loss:
----    gamma: 5
----  optim:
----    name: ranger
----    lr: 0.001
----    weight_decay: 0.0
----    sched:
----      name: CosineHoldDecayAnnealing
----      hold_steps: 6
----      last_epoch: -1
----      monitor: val_loss
----      reduce_on_plateau: false
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/lightning_logs.txt
---deleted file mode 100644
---index 54b8b13..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/lightning_logs.txt
---+++ /dev/null
---@@ -1,115 +0,0 @@
----GPU available: True, used: False
----TPU available: None, using: 0 TPU cores
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | punct_classifier    | TokenClassifier      | 2.6 K 
----2 | domain_classifier   | SequenceClassifier   | 257   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----35.9 K    Trainable params
----13.4 M    Non-trainable params
----13.5 M    Total params
----Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/lr_find_temp_model.ckpt
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | punct_classifier    | TokenClassifier      | 2.6 K 
----2 | domain_classifier   | SequenceClassifier   | 257   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----35.9 K    Trainable params
----13.4 M    Non-trainable params
----13.5 M    Total params
----Epoch 0, global step 100: val_loss reached 0.85947 (best 0.85947), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
----Epoch 1, global step 200: val_loss reached 0.85987 (best 0.85947), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=1.ckpt" as top 3
----Epoch 2, global step 300: val_loss reached 0.86472 (best 0.85947), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
----Epoch 3, step 400: val_loss was not in top 3
----Epoch 4, global step 500: val_loss reached 0.86165 (best 0.85947), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
----Epoch 5, step 600: val_loss was not in top 3
----Epoch 6, global step 700: val_loss reached 0.85772 (best 0.85772), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=6.ckpt" as top 3
----Epoch 7, step 800: val_loss was not in top 3
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | punct_classifier    | TokenClassifier      | 2.6 K 
----2 | domain_classifier   | SequenceClassifier   | 257   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----825 K     Trainable params
----12.7 M    Non-trainable params
----13.5 M    Total params
----LR finder stopped early due to diverging loss.
----Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/lr_find_temp_model.ckpt
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | punct_classifier    | TokenClassifier      | 2.6 K 
----2 | domain_classifier   | SequenceClassifier   | 257   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----825 K     Trainable params
----12.7 M    Non-trainable params
----13.5 M    Total params
----Epoch 0, global step 901: val_loss reached 0.32063 (best 0.32063), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.32-epoch=0.ckpt" as top 3
----Epoch 1, global step 1001: val_loss reached 0.32083 (best 0.32063), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.32-epoch=1.ckpt" as top 3
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | punct_classifier    | TokenClassifier      | 2.6 K 
----2 | domain_classifier   | SequenceClassifier   | 257   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----1.6 M     Trainable params
----11.9 M    Non-trainable params
----13.5 M    Total params
----LR finder stopped early due to diverging loss.
----Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/lr_find_temp_model.ckpt
----Failed to compute suggesting for `lr`. There might not be enough points.
----Traceback (most recent call last):
----  File "/home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 353, in suggestion
----    min_grad = np.gradient(loss).argmin()
----  File "<__array_function__ internals>", line 5, in gradient
----  File "/home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
----    raise ValueError(
----ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | punct_classifier    | TokenClassifier      | 2.6 K 
----2 | domain_classifier   | SequenceClassifier   | 257   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----1.6 M     Trainable params
----11.9 M    Non-trainable params
----13.5 M    Total params
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/nemo_error_log.txt
---deleted file mode 100644
---index e55ddc0..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/nemo_error_log.txt
---+++ /dev/null
---@@ -1,39 +0,0 @@
----[NeMo W 2021-02-06 10:29:26 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 10:29:26 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-06 10:29:37 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 10:29:37 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 10:29:41 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 10:29:50 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtools/optim/radam.py:62: UserWarning: This overload of addcmul_ is deprecated:
----    	addcmul_(Number value, Tensor tensor1, Tensor tensor2)
----    Consider using one of the following signatures instead:
----    	addcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/torch/csrc/utils/python_arg_parser.cpp:882.)
----      exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
----    
----[NeMo W 2021-02-06 10:49:14 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7efe5910e490> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-06 10:49:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7efe5910eeb0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-06 12:19:30 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 12:45:19 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
----      warnings.warn(*args, **kwargs)
----    
----Failed to compute suggesting for `lr`. There might not be enough points.
----Traceback (most recent call last):
----  File "/home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 353, in suggestion
----    min_grad = np.gradient(loss).argmin()
----  File "<__array_function__ internals>", line 5, in gradient
----  File "/home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
----    raise ValueError(
----ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index 21c5e73..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,33 +0,0 @@
----[NeMo W 2021-02-06 10:29:26 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo I 2021-02-06 10:29:26 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_10-29-26
----[NeMo I 2021-02-06 10:29:26 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-06 10:29:26 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-06 10:29:37 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 10:29:37 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 10:29:41 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 10:29:50 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtools/optim/radam.py:62: UserWarning: This overload of addcmul_ is deprecated:
----    	addcmul_(Number value, Tensor tensor1, Tensor tensor2)
----    Consider using one of the following signatures instead:
----    	addcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/torch/csrc/utils/python_arg_parser.cpp:882.)
----      exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
----    
----[NeMo W 2021-02-06 10:49:14 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7efe5910e490> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-06 10:49:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7efe5910eeb0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-06 12:19:30 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 12:45:19 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
----      warnings.warn(*args, **kwargs)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/events.out.tfevents.1612587358.intern-instance.11054.0 b/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/events.out.tfevents.1612587358.intern-instance.11054.0
---deleted file mode 100644
---index 53c6ce5..0000000
---Binary files a/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/events.out.tfevents.1612587358.intern-instance.11054.0 and /dev/null differ
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/git-info.log
---deleted file mode 100644
---index 11d90e2..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/git-info.log
---+++ /dev/null
---@@ -1,692 +0,0 @@
----commit hash: cce540d4c95ad9ed32010552a7aa1fb2486658c8
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index 3d27dd7..a7f721a 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,43 +1,43 @@
---- seed: 42
---- trainer:
-----    gpus: 1 # the number of gpus, 0 for CPU
-----    num_nodes: 1
-----    max_epochs: 2
-----    max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 4 # accumulates grads every k batches
-----    gradient_clip_val: 0
-----    amp_level: O1 # O1/O2 for mixed precision
-----    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    accelerator: ddp
-----    checkpoint_callback: false  # Provided by exp_manager
-----    logger: false #false  # Provided by exp_manager
-----    log_every_n_steps: 1  # Interval of logging.
-----    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    resume_from_checkpoint: null
-----
-----    # gpus: 0 # the number of gpus, 0 for CPU
----+    # gpus: 1 # the number of gpus, 0 for CPU
----     # num_nodes: 1
-----    # max_epochs: 10
----+    # max_epochs: 2
----     # max_steps: null # precedence over max_epochs
----     # accumulate_grad_batches: 4 # accumulates grads every k batches
----     # gradient_clip_val: 0
-----    # amp_level: O0 # O1/O2 for mixed precision
-----    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # # accelerator: ddp
----+    # amp_level: O1 # O1/O2 for mixed precision
----+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----     # checkpoint_callback: false  # Provided by exp_manager
----     # logger: false #false  # Provided by exp_manager
----     # log_every_n_steps: 1  # Interval of logging.
----     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # reload_dataloaders_every_epoch: true
----     # resume_from_checkpoint: null
---- 
----+    gpus: 0 # the number of gpus, 0 for CPU
----+    num_nodes: 1
----+    max_epochs: 8
----+    max_steps: null # precedence over max_epochs
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----+    gradient_clip_val: 0
----+    amp_level: O0 # O1/O2 for mixed precision
----+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----+    checkpoint_callback: false  # Provided by exp_manager
----+    logger: false #false  # Provided by exp_manager
----+    log_every_n_steps: 1  # Interval of logging.
----+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    reload_dataloaders_every_epoch: true
----+    resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /root/data # /home/nxingyu2/data # 
-----tmp_path: /tmp # /home/nxingyu2/data/tmp # 
----+base_path: /home/nxingyu2/data # /root/data # 
----+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
----@@ -61,7 +61,7 @@ model:
----     punct_class_weights: true
----     
----     dataset:
-----        data_dir: /root/data # /home/nxingyu2/data # 
----+        data_dir: /home/nxingyu2/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -125,27 +125,27 @@ model:
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 4
----+        alpha: 3
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: adamw
----+        name: ranger
----         lr: 1e-3
----         weight_decay: 0.00
-----
----         sched:
----             name: WarmupAnnealing #CyclicLR
----             # Scheduler params
----             warmup_steps: null
----             warmup_ratio: 0.1
----+            # hold_steps: 6
----             last_epoch: -1
---- 
----             # pytorch lightning args
----             monitor: val_loss
-----            reduce_on_plateau: true
----+            reduce_on_plateau: false
---- hydra:
----     run:
----         dir: .
----diff --git a/experiment/core/optim/lr_scheduler.py b/experiment/core/optim/lr_scheduler.py
----index a17ebf8..0c7d449 100644
------- a/experiment/core/optim/lr_scheduler.py
----+++ b/experiment/core/optim/lr_scheduler.py
----@@ -347,6 +347,29 @@ class PolynomialHoldDecayAnnealing(WarmupHoldPolicy):
----         return new_lrs
---- 
---- 
----+class CosineHoldDecayAnnealing(WarmupHoldPolicy):
----+    def __init__(self, optimizer, *, max_steps, min_lr=0.0, last_epoch=-1, **kwargs):
----+        super().__init__(optimizer=optimizer, max_steps=max_steps, last_epoch=last_epoch, min_lr=min_lr, **kwargs)
----+
----+    def _get_lr(self, step):
----+        for initial_lr in self.base_lrs:
----+            if initial_lr < self.min_lr:
----+                raise ValueError(
----+                    f"{self} received an initial learning rate that was lower than the minimum learning rate."
----+                )
----+
----+        new_lrs = [
----+            _cosine_annealing(
----+                initial_lr=initial_lr,
----+                step=step - self.hold_steps,
----+                max_steps=self.max_steps - max(self.warmup_steps, self.hold_steps),
----+                min_lr=self.min_lr,
----+            )
----+            for initial_lr in self.base_lrs
----+        ]
----+        return new_lrs
----+
----+
---- def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: SchedulerParams):
----     """
----     Checks if the scheduler name exists in the registry, and if it doesnt, adds it.
----@@ -654,4 +677,5 @@ AVAILABLE_SCHEDULERS = {
----     'ExponentialLR': pt_scheduler.ExponentialLR,
----     'ReduceLROnPlateau': pt_scheduler.ReduceLROnPlateau,
----     'CyclicLR': pt_scheduler.CyclicLR,
----+    'CosineHoldDecayAnnealing': CosineHoldDecayAnnealing,
---- }
----diff --git a/experiment/core/optim/optimizers.py b/experiment/core/optim/optimizers.py
----index 5afc4fd..f15686e 100644
------- a/experiment/core/optim/optimizers.py
----+++ b/experiment/core/optim/optimizers.py
----@@ -7,6 +7,7 @@ import torch.optim as optim
---- from omegaconf import DictConfig, OmegaConf
---- from torch.optim import adadelta, adagrad, adamax, rmsprop, rprop
---- from torch.optim.optimizer import Optimizer
----+from torchtools.optim import Ranger
---- 
---- from core.config import OptimizerParams, get_optimizer_config, register_optimizer_params
---- from core.optim.novograd import Novograd
----@@ -24,6 +25,7 @@ AVAILABLE_OPTIMIZERS = {
----     'rmsprop': rmsprop.RMSprop,
----     'rprop': rprop.Rprop,
----     'novograd': Novograd,
----+    'ranger': Ranger,
---- }
---- 
---- 
----diff --git a/experiment/info.log b/experiment/info.log
----index b211d42..5024c4f 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,473 +1,2 @@
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4da7f8e0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.008413951416451957
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e67aa00>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          22.42      11.12      14.86       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                         38.62       7.46      12.50      19530
------ (label_id: 3)                                          6.60      15.75       9.30       1746
-----. (label_id: 4)                                         33.33       0.01       0.01      17976
-----: (label_id: 5)                                          0.79      19.15       1.52        376
-----; (label_id: 6)                                          0.45       2.94       0.78        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.17      50.56       0.33         89
------------------------
-----micro avg                                                5.14       5.14       5.14      48233
-----macro avg                                               10.24      10.70       3.93      48233
-----weighted avg                                            30.92       5.14       7.15      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          22.09      11.27      14.93       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                         35.39       6.74      11.32      19813
------ (label_id: 3)                                          6.03      16.28       8.79       1708
-----. (label_id: 4)                                        100.00       0.01       0.02      18084
-----: (label_id: 5)                                          0.78      20.69       1.51        348
-----; (label_id: 6)                                          0.45       2.55       0.77        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.19      58.62       0.38         87
------------------------
-----micro avg                                                4.89       4.89       4.89      48633
-----macro avg                                               16.49      11.62       3.77      48633
-----weighted avg                                            54.38       4.89       6.67      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.008413951416451957
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e74d910>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          19.61      14.42      16.62        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         38.84       6.82      11.60        689
------ (label_id: 3)                                          4.50      15.52       6.98         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.59      20.00       1.14         10
-----; (label_id: 6)                                          2.56      16.67       4.44          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.25      50.00       0.51          4
------------------------
-----micro avg                                                5.55       5.55       5.55       1641
-----macro avg                                                6.64      12.34       4.13       1641
-----weighted avg                                            18.97       5.55       7.25       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.63      98.91       7.00       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          3.96       1.76       2.44       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.63       3.63       3.63      48233
-----macro avg                                                0.76      10.07       0.94      48233
-----weighted avg                                             0.25       3.63       0.33      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e68b8b0>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          19.61      14.42      16.62        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         38.84       6.82      11.60        689
------ (label_id: 3)                                          4.50      15.52       6.98         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.59      20.00       1.14         10
-----; (label_id: 6)                                          2.56      16.67       4.44          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.25      50.00       0.51          4
------------------------
-----micro avg                                                5.55       5.55       5.55       1641
-----macro avg                                                6.64      12.34       4.13       1641
-----weighted avg                                            18.97       5.55       7.25       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                          0.00       0.00       0.00      19813
------ (label_id: 3)                                          3.51     100.00       6.79       1708
-----. (label_id: 4)                                          0.00       0.00       0.00      18084
-----: (label_id: 5)                                          0.00       0.00       0.00        348
-----; (label_id: 6)                                          0.00       0.00       0.00        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.00       0.00       0.00         87
------------------------
-----micro avg                                                3.51       3.51       3.51      48633
-----macro avg                                                0.35      10.00       0.68      48633
-----weighted avg                                             0.12       3.51       0.24      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75eaf0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                          0.00       0.00       0.00        689
------ (label_id: 3)                                          3.53     100.00       6.83         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.00       0.00       0.00         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                3.53       3.53       3.53       1641
-----macro avg                                                0.35      10.00       0.68       1641
-----weighted avg                                             0.12       3.53       0.24       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75e160>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                          0.00       0.00       0.00        689
------ (label_id: 3)                                          3.53     100.00       6.83         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.00       0.00       0.00         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                3.53       3.53       3.53       1641
-----macro avg                                                0.35      10.00       0.68       1641
-----weighted avg                                             0.12       3.53       0.24       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                          0.00       0.00       0.00      19813
------ (label_id: 3)                                          3.51     100.00       6.79       1708
-----. (label_id: 4)                                          0.00       0.00       0.00      18084
-----: (label_id: 5)                                          0.00       0.00       0.00        348
-----; (label_id: 6)                                          0.00       0.00       0.00        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.00       0.00       0.00         87
------------------------
-----micro avg                                                3.51       3.51       3.51      48633
-----macro avg                                                0.35      10.00       0.68      48633
-----weighted avg                                             0.12       3.51       0.24      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       7470
-----! (label_id: 1)                                          0.00       0.00       0.00        148
-----, (label_id: 2)                                          0.00       0.00       0.00      28513
------ (label_id: 3)                                          3.02     100.00       5.86       2074
-----. (label_id: 4)                                          0.00       0.00       0.00      25120
-----: (label_id: 5)                                          0.00       0.00       0.00        570
-----; (label_id: 6)                                          0.00       0.00       0.00        534
-----? (label_id: 7)                                          0.00       0.00       0.00       2085
-----— (label_id: 8)                                          0.00       0.00       0.00       2073
-----… (label_id: 9)                                          0.00       0.00       0.00        142
------------------------
-----micro avg                                                3.02       3.02       3.02      68729
-----macro avg                                                0.30      10.00       0.59      68729
-----weighted avg                                             0.09       3.02       0.18      68729
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       3735
------------------------
-----micro avg                                              100.00     100.00     100.00       3735
-----macro avg                                              100.00     100.00     100.00       3735
-----weighted avg                                           100.00     100.00     100.00       3735
-----
-----[INFO] - Internal process exited
----+[INFO] - GPU available: True, used: False
----+[INFO] - TPU available: None, using: 0 TPU cores
----diff --git a/experiment/main.py b/experiment/main.py
----index cd4ca27..6f0a8ea 100644
------- a/experiment/main.py
----+++ b/experiment/main.py
----@@ -39,7 +39,7 @@ def main(cfg: DictConfig)->None:
----     
----     while(model.hparams.model.unfrozen<=cfg.model.maximum_unfrozen and model.hparams.model.unfrozen>=0):
----         trainer.current_epoch=0
-----        lr_finder = trainer.tuner.lr_find(model,min_lr=1e-08, max_lr=1e-02, num_training=80, early_stop_threshold=None)
----+        lr_finder = trainer.tuner.lr_find(model,min_lr=1e-10, max_lr=1e-02, num_training=80, early_stop_threshold=None)
----         # Results can be found in
----         pp(lr_finder.results)
----         new_lr = lr_finder.suggestion()
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index 782fbfa..fa37b4c 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -374,6 +374,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             scheduler_config = None
---- 
----+
----         # Check if caller provided optimizer name, default to Adam otherwise
----         optimizer_cls = optim_config.get('_target_', None)
---- 
----diff --git a/linuxcommands.txt b/linuxcommands.txt
----index e72bad1..adc1fb0 100644
------- a/linuxcommands.txt
----+++ b/linuxcommands.txt
----@@ -48,7 +48,7 @@ git clone git@github.com:ngxingyu/dotfiles.git
---- echo 'alias nv="~/nvim.appimage"' >> ~/.bashrc
---- source ~/.profile
---- 
-----
----+ls -b | head -30 | xargs ls -d
---- 
---- 
---- conda install script
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/hparams.yaml
---deleted file mode 100644
---index 75d484c..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/hparams.yaml
---+++ /dev/null
---@@ -1,107 +0,0 @@
----seed: 42
----trainer:
----  gpus: 0
----  num_nodes: 1
----  max_epochs: 8
----  max_steps: null
----  accumulate_grad_batches: 4
----  gradient_clip_val: 0
----  amp_level: O0
----  precision: 32
----  checkpoint_callback: false
----  logger: false
----  log_every_n_steps: 1
----  val_check_interval: 1.0
----  reload_dataloaders_every_epoch: true
----  resume_from_checkpoint: null
----exp_manager:
----  exp_dir: /home/nxingyu2/project/
----  name: Punctuation_with_Domain_discriminator
----  create_tensorboard_logger: true
----  create_checkpoint_callback: true
----base_path: /home/nxingyu2/data
----tmp_path: /home/nxingyu2/data/tmp
----model:
----  nemo_path: null
----  transformer_path: google/electra-small-discriminator
----  unfrozen: 0
----  maximum_unfrozen: 2
----  unfreeze_step: 1
----  punct_label_ids:
----  - ''
----  - '!'
----  - ','
----  - '-'
----  - .
----  - ':'
----  - ;
----  - '?'
----  - —
----  - …
----  punct_class_weights: true
----  dataset:
----    data_dir: /home/nxingyu2/data
----    labelled:
----    - /home/nxingyu2/data/ted_talks_processed
----    unlabelled: null
----    max_seq_length: 128
----    pad_label: ''
----    ignore_extra_tokens: false
----    ignore_start_end: false
----    use_cache: false
----    num_workers: 0
----    pin_memory: true
----    drop_last: false
----    num_labels: 10
----    num_domains: 1
----    test_unlabelled: true
----    train_ds:
----      shuffle: true
----      num_samples: -1
----      batch_size: 8
----    validation_ds:
----      shuffle: true
----      num_samples: -1
----      batch_size: 8
----  tokenizer:
----    tokenizer_name: google/electra-small-discriminator
----    vocab_file: null
----    tokenizer_model: null
----    special_tokens: null
----  language_model:
----    pretrained_model_name: google/electra-small-discriminator
----    lm_checkpoint: null
----    config_file: null
----    config: null
----  punct_head:
----    punct_num_fc_layers: 1
----    fc_dropout: 0.1
----    activation: relu
----    log_softmax: false
----    use_transformer_init: true
----    loss: dice
----  domain_head:
----    domain_num_fc_layers: 1
----    fc_dropout: 0.1
----    activation: relu
----    log_softmax: false
----    use_transformer_init: true
----    loss: cel
----    gamma: 0.1
----  dice_loss:
----    epsilon: 0.01
----    alpha: 3
----    macro_average: true
----  focal_loss:
----    gamma: 5
----  optim:
----    name: ranger
----    lr: 0.001
----    weight_decay: 0.0
----    sched:
----      name: WarmupAnnealing
----      warmup_steps: null
----      warmup_ratio: 0.1
----      last_epoch: -1
----      monitor: val_loss
----      reduce_on_plateau: false
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/lightning_logs.txt
---deleted file mode 100644
---index 9e41c86..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/lightning_logs.txt
---+++ /dev/null
---@@ -1,79 +0,0 @@
----GPU available: True, used: False
----TPU available: None, using: 0 TPU cores
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | punct_classifier    | TokenClassifier      | 2.6 K 
----2 | domain_classifier   | SequenceClassifier   | 257   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----35.9 K    Trainable params
----13.4 M    Non-trainable params
----13.5 M    Total params
----Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/lr_find_temp_model.ckpt
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | punct_classifier    | TokenClassifier      | 2.6 K 
----2 | domain_classifier   | SequenceClassifier   | 257   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----35.9 K    Trainable params
----13.4 M    Non-trainable params
----13.5 M    Total params
----Epoch 0, global step 100: val_loss reached 0.85947 (best 0.85947), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
----Epoch 1, global step 200: val_loss reached 0.85987 (best 0.85947), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=1.ckpt" as top 3
----Epoch 2, global step 300: val_loss reached 0.86473 (best 0.85947), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
----Epoch 3, step 400: val_loss was not in top 3
----Epoch 4, global step 500: val_loss reached 0.86166 (best 0.85947), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
----Epoch 5, step 600: val_loss was not in top 3
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | punct_classifier    | TokenClassifier      | 2.6 K 
----2 | domain_classifier   | SequenceClassifier   | 257   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----825 K     Trainable params
----12.7 M    Non-trainable params
----13.5 M    Total params
----LR finder stopped early due to diverging loss.
----Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/lr_find_temp_model.ckpt
----Failed to compute suggesting for `lr`. There might not be enough points.
----Traceback (most recent call last):
----  File "/home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 353, in suggestion
----    min_grad = np.gradient(loss).argmin()
----  File "<__array_function__ internals>", line 5, in gradient
----  File "/home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
----    raise ValueError(
----ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | punct_classifier    | TokenClassifier      | 2.6 K 
----2 | domain_classifier   | SequenceClassifier   | 257   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----825 K     Trainable params
----12.7 M    Non-trainable params
----13.5 M    Total params
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/nemo_error_log.txt
---deleted file mode 100644
---index dc28782..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/nemo_error_log.txt
---+++ /dev/null
---@@ -1,39 +0,0 @@
----[NeMo W 2021-02-06 12:47:20 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 12:47:20 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-06 12:47:30 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 12:47:30 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 12:47:35 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 12:47:43 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtools/optim/radam.py:62: UserWarning: This overload of addcmul_ is deprecated:
----    	addcmul_(Number value, Tensor tensor1, Tensor tensor2)
----    Consider using one of the following signatures instead:
----    	addcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/torch/csrc/utils/python_arg_parser.cpp:882.)
----      exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
----    
----[NeMo W 2021-02-06 13:06:22 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f82e4ea6af0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-06 13:06:50 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f82e4f64af0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-06 14:00:15 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 14:00:27 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
----      warnings.warn(*args, **kwargs)
----    
----Failed to compute suggesting for `lr`. There might not be enough points.
----Traceback (most recent call last):
----  File "/home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 353, in suggestion
----    min_grad = np.gradient(loss).argmin()
----  File "<__array_function__ internals>", line 5, in gradient
----  File "/home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
----    raise ValueError(
----ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index cdb87fd..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,33 +0,0 @@
----[NeMo W 2021-02-06 12:47:20 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo I 2021-02-06 12:47:20 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_12-47-20
----[NeMo I 2021-02-06 12:47:20 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-06 12:47:20 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-06 12:47:30 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 12:47:30 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 12:47:35 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 12:47:43 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtools/optim/radam.py:62: UserWarning: This overload of addcmul_ is deprecated:
----    	addcmul_(Number value, Tensor tensor1, Tensor tensor2)
----    Consider using one of the following signatures instead:
----    	addcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/torch/csrc/utils/python_arg_parser.cpp:882.)
----      exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
----    
----[NeMo W 2021-02-06 13:06:22 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f82e4ea6af0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-06 13:06:50 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f82e4f64af0> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-06 14:00:15 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 14:00:27 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
----      warnings.warn(*args, **kwargs)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/git-info.log
---deleted file mode 100644
---index c258a03..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/git-info.log
---+++ /dev/null
---@@ -1,697 +0,0 @@
----commit hash: cce540d4c95ad9ed32010552a7aa1fb2486658c8
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index 3d27dd7..7fe52d9 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,47 +1,47 @@
---- seed: 42
---- trainer:
-----    gpus: 1 # the number of gpus, 0 for CPU
-----    num_nodes: 1
-----    max_epochs: 2
-----    max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 4 # accumulates grads every k batches
-----    gradient_clip_val: 0
-----    amp_level: O1 # O1/O2 for mixed precision
-----    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    accelerator: ddp
-----    checkpoint_callback: false  # Provided by exp_manager
-----    logger: false #false  # Provided by exp_manager
-----    log_every_n_steps: 1  # Interval of logging.
-----    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    resume_from_checkpoint: null
-----
-----    # gpus: 0 # the number of gpus, 0 for CPU
----+    # gpus: 1 # the number of gpus, 0 for CPU
----     # num_nodes: 1
-----    # max_epochs: 10
----+    # max_epochs: 2
----     # max_steps: null # precedence over max_epochs
----     # accumulate_grad_batches: 4 # accumulates grads every k batches
----     # gradient_clip_val: 0
-----    # amp_level: O0 # O1/O2 for mixed precision
-----    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # # accelerator: ddp
----+    # amp_level: O1 # O1/O2 for mixed precision
----+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----     # checkpoint_callback: false  # Provided by exp_manager
----     # logger: false #false  # Provided by exp_manager
----     # log_every_n_steps: 1  # Interval of logging.
----     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # reload_dataloaders_every_epoch: true
----     # resume_from_checkpoint: null
---- 
----+    gpus: 0 # the number of gpus, 0 for CPU
----+    num_nodes: 1
----+    max_epochs: 8
----+    max_steps: null # precedence over max_epochs
----+    accumulate_grad_batches: 8 # accumulates grads every k batches
----+    gradient_clip_val: 16
----+    amp_level: O0 # O1/O2 for mixed precision
----+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----+    checkpoint_callback: false  # Provided by exp_manager
----+    logger: false #false  # Provided by exp_manager
----+    log_every_n_steps: 1  # Interval of logging.
----+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    reload_dataloaders_every_epoch: true
----+    resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /root/data # /home/nxingyu2/data # 
-----tmp_path: /tmp # /home/nxingyu2/data/tmp # 
----+base_path: /home/nxingyu2/data # /root/data # 
----+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
-----    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----+    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----     unfrozen: 0
----     maximum_unfrozen: 2
----     unfreeze_step: 1
----@@ -61,7 +61,7 @@ model:
----     punct_class_weights: true
----     
----     dataset:
-----        data_dir: /root/data # /home/nxingyu2/data # 
----+        data_dir: /home/nxingyu2/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -125,27 +125,27 @@ model:
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 4
----+        alpha: 3
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: adamw
----+        name: ranger
----         lr: 1e-3
----         weight_decay: 0.00
-----
----         sched:
----             name: WarmupAnnealing #CyclicLR
----             # Scheduler params
----             warmup_steps: null
----             warmup_ratio: 0.1
----+            # hold_steps: 6
----             last_epoch: -1
---- 
----             # pytorch lightning args
----             monitor: val_loss
-----            reduce_on_plateau: true
----+            reduce_on_plateau: false
---- hydra:
----     run:
----         dir: .
----diff --git a/experiment/core/optim/lr_scheduler.py b/experiment/core/optim/lr_scheduler.py
----index a17ebf8..0c7d449 100644
------- a/experiment/core/optim/lr_scheduler.py
----+++ b/experiment/core/optim/lr_scheduler.py
----@@ -347,6 +347,29 @@ class PolynomialHoldDecayAnnealing(WarmupHoldPolicy):
----         return new_lrs
---- 
---- 
----+class CosineHoldDecayAnnealing(WarmupHoldPolicy):
----+    def __init__(self, optimizer, *, max_steps, min_lr=0.0, last_epoch=-1, **kwargs):
----+        super().__init__(optimizer=optimizer, max_steps=max_steps, last_epoch=last_epoch, min_lr=min_lr, **kwargs)
----+
----+    def _get_lr(self, step):
----+        for initial_lr in self.base_lrs:
----+            if initial_lr < self.min_lr:
----+                raise ValueError(
----+                    f"{self} received an initial learning rate that was lower than the minimum learning rate."
----+                )
----+
----+        new_lrs = [
----+            _cosine_annealing(
----+                initial_lr=initial_lr,
----+                step=step - self.hold_steps,
----+                max_steps=self.max_steps - max(self.warmup_steps, self.hold_steps),
----+                min_lr=self.min_lr,
----+            )
----+            for initial_lr in self.base_lrs
----+        ]
----+        return new_lrs
----+
----+
---- def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: SchedulerParams):
----     """
----     Checks if the scheduler name exists in the registry, and if it doesnt, adds it.
----@@ -654,4 +677,5 @@ AVAILABLE_SCHEDULERS = {
----     'ExponentialLR': pt_scheduler.ExponentialLR,
----     'ReduceLROnPlateau': pt_scheduler.ReduceLROnPlateau,
----     'CyclicLR': pt_scheduler.CyclicLR,
----+    'CosineHoldDecayAnnealing': CosineHoldDecayAnnealing,
---- }
----diff --git a/experiment/core/optim/optimizers.py b/experiment/core/optim/optimizers.py
----index 5afc4fd..f15686e 100644
------- a/experiment/core/optim/optimizers.py
----+++ b/experiment/core/optim/optimizers.py
----@@ -7,6 +7,7 @@ import torch.optim as optim
---- from omegaconf import DictConfig, OmegaConf
---- from torch.optim import adadelta, adagrad, adamax, rmsprop, rprop
---- from torch.optim.optimizer import Optimizer
----+from torchtools.optim import Ranger
---- 
---- from core.config import OptimizerParams, get_optimizer_config, register_optimizer_params
---- from core.optim.novograd import Novograd
----@@ -24,6 +25,7 @@ AVAILABLE_OPTIMIZERS = {
----     'rmsprop': rmsprop.RMSprop,
----     'rprop': rprop.Rprop,
----     'novograd': Novograd,
----+    'ranger': Ranger,
---- }
---- 
---- 
----diff --git a/experiment/info.log b/experiment/info.log
----index b211d42..5024c4f 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,473 +1,2 @@
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4da7f8e0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.008413951416451957
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e67aa00>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          22.42      11.12      14.86       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                         38.62       7.46      12.50      19530
------ (label_id: 3)                                          6.60      15.75       9.30       1746
-----. (label_id: 4)                                         33.33       0.01       0.01      17976
-----: (label_id: 5)                                          0.79      19.15       1.52        376
-----; (label_id: 6)                                          0.45       2.94       0.78        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.17      50.56       0.33         89
------------------------
-----micro avg                                                5.14       5.14       5.14      48233
-----macro avg                                               10.24      10.70       3.93      48233
-----weighted avg                                            30.92       5.14       7.15      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          22.09      11.27      14.93       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                         35.39       6.74      11.32      19813
------ (label_id: 3)                                          6.03      16.28       8.79       1708
-----. (label_id: 4)                                        100.00       0.01       0.02      18084
-----: (label_id: 5)                                          0.78      20.69       1.51        348
-----; (label_id: 6)                                          0.45       2.55       0.77        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.19      58.62       0.38         87
------------------------
-----micro avg                                                4.89       4.89       4.89      48633
-----macro avg                                               16.49      11.62       3.77      48633
-----weighted avg                                            54.38       4.89       6.67      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.008413951416451957
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e74d910>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          19.61      14.42      16.62        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         38.84       6.82      11.60        689
------ (label_id: 3)                                          4.50      15.52       6.98         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.59      20.00       1.14         10
-----; (label_id: 6)                                          2.56      16.67       4.44          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.25      50.00       0.51          4
------------------------
-----micro avg                                                5.55       5.55       5.55       1641
-----macro avg                                                6.64      12.34       4.13       1641
-----weighted avg                                            18.97       5.55       7.25       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.63      98.91       7.00       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          3.96       1.76       2.44       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.63       3.63       3.63      48233
-----macro avg                                                0.76      10.07       0.94      48233
-----weighted avg                                             0.25       3.63       0.33      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e68b8b0>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          19.61      14.42      16.62        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         38.84       6.82      11.60        689
------ (label_id: 3)                                          4.50      15.52       6.98         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.59      20.00       1.14         10
-----; (label_id: 6)                                          2.56      16.67       4.44          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.25      50.00       0.51          4
------------------------
-----micro avg                                                5.55       5.55       5.55       1641
-----macro avg                                                6.64      12.34       4.13       1641
-----weighted avg                                            18.97       5.55       7.25       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                          0.00       0.00       0.00      19813
------ (label_id: 3)                                          3.51     100.00       6.79       1708
-----. (label_id: 4)                                          0.00       0.00       0.00      18084
-----: (label_id: 5)                                          0.00       0.00       0.00        348
-----; (label_id: 6)                                          0.00       0.00       0.00        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.00       0.00       0.00         87
------------------------
-----micro avg                                                3.51       3.51       3.51      48633
-----macro avg                                                0.35      10.00       0.68      48633
-----weighted avg                                             0.12       3.51       0.24      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75eaf0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                          0.00       0.00       0.00        689
------ (label_id: 3)                                          3.53     100.00       6.83         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.00       0.00       0.00         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                3.53       3.53       3.53       1641
-----macro avg                                                0.35      10.00       0.68       1641
-----weighted avg                                             0.12       3.53       0.24       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75e160>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                          0.00       0.00       0.00        689
------ (label_id: 3)                                          3.53     100.00       6.83         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.00       0.00       0.00         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                3.53       3.53       3.53       1641
-----macro avg                                                0.35      10.00       0.68       1641
-----weighted avg                                             0.12       3.53       0.24       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                          0.00       0.00       0.00      19813
------ (label_id: 3)                                          3.51     100.00       6.79       1708
-----. (label_id: 4)                                          0.00       0.00       0.00      18084
-----: (label_id: 5)                                          0.00       0.00       0.00        348
-----; (label_id: 6)                                          0.00       0.00       0.00        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.00       0.00       0.00         87
------------------------
-----micro avg                                                3.51       3.51       3.51      48633
-----macro avg                                                0.35      10.00       0.68      48633
-----weighted avg                                             0.12       3.51       0.24      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       7470
-----! (label_id: 1)                                          0.00       0.00       0.00        148
-----, (label_id: 2)                                          0.00       0.00       0.00      28513
------ (label_id: 3)                                          3.02     100.00       5.86       2074
-----. (label_id: 4)                                          0.00       0.00       0.00      25120
-----: (label_id: 5)                                          0.00       0.00       0.00        570
-----; (label_id: 6)                                          0.00       0.00       0.00        534
-----? (label_id: 7)                                          0.00       0.00       0.00       2085
-----— (label_id: 8)                                          0.00       0.00       0.00       2073
-----… (label_id: 9)                                          0.00       0.00       0.00        142
------------------------
-----micro avg                                                3.02       3.02       3.02      68729
-----macro avg                                                0.30      10.00       0.59      68729
-----weighted avg                                             0.09       3.02       0.18      68729
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       3735
------------------------
-----micro avg                                              100.00     100.00     100.00       3735
-----macro avg                                              100.00     100.00     100.00       3735
-----weighted avg                                           100.00     100.00     100.00       3735
-----
-----[INFO] - Internal process exited
----+[INFO] - GPU available: True, used: False
----+[INFO] - TPU available: None, using: 0 TPU cores
----diff --git a/experiment/main.py b/experiment/main.py
----index cd4ca27..6f0a8ea 100644
------- a/experiment/main.py
----+++ b/experiment/main.py
----@@ -39,7 +39,7 @@ def main(cfg: DictConfig)->None:
----     
----     while(model.hparams.model.unfrozen<=cfg.model.maximum_unfrozen and model.hparams.model.unfrozen>=0):
----         trainer.current_epoch=0
-----        lr_finder = trainer.tuner.lr_find(model,min_lr=1e-08, max_lr=1e-02, num_training=80, early_stop_threshold=None)
----+        lr_finder = trainer.tuner.lr_find(model,min_lr=1e-10, max_lr=1e-02, num_training=80, early_stop_threshold=None)
----         # Results can be found in
----         pp(lr_finder.results)
----         new_lr = lr_finder.suggestion()
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index 782fbfa..fa37b4c 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -374,6 +374,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             scheduler_config = None
---- 
----+
----         # Check if caller provided optimizer name, default to Adam otherwise
----         optimizer_cls = optim_config.get('_target_', None)
---- 
----diff --git a/linuxcommands.txt b/linuxcommands.txt
----index e72bad1..adc1fb0 100644
------- a/linuxcommands.txt
----+++ b/linuxcommands.txt
----@@ -48,7 +48,7 @@ git clone git@github.com:ngxingyu/dotfiles.git
---- echo 'alias nv="~/nvim.appimage"' >> ~/.bashrc
---- source ~/.profile
---- 
-----
----+ls -b | head -30 | xargs ls -d
---- 
---- 
---- conda install script
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/lightning_logs.txt
---deleted file mode 100644
---index 01e1664..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/lightning_logs.txt
---+++ /dev/null
---@@ -1,2 +0,0 @@
----GPU available: True, used: False
----TPU available: None, using: 0 TPU cores
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/nemo_error_log.txt
---deleted file mode 100644
---index b0fa254..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/nemo_error_log.txt
---+++ /dev/null
---@@ -1,4 +0,0 @@
----[NeMo W 2021-02-06 14:03:12 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 14:03:12 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index b7a840f..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,6 +0,0 @@
----[NeMo W 2021-02-06 14:03:12 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo I 2021-02-06 14:03:12 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_14-03-12
----[NeMo I 2021-02-06 14:03:12 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-06 14:03:12 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/events.out.tfevents.1612594376.intern-instance.19602.0 b/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/events.out.tfevents.1612594376.intern-instance.19602.0
---deleted file mode 100644
---index 94520e4..0000000
---Binary files a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/events.out.tfevents.1612594376.intern-instance.19602.0 and /dev/null differ
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/git-info.log
---deleted file mode 100644
---index c258a03..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/git-info.log
---+++ /dev/null
---@@ -1,697 +0,0 @@
----commit hash: cce540d4c95ad9ed32010552a7aa1fb2486658c8
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index 3d27dd7..7fe52d9 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,47 +1,47 @@
---- seed: 42
---- trainer:
-----    gpus: 1 # the number of gpus, 0 for CPU
-----    num_nodes: 1
-----    max_epochs: 2
-----    max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 4 # accumulates grads every k batches
-----    gradient_clip_val: 0
-----    amp_level: O1 # O1/O2 for mixed precision
-----    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    accelerator: ddp
-----    checkpoint_callback: false  # Provided by exp_manager
-----    logger: false #false  # Provided by exp_manager
-----    log_every_n_steps: 1  # Interval of logging.
-----    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    resume_from_checkpoint: null
-----
-----    # gpus: 0 # the number of gpus, 0 for CPU
----+    # gpus: 1 # the number of gpus, 0 for CPU
----     # num_nodes: 1
-----    # max_epochs: 10
----+    # max_epochs: 2
----     # max_steps: null # precedence over max_epochs
----     # accumulate_grad_batches: 4 # accumulates grads every k batches
----     # gradient_clip_val: 0
-----    # amp_level: O0 # O1/O2 for mixed precision
-----    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # # accelerator: ddp
----+    # amp_level: O1 # O1/O2 for mixed precision
----+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----     # checkpoint_callback: false  # Provided by exp_manager
----     # logger: false #false  # Provided by exp_manager
----     # log_every_n_steps: 1  # Interval of logging.
----     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # reload_dataloaders_every_epoch: true
----     # resume_from_checkpoint: null
---- 
----+    gpus: 0 # the number of gpus, 0 for CPU
----+    num_nodes: 1
----+    max_epochs: 8
----+    max_steps: null # precedence over max_epochs
----+    accumulate_grad_batches: 8 # accumulates grads every k batches
----+    gradient_clip_val: 16
----+    amp_level: O0 # O1/O2 for mixed precision
----+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----+    checkpoint_callback: false  # Provided by exp_manager
----+    logger: false #false  # Provided by exp_manager
----+    log_every_n_steps: 1  # Interval of logging.
----+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    reload_dataloaders_every_epoch: true
----+    resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /root/data # /home/nxingyu2/data # 
-----tmp_path: /tmp # /home/nxingyu2/data/tmp # 
----+base_path: /home/nxingyu2/data # /root/data # 
----+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
-----    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----+    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----     unfrozen: 0
----     maximum_unfrozen: 2
----     unfreeze_step: 1
----@@ -61,7 +61,7 @@ model:
----     punct_class_weights: true
----     
----     dataset:
-----        data_dir: /root/data # /home/nxingyu2/data # 
----+        data_dir: /home/nxingyu2/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -125,27 +125,27 @@ model:
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 4
----+        alpha: 3
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: adamw
----+        name: ranger
----         lr: 1e-3
----         weight_decay: 0.00
-----
----         sched:
----             name: WarmupAnnealing #CyclicLR
----             # Scheduler params
----             warmup_steps: null
----             warmup_ratio: 0.1
----+            # hold_steps: 6
----             last_epoch: -1
---- 
----             # pytorch lightning args
----             monitor: val_loss
-----            reduce_on_plateau: true
----+            reduce_on_plateau: false
---- hydra:
----     run:
----         dir: .
----diff --git a/experiment/core/optim/lr_scheduler.py b/experiment/core/optim/lr_scheduler.py
----index a17ebf8..0c7d449 100644
------- a/experiment/core/optim/lr_scheduler.py
----+++ b/experiment/core/optim/lr_scheduler.py
----@@ -347,6 +347,29 @@ class PolynomialHoldDecayAnnealing(WarmupHoldPolicy):
----         return new_lrs
---- 
---- 
----+class CosineHoldDecayAnnealing(WarmupHoldPolicy):
----+    def __init__(self, optimizer, *, max_steps, min_lr=0.0, last_epoch=-1, **kwargs):
----+        super().__init__(optimizer=optimizer, max_steps=max_steps, last_epoch=last_epoch, min_lr=min_lr, **kwargs)
----+
----+    def _get_lr(self, step):
----+        for initial_lr in self.base_lrs:
----+            if initial_lr < self.min_lr:
----+                raise ValueError(
----+                    f"{self} received an initial learning rate that was lower than the minimum learning rate."
----+                )
----+
----+        new_lrs = [
----+            _cosine_annealing(
----+                initial_lr=initial_lr,
----+                step=step - self.hold_steps,
----+                max_steps=self.max_steps - max(self.warmup_steps, self.hold_steps),
----+                min_lr=self.min_lr,
----+            )
----+            for initial_lr in self.base_lrs
----+        ]
----+        return new_lrs
----+
----+
---- def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: SchedulerParams):
----     """
----     Checks if the scheduler name exists in the registry, and if it doesnt, adds it.
----@@ -654,4 +677,5 @@ AVAILABLE_SCHEDULERS = {
----     'ExponentialLR': pt_scheduler.ExponentialLR,
----     'ReduceLROnPlateau': pt_scheduler.ReduceLROnPlateau,
----     'CyclicLR': pt_scheduler.CyclicLR,
----+    'CosineHoldDecayAnnealing': CosineHoldDecayAnnealing,
---- }
----diff --git a/experiment/core/optim/optimizers.py b/experiment/core/optim/optimizers.py
----index 5afc4fd..f15686e 100644
------- a/experiment/core/optim/optimizers.py
----+++ b/experiment/core/optim/optimizers.py
----@@ -7,6 +7,7 @@ import torch.optim as optim
---- from omegaconf import DictConfig, OmegaConf
---- from torch.optim import adadelta, adagrad, adamax, rmsprop, rprop
---- from torch.optim.optimizer import Optimizer
----+from torchtools.optim import Ranger
---- 
---- from core.config import OptimizerParams, get_optimizer_config, register_optimizer_params
---- from core.optim.novograd import Novograd
----@@ -24,6 +25,7 @@ AVAILABLE_OPTIMIZERS = {
----     'rmsprop': rmsprop.RMSprop,
----     'rprop': rprop.Rprop,
----     'novograd': Novograd,
----+    'ranger': Ranger,
---- }
---- 
---- 
----diff --git a/experiment/info.log b/experiment/info.log
----index b211d42..5024c4f 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,473 +1,2 @@
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4da7f8e0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.008413951416451957
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e67aa00>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          22.42      11.12      14.86       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                         38.62       7.46      12.50      19530
------ (label_id: 3)                                          6.60      15.75       9.30       1746
-----. (label_id: 4)                                         33.33       0.01       0.01      17976
-----: (label_id: 5)                                          0.79      19.15       1.52        376
-----; (label_id: 6)                                          0.45       2.94       0.78        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.17      50.56       0.33         89
------------------------
-----micro avg                                                5.14       5.14       5.14      48233
-----macro avg                                               10.24      10.70       3.93      48233
-----weighted avg                                            30.92       5.14       7.15      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          22.09      11.27      14.93       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                         35.39       6.74      11.32      19813
------ (label_id: 3)                                          6.03      16.28       8.79       1708
-----. (label_id: 4)                                        100.00       0.01       0.02      18084
-----: (label_id: 5)                                          0.78      20.69       1.51        348
-----; (label_id: 6)                                          0.45       2.55       0.77        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.19      58.62       0.38         87
------------------------
-----micro avg                                                4.89       4.89       4.89      48633
-----macro avg                                               16.49      11.62       3.77      48633
-----weighted avg                                            54.38       4.89       6.67      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.008413951416451957
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e74d910>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          19.61      14.42      16.62        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         38.84       6.82      11.60        689
------ (label_id: 3)                                          4.50      15.52       6.98         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.59      20.00       1.14         10
-----; (label_id: 6)                                          2.56      16.67       4.44          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.25      50.00       0.51          4
------------------------
-----micro avg                                                5.55       5.55       5.55       1641
-----macro avg                                                6.64      12.34       4.13       1641
-----weighted avg                                            18.97       5.55       7.25       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.63      98.91       7.00       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          3.96       1.76       2.44       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.63       3.63       3.63      48233
-----macro avg                                                0.76      10.07       0.94      48233
-----weighted avg                                             0.25       3.63       0.33      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e68b8b0>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          19.61      14.42      16.62        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         38.84       6.82      11.60        689
------ (label_id: 3)                                          4.50      15.52       6.98         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.59      20.00       1.14         10
-----; (label_id: 6)                                          2.56      16.67       4.44          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.25      50.00       0.51          4
------------------------
-----micro avg                                                5.55       5.55       5.55       1641
-----macro avg                                                6.64      12.34       4.13       1641
-----weighted avg                                            18.97       5.55       7.25       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                          0.00       0.00       0.00      19813
------ (label_id: 3)                                          3.51     100.00       6.79       1708
-----. (label_id: 4)                                          0.00       0.00       0.00      18084
-----: (label_id: 5)                                          0.00       0.00       0.00        348
-----; (label_id: 6)                                          0.00       0.00       0.00        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.00       0.00       0.00         87
------------------------
-----micro avg                                                3.51       3.51       3.51      48633
-----macro avg                                                0.35      10.00       0.68      48633
-----weighted avg                                             0.12       3.51       0.24      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75eaf0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                          0.00       0.00       0.00        689
------ (label_id: 3)                                          3.53     100.00       6.83         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.00       0.00       0.00         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                3.53       3.53       3.53       1641
-----macro avg                                                0.35      10.00       0.68       1641
-----weighted avg                                             0.12       3.53       0.24       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75e160>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                          0.00       0.00       0.00        689
------ (label_id: 3)                                          3.53     100.00       6.83         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.00       0.00       0.00         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                3.53       3.53       3.53       1641
-----macro avg                                                0.35      10.00       0.68       1641
-----weighted avg                                             0.12       3.53       0.24       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                          0.00       0.00       0.00      19813
------ (label_id: 3)                                          3.51     100.00       6.79       1708
-----. (label_id: 4)                                          0.00       0.00       0.00      18084
-----: (label_id: 5)                                          0.00       0.00       0.00        348
-----; (label_id: 6)                                          0.00       0.00       0.00        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.00       0.00       0.00         87
------------------------
-----micro avg                                                3.51       3.51       3.51      48633
-----macro avg                                                0.35      10.00       0.68      48633
-----weighted avg                                             0.12       3.51       0.24      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       7470
-----! (label_id: 1)                                          0.00       0.00       0.00        148
-----, (label_id: 2)                                          0.00       0.00       0.00      28513
------ (label_id: 3)                                          3.02     100.00       5.86       2074
-----. (label_id: 4)                                          0.00       0.00       0.00      25120
-----: (label_id: 5)                                          0.00       0.00       0.00        570
-----; (label_id: 6)                                          0.00       0.00       0.00        534
-----? (label_id: 7)                                          0.00       0.00       0.00       2085
-----— (label_id: 8)                                          0.00       0.00       0.00       2073
-----… (label_id: 9)                                          0.00       0.00       0.00        142
------------------------
-----micro avg                                                3.02       3.02       3.02      68729
-----macro avg                                                0.30      10.00       0.59      68729
-----weighted avg                                             0.09       3.02       0.18      68729
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       3735
------------------------
-----micro avg                                              100.00     100.00     100.00       3735
-----macro avg                                              100.00     100.00     100.00       3735
-----weighted avg                                           100.00     100.00     100.00       3735
-----
-----[INFO] - Internal process exited
----+[INFO] - GPU available: True, used: False
----+[INFO] - TPU available: None, using: 0 TPU cores
----diff --git a/experiment/main.py b/experiment/main.py
----index cd4ca27..6f0a8ea 100644
------- a/experiment/main.py
----+++ b/experiment/main.py
----@@ -39,7 +39,7 @@ def main(cfg: DictConfig)->None:
----     
----     while(model.hparams.model.unfrozen<=cfg.model.maximum_unfrozen and model.hparams.model.unfrozen>=0):
----         trainer.current_epoch=0
-----        lr_finder = trainer.tuner.lr_find(model,min_lr=1e-08, max_lr=1e-02, num_training=80, early_stop_threshold=None)
----+        lr_finder = trainer.tuner.lr_find(model,min_lr=1e-10, max_lr=1e-02, num_training=80, early_stop_threshold=None)
----         # Results can be found in
----         pp(lr_finder.results)
----         new_lr = lr_finder.suggestion()
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index 782fbfa..fa37b4c 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -374,6 +374,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             scheduler_config = None
---- 
----+
----         # Check if caller provided optimizer name, default to Adam otherwise
----         optimizer_cls = optim_config.get('_target_', None)
---- 
----diff --git a/linuxcommands.txt b/linuxcommands.txt
----index e72bad1..adc1fb0 100644
------- a/linuxcommands.txt
----+++ b/linuxcommands.txt
----@@ -48,7 +48,7 @@ git clone git@github.com:ngxingyu/dotfiles.git
---- echo 'alias nv="~/nvim.appimage"' >> ~/.bashrc
---- source ~/.profile
---- 
-----
----+ls -b | head -30 | xargs ls -d
---- 
---- 
---- conda install script
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/hparams.yaml
---deleted file mode 100644
---index 5cff5a5..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/hparams.yaml
---+++ /dev/null
---@@ -1,107 +0,0 @@
----seed: 42
----trainer:
----  gpus: 0
----  num_nodes: 1
----  max_epochs: 8
----  max_steps: null
----  accumulate_grad_batches: 8
----  gradient_clip_val: 16
----  amp_level: O0
----  precision: 32
----  checkpoint_callback: false
----  logger: false
----  log_every_n_steps: 1
----  val_check_interval: 1.0
----  reload_dataloaders_every_epoch: true
----  resume_from_checkpoint: null
----exp_manager:
----  exp_dir: /home/nxingyu2/project/
----  name: Punctuation_with_Domain_discriminator
----  create_tensorboard_logger: true
----  create_checkpoint_callback: true
----base_path: /home/nxingyu2/data
----tmp_path: /home/nxingyu2/data/tmp
----model:
----  nemo_path: null
----  transformer_path: google/electra-base-discriminator
----  unfrozen: 0
----  maximum_unfrozen: 2
----  unfreeze_step: 1
----  punct_label_ids:
----  - ''
----  - '!'
----  - ','
----  - '-'
----  - .
----  - ':'
----  - ;
----  - '?'
----  - —
----  - …
----  punct_class_weights: true
----  dataset:
----    data_dir: /home/nxingyu2/data
----    labelled:
----    - /home/nxingyu2/data/ted_talks_processed
----    unlabelled: null
----    max_seq_length: 128
----    pad_label: ''
----    ignore_extra_tokens: false
----    ignore_start_end: false
----    use_cache: false
----    num_workers: 0
----    pin_memory: true
----    drop_last: false
----    num_labels: 10
----    num_domains: 1
----    test_unlabelled: true
----    train_ds:
----      shuffle: true
----      num_samples: -1
----      batch_size: 8
----    validation_ds:
----      shuffle: true
----      num_samples: -1
----      batch_size: 8
----  tokenizer:
----    tokenizer_name: google/electra-base-discriminator
----    vocab_file: null
----    tokenizer_model: null
----    special_tokens: null
----  language_model:
----    pretrained_model_name: google/electra-base-discriminator
----    lm_checkpoint: null
----    config_file: null
----    config: null
----  punct_head:
----    punct_num_fc_layers: 1
----    fc_dropout: 0.1
----    activation: relu
----    log_softmax: false
----    use_transformer_init: true
----    loss: dice
----  domain_head:
----    domain_num_fc_layers: 1
----    fc_dropout: 0.1
----    activation: relu
----    log_softmax: false
----    use_transformer_init: true
----    loss: cel
----    gamma: 0.1
----  dice_loss:
----    epsilon: 0.01
----    alpha: 3
----    macro_average: true
----  focal_loss:
----    gamma: 5
----  optim:
----    name: ranger
----    lr: 0.001
----    weight_decay: 0.0
----    sched:
----      name: WarmupAnnealing
----      warmup_steps: null
----      warmup_ratio: 0.1
----      last_epoch: -1
----      monitor: val_loss
----      reduce_on_plateau: false
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/lightning_logs.txt
---deleted file mode 100644
---index 1999b86..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/lightning_logs.txt
---+++ /dev/null
---@@ -1,37 +0,0 @@
----GPU available: True, used: False
----TPU available: None, using: 0 TPU cores
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 108 M 
----1 | punct_classifier    | TokenClassifier      | 7.7 K 
----2 | domain_classifier   | SequenceClassifier   | 769   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----8.5 K     Trainable params
----108 M     Non-trainable params
----108 M     Total params
----Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/lr_find_temp_model.ckpt
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 108 M 
----1 | punct_classifier    | TokenClassifier      | 7.7 K 
----2 | domain_classifier   | SequenceClassifier   | 769   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----8.5 K     Trainable params
----108 M     Non-trainable params
----108 M     Total params
----Epoch 0, global step 50: val_loss reached 0.86867 (best 0.86867), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=0.ckpt" as top 3
----Epoch 1, global step 100: val_loss reached 0.86855 (best 0.86855), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
----Epoch 2, global step 150: val_loss reached 0.86816 (best 0.86816), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=2.ckpt" as top 3
----Epoch 3, global step 200: val_loss reached 0.86018 (best 0.86018), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/nemo_error_log.txt
---deleted file mode 100644
---index 5d0635b..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/nemo_error_log.txt
---+++ /dev/null
---@@ -1,28 +0,0 @@
----[NeMo W 2021-02-06 14:03:25 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 14:03:25 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-06 14:03:39 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 14:03:39 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 14:03:47 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 14:04:27 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtools/optim/radam.py:62: UserWarning: This overload of addcmul_ is deprecated:
----    	addcmul_(Number value, Tensor tensor1, Tensor tensor2)
----    Consider using one of the following signatures instead:
----    	addcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/torch/csrc/utils/python_arg_parser.cpp:882.)
----      exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
----    
----[NeMo W 2021-02-06 14:33:14 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7fdb2f2bd0a0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-06 14:35:16 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7fdb2f2bdf70> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-06 17:15:18 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
----      warnings.warn(*args, **kwargs)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index 1df0208..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,30 +0,0 @@
----[NeMo W 2021-02-06 14:03:25 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo I 2021-02-06 14:03:25 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_14-03-25
----[NeMo I 2021-02-06 14:03:25 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-06 14:03:25 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-06 14:03:39 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 14:03:39 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 14:03:47 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 14:04:27 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtools/optim/radam.py:62: UserWarning: This overload of addcmul_ is deprecated:
----    	addcmul_(Number value, Tensor tensor1, Tensor tensor2)
----    Consider using one of the following signatures instead:
----    	addcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/torch/csrc/utils/python_arg_parser.cpp:882.)
----      exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
----    
----[NeMo W 2021-02-06 14:33:14 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7fdb2f2bd0a0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-06 14:35:16 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7fdb2f2bdf70> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-06 17:15:18 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
----      warnings.warn(*args, **kwargs)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/events.out.tfevents.1612603306.intern-instance.26445.0 b/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/events.out.tfevents.1612603306.intern-instance.26445.0
---deleted file mode 100644
---index 00322d6..0000000
---Binary files a/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/events.out.tfevents.1612603306.intern-instance.26445.0 and /dev/null differ
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/git-info.log
---deleted file mode 100644
---index adfc583..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/git-info.log
---+++ /dev/null
---@@ -1,697 +0,0 @@
----commit hash: cce540d4c95ad9ed32010552a7aa1fb2486658c8
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index 3d27dd7..fe58670 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,47 +1,47 @@
---- seed: 42
---- trainer:
-----    gpus: 1 # the number of gpus, 0 for CPU
-----    num_nodes: 1
-----    max_epochs: 2
-----    max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 4 # accumulates grads every k batches
-----    gradient_clip_val: 0
-----    amp_level: O1 # O1/O2 for mixed precision
-----    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    accelerator: ddp
-----    checkpoint_callback: false  # Provided by exp_manager
-----    logger: false #false  # Provided by exp_manager
-----    log_every_n_steps: 1  # Interval of logging.
-----    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    resume_from_checkpoint: null
-----
-----    # gpus: 0 # the number of gpus, 0 for CPU
----+    # gpus: 1 # the number of gpus, 0 for CPU
----     # num_nodes: 1
-----    # max_epochs: 10
----+    # max_epochs: 2
----     # max_steps: null # precedence over max_epochs
----     # accumulate_grad_batches: 4 # accumulates grads every k batches
----     # gradient_clip_val: 0
-----    # amp_level: O0 # O1/O2 for mixed precision
-----    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # # accelerator: ddp
----+    # amp_level: O1 # O1/O2 for mixed precision
----+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----     # checkpoint_callback: false  # Provided by exp_manager
----     # logger: false #false  # Provided by exp_manager
----     # log_every_n_steps: 1  # Interval of logging.
----     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # reload_dataloaders_every_epoch: true
----     # resume_from_checkpoint: null
---- 
----+    gpus: 0 # the number of gpus, 0 for CPU
----+    num_nodes: 1
----+    max_epochs: 8
----+    max_steps: null # precedence over max_epochs
----+    accumulate_grad_batches: 1 # accumulates grads every k batches
----+    gradient_clip_val: 0
----+    amp_level: O0 # O1/O2 for mixed precision
----+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # accelerator: ddp
----+    checkpoint_callback: false  # Provided by exp_manager
----+    logger: false #false  # Provided by exp_manager
----+    log_every_n_steps: 1  # Interval of logging.
----+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    reload_dataloaders_every_epoch: true
----+    resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /root/experiments # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /root/data # /home/nxingyu2/data # 
-----tmp_path: /tmp # /home/nxingyu2/data/tmp # 
----+base_path: /home/nxingyu2/data # /root/data # 
----+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
-----    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----+    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----     unfrozen: 0
----     maximum_unfrozen: 2
----     unfreeze_step: 1
----@@ -61,7 +61,7 @@ model:
----     punct_class_weights: true
----     
----     dataset:
-----        data_dir: /root/data # /home/nxingyu2/data # 
----+        data_dir: /home/nxingyu2/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -125,27 +125,27 @@ model:
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 4
----+        alpha: 3
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: adamw
----+        name: novograd
----         lr: 1e-3
----         weight_decay: 0.00
-----
----         sched:
----             name: WarmupAnnealing #CyclicLR
----             # Scheduler params
----             warmup_steps: null
----             warmup_ratio: 0.1
----+            # hold_steps: 6
----             last_epoch: -1
---- 
----             # pytorch lightning args
----             monitor: val_loss
-----            reduce_on_plateau: true
----+            reduce_on_plateau: false
---- hydra:
----     run:
----         dir: .
----diff --git a/experiment/core/optim/lr_scheduler.py b/experiment/core/optim/lr_scheduler.py
----index a17ebf8..0c7d449 100644
------- a/experiment/core/optim/lr_scheduler.py
----+++ b/experiment/core/optim/lr_scheduler.py
----@@ -347,6 +347,29 @@ class PolynomialHoldDecayAnnealing(WarmupHoldPolicy):
----         return new_lrs
---- 
---- 
----+class CosineHoldDecayAnnealing(WarmupHoldPolicy):
----+    def __init__(self, optimizer, *, max_steps, min_lr=0.0, last_epoch=-1, **kwargs):
----+        super().__init__(optimizer=optimizer, max_steps=max_steps, last_epoch=last_epoch, min_lr=min_lr, **kwargs)
----+
----+    def _get_lr(self, step):
----+        for initial_lr in self.base_lrs:
----+            if initial_lr < self.min_lr:
----+                raise ValueError(
----+                    f"{self} received an initial learning rate that was lower than the minimum learning rate."
----+                )
----+
----+        new_lrs = [
----+            _cosine_annealing(
----+                initial_lr=initial_lr,
----+                step=step - self.hold_steps,
----+                max_steps=self.max_steps - max(self.warmup_steps, self.hold_steps),
----+                min_lr=self.min_lr,
----+            )
----+            for initial_lr in self.base_lrs
----+        ]
----+        return new_lrs
----+
----+
---- def register_scheduler(name: str, scheduler: _LRScheduler, scheduler_params: SchedulerParams):
----     """
----     Checks if the scheduler name exists in the registry, and if it doesnt, adds it.
----@@ -654,4 +677,5 @@ AVAILABLE_SCHEDULERS = {
----     'ExponentialLR': pt_scheduler.ExponentialLR,
----     'ReduceLROnPlateau': pt_scheduler.ReduceLROnPlateau,
----     'CyclicLR': pt_scheduler.CyclicLR,
----+    'CosineHoldDecayAnnealing': CosineHoldDecayAnnealing,
---- }
----diff --git a/experiment/core/optim/optimizers.py b/experiment/core/optim/optimizers.py
----index 5afc4fd..f15686e 100644
------- a/experiment/core/optim/optimizers.py
----+++ b/experiment/core/optim/optimizers.py
----@@ -7,6 +7,7 @@ import torch.optim as optim
---- from omegaconf import DictConfig, OmegaConf
---- from torch.optim import adadelta, adagrad, adamax, rmsprop, rprop
---- from torch.optim.optimizer import Optimizer
----+from torchtools.optim import Ranger
---- 
---- from core.config import OptimizerParams, get_optimizer_config, register_optimizer_params
---- from core.optim.novograd import Novograd
----@@ -24,6 +25,7 @@ AVAILABLE_OPTIMIZERS = {
----     'rmsprop': rmsprop.RMSprop,
----     'rprop': rprop.Rprop,
----     'novograd': Novograd,
----+    'ranger': Ranger,
---- }
---- 
---- 
----diff --git a/experiment/info.log b/experiment/info.log
----index b211d42..5024c4f 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,473 +1,2 @@
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4da7f8e0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.008413951416451957
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e67aa00>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          24.14      10.10      14.24        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         34.00       2.47       4.60        689
------ (label_id: 3)                                          4.42       8.62       5.85         58
-----. (label_id: 4)                                         50.00       1.57       3.05        572
-----: (label_id: 5)                                          0.55      40.00       1.09         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          2.18      17.39       3.87         46
-----— (label_id: 8)                                          1.42       5.71       2.27         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                4.02       4.02       4.02       1641
-----macro avg                                               11.67       8.59       3.50       1641
-----weighted avg                                            35.01       4.02       5.17       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          22.42      11.12      14.86       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                         38.62       7.46      12.50      19530
------ (label_id: 3)                                          6.60      15.75       9.30       1746
-----. (label_id: 4)                                         33.33       0.01       0.01      17976
-----: (label_id: 5)                                          0.79      19.15       1.52        376
-----; (label_id: 6)                                          0.45       2.94       0.78        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.17      50.56       0.33         89
------------------------
-----micro avg                                                5.14       5.14       5.14      48233
-----macro avg                                               10.24      10.70       3.93      48233
-----weighted avg                                            30.92       5.14       7.15      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          22.09      11.27      14.93       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                         35.39       6.74      11.32      19813
------ (label_id: 3)                                          6.03      16.28       8.79       1708
-----. (label_id: 4)                                        100.00       0.01       0.02      18084
-----: (label_id: 5)                                          0.78      20.69       1.51        348
-----; (label_id: 6)                                          0.45       2.55       0.77        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.19      58.62       0.38         87
------------------------
-----micro avg                                                4.89       4.89       4.89      48633
-----macro avg                                               16.49      11.62       3.77      48633
-----weighted avg                                            54.38       4.89       6.67      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.008413951416451957
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e74d910>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          19.61      14.42      16.62        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         38.84       6.82      11.60        689
------ (label_id: 3)                                          4.50      15.52       6.98         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.59      20.00       1.14         10
-----; (label_id: 6)                                          2.56      16.67       4.44          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.25      50.00       0.51          4
------------------------
-----micro avg                                                5.55       5.55       5.55       1641
-----macro avg                                                6.64      12.34       4.13       1641
-----weighted avg                                            18.97       5.55       7.25       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.63      98.91       7.00       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          3.96       1.76       2.44       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.63       3.63       3.63      48233
-----macro avg                                                0.76      10.07       0.94      48233
-----weighted avg                                             0.25       3.63       0.33      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e68b8b0>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                          19.61      14.42      16.62        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                         38.84       6.82      11.60        689
------ (label_id: 3)                                          4.50      15.52       6.98         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.59      20.00       1.14         10
-----; (label_id: 6)                                          2.56      16.67       4.44          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.25      50.00       0.51          4
------------------------
-----micro avg                                                5.55       5.55       5.55       1641
-----macro avg                                                6.64      12.34       4.13       1641
-----weighted avg                                            18.97       5.55       7.25       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                          0.00       0.00       0.00      19813
------ (label_id: 3)                                          3.51     100.00       6.79       1708
-----. (label_id: 4)                                          0.00       0.00       0.00      18084
-----: (label_id: 5)                                          0.00       0.00       0.00        348
-----; (label_id: 6)                                          0.00       0.00       0.00        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.00       0.00       0.00         87
------------------------
-----micro avg                                                3.51       3.51       3.51      48633
-----macro avg                                                0.35      10.00       0.68      48633
-----weighted avg                                             0.12       3.51       0.24      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75eaf0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                          0.00       0.00       0.00        689
------ (label_id: 3)                                          3.53     100.00       6.83         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.00       0.00       0.00         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                3.53       3.53       3.53       1641
-----macro avg                                                0.35      10.00       0.68       1641
-----weighted avg                                             0.12       3.53       0.24       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Optimizer config = AdamW (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.9, 0.999)
-----    eps: 1e-08
-----    lr: 0.00031622776601683794
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fee4e75e160>" 
-----will be used during training (effective maximum steps = 200) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 200
-----)
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        208
-----! (label_id: 1)                                          0.00       0.00       0.00         13
-----, (label_id: 2)                                          0.00       0.00       0.00        689
------ (label_id: 3)                                          3.53     100.00       6.83         58
-----. (label_id: 4)                                          0.00       0.00       0.00        572
-----: (label_id: 5)                                          0.00       0.00       0.00         10
-----; (label_id: 6)                                          0.00       0.00       0.00          6
-----? (label_id: 7)                                          0.00       0.00       0.00         46
-----— (label_id: 8)                                          0.00       0.00       0.00         35
-----… (label_id: 9)                                          0.00       0.00       0.00          4
------------------------
-----micro avg                                                3.53       3.53       3.53       1641
-----macro avg                                                0.35      10.00       0.68       1641
-----weighted avg                                             0.12       3.53       0.24       1641
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        104
------------------------
-----micro avg                                              100.00     100.00     100.00        104
-----macro avg                                              100.00     100.00     100.00        104
-----weighted avg                                           100.00     100.00     100.00        104
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5614
-----! (label_id: 1)                                          0.00       0.00       0.00        108
-----, (label_id: 2)                                          0.00       0.00       0.00      19530
------ (label_id: 3)                                          3.62     100.00       6.99       1746
-----. (label_id: 4)                                          0.00       0.00       0.00      17976
-----: (label_id: 5)                                          0.00       0.00       0.00        376
-----; (label_id: 6)                                          0.00       0.00       0.00        170
-----? (label_id: 7)                                          0.00       0.00       0.00       1418
-----— (label_id: 8)                                          0.00       0.00       0.00       1206
-----… (label_id: 9)                                          0.00       0.00       0.00         89
------------------------
-----micro avg                                                3.62       3.62       3.62      48233
-----macro avg                                                0.36      10.00       0.70      48233
-----weighted avg                                             0.13       3.62       0.25      48233
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2807
------------------------
-----micro avg                                              100.00     100.00     100.00       2807
-----macro avg                                              100.00     100.00     100.00       2807
-----weighted avg                                           100.00     100.00     100.00       2807
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       5634
-----! (label_id: 1)                                          0.00       0.00       0.00        113
-----, (label_id: 2)                                          0.00       0.00       0.00      19813
------ (label_id: 3)                                          3.51     100.00       6.79       1708
-----. (label_id: 4)                                          0.00       0.00       0.00      18084
-----: (label_id: 5)                                          0.00       0.00       0.00        348
-----; (label_id: 6)                                          0.00       0.00       0.00        196
-----? (label_id: 7)                                          0.00       0.00       0.00       1392
-----— (label_id: 8)                                          0.00       0.00       0.00       1258
-----… (label_id: 9)                                          0.00       0.00       0.00         87
------------------------
-----micro avg                                                3.51       3.51       3.51      48633
-----macro avg                                                0.35      10.00       0.68      48633
-----weighted avg                                             0.12       3.51       0.24      48633
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2817
------------------------
-----micro avg                                              100.00     100.00     100.00       2817
-----macro avg                                              100.00     100.00     100.00       2817
-----weighted avg                                           100.00     100.00     100.00       2817
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00       7470
-----! (label_id: 1)                                          0.00       0.00       0.00        148
-----, (label_id: 2)                                          0.00       0.00       0.00      28513
------ (label_id: 3)                                          3.02     100.00       5.86       2074
-----. (label_id: 4)                                          0.00       0.00       0.00      25120
-----: (label_id: 5)                                          0.00       0.00       0.00        570
-----; (label_id: 6)                                          0.00       0.00       0.00        534
-----? (label_id: 7)                                          0.00       0.00       0.00       2085
-----— (label_id: 8)                                          0.00       0.00       0.00       2073
-----… (label_id: 9)                                          0.00       0.00       0.00        142
------------------------
-----micro avg                                                3.02       3.02       3.02      68729
-----macro avg                                                0.30      10.00       0.59      68729
-----weighted avg                                             0.09       3.02       0.18      68729
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       3735
------------------------
-----micro avg                                              100.00     100.00     100.00       3735
-----macro avg                                              100.00     100.00     100.00       3735
-----weighted avg                                           100.00     100.00     100.00       3735
-----
-----[INFO] - Internal process exited
----+[INFO] - GPU available: True, used: False
----+[INFO] - TPU available: None, using: 0 TPU cores
----diff --git a/experiment/main.py b/experiment/main.py
----index cd4ca27..6f0a8ea 100644
------- a/experiment/main.py
----+++ b/experiment/main.py
----@@ -39,7 +39,7 @@ def main(cfg: DictConfig)->None:
----     
----     while(model.hparams.model.unfrozen<=cfg.model.maximum_unfrozen and model.hparams.model.unfrozen>=0):
----         trainer.current_epoch=0
-----        lr_finder = trainer.tuner.lr_find(model,min_lr=1e-08, max_lr=1e-02, num_training=80, early_stop_threshold=None)
----+        lr_finder = trainer.tuner.lr_find(model,min_lr=1e-10, max_lr=1e-02, num_training=80, early_stop_threshold=None)
----         # Results can be found in
----         pp(lr_finder.results)
----         new_lr = lr_finder.suggestion()
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index 782fbfa..fa37b4c 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -374,6 +374,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             scheduler_config = None
---- 
----+
----         # Check if caller provided optimizer name, default to Adam otherwise
----         optimizer_cls = optim_config.get('_target_', None)
---- 
----diff --git a/linuxcommands.txt b/linuxcommands.txt
----index e72bad1..adc1fb0 100644
------- a/linuxcommands.txt
----+++ b/linuxcommands.txt
----@@ -48,7 +48,7 @@ git clone git@github.com:ngxingyu/dotfiles.git
---- echo 'alias nv="~/nvim.appimage"' >> ~/.bashrc
---- source ~/.profile
---- 
-----
----+ls -b | head -30 | xargs ls -d
---- 
---- 
---- conda install script
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/hparams.yaml
---deleted file mode 100644
---index af74c43..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/hparams.yaml
---+++ /dev/null
---@@ -1,107 +0,0 @@
----seed: 42
----trainer:
----  gpus: 0
----  num_nodes: 1
----  max_epochs: 8
----  max_steps: null
----  accumulate_grad_batches: 1
----  gradient_clip_val: 0
----  amp_level: O0
----  precision: 32
----  checkpoint_callback: false
----  logger: false
----  log_every_n_steps: 1
----  val_check_interval: 1.0
----  reload_dataloaders_every_epoch: true
----  resume_from_checkpoint: null
----exp_manager:
----  exp_dir: /home/nxingyu2/project/
----  name: Punctuation_with_Domain_discriminator
----  create_tensorboard_logger: true
----  create_checkpoint_callback: true
----base_path: /home/nxingyu2/data
----tmp_path: /home/nxingyu2/data/tmp
----model:
----  nemo_path: null
----  transformer_path: google/electra-base-discriminator
----  unfrozen: 0
----  maximum_unfrozen: 2
----  unfreeze_step: 1
----  punct_label_ids:
----  - ''
----  - '!'
----  - ','
----  - '-'
----  - .
----  - ':'
----  - ;
----  - '?'
----  - —
----  - …
----  punct_class_weights: true
----  dataset:
----    data_dir: /home/nxingyu2/data
----    labelled:
----    - /home/nxingyu2/data/ted_talks_processed
----    unlabelled: null
----    max_seq_length: 128
----    pad_label: ''
----    ignore_extra_tokens: false
----    ignore_start_end: false
----    use_cache: false
----    num_workers: 0
----    pin_memory: true
----    drop_last: false
----    num_labels: 10
----    num_domains: 1
----    test_unlabelled: true
----    train_ds:
----      shuffle: true
----      num_samples: -1
----      batch_size: 8
----    validation_ds:
----      shuffle: true
----      num_samples: -1
----      batch_size: 8
----  tokenizer:
----    tokenizer_name: google/electra-base-discriminator
----    vocab_file: null
----    tokenizer_model: null
----    special_tokens: null
----  language_model:
----    pretrained_model_name: google/electra-base-discriminator
----    lm_checkpoint: null
----    config_file: null
----    config: null
----  punct_head:
----    punct_num_fc_layers: 1
----    fc_dropout: 0.1
----    activation: relu
----    log_softmax: false
----    use_transformer_init: true
----    loss: dice
----  domain_head:
----    domain_num_fc_layers: 1
----    fc_dropout: 0.1
----    activation: relu
----    log_softmax: false
----    use_transformer_init: true
----    loss: cel
----    gamma: 0.1
----  dice_loss:
----    epsilon: 0.01
----    alpha: 3
----    macro_average: true
----  focal_loss:
----    gamma: 5
----  optim:
----    name: novograd
----    lr: 0.001
----    weight_decay: 0.0
----    sched:
----      name: WarmupAnnealing
----      warmup_steps: null
----      warmup_ratio: 0.1
----      last_epoch: -1
----      monitor: val_loss
----      reduce_on_plateau: false
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lightning_logs.txt
---deleted file mode 100644
---index 851eabd..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lightning_logs.txt
---+++ /dev/null
---@@ -1,40 +0,0 @@
----GPU available: True, used: False
----TPU available: None, using: 0 TPU cores
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 108 M 
----1 | punct_classifier    | TokenClassifier      | 7.7 K 
----2 | domain_classifier   | SequenceClassifier   | 769   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----8.5 K     Trainable params
----108 M     Non-trainable params
----108 M     Total params
----Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 108 M 
----1 | punct_classifier    | TokenClassifier      | 7.7 K 
----2 | domain_classifier   | SequenceClassifier   | 769   
----3 | punctuation_loss    | FocalDiceLoss        | 0     
----4 | domain_loss         | CrossEntropyLoss     | 0     
----5 | agg_loss            | AggregatorLoss       | 0     
----6 | punct_class_report  | ClassificationReport | 0     
----7 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----8.5 K     Trainable params
----108 M     Non-trainable params
----108 M     Total params
----Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
----Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
----Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
----Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
----Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
----Epoch 5, step 2394: val_loss was not in top 3
----Epoch 6, step 2793: val_loss was not in top 3
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/nemo_error_log.txt
---deleted file mode 100644
---index 1adb293..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/nemo_error_log.txt
---+++ /dev/null
---@@ -1,19 +0,0 @@
----[NeMo W 2021-02-06 17:15:23 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 17:15:23 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-06 17:15:37 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 17:15:37 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 17:15:45 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 17:50:54 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f224f0c1d90> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-06 17:53:06 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f224f0c1a90> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index 963f28e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,21 +0,0 @@
----[NeMo W 2021-02-06 17:15:23 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo I 2021-02-06 17:15:23 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23
----[NeMo I 2021-02-06 17:15:23 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-06 17:15:23 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-06 17:15:37 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 17:15:37 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 17:15:45 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-06 17:50:54 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f224f0c1d90> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-06 17:53:06 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f224f0c1a90> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/git-info.log
---deleted file mode 100644
---index 44fff95..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/git-info.log
---+++ /dev/null
---@@ -1,623 +0,0 @@
----commit hash: 939a671c8c117db6975316767ced5d95449e2b27
----diff --git a/README.md b/README.md
----index d52dba5..0b403d3 100644
------- a/README.md
----+++ b/README.md
----@@ -333,4 +333,71 @@ label                                                precision    recall       f
----  'punct_f1': 0.5858508944511414,
----  'punct_precision': 0.30176490545272827,
----  'punct_recall': 10.0,
----- 'test_loss': 0.8140875697135925}
----\ No newline at end of file
----+ 'test_loss': 0.8140875697135925}
----+
----+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
----+
----+ (label_id: 0)                                          62.15     100.00      76.66       5154
----+! (label_id: 1)                                          0.00       0.00       0.00        108
----+, (label_id: 2)                                          0.00       0.00       0.00      18022
----+- (label_id: 3)                                          0.00       0.00       0.00       1557
----+. (label_id: 4)                                         41.74      94.01      57.81      15164
----+: (label_id: 5)                                          0.00       0.00       0.00        319
----+; (label_id: 6)                                          0.00       0.00       0.00         88
----+? (label_id: 7)                                          0.00       0.00       0.00       1217
----+ (label_id: 8)                                          0.00       0.00       0.00        752
----+… (label_id: 9)                                          0.00       0.00       0.00         67
----+-------------------
----+micro avg                                               45.72      45.72      45.72      42448
----+macro avg                                               10.39      19.40      13.45      42448
----+weighted avg                                            22.46      45.72      29.96      42448
----+{ 'punct_f1': 13.446383476257324,
----+ 'punct_precision': 10.388500213623047,
----+ 'punct_recall': 19.400554656982422,
----+ 'test_loss': 0.44148480892181396}
----+
----+
----+ ## Log for 8/2/2021
----+
----+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
----+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
----+
----+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
----+End frozen 
----+micro avg                                               41.42      41.42      41.42      33406
----+macro avg                                               11.01      13.54      11.07      33406
----+weighted avg                                            34.88      41.42      34.20      33406
----+
----+1st layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               36.65      36.65      36.65      33463
----+macro avg                                               10.71       9.91       8.26      33463
----+weighted avg                                            34.32      36.65      31.08      33463
----+
----+2nd layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               35.72      35.72      35.72      42448
----+macro avg                                                3.57      10.00       5.26      42448
----+weighted avg                                            12.76      35.72      18.81      42448
----+
----+{'punct_f1': 5.264181137084961,
----+ 'punct_precision': 3.572371006011963,
----+ 'punct_recall': 10.0,
----+ 'test_loss': 18.49854850769043}
----+
----+
----+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
----+alpha from 3->4 seems to reduce convergence rate.
----+
----+micro avg                                               50.98      50.98      50.98      33463
----+macro avg                                               25.99      25.38      23.38      33463
----+weighted avg                                            50.31      50.98      48.27      33463
----+
----+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
----+micro avg                                               58.55      58.55      58.55      39340
----+macro avg                                               30.02      29.74      29.52      39340
----+weighted avg                                            57.91      58.55      57.51      39340
----+
----+still increasing?!
----+{'punct_f1': 29.523975372314453,
----+ 'punct_precision': 30.015613555908203,
----+ 'punct_recall': 29.738296508789062,
----+ 'test_loss': 0.3690211772918701}
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index fe58670..872a5ed 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,49 +1,49 @@
---- seed: 42
---- trainer:
-----    # gpus: 1 # the number of gpus, 0 for CPU
-----    # num_nodes: 1
-----    # max_epochs: 2
-----    # max_steps: null # precedence over max_epochs
-----    # accumulate_grad_batches: 4 # accumulates grads every k batches
-----    # gradient_clip_val: 0
-----    # amp_level: O1 # O1/O2 for mixed precision
-----    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
-----    # checkpoint_callback: false  # Provided by exp_manager
-----    # logger: false #false  # Provided by exp_manager
-----    # log_every_n_steps: 1  # Interval of logging.
-----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # resume_from_checkpoint: null
-----
-----    gpus: 0 # the number of gpus, 0 for CPU
----+    gpus: 1 # the number of gpus, 0 for CPU
----     num_nodes: 1
-----    max_epochs: 8
----+    max_epochs: 10
----     max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 1 # accumulates grads every k batches
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----     gradient_clip_val: 0
-----    amp_level: O0 # O1/O2 for mixed precision
-----    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
----+    amp_level: O1 # O1/O2 for mixed precision
----+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    accelerator: ddp
----     checkpoint_callback: false  # Provided by exp_manager
----     logger: false #false  # Provided by exp_manager
----     log_every_n_steps: 1  # Interval of logging.
----     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    reload_dataloaders_every_epoch: true
----     resume_from_checkpoint: null
---- 
----+    # gpus: 0 # the number of gpus, 0 for CPU
----+    # num_nodes: 1
----+    # max_epochs: 8
----+    # max_steps: null # precedence over max_epochs
----+    # accumulate_grad_batches: 1 # accumulates grads every k batches
----+    # gradient_clip_val: 0
----+    # amp_level: O0 # O1/O2 for mixed precision
----+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # # accelerator: ddp
----+    # checkpoint_callback: false  # Provided by exp_manager
----+    # logger: false #false  # Provided by exp_manager
----+    # log_every_n_steps: 1  # Interval of logging.
----+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    # reload_dataloaders_every_epoch: true
----+    # resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /home/nxingyu2/data # /root/data # 
-----tmp_path: /home/nxingyu2/data/tmp # /tmp # 
----+base_path: /home/nxingyu/data # /root/data # 
----+tmp_path: /home/nxingyu/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
-----    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----     unfrozen: 0
-----    maximum_unfrozen: 2
----+    maximum_unfrozen: 1
----     unfreeze_step: 1
----     # unfreeze_every: 3
----     punct_label_ids:
----@@ -58,10 +58,10 @@ model:
----         - "—"
----         - "…"
---- 
-----    punct_class_weights: true
----+    punct_class_weights: false
----     
----     dataset:
-----        data_dir: /home/nxingyu2/data # /root/data # 
----+        data_dir: /home/nxingyu/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -106,6 +106,11 @@ model:
----         config: null
----         # unfrozen_layers: 1
---- 
----+    mlp:
----+        num_fc_layers: 2
----+        fc_dropout: 0.1
----+        activation: 'relu'
----+        
----     punct_head:
----         punct_num_fc_layers: 1
----         fc_dropout: 0.1
----@@ -117,22 +122,24 @@ model:
----     domain_head:
----         domain_num_fc_layers: 1
----         fc_dropout: 0.1
-----        activation: 'relu'
----+        activation: 'gelu'
----         log_softmax: false
----         use_transformer_init: true
----         loss: 'cel'
----         gamma: 0.1 # coefficient of gradient reversal
----+        pooling: 'token'
----+        idx_conditioned_on: 0
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 3
----+        alpha: 4
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: novograd
----+        name: adamw
----         lr: 1e-3
----         weight_decay: 0.00
----         sched:
----diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
----index d4ff927..b4b92b0 100644
------- a/experiment/core/layers/sequence_classifier.py
----+++ b/experiment/core/layers/sequence_classifier.py
----@@ -20,7 +20,8 @@ class SequenceClassifier(nn.Module):
----         log_softmax: bool = True,
----         dropout: float = 0.0,
----         use_transformer_init: bool = True,
-----        idx_conditioned_on: int = 0,
----+        pooling: str = 'mean', # mean, max, mean_max, token
----+        idx_conditioned_on: int = None,
----     ):
----         """
----         Initializes the SequenceClassifier module.
----@@ -37,6 +38,7 @@ class SequenceClassifier(nn.Module):
----         super().__init__()
----         self.log_softmax = log_softmax
----         self._idx_conditioned_on = idx_conditioned_on
----+        self.pooling = pooling
----         self.mlp = MultiLayerPerceptron(
----             hidden_size=hidden_size,
----             num_classes=num_classes,
----@@ -48,7 +50,23 @@ class SequenceClassifier(nn.Module):
----         if use_transformer_init:
----             self.apply(lambda module: transformer_weights_init(module, xavier=False))
---- 
-----    def forward(self, hidden_states):
----+    def forward(self, hidden_states, subtoken_mask=None):
----         hidden_states = self.dropout(hidden_states)
-----        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
----+        if self.pooling=='token':
----+            pooled = hidden_states[:, self._idx_conditioned_on]
----+        else:
----+            if subtoken_mask==None:
----+                ct=hidden_states.shape[1] # Seq len
----+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
----+            else:
----+                ct = torch.sum(subtoken_mask,axis=1)
----+            pooled_sum = torch.sum(hidden_states,axis=1)
----+            if self.pooling=='mean' or self.pooling == 'mean_max':
----+                pooled_mean = pooled_sum//ct
----+            if self.pooling=='max' or self.pooling=='mean_max':
----+                pooled_max = torch.max(hidden_states,axis=1)[0]
----+            pooled=pooled_mean if self.pooling=='mean' else \
----+                pooled_max if self.pooling=='max' else \
----+                    torch.cat([pooled_mean,pooled_max],axis=-1)
----+        logits = self.mlp(pooled)
----         return logits
----diff --git a/experiment/info.log b/experiment/info.log
----index 2471fe9..e69de29 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,300 +0,0 @@
-----[INFO] - GPU available: True, used: False
-----[INFO] - TPU available: None, using: 0 TPU cores
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        184
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.23       0.34       0.53        594
------ (label_id: 3)                                          3.06      25.42       5.46         59
-----. (label_id: 4)                                         47.22      12.98      20.36        524
-----: (label_id: 5)                                          0.00       0.00       0.00         18
-----; (label_id: 6)                                          0.00       0.00       0.00         13
-----? (label_id: 7)                                          8.45       6.32       7.23         95
-----— (label_id: 8)                                          0.00       0.00       0.00         12
-----… (label_id: 9)                                          0.00       0.00       0.00          0
------------------------
-----micro avg                                                6.05       6.05       6.05       1503
-----macro avg                                                6.66       5.01       3.73       1503
-----weighted avg                                            17.61       6.05       7.98       1503
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00         92
------------------------
-----micro avg                                              100.00     100.00     100.00         92
-----macro avg                                              100.00     100.00     100.00         92
-----weighted avg                                           100.00     100.00     100.00         92
-----
-----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 1.5848931924611143e-08
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
-----will be used during training (effective maximum steps = 3192) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 3192
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        202
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.62       0.45       0.70        669
------ (label_id: 3)                                          3.48      27.27       6.17         66
-----. (label_id: 4)                                         45.06      13.01      20.19        561
-----: (label_id: 5)                                          1.52       6.67       2.47         15
-----; (label_id: 6)                                          0.00       0.00       0.00         15
-----? (label_id: 7)                                          8.70       7.32       7.95         82
-----— (label_id: 8)                                          0.00       0.00       0.00         13
-----… (label_id: 9)                                          0.00       0.00       0.00          1
------------------------
-----micro avg                                                6.20       6.20       6.20       1628
-----macro avg                                                6.04       5.47       3.75       1628
-----weighted avg                                            16.79       6.20       7.92       1628
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        101
------------------------
-----micro avg                                              100.00     100.00     100.00        101
-----macro avg                                              100.00     100.00     100.00        101
-----weighted avg                                           100.00     100.00     100.00        101
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.34       0.73       0.46       4402
-----! (label_id: 1)                                          0.42      13.95       0.82        129
-----, (label_id: 2)                                          2.53       0.64       1.03      15243
------ (label_id: 3)                                          2.45      21.03       4.38       1322
-----. (label_id: 4)                                         44.00      11.40      18.11      12542
-----: (label_id: 5)                                          0.43       1.41       0.65        354
-----; (label_id: 6)                                          0.00       0.00       0.00        163
-----? (label_id: 7)                                          4.16       6.27       5.00       1117
-----— (label_id: 8)                                          3.00       0.61       1.02        488
-----… (label_id: 9)                                          0.97       6.17       1.68         81
------------------------
-----micro avg                                                5.41       5.41       5.41      35841
-----macro avg                                                5.83       6.22       3.32      35841
-----weighted avg                                            16.78       5.41       7.18      35841
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2201
------------------------
-----micro avg                                              100.00     100.00     100.00       2201
-----macro avg                                              100.00     100.00     100.00       2201
-----weighted avg                                           100.00     100.00     100.00       2201
-----
-----[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.20       0.43       0.27       4226
-----! (label_id: 1)                                          0.44      14.17       0.86        127
-----, (label_id: 2)                                          1.93       0.49       0.78      14611
------ (label_id: 3)                                          2.23      19.56       4.01       1237
-----. (label_id: 4)                                         43.37      11.25      17.86      11977
-----: (label_id: 5)                                          0.68       2.34       1.05        342
-----; (label_id: 6)                                          0.00       0.00       0.00        129
-----? (label_id: 7)                                          5.16       7.47       6.10       1058
-----— (label_id: 8)                                          2.15       0.49       0.80        409
-----… (label_id: 9)                                          0.69       4.23       1.19         71
------------------------
-----micro avg                                                5.23       5.23       5.23      34187
-----macro avg                                                5.68       6.04       3.29      34187
-----weighted avg                                            16.32       5.23       6.98      34187
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2113
------------------------
-----micro avg                                              100.00     100.00     100.00       2113
-----macro avg                                              100.00     100.00     100.00       2113
-----weighted avg                                           100.00     100.00     100.00       2113
-----
-----[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.61       0.39       4228
-----! (label_id: 1)                                          0.30       8.28       0.58        145
-----, (label_id: 2)                                          2.27       0.58       0.92      14495
------ (label_id: 3)                                          2.64      21.78       4.70       1327
-----. (label_id: 4)                                         44.87      11.66      18.51      12193
-----: (label_id: 5)                                          0.60       1.93       0.91        362
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.19       6.40       5.07       1078
-----— (label_id: 8)                                          1.16       0.22       0.37        459
-----… (label_id: 9)                                          0.85       4.17       1.41         96
------------------------
-----micro avg                                                5.54       5.54       5.54      34547
-----macro avg                                                5.72       5.56       3.29      34547
-----weighted avg                                            17.08       5.54       7.33      34547
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2114
------------------------
-----micro avg                                              100.00     100.00     100.00       2114
-----macro avg                                              100.00     100.00     100.00       2114
-----weighted avg                                           100.00     100.00     100.00       2114
-----
-----[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.29       0.63       0.40       4444
-----! (label_id: 1)                                          0.38      10.67       0.74        150
-----, (label_id: 2)                                          2.32       0.59       0.94      15290
------ (label_id: 3)                                          2.34      20.28       4.19       1292
-----. (label_id: 4)                                         43.85      11.68      18.44      12599
-----: (label_id: 5)                                          0.41       1.28       0.62        392
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.24       6.30       5.07       1111
-----— (label_id: 8)                                          0.00       0.00       0.00        456
-----… (label_id: 9)                                          0.38       2.41       0.65         83
------------------------
-----micro avg                                                5.40       5.40       5.40      35981
-----macro avg                                                5.42       5.38       3.11      35981
-----weighted avg                                            16.59       5.40       7.22      35981
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2222
------------------------
-----micro avg                                              100.00     100.00     100.00       2222
-----macro avg                                              100.00     100.00     100.00       2222
-----weighted avg                                           100.00     100.00     100.00       2222
-----
-----[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.35       0.73       0.48       3844
-----! (label_id: 1)                                          0.54      14.62       1.04        130
-----, (label_id: 2)                                          2.32       0.59       0.94      13056
------ (label_id: 3)                                          2.67      22.28       4.77       1194
-----. (label_id: 4)                                         44.45      11.95      18.84      10791
-----: (label_id: 5)                                          0.84       3.21       1.33        280
-----; (label_id: 6)                                          0.00       0.00       0.00        140
-----? (label_id: 7)                                          4.17       6.56       5.10        914
-----— (label_id: 8)                                          0.00       0.00       0.00        401
-----… (label_id: 9)                                          0.48       2.63       0.81         76
------------------------
-----micro avg                                                5.68       5.68       5.68      30826
-----macro avg                                                5.58       6.26       3.33      30826
-----weighted avg                                            16.82       5.68       7.41      30826
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1922
------------------------
-----micro avg                                              100.00     100.00     100.00       1922
-----macro avg                                              100.00     100.00     100.00       1922
-----weighted avg                                           100.00     100.00     100.00       1922
-----
-----[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.60       0.39       3970
-----! (label_id: 1)                                          0.35      10.66       0.68        122
-----, (label_id: 2)                                          2.09       0.53       0.85      13469
------ (label_id: 3)                                          2.29      19.32       4.10       1201
-----. (label_id: 4)                                         43.43      11.24      17.86      11227
-----: (label_id: 5)                                          0.63       2.30       0.99        304
-----; (label_id: 6)                                          0.00       0.00       0.00        141
-----? (label_id: 7)                                          4.52       6.86       5.45       1006
-----— (label_id: 8)                                          1.15       0.23       0.38        444
-----… (label_id: 9)                                          0.45       2.67       0.78         75
------------------------
-----micro avg                                                5.26       5.26       5.26      31959
-----macro avg                                                5.52       5.44       3.15      31959
-----weighted avg                                            16.42       5.26       7.02      31959
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1985
------------------------
-----micro avg                                              100.00     100.00     100.00       1985
-----macro avg                                              100.00     100.00     100.00       1985
-----weighted avg                                           100.00     100.00     100.00       1985
-----
-----[INFO] - Epoch 5, step 2394: val_loss was not in top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.23       0.48       0.31       4126
-----! (label_id: 1)                                          0.29       9.40       0.56        117
-----, (label_id: 2)                                          1.91       0.49       0.77      14019
------ (label_id: 3)                                          2.52      22.59       4.53       1164
-----. (label_id: 4)                                         44.15      11.65      18.44      11789
-----: (label_id: 5)                                          0.72       2.41       1.11        332
-----; (label_id: 6)                                          0.56       0.61       0.58        165
-----? (label_id: 7)                                          3.89       6.53       4.88        980
-----— (label_id: 8)                                          2.30       0.47       0.77        430
-----… (label_id: 9)                                          1.18       8.33       2.07         60
------------------------
-----micro avg                                                5.47       5.47       5.47      33182
-----macro avg                                                5.77       6.30       3.40      33182
-----weighted avg                                            16.77       5.47       7.25      33182
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2063
------------------------
-----micro avg                                              100.00     100.00     100.00       2063
-----macro avg                                              100.00     100.00     100.00       2063
-----weighted avg                                           100.00     100.00     100.00       2063
-----
-----[INFO] - Epoch 6, step 2793: val_loss was not in top 3
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index fa37b4c..fd77a7f 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             self.hparams.model.punct_class_weights=None
---- 
----+        self.dropout=nn.Dropout(self.hparams.model.mlp.dropout)
----+        self.mlp = MultiLayerPerceptron(
----+            self.transformer.config.hidden_size,
----+            self.transformer.config.hidden_size,
----+            num_layers=self.hparams.model.mlp.num_fc_layers, 
----+            activation=self.hparams.model.mlp.activation, 
----+            log_softmax=self.hparams.model.mlp.log_softmax
----+        )
----+
----         self.punct_classifier = TokenClassifier(
----             hidden_size=self.transformer.config.hidden_size,
----             num_classes=len(self.labels_to_ids),
----@@ -80,6 +89,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----             log_softmax=self.hparams.model.domain_head.log_softmax,
----             dropout=self.hparams.model.domain_head.fc_dropout,
----             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
----+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
----         )
---- 
----         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
----@@ -119,14 +129,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
----         self.freeze()
---- 
-----    def forward(self, input_ids, attention_mask, domain_ids=None):
----+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
----         hidden_states = self.transformer(
----             input_ids=input_ids, attention_mask=attention_mask
----         )[0]
----+        hidden_states = self.dropout(hidden_states)
----+        hidden_states = self.mlp(hidden_states)
----         punct_logits = self.punct_classifier(hidden_states=hidden_states)
----         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
----         domain_logits = self.domain_classifier(
-----            hidden_states=reverse_grad_hidden_states)
----+            hidden_states=reverse_grad_hidden_states,
----+            subtoken_mask=subtoken_mask)
----         return punct_logits, domain_logits
---- 
----     def _make_step(self, batch):
----@@ -136,7 +149,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         punct_labels = batch['labels']
----         domain_labels = batch['domain']
----         punct_logits, domain_logits = self(
-----            input_ids=input_ids, attention_mask=attention_mask
----+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
----         )
----         punctuation_loss = self.punctuation_loss(
----             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/lightning_logs.txt
---deleted file mode 100644
---index 8942cad..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/lightning_logs.txt
---+++ /dev/null
---@@ -1,5 +0,0 @@
----Global seed set to 42
----GPU available: True, used: True
----TPU available: None, using: 0 TPU cores
----LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
----Using native 16bit precision.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/nemo_error_log.txt
---deleted file mode 100644
---index 7ce4d0c..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/nemo_error_log.txt
---+++ /dev/null
---@@ -1 +0,0 @@
----[NeMo W 2021-02-08 10:44:29 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index db09fa5..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,3 +0,0 @@
----[NeMo I 2021-02-08 10:44:29 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-44-29
----[NeMo I 2021-02-08 10:44:29 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-08 10:44:29 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/git-info.log
---deleted file mode 100644
---index da9a71d..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/git-info.log
---+++ /dev/null
---@@ -1,623 +0,0 @@
----commit hash: 939a671c8c117db6975316767ced5d95449e2b27
----diff --git a/README.md b/README.md
----index d52dba5..0b403d3 100644
------- a/README.md
----+++ b/README.md
----@@ -333,4 +333,71 @@ label                                                precision    recall       f
----  'punct_f1': 0.5858508944511414,
----  'punct_precision': 0.30176490545272827,
----  'punct_recall': 10.0,
----- 'test_loss': 0.8140875697135925}
----\ No newline at end of file
----+ 'test_loss': 0.8140875697135925}
----+
----+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
----+
----+ (label_id: 0)                                          62.15     100.00      76.66       5154
----+! (label_id: 1)                                          0.00       0.00       0.00        108
----+, (label_id: 2)                                          0.00       0.00       0.00      18022
----+- (label_id: 3)                                          0.00       0.00       0.00       1557
----+. (label_id: 4)                                         41.74      94.01      57.81      15164
----+: (label_id: 5)                                          0.00       0.00       0.00        319
----+; (label_id: 6)                                          0.00       0.00       0.00         88
----+? (label_id: 7)                                          0.00       0.00       0.00       1217
----+ (label_id: 8)                                          0.00       0.00       0.00        752
----+… (label_id: 9)                                          0.00       0.00       0.00         67
----+-------------------
----+micro avg                                               45.72      45.72      45.72      42448
----+macro avg                                               10.39      19.40      13.45      42448
----+weighted avg                                            22.46      45.72      29.96      42448
----+{ 'punct_f1': 13.446383476257324,
----+ 'punct_precision': 10.388500213623047,
----+ 'punct_recall': 19.400554656982422,
----+ 'test_loss': 0.44148480892181396}
----+
----+
----+ ## Log for 8/2/2021
----+
----+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
----+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
----+
----+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
----+End frozen 
----+micro avg                                               41.42      41.42      41.42      33406
----+macro avg                                               11.01      13.54      11.07      33406
----+weighted avg                                            34.88      41.42      34.20      33406
----+
----+1st layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               36.65      36.65      36.65      33463
----+macro avg                                               10.71       9.91       8.26      33463
----+weighted avg                                            34.32      36.65      31.08      33463
----+
----+2nd layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               35.72      35.72      35.72      42448
----+macro avg                                                3.57      10.00       5.26      42448
----+weighted avg                                            12.76      35.72      18.81      42448
----+
----+{'punct_f1': 5.264181137084961,
----+ 'punct_precision': 3.572371006011963,
----+ 'punct_recall': 10.0,
----+ 'test_loss': 18.49854850769043}
----+
----+
----+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
----+alpha from 3->4 seems to reduce convergence rate.
----+
----+micro avg                                               50.98      50.98      50.98      33463
----+macro avg                                               25.99      25.38      23.38      33463
----+weighted avg                                            50.31      50.98      48.27      33463
----+
----+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
----+micro avg                                               58.55      58.55      58.55      39340
----+macro avg                                               30.02      29.74      29.52      39340
----+weighted avg                                            57.91      58.55      57.51      39340
----+
----+still increasing?!
----+{'punct_f1': 29.523975372314453,
----+ 'punct_precision': 30.015613555908203,
----+ 'punct_recall': 29.738296508789062,
----+ 'test_loss': 0.3690211772918701}
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index fe58670..872a5ed 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,49 +1,49 @@
---- seed: 42
---- trainer:
-----    # gpus: 1 # the number of gpus, 0 for CPU
-----    # num_nodes: 1
-----    # max_epochs: 2
-----    # max_steps: null # precedence over max_epochs
-----    # accumulate_grad_batches: 4 # accumulates grads every k batches
-----    # gradient_clip_val: 0
-----    # amp_level: O1 # O1/O2 for mixed precision
-----    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
-----    # checkpoint_callback: false  # Provided by exp_manager
-----    # logger: false #false  # Provided by exp_manager
-----    # log_every_n_steps: 1  # Interval of logging.
-----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # resume_from_checkpoint: null
-----
-----    gpus: 0 # the number of gpus, 0 for CPU
----+    gpus: 1 # the number of gpus, 0 for CPU
----     num_nodes: 1
-----    max_epochs: 8
----+    max_epochs: 10
----     max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 1 # accumulates grads every k batches
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----     gradient_clip_val: 0
-----    amp_level: O0 # O1/O2 for mixed precision
-----    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
----+    amp_level: O1 # O1/O2 for mixed precision
----+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    accelerator: ddp
----     checkpoint_callback: false  # Provided by exp_manager
----     logger: false #false  # Provided by exp_manager
----     log_every_n_steps: 1  # Interval of logging.
----     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    reload_dataloaders_every_epoch: true
----     resume_from_checkpoint: null
---- 
----+    # gpus: 0 # the number of gpus, 0 for CPU
----+    # num_nodes: 1
----+    # max_epochs: 8
----+    # max_steps: null # precedence over max_epochs
----+    # accumulate_grad_batches: 1 # accumulates grads every k batches
----+    # gradient_clip_val: 0
----+    # amp_level: O0 # O1/O2 for mixed precision
----+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # # accelerator: ddp
----+    # checkpoint_callback: false  # Provided by exp_manager
----+    # logger: false #false  # Provided by exp_manager
----+    # log_every_n_steps: 1  # Interval of logging.
----+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    # reload_dataloaders_every_epoch: true
----+    # resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /home/nxingyu2/data # /root/data # 
-----tmp_path: /home/nxingyu2/data/tmp # /tmp # 
----+base_path: /home/nxingyu/data # /root/data # 
----+tmp_path: /home/nxingyu/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
-----    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----     unfrozen: 0
-----    maximum_unfrozen: 2
----+    maximum_unfrozen: 1
----     unfreeze_step: 1
----     # unfreeze_every: 3
----     punct_label_ids:
----@@ -58,10 +58,10 @@ model:
----         - "—"
----         - "…"
---- 
-----    punct_class_weights: true
----+    punct_class_weights: false
----     
----     dataset:
-----        data_dir: /home/nxingyu2/data # /root/data # 
----+        data_dir: /home/nxingyu/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -106,6 +106,11 @@ model:
----         config: null
----         # unfrozen_layers: 1
---- 
----+    mlp:
----+        num_fc_layers: 2
----+        fc_dropout: 0.1
----+        activation: 'relu'
----+        
----     punct_head:
----         punct_num_fc_layers: 1
----         fc_dropout: 0.1
----@@ -117,22 +122,24 @@ model:
----     domain_head:
----         domain_num_fc_layers: 1
----         fc_dropout: 0.1
-----        activation: 'relu'
----+        activation: 'gelu'
----         log_softmax: false
----         use_transformer_init: true
----         loss: 'cel'
----         gamma: 0.1 # coefficient of gradient reversal
----+        pooling: 'token'
----+        idx_conditioned_on: 0
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 3
----+        alpha: 4
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: novograd
----+        name: adamw
----         lr: 1e-3
----         weight_decay: 0.00
----         sched:
----diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
----index d4ff927..b4b92b0 100644
------- a/experiment/core/layers/sequence_classifier.py
----+++ b/experiment/core/layers/sequence_classifier.py
----@@ -20,7 +20,8 @@ class SequenceClassifier(nn.Module):
----         log_softmax: bool = True,
----         dropout: float = 0.0,
----         use_transformer_init: bool = True,
-----        idx_conditioned_on: int = 0,
----+        pooling: str = 'mean', # mean, max, mean_max, token
----+        idx_conditioned_on: int = None,
----     ):
----         """
----         Initializes the SequenceClassifier module.
----@@ -37,6 +38,7 @@ class SequenceClassifier(nn.Module):
----         super().__init__()
----         self.log_softmax = log_softmax
----         self._idx_conditioned_on = idx_conditioned_on
----+        self.pooling = pooling
----         self.mlp = MultiLayerPerceptron(
----             hidden_size=hidden_size,
----             num_classes=num_classes,
----@@ -48,7 +50,23 @@ class SequenceClassifier(nn.Module):
----         if use_transformer_init:
----             self.apply(lambda module: transformer_weights_init(module, xavier=False))
---- 
-----    def forward(self, hidden_states):
----+    def forward(self, hidden_states, subtoken_mask=None):
----         hidden_states = self.dropout(hidden_states)
-----        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
----+        if self.pooling=='token':
----+            pooled = hidden_states[:, self._idx_conditioned_on]
----+        else:
----+            if subtoken_mask==None:
----+                ct=hidden_states.shape[1] # Seq len
----+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
----+            else:
----+                ct = torch.sum(subtoken_mask,axis=1)
----+            pooled_sum = torch.sum(hidden_states,axis=1)
----+            if self.pooling=='mean' or self.pooling == 'mean_max':
----+                pooled_mean = pooled_sum//ct
----+            if self.pooling=='max' or self.pooling=='mean_max':
----+                pooled_max = torch.max(hidden_states,axis=1)[0]
----+            pooled=pooled_mean if self.pooling=='mean' else \
----+                pooled_max if self.pooling=='max' else \
----+                    torch.cat([pooled_mean,pooled_max],axis=-1)
----+        logits = self.mlp(pooled)
----         return logits
----diff --git a/experiment/info.log b/experiment/info.log
----index 2471fe9..e69de29 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,300 +0,0 @@
-----[INFO] - GPU available: True, used: False
-----[INFO] - TPU available: None, using: 0 TPU cores
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        184
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.23       0.34       0.53        594
------ (label_id: 3)                                          3.06      25.42       5.46         59
-----. (label_id: 4)                                         47.22      12.98      20.36        524
-----: (label_id: 5)                                          0.00       0.00       0.00         18
-----; (label_id: 6)                                          0.00       0.00       0.00         13
-----? (label_id: 7)                                          8.45       6.32       7.23         95
-----— (label_id: 8)                                          0.00       0.00       0.00         12
-----… (label_id: 9)                                          0.00       0.00       0.00          0
------------------------
-----micro avg                                                6.05       6.05       6.05       1503
-----macro avg                                                6.66       5.01       3.73       1503
-----weighted avg                                            17.61       6.05       7.98       1503
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00         92
------------------------
-----micro avg                                              100.00     100.00     100.00         92
-----macro avg                                              100.00     100.00     100.00         92
-----weighted avg                                           100.00     100.00     100.00         92
-----
-----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 1.5848931924611143e-08
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
-----will be used during training (effective maximum steps = 3192) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 3192
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        202
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.62       0.45       0.70        669
------ (label_id: 3)                                          3.48      27.27       6.17         66
-----. (label_id: 4)                                         45.06      13.01      20.19        561
-----: (label_id: 5)                                          1.52       6.67       2.47         15
-----; (label_id: 6)                                          0.00       0.00       0.00         15
-----? (label_id: 7)                                          8.70       7.32       7.95         82
-----— (label_id: 8)                                          0.00       0.00       0.00         13
-----… (label_id: 9)                                          0.00       0.00       0.00          1
------------------------
-----micro avg                                                6.20       6.20       6.20       1628
-----macro avg                                                6.04       5.47       3.75       1628
-----weighted avg                                            16.79       6.20       7.92       1628
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        101
------------------------
-----micro avg                                              100.00     100.00     100.00        101
-----macro avg                                              100.00     100.00     100.00        101
-----weighted avg                                           100.00     100.00     100.00        101
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.34       0.73       0.46       4402
-----! (label_id: 1)                                          0.42      13.95       0.82        129
-----, (label_id: 2)                                          2.53       0.64       1.03      15243
------ (label_id: 3)                                          2.45      21.03       4.38       1322
-----. (label_id: 4)                                         44.00      11.40      18.11      12542
-----: (label_id: 5)                                          0.43       1.41       0.65        354
-----; (label_id: 6)                                          0.00       0.00       0.00        163
-----? (label_id: 7)                                          4.16       6.27       5.00       1117
-----— (label_id: 8)                                          3.00       0.61       1.02        488
-----… (label_id: 9)                                          0.97       6.17       1.68         81
------------------------
-----micro avg                                                5.41       5.41       5.41      35841
-----macro avg                                                5.83       6.22       3.32      35841
-----weighted avg                                            16.78       5.41       7.18      35841
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2201
------------------------
-----micro avg                                              100.00     100.00     100.00       2201
-----macro avg                                              100.00     100.00     100.00       2201
-----weighted avg                                           100.00     100.00     100.00       2201
-----
-----[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.20       0.43       0.27       4226
-----! (label_id: 1)                                          0.44      14.17       0.86        127
-----, (label_id: 2)                                          1.93       0.49       0.78      14611
------ (label_id: 3)                                          2.23      19.56       4.01       1237
-----. (label_id: 4)                                         43.37      11.25      17.86      11977
-----: (label_id: 5)                                          0.68       2.34       1.05        342
-----; (label_id: 6)                                          0.00       0.00       0.00        129
-----? (label_id: 7)                                          5.16       7.47       6.10       1058
-----— (label_id: 8)                                          2.15       0.49       0.80        409
-----… (label_id: 9)                                          0.69       4.23       1.19         71
------------------------
-----micro avg                                                5.23       5.23       5.23      34187
-----macro avg                                                5.68       6.04       3.29      34187
-----weighted avg                                            16.32       5.23       6.98      34187
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2113
------------------------
-----micro avg                                              100.00     100.00     100.00       2113
-----macro avg                                              100.00     100.00     100.00       2113
-----weighted avg                                           100.00     100.00     100.00       2113
-----
-----[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.61       0.39       4228
-----! (label_id: 1)                                          0.30       8.28       0.58        145
-----, (label_id: 2)                                          2.27       0.58       0.92      14495
------ (label_id: 3)                                          2.64      21.78       4.70       1327
-----. (label_id: 4)                                         44.87      11.66      18.51      12193
-----: (label_id: 5)                                          0.60       1.93       0.91        362
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.19       6.40       5.07       1078
-----— (label_id: 8)                                          1.16       0.22       0.37        459
-----… (label_id: 9)                                          0.85       4.17       1.41         96
------------------------
-----micro avg                                                5.54       5.54       5.54      34547
-----macro avg                                                5.72       5.56       3.29      34547
-----weighted avg                                            17.08       5.54       7.33      34547
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2114
------------------------
-----micro avg                                              100.00     100.00     100.00       2114
-----macro avg                                              100.00     100.00     100.00       2114
-----weighted avg                                           100.00     100.00     100.00       2114
-----
-----[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.29       0.63       0.40       4444
-----! (label_id: 1)                                          0.38      10.67       0.74        150
-----, (label_id: 2)                                          2.32       0.59       0.94      15290
------ (label_id: 3)                                          2.34      20.28       4.19       1292
-----. (label_id: 4)                                         43.85      11.68      18.44      12599
-----: (label_id: 5)                                          0.41       1.28       0.62        392
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.24       6.30       5.07       1111
-----— (label_id: 8)                                          0.00       0.00       0.00        456
-----… (label_id: 9)                                          0.38       2.41       0.65         83
------------------------
-----micro avg                                                5.40       5.40       5.40      35981
-----macro avg                                                5.42       5.38       3.11      35981
-----weighted avg                                            16.59       5.40       7.22      35981
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2222
------------------------
-----micro avg                                              100.00     100.00     100.00       2222
-----macro avg                                              100.00     100.00     100.00       2222
-----weighted avg                                           100.00     100.00     100.00       2222
-----
-----[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.35       0.73       0.48       3844
-----! (label_id: 1)                                          0.54      14.62       1.04        130
-----, (label_id: 2)                                          2.32       0.59       0.94      13056
------ (label_id: 3)                                          2.67      22.28       4.77       1194
-----. (label_id: 4)                                         44.45      11.95      18.84      10791
-----: (label_id: 5)                                          0.84       3.21       1.33        280
-----; (label_id: 6)                                          0.00       0.00       0.00        140
-----? (label_id: 7)                                          4.17       6.56       5.10        914
-----— (label_id: 8)                                          0.00       0.00       0.00        401
-----… (label_id: 9)                                          0.48       2.63       0.81         76
------------------------
-----micro avg                                                5.68       5.68       5.68      30826
-----macro avg                                                5.58       6.26       3.33      30826
-----weighted avg                                            16.82       5.68       7.41      30826
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1922
------------------------
-----micro avg                                              100.00     100.00     100.00       1922
-----macro avg                                              100.00     100.00     100.00       1922
-----weighted avg                                           100.00     100.00     100.00       1922
-----
-----[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.60       0.39       3970
-----! (label_id: 1)                                          0.35      10.66       0.68        122
-----, (label_id: 2)                                          2.09       0.53       0.85      13469
------ (label_id: 3)                                          2.29      19.32       4.10       1201
-----. (label_id: 4)                                         43.43      11.24      17.86      11227
-----: (label_id: 5)                                          0.63       2.30       0.99        304
-----; (label_id: 6)                                          0.00       0.00       0.00        141
-----? (label_id: 7)                                          4.52       6.86       5.45       1006
-----— (label_id: 8)                                          1.15       0.23       0.38        444
-----… (label_id: 9)                                          0.45       2.67       0.78         75
------------------------
-----micro avg                                                5.26       5.26       5.26      31959
-----macro avg                                                5.52       5.44       3.15      31959
-----weighted avg                                            16.42       5.26       7.02      31959
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1985
------------------------
-----micro avg                                              100.00     100.00     100.00       1985
-----macro avg                                              100.00     100.00     100.00       1985
-----weighted avg                                           100.00     100.00     100.00       1985
-----
-----[INFO] - Epoch 5, step 2394: val_loss was not in top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.23       0.48       0.31       4126
-----! (label_id: 1)                                          0.29       9.40       0.56        117
-----, (label_id: 2)                                          1.91       0.49       0.77      14019
------ (label_id: 3)                                          2.52      22.59       4.53       1164
-----. (label_id: 4)                                         44.15      11.65      18.44      11789
-----: (label_id: 5)                                          0.72       2.41       1.11        332
-----; (label_id: 6)                                          0.56       0.61       0.58        165
-----? (label_id: 7)                                          3.89       6.53       4.88        980
-----— (label_id: 8)                                          2.30       0.47       0.77        430
-----… (label_id: 9)                                          1.18       8.33       2.07         60
------------------------
-----micro avg                                                5.47       5.47       5.47      33182
-----macro avg                                                5.77       6.30       3.40      33182
-----weighted avg                                            16.77       5.47       7.25      33182
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2063
------------------------
-----micro avg                                              100.00     100.00     100.00       2063
-----macro avg                                              100.00     100.00     100.00       2063
-----weighted avg                                           100.00     100.00     100.00       2063
-----
-----[INFO] - Epoch 6, step 2793: val_loss was not in top 3
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index fa37b4c..ae8f205 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             self.hparams.model.punct_class_weights=None
---- 
----+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.dropout)
----+        self.mlp = MultiLayerPerceptron(
----+            self.transformer.config.hidden_size,
----+            self.transformer.config.hidden_size,
----+            num_layers=self.hparams.model.mlp.num_fc_layers, 
----+            activation=self.hparams.model.mlp.activation, 
----+            log_softmax=self.hparams.model.mlp.log_softmax
----+        )
----+
----         self.punct_classifier = TokenClassifier(
----             hidden_size=self.transformer.config.hidden_size,
----             num_classes=len(self.labels_to_ids),
----@@ -80,6 +89,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----             log_softmax=self.hparams.model.domain_head.log_softmax,
----             dropout=self.hparams.model.domain_head.fc_dropout,
----             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
----+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
----         )
---- 
----         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
----@@ -119,14 +129,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
----         self.freeze()
---- 
-----    def forward(self, input_ids, attention_mask, domain_ids=None):
----+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
----         hidden_states = self.transformer(
----             input_ids=input_ids, attention_mask=attention_mask
----         )[0]
----+        hidden_states = self.dropout(hidden_states)
----+        hidden_states = self.mlp(hidden_states)
----         punct_logits = self.punct_classifier(hidden_states=hidden_states)
----         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
----         domain_logits = self.domain_classifier(
-----            hidden_states=reverse_grad_hidden_states)
----+            hidden_states=reverse_grad_hidden_states,
----+            subtoken_mask=subtoken_mask)
----         return punct_logits, domain_logits
---- 
----     def _make_step(self, batch):
----@@ -136,7 +149,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         punct_labels = batch['labels']
----         domain_labels = batch['domain']
----         punct_logits, domain_logits = self(
-----            input_ids=input_ids, attention_mask=attention_mask
----+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
----         )
----         punctuation_loss = self.punctuation_loss(
----             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/lightning_logs.txt
---deleted file mode 100644
---index 8942cad..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/lightning_logs.txt
---+++ /dev/null
---@@ -1,5 +0,0 @@
----Global seed set to 42
----GPU available: True, used: True
----TPU available: None, using: 0 TPU cores
----LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
----Using native 16bit precision.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/nemo_error_log.txt
---deleted file mode 100644
---index 9358c62..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/nemo_error_log.txt
---+++ /dev/null
---@@ -1 +0,0 @@
----[NeMo W 2021-02-08 10:45:17 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index 141e96e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,3 +0,0 @@
----[NeMo I 2021-02-08 10:45:16 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-45-16
----[NeMo I 2021-02-08 10:45:16 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-08 10:45:17 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/git-info.log
---deleted file mode 100644
---index de2f700..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/git-info.log
---+++ /dev/null
---@@ -1,623 +0,0 @@
----commit hash: 939a671c8c117db6975316767ced5d95449e2b27
----diff --git a/README.md b/README.md
----index d52dba5..0b403d3 100644
------- a/README.md
----+++ b/README.md
----@@ -333,4 +333,71 @@ label                                                precision    recall       f
----  'punct_f1': 0.5858508944511414,
----  'punct_precision': 0.30176490545272827,
----  'punct_recall': 10.0,
----- 'test_loss': 0.8140875697135925}
----\ No newline at end of file
----+ 'test_loss': 0.8140875697135925}
----+
----+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
----+
----+ (label_id: 0)                                          62.15     100.00      76.66       5154
----+! (label_id: 1)                                          0.00       0.00       0.00        108
----+, (label_id: 2)                                          0.00       0.00       0.00      18022
----+- (label_id: 3)                                          0.00       0.00       0.00       1557
----+. (label_id: 4)                                         41.74      94.01      57.81      15164
----+: (label_id: 5)                                          0.00       0.00       0.00        319
----+; (label_id: 6)                                          0.00       0.00       0.00         88
----+? (label_id: 7)                                          0.00       0.00       0.00       1217
----+ (label_id: 8)                                          0.00       0.00       0.00        752
----+… (label_id: 9)                                          0.00       0.00       0.00         67
----+-------------------
----+micro avg                                               45.72      45.72      45.72      42448
----+macro avg                                               10.39      19.40      13.45      42448
----+weighted avg                                            22.46      45.72      29.96      42448
----+{ 'punct_f1': 13.446383476257324,
----+ 'punct_precision': 10.388500213623047,
----+ 'punct_recall': 19.400554656982422,
----+ 'test_loss': 0.44148480892181396}
----+
----+
----+ ## Log for 8/2/2021
----+
----+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
----+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
----+
----+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
----+End frozen 
----+micro avg                                               41.42      41.42      41.42      33406
----+macro avg                                               11.01      13.54      11.07      33406
----+weighted avg                                            34.88      41.42      34.20      33406
----+
----+1st layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               36.65      36.65      36.65      33463
----+macro avg                                               10.71       9.91       8.26      33463
----+weighted avg                                            34.32      36.65      31.08      33463
----+
----+2nd layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               35.72      35.72      35.72      42448
----+macro avg                                                3.57      10.00       5.26      42448
----+weighted avg                                            12.76      35.72      18.81      42448
----+
----+{'punct_f1': 5.264181137084961,
----+ 'punct_precision': 3.572371006011963,
----+ 'punct_recall': 10.0,
----+ 'test_loss': 18.49854850769043}
----+
----+
----+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
----+alpha from 3->4 seems to reduce convergence rate.
----+
----+micro avg                                               50.98      50.98      50.98      33463
----+macro avg                                               25.99      25.38      23.38      33463
----+weighted avg                                            50.31      50.98      48.27      33463
----+
----+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
----+micro avg                                               58.55      58.55      58.55      39340
----+macro avg                                               30.02      29.74      29.52      39340
----+weighted avg                                            57.91      58.55      57.51      39340
----+
----+still increasing?!
----+{'punct_f1': 29.523975372314453,
----+ 'punct_precision': 30.015613555908203,
----+ 'punct_recall': 29.738296508789062,
----+ 'test_loss': 0.3690211772918701}
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index fe58670..872a5ed 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,49 +1,49 @@
---- seed: 42
---- trainer:
-----    # gpus: 1 # the number of gpus, 0 for CPU
-----    # num_nodes: 1
-----    # max_epochs: 2
-----    # max_steps: null # precedence over max_epochs
-----    # accumulate_grad_batches: 4 # accumulates grads every k batches
-----    # gradient_clip_val: 0
-----    # amp_level: O1 # O1/O2 for mixed precision
-----    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
-----    # checkpoint_callback: false  # Provided by exp_manager
-----    # logger: false #false  # Provided by exp_manager
-----    # log_every_n_steps: 1  # Interval of logging.
-----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # resume_from_checkpoint: null
-----
-----    gpus: 0 # the number of gpus, 0 for CPU
----+    gpus: 1 # the number of gpus, 0 for CPU
----     num_nodes: 1
-----    max_epochs: 8
----+    max_epochs: 10
----     max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 1 # accumulates grads every k batches
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----     gradient_clip_val: 0
-----    amp_level: O0 # O1/O2 for mixed precision
-----    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
----+    amp_level: O1 # O1/O2 for mixed precision
----+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    accelerator: ddp
----     checkpoint_callback: false  # Provided by exp_manager
----     logger: false #false  # Provided by exp_manager
----     log_every_n_steps: 1  # Interval of logging.
----     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    reload_dataloaders_every_epoch: true
----     resume_from_checkpoint: null
---- 
----+    # gpus: 0 # the number of gpus, 0 for CPU
----+    # num_nodes: 1
----+    # max_epochs: 8
----+    # max_steps: null # precedence over max_epochs
----+    # accumulate_grad_batches: 1 # accumulates grads every k batches
----+    # gradient_clip_val: 0
----+    # amp_level: O0 # O1/O2 for mixed precision
----+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # # accelerator: ddp
----+    # checkpoint_callback: false  # Provided by exp_manager
----+    # logger: false #false  # Provided by exp_manager
----+    # log_every_n_steps: 1  # Interval of logging.
----+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    # reload_dataloaders_every_epoch: true
----+    # resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /home/nxingyu2/data # /root/data # 
-----tmp_path: /home/nxingyu2/data/tmp # /tmp # 
----+base_path: /home/nxingyu/data # /root/data # 
----+tmp_path: /home/nxingyu/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
-----    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----     unfrozen: 0
-----    maximum_unfrozen: 2
----+    maximum_unfrozen: 1
----     unfreeze_step: 1
----     # unfreeze_every: 3
----     punct_label_ids:
----@@ -58,10 +58,10 @@ model:
----         - "—"
----         - "…"
---- 
-----    punct_class_weights: true
----+    punct_class_weights: false
----     
----     dataset:
-----        data_dir: /home/nxingyu2/data # /root/data # 
----+        data_dir: /home/nxingyu/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -106,6 +106,11 @@ model:
----         config: null
----         # unfrozen_layers: 1
---- 
----+    mlp:
----+        num_fc_layers: 2
----+        fc_dropout: 0.1
----+        activation: 'relu'
----+        
----     punct_head:
----         punct_num_fc_layers: 1
----         fc_dropout: 0.1
----@@ -117,22 +122,24 @@ model:
----     domain_head:
----         domain_num_fc_layers: 1
----         fc_dropout: 0.1
-----        activation: 'relu'
----+        activation: 'gelu'
----         log_softmax: false
----         use_transformer_init: true
----         loss: 'cel'
----         gamma: 0.1 # coefficient of gradient reversal
----+        pooling: 'token'
----+        idx_conditioned_on: 0
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 3
----+        alpha: 4
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: novograd
----+        name: adamw
----         lr: 1e-3
----         weight_decay: 0.00
----         sched:
----diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
----index d4ff927..b4b92b0 100644
------- a/experiment/core/layers/sequence_classifier.py
----+++ b/experiment/core/layers/sequence_classifier.py
----@@ -20,7 +20,8 @@ class SequenceClassifier(nn.Module):
----         log_softmax: bool = True,
----         dropout: float = 0.0,
----         use_transformer_init: bool = True,
-----        idx_conditioned_on: int = 0,
----+        pooling: str = 'mean', # mean, max, mean_max, token
----+        idx_conditioned_on: int = None,
----     ):
----         """
----         Initializes the SequenceClassifier module.
----@@ -37,6 +38,7 @@ class SequenceClassifier(nn.Module):
----         super().__init__()
----         self.log_softmax = log_softmax
----         self._idx_conditioned_on = idx_conditioned_on
----+        self.pooling = pooling
----         self.mlp = MultiLayerPerceptron(
----             hidden_size=hidden_size,
----             num_classes=num_classes,
----@@ -48,7 +50,23 @@ class SequenceClassifier(nn.Module):
----         if use_transformer_init:
----             self.apply(lambda module: transformer_weights_init(module, xavier=False))
---- 
-----    def forward(self, hidden_states):
----+    def forward(self, hidden_states, subtoken_mask=None):
----         hidden_states = self.dropout(hidden_states)
-----        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
----+        if self.pooling=='token':
----+            pooled = hidden_states[:, self._idx_conditioned_on]
----+        else:
----+            if subtoken_mask==None:
----+                ct=hidden_states.shape[1] # Seq len
----+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
----+            else:
----+                ct = torch.sum(subtoken_mask,axis=1)
----+            pooled_sum = torch.sum(hidden_states,axis=1)
----+            if self.pooling=='mean' or self.pooling == 'mean_max':
----+                pooled_mean = pooled_sum//ct
----+            if self.pooling=='max' or self.pooling=='mean_max':
----+                pooled_max = torch.max(hidden_states,axis=1)[0]
----+            pooled=pooled_mean if self.pooling=='mean' else \
----+                pooled_max if self.pooling=='max' else \
----+                    torch.cat([pooled_mean,pooled_max],axis=-1)
----+        logits = self.mlp(pooled)
----         return logits
----diff --git a/experiment/info.log b/experiment/info.log
----index 2471fe9..e69de29 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,300 +0,0 @@
-----[INFO] - GPU available: True, used: False
-----[INFO] - TPU available: None, using: 0 TPU cores
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        184
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.23       0.34       0.53        594
------ (label_id: 3)                                          3.06      25.42       5.46         59
-----. (label_id: 4)                                         47.22      12.98      20.36        524
-----: (label_id: 5)                                          0.00       0.00       0.00         18
-----; (label_id: 6)                                          0.00       0.00       0.00         13
-----? (label_id: 7)                                          8.45       6.32       7.23         95
-----— (label_id: 8)                                          0.00       0.00       0.00         12
-----… (label_id: 9)                                          0.00       0.00       0.00          0
------------------------
-----micro avg                                                6.05       6.05       6.05       1503
-----macro avg                                                6.66       5.01       3.73       1503
-----weighted avg                                            17.61       6.05       7.98       1503
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00         92
------------------------
-----micro avg                                              100.00     100.00     100.00         92
-----macro avg                                              100.00     100.00     100.00         92
-----weighted avg                                           100.00     100.00     100.00         92
-----
-----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 1.5848931924611143e-08
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
-----will be used during training (effective maximum steps = 3192) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 3192
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        202
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.62       0.45       0.70        669
------ (label_id: 3)                                          3.48      27.27       6.17         66
-----. (label_id: 4)                                         45.06      13.01      20.19        561
-----: (label_id: 5)                                          1.52       6.67       2.47         15
-----; (label_id: 6)                                          0.00       0.00       0.00         15
-----? (label_id: 7)                                          8.70       7.32       7.95         82
-----— (label_id: 8)                                          0.00       0.00       0.00         13
-----… (label_id: 9)                                          0.00       0.00       0.00          1
------------------------
-----micro avg                                                6.20       6.20       6.20       1628
-----macro avg                                                6.04       5.47       3.75       1628
-----weighted avg                                            16.79       6.20       7.92       1628
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        101
------------------------
-----micro avg                                              100.00     100.00     100.00        101
-----macro avg                                              100.00     100.00     100.00        101
-----weighted avg                                           100.00     100.00     100.00        101
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.34       0.73       0.46       4402
-----! (label_id: 1)                                          0.42      13.95       0.82        129
-----, (label_id: 2)                                          2.53       0.64       1.03      15243
------ (label_id: 3)                                          2.45      21.03       4.38       1322
-----. (label_id: 4)                                         44.00      11.40      18.11      12542
-----: (label_id: 5)                                          0.43       1.41       0.65        354
-----; (label_id: 6)                                          0.00       0.00       0.00        163
-----? (label_id: 7)                                          4.16       6.27       5.00       1117
-----— (label_id: 8)                                          3.00       0.61       1.02        488
-----… (label_id: 9)                                          0.97       6.17       1.68         81
------------------------
-----micro avg                                                5.41       5.41       5.41      35841
-----macro avg                                                5.83       6.22       3.32      35841
-----weighted avg                                            16.78       5.41       7.18      35841
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2201
------------------------
-----micro avg                                              100.00     100.00     100.00       2201
-----macro avg                                              100.00     100.00     100.00       2201
-----weighted avg                                           100.00     100.00     100.00       2201
-----
-----[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.20       0.43       0.27       4226
-----! (label_id: 1)                                          0.44      14.17       0.86        127
-----, (label_id: 2)                                          1.93       0.49       0.78      14611
------ (label_id: 3)                                          2.23      19.56       4.01       1237
-----. (label_id: 4)                                         43.37      11.25      17.86      11977
-----: (label_id: 5)                                          0.68       2.34       1.05        342
-----; (label_id: 6)                                          0.00       0.00       0.00        129
-----? (label_id: 7)                                          5.16       7.47       6.10       1058
-----— (label_id: 8)                                          2.15       0.49       0.80        409
-----… (label_id: 9)                                          0.69       4.23       1.19         71
------------------------
-----micro avg                                                5.23       5.23       5.23      34187
-----macro avg                                                5.68       6.04       3.29      34187
-----weighted avg                                            16.32       5.23       6.98      34187
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2113
------------------------
-----micro avg                                              100.00     100.00     100.00       2113
-----macro avg                                              100.00     100.00     100.00       2113
-----weighted avg                                           100.00     100.00     100.00       2113
-----
-----[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.61       0.39       4228
-----! (label_id: 1)                                          0.30       8.28       0.58        145
-----, (label_id: 2)                                          2.27       0.58       0.92      14495
------ (label_id: 3)                                          2.64      21.78       4.70       1327
-----. (label_id: 4)                                         44.87      11.66      18.51      12193
-----: (label_id: 5)                                          0.60       1.93       0.91        362
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.19       6.40       5.07       1078
-----— (label_id: 8)                                          1.16       0.22       0.37        459
-----… (label_id: 9)                                          0.85       4.17       1.41         96
------------------------
-----micro avg                                                5.54       5.54       5.54      34547
-----macro avg                                                5.72       5.56       3.29      34547
-----weighted avg                                            17.08       5.54       7.33      34547
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2114
------------------------
-----micro avg                                              100.00     100.00     100.00       2114
-----macro avg                                              100.00     100.00     100.00       2114
-----weighted avg                                           100.00     100.00     100.00       2114
-----
-----[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.29       0.63       0.40       4444
-----! (label_id: 1)                                          0.38      10.67       0.74        150
-----, (label_id: 2)                                          2.32       0.59       0.94      15290
------ (label_id: 3)                                          2.34      20.28       4.19       1292
-----. (label_id: 4)                                         43.85      11.68      18.44      12599
-----: (label_id: 5)                                          0.41       1.28       0.62        392
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.24       6.30       5.07       1111
-----— (label_id: 8)                                          0.00       0.00       0.00        456
-----… (label_id: 9)                                          0.38       2.41       0.65         83
------------------------
-----micro avg                                                5.40       5.40       5.40      35981
-----macro avg                                                5.42       5.38       3.11      35981
-----weighted avg                                            16.59       5.40       7.22      35981
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2222
------------------------
-----micro avg                                              100.00     100.00     100.00       2222
-----macro avg                                              100.00     100.00     100.00       2222
-----weighted avg                                           100.00     100.00     100.00       2222
-----
-----[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.35       0.73       0.48       3844
-----! (label_id: 1)                                          0.54      14.62       1.04        130
-----, (label_id: 2)                                          2.32       0.59       0.94      13056
------ (label_id: 3)                                          2.67      22.28       4.77       1194
-----. (label_id: 4)                                         44.45      11.95      18.84      10791
-----: (label_id: 5)                                          0.84       3.21       1.33        280
-----; (label_id: 6)                                          0.00       0.00       0.00        140
-----? (label_id: 7)                                          4.17       6.56       5.10        914
-----— (label_id: 8)                                          0.00       0.00       0.00        401
-----… (label_id: 9)                                          0.48       2.63       0.81         76
------------------------
-----micro avg                                                5.68       5.68       5.68      30826
-----macro avg                                                5.58       6.26       3.33      30826
-----weighted avg                                            16.82       5.68       7.41      30826
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1922
------------------------
-----micro avg                                              100.00     100.00     100.00       1922
-----macro avg                                              100.00     100.00     100.00       1922
-----weighted avg                                           100.00     100.00     100.00       1922
-----
-----[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.60       0.39       3970
-----! (label_id: 1)                                          0.35      10.66       0.68        122
-----, (label_id: 2)                                          2.09       0.53       0.85      13469
------ (label_id: 3)                                          2.29      19.32       4.10       1201
-----. (label_id: 4)                                         43.43      11.24      17.86      11227
-----: (label_id: 5)                                          0.63       2.30       0.99        304
-----; (label_id: 6)                                          0.00       0.00       0.00        141
-----? (label_id: 7)                                          4.52       6.86       5.45       1006
-----— (label_id: 8)                                          1.15       0.23       0.38        444
-----… (label_id: 9)                                          0.45       2.67       0.78         75
------------------------
-----micro avg                                                5.26       5.26       5.26      31959
-----macro avg                                                5.52       5.44       3.15      31959
-----weighted avg                                            16.42       5.26       7.02      31959
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1985
------------------------
-----micro avg                                              100.00     100.00     100.00       1985
-----macro avg                                              100.00     100.00     100.00       1985
-----weighted avg                                           100.00     100.00     100.00       1985
-----
-----[INFO] - Epoch 5, step 2394: val_loss was not in top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.23       0.48       0.31       4126
-----! (label_id: 1)                                          0.29       9.40       0.56        117
-----, (label_id: 2)                                          1.91       0.49       0.77      14019
------ (label_id: 3)                                          2.52      22.59       4.53       1164
-----. (label_id: 4)                                         44.15      11.65      18.44      11789
-----: (label_id: 5)                                          0.72       2.41       1.11        332
-----; (label_id: 6)                                          0.56       0.61       0.58        165
-----? (label_id: 7)                                          3.89       6.53       4.88        980
-----— (label_id: 8)                                          2.30       0.47       0.77        430
-----… (label_id: 9)                                          1.18       8.33       2.07         60
------------------------
-----micro avg                                                5.47       5.47       5.47      33182
-----macro avg                                                5.77       6.30       3.40      33182
-----weighted avg                                            16.77       5.47       7.25      33182
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2063
------------------------
-----micro avg                                              100.00     100.00     100.00       2063
-----macro avg                                              100.00     100.00     100.00       2063
-----weighted avg                                           100.00     100.00     100.00       2063
-----
-----[INFO] - Epoch 6, step 2793: val_loss was not in top 3
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index fa37b4c..6b5ff89 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             self.hparams.model.punct_class_weights=None
---- 
----+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
----+        self.mlp = MultiLayerPerceptron(
----+            self.transformer.config.hidden_size,
----+            self.transformer.config.hidden_size,
----+            num_layers=self.hparams.model.mlp.num_fc_layers, 
----+            activation=self.hparams.model.mlp.activation, 
----+            log_softmax=self.hparams.model.mlp.log_softmax
----+        )
----+
----         self.punct_classifier = TokenClassifier(
----             hidden_size=self.transformer.config.hidden_size,
----             num_classes=len(self.labels_to_ids),
----@@ -80,6 +89,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----             log_softmax=self.hparams.model.domain_head.log_softmax,
----             dropout=self.hparams.model.domain_head.fc_dropout,
----             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
----+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
----         )
---- 
----         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
----@@ -119,14 +129,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
----         self.freeze()
---- 
-----    def forward(self, input_ids, attention_mask, domain_ids=None):
----+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
----         hidden_states = self.transformer(
----             input_ids=input_ids, attention_mask=attention_mask
----         )[0]
----+        hidden_states = self.dropout(hidden_states)
----+        hidden_states = self.mlp(hidden_states)
----         punct_logits = self.punct_classifier(hidden_states=hidden_states)
----         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
----         domain_logits = self.domain_classifier(
-----            hidden_states=reverse_grad_hidden_states)
----+            hidden_states=reverse_grad_hidden_states,
----+            subtoken_mask=subtoken_mask)
----         return punct_logits, domain_logits
---- 
----     def _make_step(self, batch):
----@@ -136,7 +149,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         punct_labels = batch['labels']
----         domain_labels = batch['domain']
----         punct_logits, domain_logits = self(
-----            input_ids=input_ids, attention_mask=attention_mask
----+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
----         )
----         punctuation_loss = self.punctuation_loss(
----             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/lightning_logs.txt
---deleted file mode 100644
---index 8942cad..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/lightning_logs.txt
---+++ /dev/null
---@@ -1,5 +0,0 @@
----Global seed set to 42
----GPU available: True, used: True
----TPU available: None, using: 0 TPU cores
----LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
----Using native 16bit precision.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/nemo_error_log.txt
---deleted file mode 100644
---index 135ccc7..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/nemo_error_log.txt
---+++ /dev/null
---@@ -1 +0,0 @@
----[NeMo W 2021-02-08 10:45:45 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index 809269b..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,3 +0,0 @@
----[NeMo I 2021-02-08 10:45:45 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-45-45
----[NeMo I 2021-02-08 10:45:45 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-08 10:45:45 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/git-info.log
---deleted file mode 100644
---index b80d6d5..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/git-info.log
---+++ /dev/null
---@@ -1,624 +0,0 @@
----commit hash: 939a671c8c117db6975316767ced5d95449e2b27
----diff --git a/README.md b/README.md
----index d52dba5..0b403d3 100644
------- a/README.md
----+++ b/README.md
----@@ -333,4 +333,71 @@ label                                                precision    recall       f
----  'punct_f1': 0.5858508944511414,
----  'punct_precision': 0.30176490545272827,
----  'punct_recall': 10.0,
----- 'test_loss': 0.8140875697135925}
----\ No newline at end of file
----+ 'test_loss': 0.8140875697135925}
----+
----+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
----+
----+ (label_id: 0)                                          62.15     100.00      76.66       5154
----+! (label_id: 1)                                          0.00       0.00       0.00        108
----+, (label_id: 2)                                          0.00       0.00       0.00      18022
----+- (label_id: 3)                                          0.00       0.00       0.00       1557
----+. (label_id: 4)                                         41.74      94.01      57.81      15164
----+: (label_id: 5)                                          0.00       0.00       0.00        319
----+; (label_id: 6)                                          0.00       0.00       0.00         88
----+? (label_id: 7)                                          0.00       0.00       0.00       1217
----+ (label_id: 8)                                          0.00       0.00       0.00        752
----+… (label_id: 9)                                          0.00       0.00       0.00         67
----+-------------------
----+micro avg                                               45.72      45.72      45.72      42448
----+macro avg                                               10.39      19.40      13.45      42448
----+weighted avg                                            22.46      45.72      29.96      42448
----+{ 'punct_f1': 13.446383476257324,
----+ 'punct_precision': 10.388500213623047,
----+ 'punct_recall': 19.400554656982422,
----+ 'test_loss': 0.44148480892181396}
----+
----+
----+ ## Log for 8/2/2021
----+
----+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
----+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
----+
----+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
----+End frozen 
----+micro avg                                               41.42      41.42      41.42      33406
----+macro avg                                               11.01      13.54      11.07      33406
----+weighted avg                                            34.88      41.42      34.20      33406
----+
----+1st layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               36.65      36.65      36.65      33463
----+macro avg                                               10.71       9.91       8.26      33463
----+weighted avg                                            34.32      36.65      31.08      33463
----+
----+2nd layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               35.72      35.72      35.72      42448
----+macro avg                                                3.57      10.00       5.26      42448
----+weighted avg                                            12.76      35.72      18.81      42448
----+
----+{'punct_f1': 5.264181137084961,
----+ 'punct_precision': 3.572371006011963,
----+ 'punct_recall': 10.0,
----+ 'test_loss': 18.49854850769043}
----+
----+
----+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
----+alpha from 3->4 seems to reduce convergence rate.
----+
----+micro avg                                               50.98      50.98      50.98      33463
----+macro avg                                               25.99      25.38      23.38      33463
----+weighted avg                                            50.31      50.98      48.27      33463
----+
----+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
----+micro avg                                               58.55      58.55      58.55      39340
----+macro avg                                               30.02      29.74      29.52      39340
----+weighted avg                                            57.91      58.55      57.51      39340
----+
----+still increasing?!
----+{'punct_f1': 29.523975372314453,
----+ 'punct_precision': 30.015613555908203,
----+ 'punct_recall': 29.738296508789062,
----+ 'test_loss': 0.3690211772918701}
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index fe58670..69c6f92 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,49 +1,49 @@
---- seed: 42
---- trainer:
-----    # gpus: 1 # the number of gpus, 0 for CPU
-----    # num_nodes: 1
-----    # max_epochs: 2
-----    # max_steps: null # precedence over max_epochs
-----    # accumulate_grad_batches: 4 # accumulates grads every k batches
-----    # gradient_clip_val: 0
-----    # amp_level: O1 # O1/O2 for mixed precision
-----    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
-----    # checkpoint_callback: false  # Provided by exp_manager
-----    # logger: false #false  # Provided by exp_manager
-----    # log_every_n_steps: 1  # Interval of logging.
-----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # resume_from_checkpoint: null
-----
-----    gpus: 0 # the number of gpus, 0 for CPU
----+    gpus: 1 # the number of gpus, 0 for CPU
----     num_nodes: 1
-----    max_epochs: 8
----+    max_epochs: 10
----     max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 1 # accumulates grads every k batches
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----     gradient_clip_val: 0
-----    amp_level: O0 # O1/O2 for mixed precision
-----    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
----+    amp_level: O1 # O1/O2 for mixed precision
----+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    accelerator: ddp
----     checkpoint_callback: false  # Provided by exp_manager
----     logger: false #false  # Provided by exp_manager
----     log_every_n_steps: 1  # Interval of logging.
----     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    reload_dataloaders_every_epoch: true
----     resume_from_checkpoint: null
---- 
----+    # gpus: 0 # the number of gpus, 0 for CPU
----+    # num_nodes: 1
----+    # max_epochs: 8
----+    # max_steps: null # precedence over max_epochs
----+    # accumulate_grad_batches: 1 # accumulates grads every k batches
----+    # gradient_clip_val: 0
----+    # amp_level: O0 # O1/O2 for mixed precision
----+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # # accelerator: ddp
----+    # checkpoint_callback: false  # Provided by exp_manager
----+    # logger: false #false  # Provided by exp_manager
----+    # log_every_n_steps: 1  # Interval of logging.
----+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    # reload_dataloaders_every_epoch: true
----+    # resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /home/nxingyu2/data # /root/data # 
-----tmp_path: /home/nxingyu2/data/tmp # /tmp # 
----+base_path: /home/nxingyu/data # /root/data # 
----+tmp_path: /home/nxingyu/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
-----    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----     unfrozen: 0
-----    maximum_unfrozen: 2
----+    maximum_unfrozen: 1
----     unfreeze_step: 1
----     # unfreeze_every: 3
----     punct_label_ids:
----@@ -58,10 +58,10 @@ model:
----         - "—"
----         - "…"
---- 
-----    punct_class_weights: true
----+    punct_class_weights: false
----     
----     dataset:
-----        data_dir: /home/nxingyu2/data # /root/data # 
----+        data_dir: /home/nxingyu/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -106,6 +106,12 @@ model:
----         config: null
----         # unfrozen_layers: 1
---- 
----+    mlp:
----+        num_fc_layers: 2
----+        fc_dropout: 0.1
----+        log_softmax: false
----+        activation: 'relu'
----+        
----     punct_head:
----         punct_num_fc_layers: 1
----         fc_dropout: 0.1
----@@ -117,22 +123,24 @@ model:
----     domain_head:
----         domain_num_fc_layers: 1
----         fc_dropout: 0.1
-----        activation: 'relu'
----+        activation: 'gelu'
----         log_softmax: false
----         use_transformer_init: true
----         loss: 'cel'
----         gamma: 0.1 # coefficient of gradient reversal
----+        pooling: 'token'
----+        idx_conditioned_on: 0
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 3
----+        alpha: 4
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: novograd
----+        name: adamw
----         lr: 1e-3
----         weight_decay: 0.00
----         sched:
----diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
----index d4ff927..b4b92b0 100644
------- a/experiment/core/layers/sequence_classifier.py
----+++ b/experiment/core/layers/sequence_classifier.py
----@@ -20,7 +20,8 @@ class SequenceClassifier(nn.Module):
----         log_softmax: bool = True,
----         dropout: float = 0.0,
----         use_transformer_init: bool = True,
-----        idx_conditioned_on: int = 0,
----+        pooling: str = 'mean', # mean, max, mean_max, token
----+        idx_conditioned_on: int = None,
----     ):
----         """
----         Initializes the SequenceClassifier module.
----@@ -37,6 +38,7 @@ class SequenceClassifier(nn.Module):
----         super().__init__()
----         self.log_softmax = log_softmax
----         self._idx_conditioned_on = idx_conditioned_on
----+        self.pooling = pooling
----         self.mlp = MultiLayerPerceptron(
----             hidden_size=hidden_size,
----             num_classes=num_classes,
----@@ -48,7 +50,23 @@ class SequenceClassifier(nn.Module):
----         if use_transformer_init:
----             self.apply(lambda module: transformer_weights_init(module, xavier=False))
---- 
-----    def forward(self, hidden_states):
----+    def forward(self, hidden_states, subtoken_mask=None):
----         hidden_states = self.dropout(hidden_states)
-----        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
----+        if self.pooling=='token':
----+            pooled = hidden_states[:, self._idx_conditioned_on]
----+        else:
----+            if subtoken_mask==None:
----+                ct=hidden_states.shape[1] # Seq len
----+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
----+            else:
----+                ct = torch.sum(subtoken_mask,axis=1)
----+            pooled_sum = torch.sum(hidden_states,axis=1)
----+            if self.pooling=='mean' or self.pooling == 'mean_max':
----+                pooled_mean = pooled_sum//ct
----+            if self.pooling=='max' or self.pooling=='mean_max':
----+                pooled_max = torch.max(hidden_states,axis=1)[0]
----+            pooled=pooled_mean if self.pooling=='mean' else \
----+                pooled_max if self.pooling=='max' else \
----+                    torch.cat([pooled_mean,pooled_max],axis=-1)
----+        logits = self.mlp(pooled)
----         return logits
----diff --git a/experiment/info.log b/experiment/info.log
----index 2471fe9..e69de29 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,300 +0,0 @@
-----[INFO] - GPU available: True, used: False
-----[INFO] - TPU available: None, using: 0 TPU cores
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        184
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.23       0.34       0.53        594
------ (label_id: 3)                                          3.06      25.42       5.46         59
-----. (label_id: 4)                                         47.22      12.98      20.36        524
-----: (label_id: 5)                                          0.00       0.00       0.00         18
-----; (label_id: 6)                                          0.00       0.00       0.00         13
-----? (label_id: 7)                                          8.45       6.32       7.23         95
-----— (label_id: 8)                                          0.00       0.00       0.00         12
-----… (label_id: 9)                                          0.00       0.00       0.00          0
------------------------
-----micro avg                                                6.05       6.05       6.05       1503
-----macro avg                                                6.66       5.01       3.73       1503
-----weighted avg                                            17.61       6.05       7.98       1503
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00         92
------------------------
-----micro avg                                              100.00     100.00     100.00         92
-----macro avg                                              100.00     100.00     100.00         92
-----weighted avg                                           100.00     100.00     100.00         92
-----
-----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 1.5848931924611143e-08
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
-----will be used during training (effective maximum steps = 3192) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 3192
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        202
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.62       0.45       0.70        669
------ (label_id: 3)                                          3.48      27.27       6.17         66
-----. (label_id: 4)                                         45.06      13.01      20.19        561
-----: (label_id: 5)                                          1.52       6.67       2.47         15
-----; (label_id: 6)                                          0.00       0.00       0.00         15
-----? (label_id: 7)                                          8.70       7.32       7.95         82
-----— (label_id: 8)                                          0.00       0.00       0.00         13
-----… (label_id: 9)                                          0.00       0.00       0.00          1
------------------------
-----micro avg                                                6.20       6.20       6.20       1628
-----macro avg                                                6.04       5.47       3.75       1628
-----weighted avg                                            16.79       6.20       7.92       1628
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        101
------------------------
-----micro avg                                              100.00     100.00     100.00        101
-----macro avg                                              100.00     100.00     100.00        101
-----weighted avg                                           100.00     100.00     100.00        101
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.34       0.73       0.46       4402
-----! (label_id: 1)                                          0.42      13.95       0.82        129
-----, (label_id: 2)                                          2.53       0.64       1.03      15243
------ (label_id: 3)                                          2.45      21.03       4.38       1322
-----. (label_id: 4)                                         44.00      11.40      18.11      12542
-----: (label_id: 5)                                          0.43       1.41       0.65        354
-----; (label_id: 6)                                          0.00       0.00       0.00        163
-----? (label_id: 7)                                          4.16       6.27       5.00       1117
-----— (label_id: 8)                                          3.00       0.61       1.02        488
-----… (label_id: 9)                                          0.97       6.17       1.68         81
------------------------
-----micro avg                                                5.41       5.41       5.41      35841
-----macro avg                                                5.83       6.22       3.32      35841
-----weighted avg                                            16.78       5.41       7.18      35841
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2201
------------------------
-----micro avg                                              100.00     100.00     100.00       2201
-----macro avg                                              100.00     100.00     100.00       2201
-----weighted avg                                           100.00     100.00     100.00       2201
-----
-----[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.20       0.43       0.27       4226
-----! (label_id: 1)                                          0.44      14.17       0.86        127
-----, (label_id: 2)                                          1.93       0.49       0.78      14611
------ (label_id: 3)                                          2.23      19.56       4.01       1237
-----. (label_id: 4)                                         43.37      11.25      17.86      11977
-----: (label_id: 5)                                          0.68       2.34       1.05        342
-----; (label_id: 6)                                          0.00       0.00       0.00        129
-----? (label_id: 7)                                          5.16       7.47       6.10       1058
-----— (label_id: 8)                                          2.15       0.49       0.80        409
-----… (label_id: 9)                                          0.69       4.23       1.19         71
------------------------
-----micro avg                                                5.23       5.23       5.23      34187
-----macro avg                                                5.68       6.04       3.29      34187
-----weighted avg                                            16.32       5.23       6.98      34187
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2113
------------------------
-----micro avg                                              100.00     100.00     100.00       2113
-----macro avg                                              100.00     100.00     100.00       2113
-----weighted avg                                           100.00     100.00     100.00       2113
-----
-----[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.61       0.39       4228
-----! (label_id: 1)                                          0.30       8.28       0.58        145
-----, (label_id: 2)                                          2.27       0.58       0.92      14495
------ (label_id: 3)                                          2.64      21.78       4.70       1327
-----. (label_id: 4)                                         44.87      11.66      18.51      12193
-----: (label_id: 5)                                          0.60       1.93       0.91        362
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.19       6.40       5.07       1078
-----— (label_id: 8)                                          1.16       0.22       0.37        459
-----… (label_id: 9)                                          0.85       4.17       1.41         96
------------------------
-----micro avg                                                5.54       5.54       5.54      34547
-----macro avg                                                5.72       5.56       3.29      34547
-----weighted avg                                            17.08       5.54       7.33      34547
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2114
------------------------
-----micro avg                                              100.00     100.00     100.00       2114
-----macro avg                                              100.00     100.00     100.00       2114
-----weighted avg                                           100.00     100.00     100.00       2114
-----
-----[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.29       0.63       0.40       4444
-----! (label_id: 1)                                          0.38      10.67       0.74        150
-----, (label_id: 2)                                          2.32       0.59       0.94      15290
------ (label_id: 3)                                          2.34      20.28       4.19       1292
-----. (label_id: 4)                                         43.85      11.68      18.44      12599
-----: (label_id: 5)                                          0.41       1.28       0.62        392
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.24       6.30       5.07       1111
-----— (label_id: 8)                                          0.00       0.00       0.00        456
-----… (label_id: 9)                                          0.38       2.41       0.65         83
------------------------
-----micro avg                                                5.40       5.40       5.40      35981
-----macro avg                                                5.42       5.38       3.11      35981
-----weighted avg                                            16.59       5.40       7.22      35981
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2222
------------------------
-----micro avg                                              100.00     100.00     100.00       2222
-----macro avg                                              100.00     100.00     100.00       2222
-----weighted avg                                           100.00     100.00     100.00       2222
-----
-----[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.35       0.73       0.48       3844
-----! (label_id: 1)                                          0.54      14.62       1.04        130
-----, (label_id: 2)                                          2.32       0.59       0.94      13056
------ (label_id: 3)                                          2.67      22.28       4.77       1194
-----. (label_id: 4)                                         44.45      11.95      18.84      10791
-----: (label_id: 5)                                          0.84       3.21       1.33        280
-----; (label_id: 6)                                          0.00       0.00       0.00        140
-----? (label_id: 7)                                          4.17       6.56       5.10        914
-----— (label_id: 8)                                          0.00       0.00       0.00        401
-----… (label_id: 9)                                          0.48       2.63       0.81         76
------------------------
-----micro avg                                                5.68       5.68       5.68      30826
-----macro avg                                                5.58       6.26       3.33      30826
-----weighted avg                                            16.82       5.68       7.41      30826
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1922
------------------------
-----micro avg                                              100.00     100.00     100.00       1922
-----macro avg                                              100.00     100.00     100.00       1922
-----weighted avg                                           100.00     100.00     100.00       1922
-----
-----[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.60       0.39       3970
-----! (label_id: 1)                                          0.35      10.66       0.68        122
-----, (label_id: 2)                                          2.09       0.53       0.85      13469
------ (label_id: 3)                                          2.29      19.32       4.10       1201
-----. (label_id: 4)                                         43.43      11.24      17.86      11227
-----: (label_id: 5)                                          0.63       2.30       0.99        304
-----; (label_id: 6)                                          0.00       0.00       0.00        141
-----? (label_id: 7)                                          4.52       6.86       5.45       1006
-----— (label_id: 8)                                          1.15       0.23       0.38        444
-----… (label_id: 9)                                          0.45       2.67       0.78         75
------------------------
-----micro avg                                                5.26       5.26       5.26      31959
-----macro avg                                                5.52       5.44       3.15      31959
-----weighted avg                                            16.42       5.26       7.02      31959
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1985
------------------------
-----micro avg                                              100.00     100.00     100.00       1985
-----macro avg                                              100.00     100.00     100.00       1985
-----weighted avg                                           100.00     100.00     100.00       1985
-----
-----[INFO] - Epoch 5, step 2394: val_loss was not in top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.23       0.48       0.31       4126
-----! (label_id: 1)                                          0.29       9.40       0.56        117
-----, (label_id: 2)                                          1.91       0.49       0.77      14019
------ (label_id: 3)                                          2.52      22.59       4.53       1164
-----. (label_id: 4)                                         44.15      11.65      18.44      11789
-----: (label_id: 5)                                          0.72       2.41       1.11        332
-----; (label_id: 6)                                          0.56       0.61       0.58        165
-----? (label_id: 7)                                          3.89       6.53       4.88        980
-----— (label_id: 8)                                          2.30       0.47       0.77        430
-----… (label_id: 9)                                          1.18       8.33       2.07         60
------------------------
-----micro avg                                                5.47       5.47       5.47      33182
-----macro avg                                                5.77       6.30       3.40      33182
-----weighted avg                                            16.77       5.47       7.25      33182
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2063
------------------------
-----micro avg                                              100.00     100.00     100.00       2063
-----macro avg                                              100.00     100.00     100.00       2063
-----weighted avg                                           100.00     100.00     100.00       2063
-----
-----[INFO] - Epoch 6, step 2793: val_loss was not in top 3
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index fa37b4c..6b5ff89 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             self.hparams.model.punct_class_weights=None
---- 
----+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
----+        self.mlp = MultiLayerPerceptron(
----+            self.transformer.config.hidden_size,
----+            self.transformer.config.hidden_size,
----+            num_layers=self.hparams.model.mlp.num_fc_layers, 
----+            activation=self.hparams.model.mlp.activation, 
----+            log_softmax=self.hparams.model.mlp.log_softmax
----+        )
----+
----         self.punct_classifier = TokenClassifier(
----             hidden_size=self.transformer.config.hidden_size,
----             num_classes=len(self.labels_to_ids),
----@@ -80,6 +89,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----             log_softmax=self.hparams.model.domain_head.log_softmax,
----             dropout=self.hparams.model.domain_head.fc_dropout,
----             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
----+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
----         )
---- 
----         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
----@@ -119,14 +129,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
----         self.freeze()
---- 
-----    def forward(self, input_ids, attention_mask, domain_ids=None):
----+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
----         hidden_states = self.transformer(
----             input_ids=input_ids, attention_mask=attention_mask
----         )[0]
----+        hidden_states = self.dropout(hidden_states)
----+        hidden_states = self.mlp(hidden_states)
----         punct_logits = self.punct_classifier(hidden_states=hidden_states)
----         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
----         domain_logits = self.domain_classifier(
-----            hidden_states=reverse_grad_hidden_states)
----+            hidden_states=reverse_grad_hidden_states,
----+            subtoken_mask=subtoken_mask)
----         return punct_logits, domain_logits
---- 
----     def _make_step(self, batch):
----@@ -136,7 +149,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         punct_labels = batch['labels']
----         domain_labels = batch['domain']
----         punct_logits, domain_logits = self(
-----            input_ids=input_ids, attention_mask=attention_mask
----+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
----         )
----         punctuation_loss = self.punctuation_loss(
----             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/lightning_logs.txt
---deleted file mode 100644
---index 9fa336d..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/lightning_logs.txt
---+++ /dev/null
---@@ -1,24 +0,0 @@
----Global seed set to 42
----GPU available: True, used: True
----TPU available: None, using: 0 TPU cores
----LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
----Using native 16bit precision.
----Global seed set to 42
----initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | dropout             | Dropout              | 0     
----2 | mlp                 | MultiLayerPerceptron | 131 K 
----3 | punct_classifier    | TokenClassifier      | 2.6 K 
----4 | domain_classifier   | SequenceClassifier   | 257   
----5 | punctuation_loss    | FocalDiceLoss        | 0     
----6 | domain_loss         | CrossEntropyLoss     | 0     
----7 | agg_loss            | AggregatorLoss       | 0     
----8 | punct_class_report  | ClassificationReport | 0     
----9 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----167 K     Trainable params
----13.4 M    Non-trainable params
----13.6 M    Total params
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/nemo_error_log.txt
---deleted file mode 100644
---index 97cb5eb..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/nemo_error_log.txt
---+++ /dev/null
---@@ -1,7 +0,0 @@
----[NeMo W 2021-02-08 10:46:07 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-08 10:46:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 10:46:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index d4331a1..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,9 +0,0 @@
----[NeMo I 2021-02-08 10:46:07 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-46-07
----[NeMo I 2021-02-08 10:46:07 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-08 10:46:07 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-08 10:46:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 10:46:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/git-info.log
---deleted file mode 100644
---index 10d12b7..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/git-info.log
---+++ /dev/null
---@@ -1,623 +0,0 @@
----commit hash: 939a671c8c117db6975316767ced5d95449e2b27
----diff --git a/README.md b/README.md
----index d52dba5..0b403d3 100644
------- a/README.md
----+++ b/README.md
----@@ -333,4 +333,71 @@ label                                                precision    recall       f
----  'punct_f1': 0.5858508944511414,
----  'punct_precision': 0.30176490545272827,
----  'punct_recall': 10.0,
----- 'test_loss': 0.8140875697135925}
----\ No newline at end of file
----+ 'test_loss': 0.8140875697135925}
----+
----+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
----+
----+ (label_id: 0)                                          62.15     100.00      76.66       5154
----+! (label_id: 1)                                          0.00       0.00       0.00        108
----+, (label_id: 2)                                          0.00       0.00       0.00      18022
----+- (label_id: 3)                                          0.00       0.00       0.00       1557
----+. (label_id: 4)                                         41.74      94.01      57.81      15164
----+: (label_id: 5)                                          0.00       0.00       0.00        319
----+; (label_id: 6)                                          0.00       0.00       0.00         88
----+? (label_id: 7)                                          0.00       0.00       0.00       1217
----+ (label_id: 8)                                          0.00       0.00       0.00        752
----+… (label_id: 9)                                          0.00       0.00       0.00         67
----+-------------------
----+micro avg                                               45.72      45.72      45.72      42448
----+macro avg                                               10.39      19.40      13.45      42448
----+weighted avg                                            22.46      45.72      29.96      42448
----+{ 'punct_f1': 13.446383476257324,
----+ 'punct_precision': 10.388500213623047,
----+ 'punct_recall': 19.400554656982422,
----+ 'test_loss': 0.44148480892181396}
----+
----+
----+ ## Log for 8/2/2021
----+
----+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
----+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
----+
----+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
----+End frozen 
----+micro avg                                               41.42      41.42      41.42      33406
----+macro avg                                               11.01      13.54      11.07      33406
----+weighted avg                                            34.88      41.42      34.20      33406
----+
----+1st layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               36.65      36.65      36.65      33463
----+macro avg                                               10.71       9.91       8.26      33463
----+weighted avg                                            34.32      36.65      31.08      33463
----+
----+2nd layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               35.72      35.72      35.72      42448
----+macro avg                                                3.57      10.00       5.26      42448
----+weighted avg                                            12.76      35.72      18.81      42448
----+
----+{'punct_f1': 5.264181137084961,
----+ 'punct_precision': 3.572371006011963,
----+ 'punct_recall': 10.0,
----+ 'test_loss': 18.49854850769043}
----+
----+
----+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
----+alpha from 3->4 seems to reduce convergence rate.
----+
----+micro avg                                               50.98      50.98      50.98      33463
----+macro avg                                               25.99      25.38      23.38      33463
----+weighted avg                                            50.31      50.98      48.27      33463
----+
----+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
----+micro avg                                               58.55      58.55      58.55      39340
----+macro avg                                               30.02      29.74      29.52      39340
----+weighted avg                                            57.91      58.55      57.51      39340
----+
----+still increasing?!
----+{'punct_f1': 29.523975372314453,
----+ 'punct_precision': 30.015613555908203,
----+ 'punct_recall': 29.738296508789062,
----+ 'test_loss': 0.3690211772918701}
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index fe58670..716a889 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,49 +1,49 @@
---- seed: 42
---- trainer:
-----    # gpus: 1 # the number of gpus, 0 for CPU
-----    # num_nodes: 1
-----    # max_epochs: 2
-----    # max_steps: null # precedence over max_epochs
-----    # accumulate_grad_batches: 4 # accumulates grads every k batches
-----    # gradient_clip_val: 0
-----    # amp_level: O1 # O1/O2 for mixed precision
-----    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
-----    # checkpoint_callback: false  # Provided by exp_manager
-----    # logger: false #false  # Provided by exp_manager
-----    # log_every_n_steps: 1  # Interval of logging.
-----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # resume_from_checkpoint: null
-----
-----    gpus: 0 # the number of gpus, 0 for CPU
----+    gpus: 1 # the number of gpus, 0 for CPU
----     num_nodes: 1
-----    max_epochs: 8
----+    max_epochs: 10
----     max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 1 # accumulates grads every k batches
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----     gradient_clip_val: 0
-----    amp_level: O0 # O1/O2 for mixed precision
-----    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
----+    amp_level: O1 # O1/O2 for mixed precision
----+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    accelerator: ddp
----     checkpoint_callback: false  # Provided by exp_manager
----     logger: false #false  # Provided by exp_manager
----     log_every_n_steps: 1  # Interval of logging.
----     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    reload_dataloaders_every_epoch: true
----     resume_from_checkpoint: null
---- 
----+    # gpus: 0 # the number of gpus, 0 for CPU
----+    # num_nodes: 1
----+    # max_epochs: 8
----+    # max_steps: null # precedence over max_epochs
----+    # accumulate_grad_batches: 1 # accumulates grads every k batches
----+    # gradient_clip_val: 0
----+    # amp_level: O0 # O1/O2 for mixed precision
----+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # # accelerator: ddp
----+    # checkpoint_callback: false  # Provided by exp_manager
----+    # logger: false #false  # Provided by exp_manager
----+    # log_every_n_steps: 1  # Interval of logging.
----+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    # reload_dataloaders_every_epoch: true
----+    # resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /home/nxingyu2/data # /root/data # 
-----tmp_path: /home/nxingyu2/data/tmp # /tmp # 
----+base_path: /home/nxingyu/data # /root/data # 
----+tmp_path: /home/nxingyu/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
-----    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----     unfrozen: 0
-----    maximum_unfrozen: 2
----+    maximum_unfrozen: 1
----     unfreeze_step: 1
----     # unfreeze_every: 3
----     punct_label_ids:
----@@ -58,10 +58,10 @@ model:
----         - "—"
----         - "…"
---- 
-----    punct_class_weights: true
----+    punct_class_weights: false
----     
----     dataset:
-----        data_dir: /home/nxingyu2/data # /root/data # 
----+        data_dir: /home/nxingyu/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -106,6 +106,12 @@ model:
----         config: null
----         # unfrozen_layers: 1
---- 
----+    mlp:
----+        num_fc_layers: 2
----+        fc_dropout: 0.1
----+        log_softmax: false
----+        activation: 'gelu'
----+        
----     punct_head:
----         punct_num_fc_layers: 1
----         fc_dropout: 0.1
----@@ -122,17 +128,19 @@ model:
----         use_transformer_init: true
----         loss: 'cel'
----         gamma: 0.1 # coefficient of gradient reversal
----+        pooling: 'token'
----+        idx_conditioned_on: 0
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 3
----+        alpha: 4
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: novograd
----+        name: adamw
----         lr: 1e-3
----         weight_decay: 0.00
----         sched:
----diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
----index d4ff927..1a6fe93 100644
------- a/experiment/core/layers/sequence_classifier.py
----+++ b/experiment/core/layers/sequence_classifier.py
----@@ -1,3 +1,4 @@
----+import torch
---- from torch import nn
---- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
---- from core.utils import transformer_weights_init
----@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
----         log_softmax: bool = True,
----         dropout: float = 0.0,
----         use_transformer_init: bool = True,
-----        idx_conditioned_on: int = 0,
----+        pooling: str = 'mean', # mean, max, mean_max, token
----+        idx_conditioned_on: int = None,
----     ):
----         """
----         Initializes the SequenceClassifier module.
----@@ -37,6 +39,7 @@ class SequenceClassifier(nn.Module):
----         super().__init__()
----         self.log_softmax = log_softmax
----         self._idx_conditioned_on = idx_conditioned_on
----+        self.pooling = pooling
----         self.mlp = MultiLayerPerceptron(
----             hidden_size=hidden_size,
----             num_classes=num_classes,
----@@ -48,7 +51,23 @@ class SequenceClassifier(nn.Module):
----         if use_transformer_init:
----             self.apply(lambda module: transformer_weights_init(module, xavier=False))
---- 
-----    def forward(self, hidden_states):
----+    def forward(self, hidden_states, subtoken_mask=None):
----         hidden_states = self.dropout(hidden_states)
-----        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
----+        if self.pooling=='token':
----+            pooled = hidden_states[:, self._idx_conditioned_on]
----+        else:
----+            if subtoken_mask==None:
----+                ct=hidden_states.shape[1] # Seq len
----+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
----+            else:
----+                ct = torch.sum(subtoken_mask,axis=1)
----+            pooled_sum = torch.sum(hidden_states,axis=1)
----+            if self.pooling=='mean' or self.pooling == 'mean_max':
----+                pooled_mean = pooled_sum//ct
----+            if self.pooling=='max' or self.pooling=='mean_max':
----+                pooled_max = torch.max(hidden_states,axis=1)[0]
----+            pooled=pooled_mean if self.pooling=='mean' else \
----+                pooled_max if self.pooling=='max' else \
----+                    torch.cat([pooled_mean,pooled_max],axis=-1)
----+        logits = self.mlp(pooled)
----         return logits
----diff --git a/experiment/info.log b/experiment/info.log
----index 2471fe9..e69de29 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,300 +0,0 @@
-----[INFO] - GPU available: True, used: False
-----[INFO] - TPU available: None, using: 0 TPU cores
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        184
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.23       0.34       0.53        594
------ (label_id: 3)                                          3.06      25.42       5.46         59
-----. (label_id: 4)                                         47.22      12.98      20.36        524
-----: (label_id: 5)                                          0.00       0.00       0.00         18
-----; (label_id: 6)                                          0.00       0.00       0.00         13
-----? (label_id: 7)                                          8.45       6.32       7.23         95
-----— (label_id: 8)                                          0.00       0.00       0.00         12
-----… (label_id: 9)                                          0.00       0.00       0.00          0
------------------------
-----micro avg                                                6.05       6.05       6.05       1503
-----macro avg                                                6.66       5.01       3.73       1503
-----weighted avg                                            17.61       6.05       7.98       1503
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00         92
------------------------
-----micro avg                                              100.00     100.00     100.00         92
-----macro avg                                              100.00     100.00     100.00         92
-----weighted avg                                           100.00     100.00     100.00         92
-----
-----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 1.5848931924611143e-08
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
-----will be used during training (effective maximum steps = 3192) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 3192
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        202
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.62       0.45       0.70        669
------ (label_id: 3)                                          3.48      27.27       6.17         66
-----. (label_id: 4)                                         45.06      13.01      20.19        561
-----: (label_id: 5)                                          1.52       6.67       2.47         15
-----; (label_id: 6)                                          0.00       0.00       0.00         15
-----? (label_id: 7)                                          8.70       7.32       7.95         82
-----— (label_id: 8)                                          0.00       0.00       0.00         13
-----… (label_id: 9)                                          0.00       0.00       0.00          1
------------------------
-----micro avg                                                6.20       6.20       6.20       1628
-----macro avg                                                6.04       5.47       3.75       1628
-----weighted avg                                            16.79       6.20       7.92       1628
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        101
------------------------
-----micro avg                                              100.00     100.00     100.00        101
-----macro avg                                              100.00     100.00     100.00        101
-----weighted avg                                           100.00     100.00     100.00        101
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.34       0.73       0.46       4402
-----! (label_id: 1)                                          0.42      13.95       0.82        129
-----, (label_id: 2)                                          2.53       0.64       1.03      15243
------ (label_id: 3)                                          2.45      21.03       4.38       1322
-----. (label_id: 4)                                         44.00      11.40      18.11      12542
-----: (label_id: 5)                                          0.43       1.41       0.65        354
-----; (label_id: 6)                                          0.00       0.00       0.00        163
-----? (label_id: 7)                                          4.16       6.27       5.00       1117
-----— (label_id: 8)                                          3.00       0.61       1.02        488
-----… (label_id: 9)                                          0.97       6.17       1.68         81
------------------------
-----micro avg                                                5.41       5.41       5.41      35841
-----macro avg                                                5.83       6.22       3.32      35841
-----weighted avg                                            16.78       5.41       7.18      35841
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2201
------------------------
-----micro avg                                              100.00     100.00     100.00       2201
-----macro avg                                              100.00     100.00     100.00       2201
-----weighted avg                                           100.00     100.00     100.00       2201
-----
-----[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.20       0.43       0.27       4226
-----! (label_id: 1)                                          0.44      14.17       0.86        127
-----, (label_id: 2)                                          1.93       0.49       0.78      14611
------ (label_id: 3)                                          2.23      19.56       4.01       1237
-----. (label_id: 4)                                         43.37      11.25      17.86      11977
-----: (label_id: 5)                                          0.68       2.34       1.05        342
-----; (label_id: 6)                                          0.00       0.00       0.00        129
-----? (label_id: 7)                                          5.16       7.47       6.10       1058
-----— (label_id: 8)                                          2.15       0.49       0.80        409
-----… (label_id: 9)                                          0.69       4.23       1.19         71
------------------------
-----micro avg                                                5.23       5.23       5.23      34187
-----macro avg                                                5.68       6.04       3.29      34187
-----weighted avg                                            16.32       5.23       6.98      34187
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2113
------------------------
-----micro avg                                              100.00     100.00     100.00       2113
-----macro avg                                              100.00     100.00     100.00       2113
-----weighted avg                                           100.00     100.00     100.00       2113
-----
-----[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.61       0.39       4228
-----! (label_id: 1)                                          0.30       8.28       0.58        145
-----, (label_id: 2)                                          2.27       0.58       0.92      14495
------ (label_id: 3)                                          2.64      21.78       4.70       1327
-----. (label_id: 4)                                         44.87      11.66      18.51      12193
-----: (label_id: 5)                                          0.60       1.93       0.91        362
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.19       6.40       5.07       1078
-----— (label_id: 8)                                          1.16       0.22       0.37        459
-----… (label_id: 9)                                          0.85       4.17       1.41         96
------------------------
-----micro avg                                                5.54       5.54       5.54      34547
-----macro avg                                                5.72       5.56       3.29      34547
-----weighted avg                                            17.08       5.54       7.33      34547
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2114
------------------------
-----micro avg                                              100.00     100.00     100.00       2114
-----macro avg                                              100.00     100.00     100.00       2114
-----weighted avg                                           100.00     100.00     100.00       2114
-----
-----[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.29       0.63       0.40       4444
-----! (label_id: 1)                                          0.38      10.67       0.74        150
-----, (label_id: 2)                                          2.32       0.59       0.94      15290
------ (label_id: 3)                                          2.34      20.28       4.19       1292
-----. (label_id: 4)                                         43.85      11.68      18.44      12599
-----: (label_id: 5)                                          0.41       1.28       0.62        392
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.24       6.30       5.07       1111
-----— (label_id: 8)                                          0.00       0.00       0.00        456
-----… (label_id: 9)                                          0.38       2.41       0.65         83
------------------------
-----micro avg                                                5.40       5.40       5.40      35981
-----macro avg                                                5.42       5.38       3.11      35981
-----weighted avg                                            16.59       5.40       7.22      35981
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2222
------------------------
-----micro avg                                              100.00     100.00     100.00       2222
-----macro avg                                              100.00     100.00     100.00       2222
-----weighted avg                                           100.00     100.00     100.00       2222
-----
-----[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.35       0.73       0.48       3844
-----! (label_id: 1)                                          0.54      14.62       1.04        130
-----, (label_id: 2)                                          2.32       0.59       0.94      13056
------ (label_id: 3)                                          2.67      22.28       4.77       1194
-----. (label_id: 4)                                         44.45      11.95      18.84      10791
-----: (label_id: 5)                                          0.84       3.21       1.33        280
-----; (label_id: 6)                                          0.00       0.00       0.00        140
-----? (label_id: 7)                                          4.17       6.56       5.10        914
-----— (label_id: 8)                                          0.00       0.00       0.00        401
-----… (label_id: 9)                                          0.48       2.63       0.81         76
------------------------
-----micro avg                                                5.68       5.68       5.68      30826
-----macro avg                                                5.58       6.26       3.33      30826
-----weighted avg                                            16.82       5.68       7.41      30826
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1922
------------------------
-----micro avg                                              100.00     100.00     100.00       1922
-----macro avg                                              100.00     100.00     100.00       1922
-----weighted avg                                           100.00     100.00     100.00       1922
-----
-----[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.60       0.39       3970
-----! (label_id: 1)                                          0.35      10.66       0.68        122
-----, (label_id: 2)                                          2.09       0.53       0.85      13469
------ (label_id: 3)                                          2.29      19.32       4.10       1201
-----. (label_id: 4)                                         43.43      11.24      17.86      11227
-----: (label_id: 5)                                          0.63       2.30       0.99        304
-----; (label_id: 6)                                          0.00       0.00       0.00        141
-----? (label_id: 7)                                          4.52       6.86       5.45       1006
-----— (label_id: 8)                                          1.15       0.23       0.38        444
-----… (label_id: 9)                                          0.45       2.67       0.78         75
------------------------
-----micro avg                                                5.26       5.26       5.26      31959
-----macro avg                                                5.52       5.44       3.15      31959
-----weighted avg                                            16.42       5.26       7.02      31959
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1985
------------------------
-----micro avg                                              100.00     100.00     100.00       1985
-----macro avg                                              100.00     100.00     100.00       1985
-----weighted avg                                           100.00     100.00     100.00       1985
-----
-----[INFO] - Epoch 5, step 2394: val_loss was not in top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.23       0.48       0.31       4126
-----! (label_id: 1)                                          0.29       9.40       0.56        117
-----, (label_id: 2)                                          1.91       0.49       0.77      14019
------ (label_id: 3)                                          2.52      22.59       4.53       1164
-----. (label_id: 4)                                         44.15      11.65      18.44      11789
-----: (label_id: 5)                                          0.72       2.41       1.11        332
-----; (label_id: 6)                                          0.56       0.61       0.58        165
-----? (label_id: 7)                                          3.89       6.53       4.88        980
-----— (label_id: 8)                                          2.30       0.47       0.77        430
-----… (label_id: 9)                                          1.18       8.33       2.07         60
------------------------
-----micro avg                                                5.47       5.47       5.47      33182
-----macro avg                                                5.77       6.30       3.40      33182
-----weighted avg                                            16.77       5.47       7.25      33182
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2063
------------------------
-----micro avg                                              100.00     100.00     100.00       2063
-----macro avg                                              100.00     100.00     100.00       2063
-----weighted avg                                           100.00     100.00     100.00       2063
-----
-----[INFO] - Epoch 6, step 2793: val_loss was not in top 3
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index fa37b4c..6b5ff89 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             self.hparams.model.punct_class_weights=None
---- 
----+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
----+        self.mlp = MultiLayerPerceptron(
----+            self.transformer.config.hidden_size,
----+            self.transformer.config.hidden_size,
----+            num_layers=self.hparams.model.mlp.num_fc_layers, 
----+            activation=self.hparams.model.mlp.activation, 
----+            log_softmax=self.hparams.model.mlp.log_softmax
----+        )
----+
----         self.punct_classifier = TokenClassifier(
----             hidden_size=self.transformer.config.hidden_size,
----             num_classes=len(self.labels_to_ids),
----@@ -80,6 +89,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----             log_softmax=self.hparams.model.domain_head.log_softmax,
----             dropout=self.hparams.model.domain_head.fc_dropout,
----             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
----+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
----         )
---- 
----         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
----@@ -119,14 +129,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
----         self.freeze()
---- 
-----    def forward(self, input_ids, attention_mask, domain_ids=None):
----+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
----         hidden_states = self.transformer(
----             input_ids=input_ids, attention_mask=attention_mask
----         )[0]
----+        hidden_states = self.dropout(hidden_states)
----+        hidden_states = self.mlp(hidden_states)
----         punct_logits = self.punct_classifier(hidden_states=hidden_states)
----         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
----         domain_logits = self.domain_classifier(
-----            hidden_states=reverse_grad_hidden_states)
----+            hidden_states=reverse_grad_hidden_states,
----+            subtoken_mask=subtoken_mask)
----         return punct_logits, domain_logits
---- 
----     def _make_step(self, batch):
----@@ -136,7 +149,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         punct_labels = batch['labels']
----         domain_labels = batch['domain']
----         punct_logits, domain_logits = self(
-----            input_ids=input_ids, attention_mask=attention_mask
----+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
----         )
----         punctuation_loss = self.punctuation_loss(
----             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/lightning_logs.txt
---deleted file mode 100644
---index 8942cad..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/lightning_logs.txt
---+++ /dev/null
---@@ -1,5 +0,0 @@
----Global seed set to 42
----GPU available: True, used: True
----TPU available: None, using: 0 TPU cores
----LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
----Using native 16bit precision.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/nemo_error_log.txt
---deleted file mode 100644
---index a6f4130..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/nemo_error_log.txt
---+++ /dev/null
---@@ -1 +0,0 @@
----[NeMo W 2021-02-08 10:47:32 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index 61a48c2..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,3 +0,0 @@
----[NeMo I 2021-02-08 10:47:32 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-47-32
----[NeMo I 2021-02-08 10:47:32 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-08 10:47:32 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/git-info.log
---deleted file mode 100644
---index 4d1615a..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/git-info.log
---+++ /dev/null
---@@ -1,623 +0,0 @@
----commit hash: 939a671c8c117db6975316767ced5d95449e2b27
----diff --git a/README.md b/README.md
----index d52dba5..0b403d3 100644
------- a/README.md
----+++ b/README.md
----@@ -333,4 +333,71 @@ label                                                precision    recall       f
----  'punct_f1': 0.5858508944511414,
----  'punct_precision': 0.30176490545272827,
----  'punct_recall': 10.0,
----- 'test_loss': 0.8140875697135925}
----\ No newline at end of file
----+ 'test_loss': 0.8140875697135925}
----+
----+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
----+
----+ (label_id: 0)                                          62.15     100.00      76.66       5154
----+! (label_id: 1)                                          0.00       0.00       0.00        108
----+, (label_id: 2)                                          0.00       0.00       0.00      18022
----+- (label_id: 3)                                          0.00       0.00       0.00       1557
----+. (label_id: 4)                                         41.74      94.01      57.81      15164
----+: (label_id: 5)                                          0.00       0.00       0.00        319
----+; (label_id: 6)                                          0.00       0.00       0.00         88
----+? (label_id: 7)                                          0.00       0.00       0.00       1217
----+ (label_id: 8)                                          0.00       0.00       0.00        752
----+… (label_id: 9)                                          0.00       0.00       0.00         67
----+-------------------
----+micro avg                                               45.72      45.72      45.72      42448
----+macro avg                                               10.39      19.40      13.45      42448
----+weighted avg                                            22.46      45.72      29.96      42448
----+{ 'punct_f1': 13.446383476257324,
----+ 'punct_precision': 10.388500213623047,
----+ 'punct_recall': 19.400554656982422,
----+ 'test_loss': 0.44148480892181396}
----+
----+
----+ ## Log for 8/2/2021
----+
----+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
----+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
----+
----+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
----+End frozen 
----+micro avg                                               41.42      41.42      41.42      33406
----+macro avg                                               11.01      13.54      11.07      33406
----+weighted avg                                            34.88      41.42      34.20      33406
----+
----+1st layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               36.65      36.65      36.65      33463
----+macro avg                                               10.71       9.91       8.26      33463
----+weighted avg                                            34.32      36.65      31.08      33463
----+
----+2nd layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               35.72      35.72      35.72      42448
----+macro avg                                                3.57      10.00       5.26      42448
----+weighted avg                                            12.76      35.72      18.81      42448
----+
----+{'punct_f1': 5.264181137084961,
----+ 'punct_precision': 3.572371006011963,
----+ 'punct_recall': 10.0,
----+ 'test_loss': 18.49854850769043}
----+
----+
----+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
----+alpha from 3->4 seems to reduce convergence rate.
----+
----+micro avg                                               50.98      50.98      50.98      33463
----+macro avg                                               25.99      25.38      23.38      33463
----+weighted avg                                            50.31      50.98      48.27      33463
----+
----+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
----+micro avg                                               58.55      58.55      58.55      39340
----+macro avg                                               30.02      29.74      29.52      39340
----+weighted avg                                            57.91      58.55      57.51      39340
----+
----+still increasing?!
----+{'punct_f1': 29.523975372314453,
----+ 'punct_precision': 30.015613555908203,
----+ 'punct_recall': 29.738296508789062,
----+ 'test_loss': 0.3690211772918701}
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index fe58670..ab37d69 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,49 +1,49 @@
---- seed: 42
---- trainer:
-----    # gpus: 1 # the number of gpus, 0 for CPU
-----    # num_nodes: 1
-----    # max_epochs: 2
-----    # max_steps: null # precedence over max_epochs
-----    # accumulate_grad_batches: 4 # accumulates grads every k batches
-----    # gradient_clip_val: 0
-----    # amp_level: O1 # O1/O2 for mixed precision
-----    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
-----    # checkpoint_callback: false  # Provided by exp_manager
-----    # logger: false #false  # Provided by exp_manager
-----    # log_every_n_steps: 1  # Interval of logging.
-----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # resume_from_checkpoint: null
-----
-----    gpus: 0 # the number of gpus, 0 for CPU
----+    gpus: 1 # the number of gpus, 0 for CPU
----     num_nodes: 1
-----    max_epochs: 8
----+    max_epochs: 10
----     max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 1 # accumulates grads every k batches
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----     gradient_clip_val: 0
-----    amp_level: O0 # O1/O2 for mixed precision
-----    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
----+    amp_level: O1 # O1/O2 for mixed precision
----+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    accelerator: ddp
----     checkpoint_callback: false  # Provided by exp_manager
----     logger: false #false  # Provided by exp_manager
----     log_every_n_steps: 1  # Interval of logging.
----     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    reload_dataloaders_every_epoch: true
----     resume_from_checkpoint: null
---- 
----+    # gpus: 0 # the number of gpus, 0 for CPU
----+    # num_nodes: 1
----+    # max_epochs: 8
----+    # max_steps: null # precedence over max_epochs
----+    # accumulate_grad_batches: 1 # accumulates grads every k batches
----+    # gradient_clip_val: 0
----+    # amp_level: O0 # O1/O2 for mixed precision
----+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # # accelerator: ddp
----+    # checkpoint_callback: false  # Provided by exp_manager
----+    # logger: false #false  # Provided by exp_manager
----+    # log_every_n_steps: 1  # Interval of logging.
----+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    # reload_dataloaders_every_epoch: true
----+    # resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /home/nxingyu2/data # /root/data # 
-----tmp_path: /home/nxingyu2/data/tmp # /tmp # 
----+base_path: /home/nxingyu/data # /root/data # 
----+tmp_path: /home/nxingyu/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
-----    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----     unfrozen: 0
-----    maximum_unfrozen: 2
----+    maximum_unfrozen: 1
----     unfreeze_step: 1
----     # unfreeze_every: 3
----     punct_label_ids:
----@@ -58,10 +58,10 @@ model:
----         - "—"
----         - "…"
---- 
-----    punct_class_weights: true
----+    punct_class_weights: false
----     
----     dataset:
-----        data_dir: /home/nxingyu2/data # /root/data # 
----+        data_dir: /home/nxingyu/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -106,6 +106,12 @@ model:
----         config: null
----         # unfrozen_layers: 1
---- 
----+    mlp:
----+        num_fc_layers: 2
----+        fc_dropout: 0.1
----+        log_softmax: false
----+        activation: 'GELU'
----+        
----     punct_head:
----         punct_num_fc_layers: 1
----         fc_dropout: 0.1
----@@ -122,17 +128,19 @@ model:
----         use_transformer_init: true
----         loss: 'cel'
----         gamma: 0.1 # coefficient of gradient reversal
----+        pooling: 'token'
----+        idx_conditioned_on: 0
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 3
----+        alpha: 4
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: novograd
----+        name: adamw
----         lr: 1e-3
----         weight_decay: 0.00
----         sched:
----diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
----index d4ff927..1a6fe93 100644
------- a/experiment/core/layers/sequence_classifier.py
----+++ b/experiment/core/layers/sequence_classifier.py
----@@ -1,3 +1,4 @@
----+import torch
---- from torch import nn
---- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
---- from core.utils import transformer_weights_init
----@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
----         log_softmax: bool = True,
----         dropout: float = 0.0,
----         use_transformer_init: bool = True,
-----        idx_conditioned_on: int = 0,
----+        pooling: str = 'mean', # mean, max, mean_max, token
----+        idx_conditioned_on: int = None,
----     ):
----         """
----         Initializes the SequenceClassifier module.
----@@ -37,6 +39,7 @@ class SequenceClassifier(nn.Module):
----         super().__init__()
----         self.log_softmax = log_softmax
----         self._idx_conditioned_on = idx_conditioned_on
----+        self.pooling = pooling
----         self.mlp = MultiLayerPerceptron(
----             hidden_size=hidden_size,
----             num_classes=num_classes,
----@@ -48,7 +51,23 @@ class SequenceClassifier(nn.Module):
----         if use_transformer_init:
----             self.apply(lambda module: transformer_weights_init(module, xavier=False))
---- 
-----    def forward(self, hidden_states):
----+    def forward(self, hidden_states, subtoken_mask=None):
----         hidden_states = self.dropout(hidden_states)
-----        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
----+        if self.pooling=='token':
----+            pooled = hidden_states[:, self._idx_conditioned_on]
----+        else:
----+            if subtoken_mask==None:
----+                ct=hidden_states.shape[1] # Seq len
----+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
----+            else:
----+                ct = torch.sum(subtoken_mask,axis=1)
----+            pooled_sum = torch.sum(hidden_states,axis=1)
----+            if self.pooling=='mean' or self.pooling == 'mean_max':
----+                pooled_mean = pooled_sum//ct
----+            if self.pooling=='max' or self.pooling=='mean_max':
----+                pooled_max = torch.max(hidden_states,axis=1)[0]
----+            pooled=pooled_mean if self.pooling=='mean' else \
----+                pooled_max if self.pooling=='max' else \
----+                    torch.cat([pooled_mean,pooled_max],axis=-1)
----+        logits = self.mlp(pooled)
----         return logits
----diff --git a/experiment/info.log b/experiment/info.log
----index 2471fe9..e69de29 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,300 +0,0 @@
-----[INFO] - GPU available: True, used: False
-----[INFO] - TPU available: None, using: 0 TPU cores
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        184
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.23       0.34       0.53        594
------ (label_id: 3)                                          3.06      25.42       5.46         59
-----. (label_id: 4)                                         47.22      12.98      20.36        524
-----: (label_id: 5)                                          0.00       0.00       0.00         18
-----; (label_id: 6)                                          0.00       0.00       0.00         13
-----? (label_id: 7)                                          8.45       6.32       7.23         95
-----— (label_id: 8)                                          0.00       0.00       0.00         12
-----… (label_id: 9)                                          0.00       0.00       0.00          0
------------------------
-----micro avg                                                6.05       6.05       6.05       1503
-----macro avg                                                6.66       5.01       3.73       1503
-----weighted avg                                            17.61       6.05       7.98       1503
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00         92
------------------------
-----micro avg                                              100.00     100.00     100.00         92
-----macro avg                                              100.00     100.00     100.00         92
-----weighted avg                                           100.00     100.00     100.00         92
-----
-----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 1.5848931924611143e-08
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
-----will be used during training (effective maximum steps = 3192) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 3192
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        202
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.62       0.45       0.70        669
------ (label_id: 3)                                          3.48      27.27       6.17         66
-----. (label_id: 4)                                         45.06      13.01      20.19        561
-----: (label_id: 5)                                          1.52       6.67       2.47         15
-----; (label_id: 6)                                          0.00       0.00       0.00         15
-----? (label_id: 7)                                          8.70       7.32       7.95         82
-----— (label_id: 8)                                          0.00       0.00       0.00         13
-----… (label_id: 9)                                          0.00       0.00       0.00          1
------------------------
-----micro avg                                                6.20       6.20       6.20       1628
-----macro avg                                                6.04       5.47       3.75       1628
-----weighted avg                                            16.79       6.20       7.92       1628
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        101
------------------------
-----micro avg                                              100.00     100.00     100.00        101
-----macro avg                                              100.00     100.00     100.00        101
-----weighted avg                                           100.00     100.00     100.00        101
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.34       0.73       0.46       4402
-----! (label_id: 1)                                          0.42      13.95       0.82        129
-----, (label_id: 2)                                          2.53       0.64       1.03      15243
------ (label_id: 3)                                          2.45      21.03       4.38       1322
-----. (label_id: 4)                                         44.00      11.40      18.11      12542
-----: (label_id: 5)                                          0.43       1.41       0.65        354
-----; (label_id: 6)                                          0.00       0.00       0.00        163
-----? (label_id: 7)                                          4.16       6.27       5.00       1117
-----— (label_id: 8)                                          3.00       0.61       1.02        488
-----… (label_id: 9)                                          0.97       6.17       1.68         81
------------------------
-----micro avg                                                5.41       5.41       5.41      35841
-----macro avg                                                5.83       6.22       3.32      35841
-----weighted avg                                            16.78       5.41       7.18      35841
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2201
------------------------
-----micro avg                                              100.00     100.00     100.00       2201
-----macro avg                                              100.00     100.00     100.00       2201
-----weighted avg                                           100.00     100.00     100.00       2201
-----
-----[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.20       0.43       0.27       4226
-----! (label_id: 1)                                          0.44      14.17       0.86        127
-----, (label_id: 2)                                          1.93       0.49       0.78      14611
------ (label_id: 3)                                          2.23      19.56       4.01       1237
-----. (label_id: 4)                                         43.37      11.25      17.86      11977
-----: (label_id: 5)                                          0.68       2.34       1.05        342
-----; (label_id: 6)                                          0.00       0.00       0.00        129
-----? (label_id: 7)                                          5.16       7.47       6.10       1058
-----— (label_id: 8)                                          2.15       0.49       0.80        409
-----… (label_id: 9)                                          0.69       4.23       1.19         71
------------------------
-----micro avg                                                5.23       5.23       5.23      34187
-----macro avg                                                5.68       6.04       3.29      34187
-----weighted avg                                            16.32       5.23       6.98      34187
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2113
------------------------
-----micro avg                                              100.00     100.00     100.00       2113
-----macro avg                                              100.00     100.00     100.00       2113
-----weighted avg                                           100.00     100.00     100.00       2113
-----
-----[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.61       0.39       4228
-----! (label_id: 1)                                          0.30       8.28       0.58        145
-----, (label_id: 2)                                          2.27       0.58       0.92      14495
------ (label_id: 3)                                          2.64      21.78       4.70       1327
-----. (label_id: 4)                                         44.87      11.66      18.51      12193
-----: (label_id: 5)                                          0.60       1.93       0.91        362
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.19       6.40       5.07       1078
-----— (label_id: 8)                                          1.16       0.22       0.37        459
-----… (label_id: 9)                                          0.85       4.17       1.41         96
------------------------
-----micro avg                                                5.54       5.54       5.54      34547
-----macro avg                                                5.72       5.56       3.29      34547
-----weighted avg                                            17.08       5.54       7.33      34547
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2114
------------------------
-----micro avg                                              100.00     100.00     100.00       2114
-----macro avg                                              100.00     100.00     100.00       2114
-----weighted avg                                           100.00     100.00     100.00       2114
-----
-----[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.29       0.63       0.40       4444
-----! (label_id: 1)                                          0.38      10.67       0.74        150
-----, (label_id: 2)                                          2.32       0.59       0.94      15290
------ (label_id: 3)                                          2.34      20.28       4.19       1292
-----. (label_id: 4)                                         43.85      11.68      18.44      12599
-----: (label_id: 5)                                          0.41       1.28       0.62        392
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.24       6.30       5.07       1111
-----— (label_id: 8)                                          0.00       0.00       0.00        456
-----… (label_id: 9)                                          0.38       2.41       0.65         83
------------------------
-----micro avg                                                5.40       5.40       5.40      35981
-----macro avg                                                5.42       5.38       3.11      35981
-----weighted avg                                            16.59       5.40       7.22      35981
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2222
------------------------
-----micro avg                                              100.00     100.00     100.00       2222
-----macro avg                                              100.00     100.00     100.00       2222
-----weighted avg                                           100.00     100.00     100.00       2222
-----
-----[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.35       0.73       0.48       3844
-----! (label_id: 1)                                          0.54      14.62       1.04        130
-----, (label_id: 2)                                          2.32       0.59       0.94      13056
------ (label_id: 3)                                          2.67      22.28       4.77       1194
-----. (label_id: 4)                                         44.45      11.95      18.84      10791
-----: (label_id: 5)                                          0.84       3.21       1.33        280
-----; (label_id: 6)                                          0.00       0.00       0.00        140
-----? (label_id: 7)                                          4.17       6.56       5.10        914
-----— (label_id: 8)                                          0.00       0.00       0.00        401
-----… (label_id: 9)                                          0.48       2.63       0.81         76
------------------------
-----micro avg                                                5.68       5.68       5.68      30826
-----macro avg                                                5.58       6.26       3.33      30826
-----weighted avg                                            16.82       5.68       7.41      30826
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1922
------------------------
-----micro avg                                              100.00     100.00     100.00       1922
-----macro avg                                              100.00     100.00     100.00       1922
-----weighted avg                                           100.00     100.00     100.00       1922
-----
-----[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.60       0.39       3970
-----! (label_id: 1)                                          0.35      10.66       0.68        122
-----, (label_id: 2)                                          2.09       0.53       0.85      13469
------ (label_id: 3)                                          2.29      19.32       4.10       1201
-----. (label_id: 4)                                         43.43      11.24      17.86      11227
-----: (label_id: 5)                                          0.63       2.30       0.99        304
-----; (label_id: 6)                                          0.00       0.00       0.00        141
-----? (label_id: 7)                                          4.52       6.86       5.45       1006
-----— (label_id: 8)                                          1.15       0.23       0.38        444
-----… (label_id: 9)                                          0.45       2.67       0.78         75
------------------------
-----micro avg                                                5.26       5.26       5.26      31959
-----macro avg                                                5.52       5.44       3.15      31959
-----weighted avg                                            16.42       5.26       7.02      31959
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1985
------------------------
-----micro avg                                              100.00     100.00     100.00       1985
-----macro avg                                              100.00     100.00     100.00       1985
-----weighted avg                                           100.00     100.00     100.00       1985
-----
-----[INFO] - Epoch 5, step 2394: val_loss was not in top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.23       0.48       0.31       4126
-----! (label_id: 1)                                          0.29       9.40       0.56        117
-----, (label_id: 2)                                          1.91       0.49       0.77      14019
------ (label_id: 3)                                          2.52      22.59       4.53       1164
-----. (label_id: 4)                                         44.15      11.65      18.44      11789
-----: (label_id: 5)                                          0.72       2.41       1.11        332
-----; (label_id: 6)                                          0.56       0.61       0.58        165
-----? (label_id: 7)                                          3.89       6.53       4.88        980
-----— (label_id: 8)                                          2.30       0.47       0.77        430
-----… (label_id: 9)                                          1.18       8.33       2.07         60
------------------------
-----micro avg                                                5.47       5.47       5.47      33182
-----macro avg                                                5.77       6.30       3.40      33182
-----weighted avg                                            16.77       5.47       7.25      33182
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2063
------------------------
-----micro avg                                              100.00     100.00     100.00       2063
-----macro avg                                              100.00     100.00     100.00       2063
-----weighted avg                                           100.00     100.00     100.00       2063
-----
-----[INFO] - Epoch 6, step 2793: val_loss was not in top 3
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index fa37b4c..6b5ff89 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             self.hparams.model.punct_class_weights=None
---- 
----+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
----+        self.mlp = MultiLayerPerceptron(
----+            self.transformer.config.hidden_size,
----+            self.transformer.config.hidden_size,
----+            num_layers=self.hparams.model.mlp.num_fc_layers, 
----+            activation=self.hparams.model.mlp.activation, 
----+            log_softmax=self.hparams.model.mlp.log_softmax
----+        )
----+
----         self.punct_classifier = TokenClassifier(
----             hidden_size=self.transformer.config.hidden_size,
----             num_classes=len(self.labels_to_ids),
----@@ -80,6 +89,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----             log_softmax=self.hparams.model.domain_head.log_softmax,
----             dropout=self.hparams.model.domain_head.fc_dropout,
----             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
----+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
----         )
---- 
----         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
----@@ -119,14 +129,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
----         self.freeze()
---- 
-----    def forward(self, input_ids, attention_mask, domain_ids=None):
----+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
----         hidden_states = self.transformer(
----             input_ids=input_ids, attention_mask=attention_mask
----         )[0]
----+        hidden_states = self.dropout(hidden_states)
----+        hidden_states = self.mlp(hidden_states)
----         punct_logits = self.punct_classifier(hidden_states=hidden_states)
----         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
----         domain_logits = self.domain_classifier(
-----            hidden_states=reverse_grad_hidden_states)
----+            hidden_states=reverse_grad_hidden_states,
----+            subtoken_mask=subtoken_mask)
----         return punct_logits, domain_logits
---- 
----     def _make_step(self, batch):
----@@ -136,7 +149,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         punct_labels = batch['labels']
----         domain_labels = batch['domain']
----         punct_logits, domain_logits = self(
-----            input_ids=input_ids, attention_mask=attention_mask
----+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
----         )
----         punctuation_loss = self.punctuation_loss(
----             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/lightning_logs.txt
---deleted file mode 100644
---index 8942cad..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/lightning_logs.txt
---+++ /dev/null
---@@ -1,5 +0,0 @@
----Global seed set to 42
----GPU available: True, used: True
----TPU available: None, using: 0 TPU cores
----LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
----Using native 16bit precision.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/nemo_error_log.txt
---deleted file mode 100644
---index c2984dc..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/nemo_error_log.txt
---+++ /dev/null
---@@ -1 +0,0 @@
----[NeMo W 2021-02-08 10:48:11 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index 0131ae4..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,3 +0,0 @@
----[NeMo I 2021-02-08 10:48:11 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-48-11
----[NeMo I 2021-02-08 10:48:11 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-08 10:48:11 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/git-info.log
---deleted file mode 100644
---index 4d1615a..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/git-info.log
---+++ /dev/null
---@@ -1,623 +0,0 @@
----commit hash: 939a671c8c117db6975316767ced5d95449e2b27
----diff --git a/README.md b/README.md
----index d52dba5..0b403d3 100644
------- a/README.md
----+++ b/README.md
----@@ -333,4 +333,71 @@ label                                                precision    recall       f
----  'punct_f1': 0.5858508944511414,
----  'punct_precision': 0.30176490545272827,
----  'punct_recall': 10.0,
----- 'test_loss': 0.8140875697135925}
----\ No newline at end of file
----+ 'test_loss': 0.8140875697135925}
----+
----+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
----+
----+ (label_id: 0)                                          62.15     100.00      76.66       5154
----+! (label_id: 1)                                          0.00       0.00       0.00        108
----+, (label_id: 2)                                          0.00       0.00       0.00      18022
----+- (label_id: 3)                                          0.00       0.00       0.00       1557
----+. (label_id: 4)                                         41.74      94.01      57.81      15164
----+: (label_id: 5)                                          0.00       0.00       0.00        319
----+; (label_id: 6)                                          0.00       0.00       0.00         88
----+? (label_id: 7)                                          0.00       0.00       0.00       1217
----+ (label_id: 8)                                          0.00       0.00       0.00        752
----+… (label_id: 9)                                          0.00       0.00       0.00         67
----+-------------------
----+micro avg                                               45.72      45.72      45.72      42448
----+macro avg                                               10.39      19.40      13.45      42448
----+weighted avg                                            22.46      45.72      29.96      42448
----+{ 'punct_f1': 13.446383476257324,
----+ 'punct_precision': 10.388500213623047,
----+ 'punct_recall': 19.400554656982422,
----+ 'test_loss': 0.44148480892181396}
----+
----+
----+ ## Log for 8/2/2021
----+
----+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
----+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
----+
----+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
----+End frozen 
----+micro avg                                               41.42      41.42      41.42      33406
----+macro avg                                               11.01      13.54      11.07      33406
----+weighted avg                                            34.88      41.42      34.20      33406
----+
----+1st layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               36.65      36.65      36.65      33463
----+macro avg                                               10.71       9.91       8.26      33463
----+weighted avg                                            34.32      36.65      31.08      33463
----+
----+2nd layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               35.72      35.72      35.72      42448
----+macro avg                                                3.57      10.00       5.26      42448
----+weighted avg                                            12.76      35.72      18.81      42448
----+
----+{'punct_f1': 5.264181137084961,
----+ 'punct_precision': 3.572371006011963,
----+ 'punct_recall': 10.0,
----+ 'test_loss': 18.49854850769043}
----+
----+
----+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
----+alpha from 3->4 seems to reduce convergence rate.
----+
----+micro avg                                               50.98      50.98      50.98      33463
----+macro avg                                               25.99      25.38      23.38      33463
----+weighted avg                                            50.31      50.98      48.27      33463
----+
----+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
----+micro avg                                               58.55      58.55      58.55      39340
----+macro avg                                               30.02      29.74      29.52      39340
----+weighted avg                                            57.91      58.55      57.51      39340
----+
----+still increasing?!
----+{'punct_f1': 29.523975372314453,
----+ 'punct_precision': 30.015613555908203,
----+ 'punct_recall': 29.738296508789062,
----+ 'test_loss': 0.3690211772918701}
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index fe58670..ab37d69 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,49 +1,49 @@
---- seed: 42
---- trainer:
-----    # gpus: 1 # the number of gpus, 0 for CPU
-----    # num_nodes: 1
-----    # max_epochs: 2
-----    # max_steps: null # precedence over max_epochs
-----    # accumulate_grad_batches: 4 # accumulates grads every k batches
-----    # gradient_clip_val: 0
-----    # amp_level: O1 # O1/O2 for mixed precision
-----    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
-----    # checkpoint_callback: false  # Provided by exp_manager
-----    # logger: false #false  # Provided by exp_manager
-----    # log_every_n_steps: 1  # Interval of logging.
-----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # resume_from_checkpoint: null
-----
-----    gpus: 0 # the number of gpus, 0 for CPU
----+    gpus: 1 # the number of gpus, 0 for CPU
----     num_nodes: 1
-----    max_epochs: 8
----+    max_epochs: 10
----     max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 1 # accumulates grads every k batches
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----     gradient_clip_val: 0
-----    amp_level: O0 # O1/O2 for mixed precision
-----    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
----+    amp_level: O1 # O1/O2 for mixed precision
----+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    accelerator: ddp
----     checkpoint_callback: false  # Provided by exp_manager
----     logger: false #false  # Provided by exp_manager
----     log_every_n_steps: 1  # Interval of logging.
----     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    reload_dataloaders_every_epoch: true
----     resume_from_checkpoint: null
---- 
----+    # gpus: 0 # the number of gpus, 0 for CPU
----+    # num_nodes: 1
----+    # max_epochs: 8
----+    # max_steps: null # precedence over max_epochs
----+    # accumulate_grad_batches: 1 # accumulates grads every k batches
----+    # gradient_clip_val: 0
----+    # amp_level: O0 # O1/O2 for mixed precision
----+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # # accelerator: ddp
----+    # checkpoint_callback: false  # Provided by exp_manager
----+    # logger: false #false  # Provided by exp_manager
----+    # log_every_n_steps: 1  # Interval of logging.
----+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    # reload_dataloaders_every_epoch: true
----+    # resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /home/nxingyu2/data # /root/data # 
-----tmp_path: /home/nxingyu2/data/tmp # /tmp # 
----+base_path: /home/nxingyu/data # /root/data # 
----+tmp_path: /home/nxingyu/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
-----    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----     unfrozen: 0
-----    maximum_unfrozen: 2
----+    maximum_unfrozen: 1
----     unfreeze_step: 1
----     # unfreeze_every: 3
----     punct_label_ids:
----@@ -58,10 +58,10 @@ model:
----         - "—"
----         - "…"
---- 
-----    punct_class_weights: true
----+    punct_class_weights: false
----     
----     dataset:
-----        data_dir: /home/nxingyu2/data # /root/data # 
----+        data_dir: /home/nxingyu/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -106,6 +106,12 @@ model:
----         config: null
----         # unfrozen_layers: 1
---- 
----+    mlp:
----+        num_fc_layers: 2
----+        fc_dropout: 0.1
----+        log_softmax: false
----+        activation: 'GELU'
----+        
----     punct_head:
----         punct_num_fc_layers: 1
----         fc_dropout: 0.1
----@@ -122,17 +128,19 @@ model:
----         use_transformer_init: true
----         loss: 'cel'
----         gamma: 0.1 # coefficient of gradient reversal
----+        pooling: 'token'
----+        idx_conditioned_on: 0
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 3
----+        alpha: 4
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: novograd
----+        name: adamw
----         lr: 1e-3
----         weight_decay: 0.00
----         sched:
----diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
----index d4ff927..1a6fe93 100644
------- a/experiment/core/layers/sequence_classifier.py
----+++ b/experiment/core/layers/sequence_classifier.py
----@@ -1,3 +1,4 @@
----+import torch
---- from torch import nn
---- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
---- from core.utils import transformer_weights_init
----@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
----         log_softmax: bool = True,
----         dropout: float = 0.0,
----         use_transformer_init: bool = True,
-----        idx_conditioned_on: int = 0,
----+        pooling: str = 'mean', # mean, max, mean_max, token
----+        idx_conditioned_on: int = None,
----     ):
----         """
----         Initializes the SequenceClassifier module.
----@@ -37,6 +39,7 @@ class SequenceClassifier(nn.Module):
----         super().__init__()
----         self.log_softmax = log_softmax
----         self._idx_conditioned_on = idx_conditioned_on
----+        self.pooling = pooling
----         self.mlp = MultiLayerPerceptron(
----             hidden_size=hidden_size,
----             num_classes=num_classes,
----@@ -48,7 +51,23 @@ class SequenceClassifier(nn.Module):
----         if use_transformer_init:
----             self.apply(lambda module: transformer_weights_init(module, xavier=False))
---- 
-----    def forward(self, hidden_states):
----+    def forward(self, hidden_states, subtoken_mask=None):
----         hidden_states = self.dropout(hidden_states)
-----        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
----+        if self.pooling=='token':
----+            pooled = hidden_states[:, self._idx_conditioned_on]
----+        else:
----+            if subtoken_mask==None:
----+                ct=hidden_states.shape[1] # Seq len
----+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
----+            else:
----+                ct = torch.sum(subtoken_mask,axis=1)
----+            pooled_sum = torch.sum(hidden_states,axis=1)
----+            if self.pooling=='mean' or self.pooling == 'mean_max':
----+                pooled_mean = pooled_sum//ct
----+            if self.pooling=='max' or self.pooling=='mean_max':
----+                pooled_max = torch.max(hidden_states,axis=1)[0]
----+            pooled=pooled_mean if self.pooling=='mean' else \
----+                pooled_max if self.pooling=='max' else \
----+                    torch.cat([pooled_mean,pooled_max],axis=-1)
----+        logits = self.mlp(pooled)
----         return logits
----diff --git a/experiment/info.log b/experiment/info.log
----index 2471fe9..e69de29 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,300 +0,0 @@
-----[INFO] - GPU available: True, used: False
-----[INFO] - TPU available: None, using: 0 TPU cores
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        184
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.23       0.34       0.53        594
------ (label_id: 3)                                          3.06      25.42       5.46         59
-----. (label_id: 4)                                         47.22      12.98      20.36        524
-----: (label_id: 5)                                          0.00       0.00       0.00         18
-----; (label_id: 6)                                          0.00       0.00       0.00         13
-----? (label_id: 7)                                          8.45       6.32       7.23         95
-----— (label_id: 8)                                          0.00       0.00       0.00         12
-----… (label_id: 9)                                          0.00       0.00       0.00          0
------------------------
-----micro avg                                                6.05       6.05       6.05       1503
-----macro avg                                                6.66       5.01       3.73       1503
-----weighted avg                                            17.61       6.05       7.98       1503
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00         92
------------------------
-----micro avg                                              100.00     100.00     100.00         92
-----macro avg                                              100.00     100.00     100.00         92
-----weighted avg                                           100.00     100.00     100.00         92
-----
-----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 1.5848931924611143e-08
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
-----will be used during training (effective maximum steps = 3192) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 3192
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        202
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.62       0.45       0.70        669
------ (label_id: 3)                                          3.48      27.27       6.17         66
-----. (label_id: 4)                                         45.06      13.01      20.19        561
-----: (label_id: 5)                                          1.52       6.67       2.47         15
-----; (label_id: 6)                                          0.00       0.00       0.00         15
-----? (label_id: 7)                                          8.70       7.32       7.95         82
-----— (label_id: 8)                                          0.00       0.00       0.00         13
-----… (label_id: 9)                                          0.00       0.00       0.00          1
------------------------
-----micro avg                                                6.20       6.20       6.20       1628
-----macro avg                                                6.04       5.47       3.75       1628
-----weighted avg                                            16.79       6.20       7.92       1628
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        101
------------------------
-----micro avg                                              100.00     100.00     100.00        101
-----macro avg                                              100.00     100.00     100.00        101
-----weighted avg                                           100.00     100.00     100.00        101
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.34       0.73       0.46       4402
-----! (label_id: 1)                                          0.42      13.95       0.82        129
-----, (label_id: 2)                                          2.53       0.64       1.03      15243
------ (label_id: 3)                                          2.45      21.03       4.38       1322
-----. (label_id: 4)                                         44.00      11.40      18.11      12542
-----: (label_id: 5)                                          0.43       1.41       0.65        354
-----; (label_id: 6)                                          0.00       0.00       0.00        163
-----? (label_id: 7)                                          4.16       6.27       5.00       1117
-----— (label_id: 8)                                          3.00       0.61       1.02        488
-----… (label_id: 9)                                          0.97       6.17       1.68         81
------------------------
-----micro avg                                                5.41       5.41       5.41      35841
-----macro avg                                                5.83       6.22       3.32      35841
-----weighted avg                                            16.78       5.41       7.18      35841
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2201
------------------------
-----micro avg                                              100.00     100.00     100.00       2201
-----macro avg                                              100.00     100.00     100.00       2201
-----weighted avg                                           100.00     100.00     100.00       2201
-----
-----[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.20       0.43       0.27       4226
-----! (label_id: 1)                                          0.44      14.17       0.86        127
-----, (label_id: 2)                                          1.93       0.49       0.78      14611
------ (label_id: 3)                                          2.23      19.56       4.01       1237
-----. (label_id: 4)                                         43.37      11.25      17.86      11977
-----: (label_id: 5)                                          0.68       2.34       1.05        342
-----; (label_id: 6)                                          0.00       0.00       0.00        129
-----? (label_id: 7)                                          5.16       7.47       6.10       1058
-----— (label_id: 8)                                          2.15       0.49       0.80        409
-----… (label_id: 9)                                          0.69       4.23       1.19         71
------------------------
-----micro avg                                                5.23       5.23       5.23      34187
-----macro avg                                                5.68       6.04       3.29      34187
-----weighted avg                                            16.32       5.23       6.98      34187
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2113
------------------------
-----micro avg                                              100.00     100.00     100.00       2113
-----macro avg                                              100.00     100.00     100.00       2113
-----weighted avg                                           100.00     100.00     100.00       2113
-----
-----[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.61       0.39       4228
-----! (label_id: 1)                                          0.30       8.28       0.58        145
-----, (label_id: 2)                                          2.27       0.58       0.92      14495
------ (label_id: 3)                                          2.64      21.78       4.70       1327
-----. (label_id: 4)                                         44.87      11.66      18.51      12193
-----: (label_id: 5)                                          0.60       1.93       0.91        362
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.19       6.40       5.07       1078
-----— (label_id: 8)                                          1.16       0.22       0.37        459
-----… (label_id: 9)                                          0.85       4.17       1.41         96
------------------------
-----micro avg                                                5.54       5.54       5.54      34547
-----macro avg                                                5.72       5.56       3.29      34547
-----weighted avg                                            17.08       5.54       7.33      34547
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2114
------------------------
-----micro avg                                              100.00     100.00     100.00       2114
-----macro avg                                              100.00     100.00     100.00       2114
-----weighted avg                                           100.00     100.00     100.00       2114
-----
-----[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.29       0.63       0.40       4444
-----! (label_id: 1)                                          0.38      10.67       0.74        150
-----, (label_id: 2)                                          2.32       0.59       0.94      15290
------ (label_id: 3)                                          2.34      20.28       4.19       1292
-----. (label_id: 4)                                         43.85      11.68      18.44      12599
-----: (label_id: 5)                                          0.41       1.28       0.62        392
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.24       6.30       5.07       1111
-----— (label_id: 8)                                          0.00       0.00       0.00        456
-----… (label_id: 9)                                          0.38       2.41       0.65         83
------------------------
-----micro avg                                                5.40       5.40       5.40      35981
-----macro avg                                                5.42       5.38       3.11      35981
-----weighted avg                                            16.59       5.40       7.22      35981
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2222
------------------------
-----micro avg                                              100.00     100.00     100.00       2222
-----macro avg                                              100.00     100.00     100.00       2222
-----weighted avg                                           100.00     100.00     100.00       2222
-----
-----[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.35       0.73       0.48       3844
-----! (label_id: 1)                                          0.54      14.62       1.04        130
-----, (label_id: 2)                                          2.32       0.59       0.94      13056
------ (label_id: 3)                                          2.67      22.28       4.77       1194
-----. (label_id: 4)                                         44.45      11.95      18.84      10791
-----: (label_id: 5)                                          0.84       3.21       1.33        280
-----; (label_id: 6)                                          0.00       0.00       0.00        140
-----? (label_id: 7)                                          4.17       6.56       5.10        914
-----— (label_id: 8)                                          0.00       0.00       0.00        401
-----… (label_id: 9)                                          0.48       2.63       0.81         76
------------------------
-----micro avg                                                5.68       5.68       5.68      30826
-----macro avg                                                5.58       6.26       3.33      30826
-----weighted avg                                            16.82       5.68       7.41      30826
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1922
------------------------
-----micro avg                                              100.00     100.00     100.00       1922
-----macro avg                                              100.00     100.00     100.00       1922
-----weighted avg                                           100.00     100.00     100.00       1922
-----
-----[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.60       0.39       3970
-----! (label_id: 1)                                          0.35      10.66       0.68        122
-----, (label_id: 2)                                          2.09       0.53       0.85      13469
------ (label_id: 3)                                          2.29      19.32       4.10       1201
-----. (label_id: 4)                                         43.43      11.24      17.86      11227
-----: (label_id: 5)                                          0.63       2.30       0.99        304
-----; (label_id: 6)                                          0.00       0.00       0.00        141
-----? (label_id: 7)                                          4.52       6.86       5.45       1006
-----— (label_id: 8)                                          1.15       0.23       0.38        444
-----… (label_id: 9)                                          0.45       2.67       0.78         75
------------------------
-----micro avg                                                5.26       5.26       5.26      31959
-----macro avg                                                5.52       5.44       3.15      31959
-----weighted avg                                            16.42       5.26       7.02      31959
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1985
------------------------
-----micro avg                                              100.00     100.00     100.00       1985
-----macro avg                                              100.00     100.00     100.00       1985
-----weighted avg                                           100.00     100.00     100.00       1985
-----
-----[INFO] - Epoch 5, step 2394: val_loss was not in top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.23       0.48       0.31       4126
-----! (label_id: 1)                                          0.29       9.40       0.56        117
-----, (label_id: 2)                                          1.91       0.49       0.77      14019
------ (label_id: 3)                                          2.52      22.59       4.53       1164
-----. (label_id: 4)                                         44.15      11.65      18.44      11789
-----: (label_id: 5)                                          0.72       2.41       1.11        332
-----; (label_id: 6)                                          0.56       0.61       0.58        165
-----? (label_id: 7)                                          3.89       6.53       4.88        980
-----— (label_id: 8)                                          2.30       0.47       0.77        430
-----… (label_id: 9)                                          1.18       8.33       2.07         60
------------------------
-----micro avg                                                5.47       5.47       5.47      33182
-----macro avg                                                5.77       6.30       3.40      33182
-----weighted avg                                            16.77       5.47       7.25      33182
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2063
------------------------
-----micro avg                                              100.00     100.00     100.00       2063
-----macro avg                                              100.00     100.00     100.00       2063
-----weighted avg                                           100.00     100.00     100.00       2063
-----
-----[INFO] - Epoch 6, step 2793: val_loss was not in top 3
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index fa37b4c..6b5ff89 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             self.hparams.model.punct_class_weights=None
---- 
----+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
----+        self.mlp = MultiLayerPerceptron(
----+            self.transformer.config.hidden_size,
----+            self.transformer.config.hidden_size,
----+            num_layers=self.hparams.model.mlp.num_fc_layers, 
----+            activation=self.hparams.model.mlp.activation, 
----+            log_softmax=self.hparams.model.mlp.log_softmax
----+        )
----+
----         self.punct_classifier = TokenClassifier(
----             hidden_size=self.transformer.config.hidden_size,
----             num_classes=len(self.labels_to_ids),
----@@ -80,6 +89,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----             log_softmax=self.hparams.model.domain_head.log_softmax,
----             dropout=self.hparams.model.domain_head.fc_dropout,
----             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
----+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
----         )
---- 
----         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
----@@ -119,14 +129,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
----         self.freeze()
---- 
-----    def forward(self, input_ids, attention_mask, domain_ids=None):
----+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
----         hidden_states = self.transformer(
----             input_ids=input_ids, attention_mask=attention_mask
----         )[0]
----+        hidden_states = self.dropout(hidden_states)
----+        hidden_states = self.mlp(hidden_states)
----         punct_logits = self.punct_classifier(hidden_states=hidden_states)
----         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
----         domain_logits = self.domain_classifier(
-----            hidden_states=reverse_grad_hidden_states)
----+            hidden_states=reverse_grad_hidden_states,
----+            subtoken_mask=subtoken_mask)
----         return punct_logits, domain_logits
---- 
----     def _make_step(self, batch):
----@@ -136,7 +149,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         punct_labels = batch['labels']
----         domain_labels = batch['domain']
----         punct_logits, domain_logits = self(
-----            input_ids=input_ids, attention_mask=attention_mask
----+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
----         )
----         punctuation_loss = self.punctuation_loss(
----             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/lightning_logs.txt
---deleted file mode 100644
---index 8942cad..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/lightning_logs.txt
---+++ /dev/null
---@@ -1,5 +0,0 @@
----Global seed set to 42
----GPU available: True, used: True
----TPU available: None, using: 0 TPU cores
----LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
----Using native 16bit precision.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/nemo_error_log.txt
---deleted file mode 100644
---index f7767e2..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/nemo_error_log.txt
---+++ /dev/null
---@@ -1 +0,0 @@
----[NeMo W 2021-02-08 10:50:33 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index 03a045a..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,3 +0,0 @@
----[NeMo I 2021-02-08 10:50:33 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-50-33
----[NeMo I 2021-02-08 10:50:33 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-08 10:50:33 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/git-info.log
---deleted file mode 100644
---index 10d12b7..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/git-info.log
---+++ /dev/null
---@@ -1,623 +0,0 @@
----commit hash: 939a671c8c117db6975316767ced5d95449e2b27
----diff --git a/README.md b/README.md
----index d52dba5..0b403d3 100644
------- a/README.md
----+++ b/README.md
----@@ -333,4 +333,71 @@ label                                                precision    recall       f
----  'punct_f1': 0.5858508944511414,
----  'punct_precision': 0.30176490545272827,
----  'punct_recall': 10.0,
----- 'test_loss': 0.8140875697135925}
----\ No newline at end of file
----+ 'test_loss': 0.8140875697135925}
----+
----+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
----+
----+ (label_id: 0)                                          62.15     100.00      76.66       5154
----+! (label_id: 1)                                          0.00       0.00       0.00        108
----+, (label_id: 2)                                          0.00       0.00       0.00      18022
----+- (label_id: 3)                                          0.00       0.00       0.00       1557
----+. (label_id: 4)                                         41.74      94.01      57.81      15164
----+: (label_id: 5)                                          0.00       0.00       0.00        319
----+; (label_id: 6)                                          0.00       0.00       0.00         88
----+? (label_id: 7)                                          0.00       0.00       0.00       1217
----+ (label_id: 8)                                          0.00       0.00       0.00        752
----+… (label_id: 9)                                          0.00       0.00       0.00         67
----+-------------------
----+micro avg                                               45.72      45.72      45.72      42448
----+macro avg                                               10.39      19.40      13.45      42448
----+weighted avg                                            22.46      45.72      29.96      42448
----+{ 'punct_f1': 13.446383476257324,
----+ 'punct_precision': 10.388500213623047,
----+ 'punct_recall': 19.400554656982422,
----+ 'test_loss': 0.44148480892181396}
----+
----+
----+ ## Log for 8/2/2021
----+
----+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
----+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
----+
----+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
----+End frozen 
----+micro avg                                               41.42      41.42      41.42      33406
----+macro avg                                               11.01      13.54      11.07      33406
----+weighted avg                                            34.88      41.42      34.20      33406
----+
----+1st layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               36.65      36.65      36.65      33463
----+macro avg                                               10.71       9.91       8.26      33463
----+weighted avg                                            34.32      36.65      31.08      33463
----+
----+2nd layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               35.72      35.72      35.72      42448
----+macro avg                                                3.57      10.00       5.26      42448
----+weighted avg                                            12.76      35.72      18.81      42448
----+
----+{'punct_f1': 5.264181137084961,
----+ 'punct_precision': 3.572371006011963,
----+ 'punct_recall': 10.0,
----+ 'test_loss': 18.49854850769043}
----+
----+
----+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
----+alpha from 3->4 seems to reduce convergence rate.
----+
----+micro avg                                               50.98      50.98      50.98      33463
----+macro avg                                               25.99      25.38      23.38      33463
----+weighted avg                                            50.31      50.98      48.27      33463
----+
----+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
----+micro avg                                               58.55      58.55      58.55      39340
----+macro avg                                               30.02      29.74      29.52      39340
----+weighted avg                                            57.91      58.55      57.51      39340
----+
----+still increasing?!
----+{'punct_f1': 29.523975372314453,
----+ 'punct_precision': 30.015613555908203,
----+ 'punct_recall': 29.738296508789062,
----+ 'test_loss': 0.3690211772918701}
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index fe58670..716a889 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,49 +1,49 @@
---- seed: 42
---- trainer:
-----    # gpus: 1 # the number of gpus, 0 for CPU
-----    # num_nodes: 1
-----    # max_epochs: 2
-----    # max_steps: null # precedence over max_epochs
-----    # accumulate_grad_batches: 4 # accumulates grads every k batches
-----    # gradient_clip_val: 0
-----    # amp_level: O1 # O1/O2 for mixed precision
-----    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
-----    # checkpoint_callback: false  # Provided by exp_manager
-----    # logger: false #false  # Provided by exp_manager
-----    # log_every_n_steps: 1  # Interval of logging.
-----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # resume_from_checkpoint: null
-----
-----    gpus: 0 # the number of gpus, 0 for CPU
----+    gpus: 1 # the number of gpus, 0 for CPU
----     num_nodes: 1
-----    max_epochs: 8
----+    max_epochs: 10
----     max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 1 # accumulates grads every k batches
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----     gradient_clip_val: 0
-----    amp_level: O0 # O1/O2 for mixed precision
-----    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
----+    amp_level: O1 # O1/O2 for mixed precision
----+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    accelerator: ddp
----     checkpoint_callback: false  # Provided by exp_manager
----     logger: false #false  # Provided by exp_manager
----     log_every_n_steps: 1  # Interval of logging.
----     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    reload_dataloaders_every_epoch: true
----     resume_from_checkpoint: null
---- 
----+    # gpus: 0 # the number of gpus, 0 for CPU
----+    # num_nodes: 1
----+    # max_epochs: 8
----+    # max_steps: null # precedence over max_epochs
----+    # accumulate_grad_batches: 1 # accumulates grads every k batches
----+    # gradient_clip_val: 0
----+    # amp_level: O0 # O1/O2 for mixed precision
----+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # # accelerator: ddp
----+    # checkpoint_callback: false  # Provided by exp_manager
----+    # logger: false #false  # Provided by exp_manager
----+    # log_every_n_steps: 1  # Interval of logging.
----+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    # reload_dataloaders_every_epoch: true
----+    # resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /home/nxingyu2/data # /root/data # 
-----tmp_path: /home/nxingyu2/data/tmp # /tmp # 
----+base_path: /home/nxingyu/data # /root/data # 
----+tmp_path: /home/nxingyu/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
-----    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----     unfrozen: 0
-----    maximum_unfrozen: 2
----+    maximum_unfrozen: 1
----     unfreeze_step: 1
----     # unfreeze_every: 3
----     punct_label_ids:
----@@ -58,10 +58,10 @@ model:
----         - "—"
----         - "…"
---- 
-----    punct_class_weights: true
----+    punct_class_weights: false
----     
----     dataset:
-----        data_dir: /home/nxingyu2/data # /root/data # 
----+        data_dir: /home/nxingyu/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -106,6 +106,12 @@ model:
----         config: null
----         # unfrozen_layers: 1
---- 
----+    mlp:
----+        num_fc_layers: 2
----+        fc_dropout: 0.1
----+        log_softmax: false
----+        activation: 'gelu'
----+        
----     punct_head:
----         punct_num_fc_layers: 1
----         fc_dropout: 0.1
----@@ -122,17 +128,19 @@ model:
----         use_transformer_init: true
----         loss: 'cel'
----         gamma: 0.1 # coefficient of gradient reversal
----+        pooling: 'token'
----+        idx_conditioned_on: 0
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 3
----+        alpha: 4
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: novograd
----+        name: adamw
----         lr: 1e-3
----         weight_decay: 0.00
----         sched:
----diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
----index d4ff927..1a6fe93 100644
------- a/experiment/core/layers/sequence_classifier.py
----+++ b/experiment/core/layers/sequence_classifier.py
----@@ -1,3 +1,4 @@
----+import torch
---- from torch import nn
---- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
---- from core.utils import transformer_weights_init
----@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
----         log_softmax: bool = True,
----         dropout: float = 0.0,
----         use_transformer_init: bool = True,
-----        idx_conditioned_on: int = 0,
----+        pooling: str = 'mean', # mean, max, mean_max, token
----+        idx_conditioned_on: int = None,
----     ):
----         """
----         Initializes the SequenceClassifier module.
----@@ -37,6 +39,7 @@ class SequenceClassifier(nn.Module):
----         super().__init__()
----         self.log_softmax = log_softmax
----         self._idx_conditioned_on = idx_conditioned_on
----+        self.pooling = pooling
----         self.mlp = MultiLayerPerceptron(
----             hidden_size=hidden_size,
----             num_classes=num_classes,
----@@ -48,7 +51,23 @@ class SequenceClassifier(nn.Module):
----         if use_transformer_init:
----             self.apply(lambda module: transformer_weights_init(module, xavier=False))
---- 
-----    def forward(self, hidden_states):
----+    def forward(self, hidden_states, subtoken_mask=None):
----         hidden_states = self.dropout(hidden_states)
-----        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
----+        if self.pooling=='token':
----+            pooled = hidden_states[:, self._idx_conditioned_on]
----+        else:
----+            if subtoken_mask==None:
----+                ct=hidden_states.shape[1] # Seq len
----+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
----+            else:
----+                ct = torch.sum(subtoken_mask,axis=1)
----+            pooled_sum = torch.sum(hidden_states,axis=1)
----+            if self.pooling=='mean' or self.pooling == 'mean_max':
----+                pooled_mean = pooled_sum//ct
----+            if self.pooling=='max' or self.pooling=='mean_max':
----+                pooled_max = torch.max(hidden_states,axis=1)[0]
----+            pooled=pooled_mean if self.pooling=='mean' else \
----+                pooled_max if self.pooling=='max' else \
----+                    torch.cat([pooled_mean,pooled_max],axis=-1)
----+        logits = self.mlp(pooled)
----         return logits
----diff --git a/experiment/info.log b/experiment/info.log
----index 2471fe9..e69de29 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,300 +0,0 @@
-----[INFO] - GPU available: True, used: False
-----[INFO] - TPU available: None, using: 0 TPU cores
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        184
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.23       0.34       0.53        594
------ (label_id: 3)                                          3.06      25.42       5.46         59
-----. (label_id: 4)                                         47.22      12.98      20.36        524
-----: (label_id: 5)                                          0.00       0.00       0.00         18
-----; (label_id: 6)                                          0.00       0.00       0.00         13
-----? (label_id: 7)                                          8.45       6.32       7.23         95
-----— (label_id: 8)                                          0.00       0.00       0.00         12
-----… (label_id: 9)                                          0.00       0.00       0.00          0
------------------------
-----micro avg                                                6.05       6.05       6.05       1503
-----macro avg                                                6.66       5.01       3.73       1503
-----weighted avg                                            17.61       6.05       7.98       1503
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00         92
------------------------
-----micro avg                                              100.00     100.00     100.00         92
-----macro avg                                              100.00     100.00     100.00         92
-----weighted avg                                           100.00     100.00     100.00         92
-----
-----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 1.5848931924611143e-08
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
-----will be used during training (effective maximum steps = 3192) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 3192
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        202
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.62       0.45       0.70        669
------ (label_id: 3)                                          3.48      27.27       6.17         66
-----. (label_id: 4)                                         45.06      13.01      20.19        561
-----: (label_id: 5)                                          1.52       6.67       2.47         15
-----; (label_id: 6)                                          0.00       0.00       0.00         15
-----? (label_id: 7)                                          8.70       7.32       7.95         82
-----— (label_id: 8)                                          0.00       0.00       0.00         13
-----… (label_id: 9)                                          0.00       0.00       0.00          1
------------------------
-----micro avg                                                6.20       6.20       6.20       1628
-----macro avg                                                6.04       5.47       3.75       1628
-----weighted avg                                            16.79       6.20       7.92       1628
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        101
------------------------
-----micro avg                                              100.00     100.00     100.00        101
-----macro avg                                              100.00     100.00     100.00        101
-----weighted avg                                           100.00     100.00     100.00        101
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.34       0.73       0.46       4402
-----! (label_id: 1)                                          0.42      13.95       0.82        129
-----, (label_id: 2)                                          2.53       0.64       1.03      15243
------ (label_id: 3)                                          2.45      21.03       4.38       1322
-----. (label_id: 4)                                         44.00      11.40      18.11      12542
-----: (label_id: 5)                                          0.43       1.41       0.65        354
-----; (label_id: 6)                                          0.00       0.00       0.00        163
-----? (label_id: 7)                                          4.16       6.27       5.00       1117
-----— (label_id: 8)                                          3.00       0.61       1.02        488
-----… (label_id: 9)                                          0.97       6.17       1.68         81
------------------------
-----micro avg                                                5.41       5.41       5.41      35841
-----macro avg                                                5.83       6.22       3.32      35841
-----weighted avg                                            16.78       5.41       7.18      35841
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2201
------------------------
-----micro avg                                              100.00     100.00     100.00       2201
-----macro avg                                              100.00     100.00     100.00       2201
-----weighted avg                                           100.00     100.00     100.00       2201
-----
-----[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.20       0.43       0.27       4226
-----! (label_id: 1)                                          0.44      14.17       0.86        127
-----, (label_id: 2)                                          1.93       0.49       0.78      14611
------ (label_id: 3)                                          2.23      19.56       4.01       1237
-----. (label_id: 4)                                         43.37      11.25      17.86      11977
-----: (label_id: 5)                                          0.68       2.34       1.05        342
-----; (label_id: 6)                                          0.00       0.00       0.00        129
-----? (label_id: 7)                                          5.16       7.47       6.10       1058
-----— (label_id: 8)                                          2.15       0.49       0.80        409
-----… (label_id: 9)                                          0.69       4.23       1.19         71
------------------------
-----micro avg                                                5.23       5.23       5.23      34187
-----macro avg                                                5.68       6.04       3.29      34187
-----weighted avg                                            16.32       5.23       6.98      34187
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2113
------------------------
-----micro avg                                              100.00     100.00     100.00       2113
-----macro avg                                              100.00     100.00     100.00       2113
-----weighted avg                                           100.00     100.00     100.00       2113
-----
-----[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.61       0.39       4228
-----! (label_id: 1)                                          0.30       8.28       0.58        145
-----, (label_id: 2)                                          2.27       0.58       0.92      14495
------ (label_id: 3)                                          2.64      21.78       4.70       1327
-----. (label_id: 4)                                         44.87      11.66      18.51      12193
-----: (label_id: 5)                                          0.60       1.93       0.91        362
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.19       6.40       5.07       1078
-----— (label_id: 8)                                          1.16       0.22       0.37        459
-----… (label_id: 9)                                          0.85       4.17       1.41         96
------------------------
-----micro avg                                                5.54       5.54       5.54      34547
-----macro avg                                                5.72       5.56       3.29      34547
-----weighted avg                                            17.08       5.54       7.33      34547
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2114
------------------------
-----micro avg                                              100.00     100.00     100.00       2114
-----macro avg                                              100.00     100.00     100.00       2114
-----weighted avg                                           100.00     100.00     100.00       2114
-----
-----[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.29       0.63       0.40       4444
-----! (label_id: 1)                                          0.38      10.67       0.74        150
-----, (label_id: 2)                                          2.32       0.59       0.94      15290
------ (label_id: 3)                                          2.34      20.28       4.19       1292
-----. (label_id: 4)                                         43.85      11.68      18.44      12599
-----: (label_id: 5)                                          0.41       1.28       0.62        392
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.24       6.30       5.07       1111
-----— (label_id: 8)                                          0.00       0.00       0.00        456
-----… (label_id: 9)                                          0.38       2.41       0.65         83
------------------------
-----micro avg                                                5.40       5.40       5.40      35981
-----macro avg                                                5.42       5.38       3.11      35981
-----weighted avg                                            16.59       5.40       7.22      35981
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2222
------------------------
-----micro avg                                              100.00     100.00     100.00       2222
-----macro avg                                              100.00     100.00     100.00       2222
-----weighted avg                                           100.00     100.00     100.00       2222
-----
-----[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.35       0.73       0.48       3844
-----! (label_id: 1)                                          0.54      14.62       1.04        130
-----, (label_id: 2)                                          2.32       0.59       0.94      13056
------ (label_id: 3)                                          2.67      22.28       4.77       1194
-----. (label_id: 4)                                         44.45      11.95      18.84      10791
-----: (label_id: 5)                                          0.84       3.21       1.33        280
-----; (label_id: 6)                                          0.00       0.00       0.00        140
-----? (label_id: 7)                                          4.17       6.56       5.10        914
-----— (label_id: 8)                                          0.00       0.00       0.00        401
-----… (label_id: 9)                                          0.48       2.63       0.81         76
------------------------
-----micro avg                                                5.68       5.68       5.68      30826
-----macro avg                                                5.58       6.26       3.33      30826
-----weighted avg                                            16.82       5.68       7.41      30826
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1922
------------------------
-----micro avg                                              100.00     100.00     100.00       1922
-----macro avg                                              100.00     100.00     100.00       1922
-----weighted avg                                           100.00     100.00     100.00       1922
-----
-----[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.60       0.39       3970
-----! (label_id: 1)                                          0.35      10.66       0.68        122
-----, (label_id: 2)                                          2.09       0.53       0.85      13469
------ (label_id: 3)                                          2.29      19.32       4.10       1201
-----. (label_id: 4)                                         43.43      11.24      17.86      11227
-----: (label_id: 5)                                          0.63       2.30       0.99        304
-----; (label_id: 6)                                          0.00       0.00       0.00        141
-----? (label_id: 7)                                          4.52       6.86       5.45       1006
-----— (label_id: 8)                                          1.15       0.23       0.38        444
-----… (label_id: 9)                                          0.45       2.67       0.78         75
------------------------
-----micro avg                                                5.26       5.26       5.26      31959
-----macro avg                                                5.52       5.44       3.15      31959
-----weighted avg                                            16.42       5.26       7.02      31959
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1985
------------------------
-----micro avg                                              100.00     100.00     100.00       1985
-----macro avg                                              100.00     100.00     100.00       1985
-----weighted avg                                           100.00     100.00     100.00       1985
-----
-----[INFO] - Epoch 5, step 2394: val_loss was not in top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.23       0.48       0.31       4126
-----! (label_id: 1)                                          0.29       9.40       0.56        117
-----, (label_id: 2)                                          1.91       0.49       0.77      14019
------ (label_id: 3)                                          2.52      22.59       4.53       1164
-----. (label_id: 4)                                         44.15      11.65      18.44      11789
-----: (label_id: 5)                                          0.72       2.41       1.11        332
-----; (label_id: 6)                                          0.56       0.61       0.58        165
-----? (label_id: 7)                                          3.89       6.53       4.88        980
-----— (label_id: 8)                                          2.30       0.47       0.77        430
-----… (label_id: 9)                                          1.18       8.33       2.07         60
------------------------
-----micro avg                                                5.47       5.47       5.47      33182
-----macro avg                                                5.77       6.30       3.40      33182
-----weighted avg                                            16.77       5.47       7.25      33182
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2063
------------------------
-----micro avg                                              100.00     100.00     100.00       2063
-----macro avg                                              100.00     100.00     100.00       2063
-----weighted avg                                           100.00     100.00     100.00       2063
-----
-----[INFO] - Epoch 6, step 2793: val_loss was not in top 3
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index fa37b4c..6b5ff89 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             self.hparams.model.punct_class_weights=None
---- 
----+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
----+        self.mlp = MultiLayerPerceptron(
----+            self.transformer.config.hidden_size,
----+            self.transformer.config.hidden_size,
----+            num_layers=self.hparams.model.mlp.num_fc_layers, 
----+            activation=self.hparams.model.mlp.activation, 
----+            log_softmax=self.hparams.model.mlp.log_softmax
----+        )
----+
----         self.punct_classifier = TokenClassifier(
----             hidden_size=self.transformer.config.hidden_size,
----             num_classes=len(self.labels_to_ids),
----@@ -80,6 +89,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----             log_softmax=self.hparams.model.domain_head.log_softmax,
----             dropout=self.hparams.model.domain_head.fc_dropout,
----             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
----+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
----         )
---- 
----         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
----@@ -119,14 +129,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
----         self.freeze()
---- 
-----    def forward(self, input_ids, attention_mask, domain_ids=None):
----+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
----         hidden_states = self.transformer(
----             input_ids=input_ids, attention_mask=attention_mask
----         )[0]
----+        hidden_states = self.dropout(hidden_states)
----+        hidden_states = self.mlp(hidden_states)
----         punct_logits = self.punct_classifier(hidden_states=hidden_states)
----         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
----         domain_logits = self.domain_classifier(
-----            hidden_states=reverse_grad_hidden_states)
----+            hidden_states=reverse_grad_hidden_states,
----+            subtoken_mask=subtoken_mask)
----         return punct_logits, domain_logits
---- 
----     def _make_step(self, batch):
----@@ -136,7 +149,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         punct_labels = batch['labels']
----         domain_labels = batch['domain']
----         punct_logits, domain_logits = self(
-----            input_ids=input_ids, attention_mask=attention_mask
----+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
----         )
----         punctuation_loss = self.punctuation_loss(
----             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/lightning_logs.txt
---deleted file mode 100644
---index 8942cad..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/lightning_logs.txt
---+++ /dev/null
---@@ -1,5 +0,0 @@
----Global seed set to 42
----GPU available: True, used: True
----TPU available: None, using: 0 TPU cores
----LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
----Using native 16bit precision.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/nemo_error_log.txt
---deleted file mode 100644
---index 96631f6..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/nemo_error_log.txt
---+++ /dev/null
---@@ -1 +0,0 @@
----[NeMo W 2021-02-08 10:51:23 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index 8d946e1..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,3 +0,0 @@
----[NeMo I 2021-02-08 10:51:23 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-51-23
----[NeMo I 2021-02-08 10:51:23 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-08 10:51:23 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/git-info.log
---deleted file mode 100644
---index 3daac52..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/git-info.log
---+++ /dev/null
---@@ -1,623 +0,0 @@
----commit hash: 939a671c8c117db6975316767ced5d95449e2b27
----diff --git a/README.md b/README.md
----index d52dba5..0b403d3 100644
------- a/README.md
----+++ b/README.md
----@@ -333,4 +333,71 @@ label                                                precision    recall       f
----  'punct_f1': 0.5858508944511414,
----  'punct_precision': 0.30176490545272827,
----  'punct_recall': 10.0,
----- 'test_loss': 0.8140875697135925}
----\ No newline at end of file
----+ 'test_loss': 0.8140875697135925}
----+
----+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
----+
----+ (label_id: 0)                                          62.15     100.00      76.66       5154
----+! (label_id: 1)                                          0.00       0.00       0.00        108
----+, (label_id: 2)                                          0.00       0.00       0.00      18022
----+- (label_id: 3)                                          0.00       0.00       0.00       1557
----+. (label_id: 4)                                         41.74      94.01      57.81      15164
----+: (label_id: 5)                                          0.00       0.00       0.00        319
----+; (label_id: 6)                                          0.00       0.00       0.00         88
----+? (label_id: 7)                                          0.00       0.00       0.00       1217
----+ (label_id: 8)                                          0.00       0.00       0.00        752
----+… (label_id: 9)                                          0.00       0.00       0.00         67
----+-------------------
----+micro avg                                               45.72      45.72      45.72      42448
----+macro avg                                               10.39      19.40      13.45      42448
----+weighted avg                                            22.46      45.72      29.96      42448
----+{ 'punct_f1': 13.446383476257324,
----+ 'punct_precision': 10.388500213623047,
----+ 'punct_recall': 19.400554656982422,
----+ 'test_loss': 0.44148480892181396}
----+
----+
----+ ## Log for 8/2/2021
----+
----+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
----+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
----+
----+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
----+End frozen 
----+micro avg                                               41.42      41.42      41.42      33406
----+macro avg                                               11.01      13.54      11.07      33406
----+weighted avg                                            34.88      41.42      34.20      33406
----+
----+1st layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               36.65      36.65      36.65      33463
----+macro avg                                               10.71       9.91       8.26      33463
----+weighted avg                                            34.32      36.65      31.08      33463
----+
----+2nd layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               35.72      35.72      35.72      42448
----+macro avg                                                3.57      10.00       5.26      42448
----+weighted avg                                            12.76      35.72      18.81      42448
----+
----+{'punct_f1': 5.264181137084961,
----+ 'punct_precision': 3.572371006011963,
----+ 'punct_recall': 10.0,
----+ 'test_loss': 18.49854850769043}
----+
----+
----+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
----+alpha from 3->4 seems to reduce convergence rate.
----+
----+micro avg                                               50.98      50.98      50.98      33463
----+macro avg                                               25.99      25.38      23.38      33463
----+weighted avg                                            50.31      50.98      48.27      33463
----+
----+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
----+micro avg                                               58.55      58.55      58.55      39340
----+macro avg                                               30.02      29.74      29.52      39340
----+weighted avg                                            57.91      58.55      57.51      39340
----+
----+still increasing?!
----+{'punct_f1': 29.523975372314453,
----+ 'punct_precision': 30.015613555908203,
----+ 'punct_recall': 29.738296508789062,
----+ 'test_loss': 0.3690211772918701}
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index fe58670..26bf21c 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,49 +1,49 @@
---- seed: 42
---- trainer:
-----    # gpus: 1 # the number of gpus, 0 for CPU
-----    # num_nodes: 1
-----    # max_epochs: 2
-----    # max_steps: null # precedence over max_epochs
-----    # accumulate_grad_batches: 4 # accumulates grads every k batches
-----    # gradient_clip_val: 0
-----    # amp_level: O1 # O1/O2 for mixed precision
-----    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
-----    # checkpoint_callback: false  # Provided by exp_manager
-----    # logger: false #false  # Provided by exp_manager
-----    # log_every_n_steps: 1  # Interval of logging.
-----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # resume_from_checkpoint: null
-----
-----    gpus: 0 # the number of gpus, 0 for CPU
----+    gpus: 1 # the number of gpus, 0 for CPU
----     num_nodes: 1
-----    max_epochs: 8
----+    max_epochs: 10
----     max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 1 # accumulates grads every k batches
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----     gradient_clip_val: 0
-----    amp_level: O0 # O1/O2 for mixed precision
-----    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
----+    amp_level: O1 # O1/O2 for mixed precision
----+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    accelerator: ddp
----     checkpoint_callback: false  # Provided by exp_manager
----     logger: false #false  # Provided by exp_manager
----     log_every_n_steps: 1  # Interval of logging.
----     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    reload_dataloaders_every_epoch: true
----     resume_from_checkpoint: null
---- 
----+    # gpus: 0 # the number of gpus, 0 for CPU
----+    # num_nodes: 1
----+    # max_epochs: 8
----+    # max_steps: null # precedence over max_epochs
----+    # accumulate_grad_batches: 1 # accumulates grads every k batches
----+    # gradient_clip_val: 0
----+    # amp_level: O0 # O1/O2 for mixed precision
----+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # # accelerator: ddp
----+    # checkpoint_callback: false  # Provided by exp_manager
----+    # logger: false #false  # Provided by exp_manager
----+    # log_every_n_steps: 1  # Interval of logging.
----+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    # reload_dataloaders_every_epoch: true
----+    # resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /home/nxingyu2/data # /root/data # 
-----tmp_path: /home/nxingyu2/data/tmp # /tmp # 
----+base_path: /home/nxingyu/data # /root/data # 
----+tmp_path: /home/nxingyu/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
-----    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----     unfrozen: 0
-----    maximum_unfrozen: 2
----+    maximum_unfrozen: 1
----     unfreeze_step: 1
----     # unfreeze_every: 3
----     punct_label_ids:
----@@ -58,10 +58,10 @@ model:
----         - "—"
----         - "…"
---- 
-----    punct_class_weights: true
----+    punct_class_weights: false
----     
----     dataset:
-----        data_dir: /home/nxingyu2/data # /root/data # 
----+        data_dir: /home/nxingyu/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -106,6 +106,12 @@ model:
----         config: null
----         # unfrozen_layers: 1
---- 
----+    mlp:
----+        num_fc_layers: 2
----+        fc_dropout: 0.1
----+        log_softmax: false
----+        activation: 'relu'
----+        
----     punct_head:
----         punct_num_fc_layers: 1
----         fc_dropout: 0.1
----@@ -122,17 +128,19 @@ model:
----         use_transformer_init: true
----         loss: 'cel'
----         gamma: 0.1 # coefficient of gradient reversal
----+        pooling: 'token'
----+        idx_conditioned_on: 0
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 3
----+        alpha: 4
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: novograd
----+        name: adamw
----         lr: 1e-3
----         weight_decay: 0.00
----         sched:
----diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
----index d4ff927..1a6fe93 100644
------- a/experiment/core/layers/sequence_classifier.py
----+++ b/experiment/core/layers/sequence_classifier.py
----@@ -1,3 +1,4 @@
----+import torch
---- from torch import nn
---- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
---- from core.utils import transformer_weights_init
----@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
----         log_softmax: bool = True,
----         dropout: float = 0.0,
----         use_transformer_init: bool = True,
-----        idx_conditioned_on: int = 0,
----+        pooling: str = 'mean', # mean, max, mean_max, token
----+        idx_conditioned_on: int = None,
----     ):
----         """
----         Initializes the SequenceClassifier module.
----@@ -37,6 +39,7 @@ class SequenceClassifier(nn.Module):
----         super().__init__()
----         self.log_softmax = log_softmax
----         self._idx_conditioned_on = idx_conditioned_on
----+        self.pooling = pooling
----         self.mlp = MultiLayerPerceptron(
----             hidden_size=hidden_size,
----             num_classes=num_classes,
----@@ -48,7 +51,23 @@ class SequenceClassifier(nn.Module):
----         if use_transformer_init:
----             self.apply(lambda module: transformer_weights_init(module, xavier=False))
---- 
-----    def forward(self, hidden_states):
----+    def forward(self, hidden_states, subtoken_mask=None):
----         hidden_states = self.dropout(hidden_states)
-----        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
----+        if self.pooling=='token':
----+            pooled = hidden_states[:, self._idx_conditioned_on]
----+        else:
----+            if subtoken_mask==None:
----+                ct=hidden_states.shape[1] # Seq len
----+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
----+            else:
----+                ct = torch.sum(subtoken_mask,axis=1)
----+            pooled_sum = torch.sum(hidden_states,axis=1)
----+            if self.pooling=='mean' or self.pooling == 'mean_max':
----+                pooled_mean = pooled_sum//ct
----+            if self.pooling=='max' or self.pooling=='mean_max':
----+                pooled_max = torch.max(hidden_states,axis=1)[0]
----+            pooled=pooled_mean if self.pooling=='mean' else \
----+                pooled_max if self.pooling=='max' else \
----+                    torch.cat([pooled_mean,pooled_max],axis=-1)
----+        logits = self.mlp(pooled)
----         return logits
----diff --git a/experiment/info.log b/experiment/info.log
----index 2471fe9..e69de29 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,300 +0,0 @@
-----[INFO] - GPU available: True, used: False
-----[INFO] - TPU available: None, using: 0 TPU cores
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        184
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.23       0.34       0.53        594
------ (label_id: 3)                                          3.06      25.42       5.46         59
-----. (label_id: 4)                                         47.22      12.98      20.36        524
-----: (label_id: 5)                                          0.00       0.00       0.00         18
-----; (label_id: 6)                                          0.00       0.00       0.00         13
-----? (label_id: 7)                                          8.45       6.32       7.23         95
-----— (label_id: 8)                                          0.00       0.00       0.00         12
-----… (label_id: 9)                                          0.00       0.00       0.00          0
------------------------
-----micro avg                                                6.05       6.05       6.05       1503
-----macro avg                                                6.66       5.01       3.73       1503
-----weighted avg                                            17.61       6.05       7.98       1503
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00         92
------------------------
-----micro avg                                              100.00     100.00     100.00         92
-----macro avg                                              100.00     100.00     100.00         92
-----weighted avg                                           100.00     100.00     100.00         92
-----
-----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 1.5848931924611143e-08
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
-----will be used during training (effective maximum steps = 3192) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 3192
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        202
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.62       0.45       0.70        669
------ (label_id: 3)                                          3.48      27.27       6.17         66
-----. (label_id: 4)                                         45.06      13.01      20.19        561
-----: (label_id: 5)                                          1.52       6.67       2.47         15
-----; (label_id: 6)                                          0.00       0.00       0.00         15
-----? (label_id: 7)                                          8.70       7.32       7.95         82
-----— (label_id: 8)                                          0.00       0.00       0.00         13
-----… (label_id: 9)                                          0.00       0.00       0.00          1
------------------------
-----micro avg                                                6.20       6.20       6.20       1628
-----macro avg                                                6.04       5.47       3.75       1628
-----weighted avg                                            16.79       6.20       7.92       1628
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        101
------------------------
-----micro avg                                              100.00     100.00     100.00        101
-----macro avg                                              100.00     100.00     100.00        101
-----weighted avg                                           100.00     100.00     100.00        101
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.34       0.73       0.46       4402
-----! (label_id: 1)                                          0.42      13.95       0.82        129
-----, (label_id: 2)                                          2.53       0.64       1.03      15243
------ (label_id: 3)                                          2.45      21.03       4.38       1322
-----. (label_id: 4)                                         44.00      11.40      18.11      12542
-----: (label_id: 5)                                          0.43       1.41       0.65        354
-----; (label_id: 6)                                          0.00       0.00       0.00        163
-----? (label_id: 7)                                          4.16       6.27       5.00       1117
-----— (label_id: 8)                                          3.00       0.61       1.02        488
-----… (label_id: 9)                                          0.97       6.17       1.68         81
------------------------
-----micro avg                                                5.41       5.41       5.41      35841
-----macro avg                                                5.83       6.22       3.32      35841
-----weighted avg                                            16.78       5.41       7.18      35841
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2201
------------------------
-----micro avg                                              100.00     100.00     100.00       2201
-----macro avg                                              100.00     100.00     100.00       2201
-----weighted avg                                           100.00     100.00     100.00       2201
-----
-----[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.20       0.43       0.27       4226
-----! (label_id: 1)                                          0.44      14.17       0.86        127
-----, (label_id: 2)                                          1.93       0.49       0.78      14611
------ (label_id: 3)                                          2.23      19.56       4.01       1237
-----. (label_id: 4)                                         43.37      11.25      17.86      11977
-----: (label_id: 5)                                          0.68       2.34       1.05        342
-----; (label_id: 6)                                          0.00       0.00       0.00        129
-----? (label_id: 7)                                          5.16       7.47       6.10       1058
-----— (label_id: 8)                                          2.15       0.49       0.80        409
-----… (label_id: 9)                                          0.69       4.23       1.19         71
------------------------
-----micro avg                                                5.23       5.23       5.23      34187
-----macro avg                                                5.68       6.04       3.29      34187
-----weighted avg                                            16.32       5.23       6.98      34187
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2113
------------------------
-----micro avg                                              100.00     100.00     100.00       2113
-----macro avg                                              100.00     100.00     100.00       2113
-----weighted avg                                           100.00     100.00     100.00       2113
-----
-----[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.61       0.39       4228
-----! (label_id: 1)                                          0.30       8.28       0.58        145
-----, (label_id: 2)                                          2.27       0.58       0.92      14495
------ (label_id: 3)                                          2.64      21.78       4.70       1327
-----. (label_id: 4)                                         44.87      11.66      18.51      12193
-----: (label_id: 5)                                          0.60       1.93       0.91        362
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.19       6.40       5.07       1078
-----— (label_id: 8)                                          1.16       0.22       0.37        459
-----… (label_id: 9)                                          0.85       4.17       1.41         96
------------------------
-----micro avg                                                5.54       5.54       5.54      34547
-----macro avg                                                5.72       5.56       3.29      34547
-----weighted avg                                            17.08       5.54       7.33      34547
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2114
------------------------
-----micro avg                                              100.00     100.00     100.00       2114
-----macro avg                                              100.00     100.00     100.00       2114
-----weighted avg                                           100.00     100.00     100.00       2114
-----
-----[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.29       0.63       0.40       4444
-----! (label_id: 1)                                          0.38      10.67       0.74        150
-----, (label_id: 2)                                          2.32       0.59       0.94      15290
------ (label_id: 3)                                          2.34      20.28       4.19       1292
-----. (label_id: 4)                                         43.85      11.68      18.44      12599
-----: (label_id: 5)                                          0.41       1.28       0.62        392
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.24       6.30       5.07       1111
-----— (label_id: 8)                                          0.00       0.00       0.00        456
-----… (label_id: 9)                                          0.38       2.41       0.65         83
------------------------
-----micro avg                                                5.40       5.40       5.40      35981
-----macro avg                                                5.42       5.38       3.11      35981
-----weighted avg                                            16.59       5.40       7.22      35981
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2222
------------------------
-----micro avg                                              100.00     100.00     100.00       2222
-----macro avg                                              100.00     100.00     100.00       2222
-----weighted avg                                           100.00     100.00     100.00       2222
-----
-----[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.35       0.73       0.48       3844
-----! (label_id: 1)                                          0.54      14.62       1.04        130
-----, (label_id: 2)                                          2.32       0.59       0.94      13056
------ (label_id: 3)                                          2.67      22.28       4.77       1194
-----. (label_id: 4)                                         44.45      11.95      18.84      10791
-----: (label_id: 5)                                          0.84       3.21       1.33        280
-----; (label_id: 6)                                          0.00       0.00       0.00        140
-----? (label_id: 7)                                          4.17       6.56       5.10        914
-----— (label_id: 8)                                          0.00       0.00       0.00        401
-----… (label_id: 9)                                          0.48       2.63       0.81         76
------------------------
-----micro avg                                                5.68       5.68       5.68      30826
-----macro avg                                                5.58       6.26       3.33      30826
-----weighted avg                                            16.82       5.68       7.41      30826
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1922
------------------------
-----micro avg                                              100.00     100.00     100.00       1922
-----macro avg                                              100.00     100.00     100.00       1922
-----weighted avg                                           100.00     100.00     100.00       1922
-----
-----[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.60       0.39       3970
-----! (label_id: 1)                                          0.35      10.66       0.68        122
-----, (label_id: 2)                                          2.09       0.53       0.85      13469
------ (label_id: 3)                                          2.29      19.32       4.10       1201
-----. (label_id: 4)                                         43.43      11.24      17.86      11227
-----: (label_id: 5)                                          0.63       2.30       0.99        304
-----; (label_id: 6)                                          0.00       0.00       0.00        141
-----? (label_id: 7)                                          4.52       6.86       5.45       1006
-----— (label_id: 8)                                          1.15       0.23       0.38        444
-----… (label_id: 9)                                          0.45       2.67       0.78         75
------------------------
-----micro avg                                                5.26       5.26       5.26      31959
-----macro avg                                                5.52       5.44       3.15      31959
-----weighted avg                                            16.42       5.26       7.02      31959
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1985
------------------------
-----micro avg                                              100.00     100.00     100.00       1985
-----macro avg                                              100.00     100.00     100.00       1985
-----weighted avg                                           100.00     100.00     100.00       1985
-----
-----[INFO] - Epoch 5, step 2394: val_loss was not in top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.23       0.48       0.31       4126
-----! (label_id: 1)                                          0.29       9.40       0.56        117
-----, (label_id: 2)                                          1.91       0.49       0.77      14019
------ (label_id: 3)                                          2.52      22.59       4.53       1164
-----. (label_id: 4)                                         44.15      11.65      18.44      11789
-----: (label_id: 5)                                          0.72       2.41       1.11        332
-----; (label_id: 6)                                          0.56       0.61       0.58        165
-----? (label_id: 7)                                          3.89       6.53       4.88        980
-----— (label_id: 8)                                          2.30       0.47       0.77        430
-----… (label_id: 9)                                          1.18       8.33       2.07         60
------------------------
-----micro avg                                                5.47       5.47       5.47      33182
-----macro avg                                                5.77       6.30       3.40      33182
-----weighted avg                                            16.77       5.47       7.25      33182
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2063
------------------------
-----micro avg                                              100.00     100.00     100.00       2063
-----macro avg                                              100.00     100.00     100.00       2063
-----weighted avg                                           100.00     100.00     100.00       2063
-----
-----[INFO] - Epoch 6, step 2793: val_loss was not in top 3
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index fa37b4c..6b5ff89 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             self.hparams.model.punct_class_weights=None
---- 
----+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
----+        self.mlp = MultiLayerPerceptron(
----+            self.transformer.config.hidden_size,
----+            self.transformer.config.hidden_size,
----+            num_layers=self.hparams.model.mlp.num_fc_layers, 
----+            activation=self.hparams.model.mlp.activation, 
----+            log_softmax=self.hparams.model.mlp.log_softmax
----+        )
----+
----         self.punct_classifier = TokenClassifier(
----             hidden_size=self.transformer.config.hidden_size,
----             num_classes=len(self.labels_to_ids),
----@@ -80,6 +89,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----             log_softmax=self.hparams.model.domain_head.log_softmax,
----             dropout=self.hparams.model.domain_head.fc_dropout,
----             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
----+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
----         )
---- 
----         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
----@@ -119,14 +129,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
----         self.freeze()
---- 
-----    def forward(self, input_ids, attention_mask, domain_ids=None):
----+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
----         hidden_states = self.transformer(
----             input_ids=input_ids, attention_mask=attention_mask
----         )[0]
----+        hidden_states = self.dropout(hidden_states)
----+        hidden_states = self.mlp(hidden_states)
----         punct_logits = self.punct_classifier(hidden_states=hidden_states)
----         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
----         domain_logits = self.domain_classifier(
-----            hidden_states=reverse_grad_hidden_states)
----+            hidden_states=reverse_grad_hidden_states,
----+            subtoken_mask=subtoken_mask)
----         return punct_logits, domain_logits
---- 
----     def _make_step(self, batch):
----@@ -136,7 +149,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         punct_labels = batch['labels']
----         domain_labels = batch['domain']
----         punct_logits, domain_logits = self(
-----            input_ids=input_ids, attention_mask=attention_mask
----+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
----         )
----         punctuation_loss = self.punctuation_loss(
----             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/lightning_logs.txt
---deleted file mode 100644
---index 9fa336d..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/lightning_logs.txt
---+++ /dev/null
---@@ -1,24 +0,0 @@
----Global seed set to 42
----GPU available: True, used: True
----TPU available: None, using: 0 TPU cores
----LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
----Using native 16bit precision.
----Global seed set to 42
----initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | dropout             | Dropout              | 0     
----2 | mlp                 | MultiLayerPerceptron | 131 K 
----3 | punct_classifier    | TokenClassifier      | 2.6 K 
----4 | domain_classifier   | SequenceClassifier   | 257   
----5 | punctuation_loss    | FocalDiceLoss        | 0     
----6 | domain_loss         | CrossEntropyLoss     | 0     
----7 | agg_loss            | AggregatorLoss       | 0     
----8 | punct_class_report  | ClassificationReport | 0     
----9 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----167 K     Trainable params
----13.4 M    Non-trainable params
----13.6 M    Total params
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/nemo_error_log.txt
---deleted file mode 100644
---index 315a43b..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/nemo_error_log.txt
---+++ /dev/null
---@@ -1,7 +0,0 @@
----[NeMo W 2021-02-08 10:51:41 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-08 10:51:58 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 10:51:58 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index 179aed7..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,9 +0,0 @@
----[NeMo I 2021-02-08 10:51:41 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-51-41
----[NeMo I 2021-02-08 10:51:41 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-08 10:51:41 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-08 10:51:58 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 10:51:58 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/git-info.log
---deleted file mode 100644
---index a2ded57..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/git-info.log
---+++ /dev/null
---@@ -1,624 +0,0 @@
----commit hash: 939a671c8c117db6975316767ced5d95449e2b27
----diff --git a/README.md b/README.md
----index d52dba5..0b403d3 100644
------- a/README.md
----+++ b/README.md
----@@ -333,4 +333,71 @@ label                                                precision    recall       f
----  'punct_f1': 0.5858508944511414,
----  'punct_precision': 0.30176490545272827,
----  'punct_recall': 10.0,
----- 'test_loss': 0.8140875697135925}
----\ No newline at end of file
----+ 'test_loss': 0.8140875697135925}
----+
----+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
----+
----+ (label_id: 0)                                          62.15     100.00      76.66       5154
----+! (label_id: 1)                                          0.00       0.00       0.00        108
----+, (label_id: 2)                                          0.00       0.00       0.00      18022
----+- (label_id: 3)                                          0.00       0.00       0.00       1557
----+. (label_id: 4)                                         41.74      94.01      57.81      15164
----+: (label_id: 5)                                          0.00       0.00       0.00        319
----+; (label_id: 6)                                          0.00       0.00       0.00         88
----+? (label_id: 7)                                          0.00       0.00       0.00       1217
----+ (label_id: 8)                                          0.00       0.00       0.00        752
----+… (label_id: 9)                                          0.00       0.00       0.00         67
----+-------------------
----+micro avg                                               45.72      45.72      45.72      42448
----+macro avg                                               10.39      19.40      13.45      42448
----+weighted avg                                            22.46      45.72      29.96      42448
----+{ 'punct_f1': 13.446383476257324,
----+ 'punct_precision': 10.388500213623047,
----+ 'punct_recall': 19.400554656982422,
----+ 'test_loss': 0.44148480892181396}
----+
----+
----+ ## Log for 8/2/2021
----+
----+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
----+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
----+
----+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
----+End frozen 
----+micro avg                                               41.42      41.42      41.42      33406
----+macro avg                                               11.01      13.54      11.07      33406
----+weighted avg                                            34.88      41.42      34.20      33406
----+
----+1st layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               36.65      36.65      36.65      33463
----+macro avg                                               10.71       9.91       8.26      33463
----+weighted avg                                            34.32      36.65      31.08      33463
----+
----+2nd layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               35.72      35.72      35.72      42448
----+macro avg                                                3.57      10.00       5.26      42448
----+weighted avg                                            12.76      35.72      18.81      42448
----+
----+{'punct_f1': 5.264181137084961,
----+ 'punct_precision': 3.572371006011963,
----+ 'punct_recall': 10.0,
----+ 'test_loss': 18.49854850769043}
----+
----+
----+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
----+alpha from 3->4 seems to reduce convergence rate.
----+
----+micro avg                                               50.98      50.98      50.98      33463
----+macro avg                                               25.99      25.38      23.38      33463
----+weighted avg                                            50.31      50.98      48.27      33463
----+
----+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
----+micro avg                                               58.55      58.55      58.55      39340
----+macro avg                                               30.02      29.74      29.52      39340
----+weighted avg                                            57.91      58.55      57.51      39340
----+
----+still increasing?!
----+{'punct_f1': 29.523975372314453,
----+ 'punct_precision': 30.015613555908203,
----+ 'punct_recall': 29.738296508789062,
----+ 'test_loss': 0.3690211772918701}
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index fe58670..26bf21c 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,49 +1,49 @@
---- seed: 42
---- trainer:
-----    # gpus: 1 # the number of gpus, 0 for CPU
-----    # num_nodes: 1
-----    # max_epochs: 2
-----    # max_steps: null # precedence over max_epochs
-----    # accumulate_grad_batches: 4 # accumulates grads every k batches
-----    # gradient_clip_val: 0
-----    # amp_level: O1 # O1/O2 for mixed precision
-----    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
-----    # checkpoint_callback: false  # Provided by exp_manager
-----    # logger: false #false  # Provided by exp_manager
-----    # log_every_n_steps: 1  # Interval of logging.
-----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # resume_from_checkpoint: null
-----
-----    gpus: 0 # the number of gpus, 0 for CPU
----+    gpus: 1 # the number of gpus, 0 for CPU
----     num_nodes: 1
-----    max_epochs: 8
----+    max_epochs: 10
----     max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 1 # accumulates grads every k batches
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----     gradient_clip_val: 0
-----    amp_level: O0 # O1/O2 for mixed precision
-----    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
----+    amp_level: O1 # O1/O2 for mixed precision
----+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    accelerator: ddp
----     checkpoint_callback: false  # Provided by exp_manager
----     logger: false #false  # Provided by exp_manager
----     log_every_n_steps: 1  # Interval of logging.
----     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    reload_dataloaders_every_epoch: true
----     resume_from_checkpoint: null
---- 
----+    # gpus: 0 # the number of gpus, 0 for CPU
----+    # num_nodes: 1
----+    # max_epochs: 8
----+    # max_steps: null # precedence over max_epochs
----+    # accumulate_grad_batches: 1 # accumulates grads every k batches
----+    # gradient_clip_val: 0
----+    # amp_level: O0 # O1/O2 for mixed precision
----+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # # accelerator: ddp
----+    # checkpoint_callback: false  # Provided by exp_manager
----+    # logger: false #false  # Provided by exp_manager
----+    # log_every_n_steps: 1  # Interval of logging.
----+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    # reload_dataloaders_every_epoch: true
----+    # resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /home/nxingyu2/data # /root/data # 
-----tmp_path: /home/nxingyu2/data/tmp # /tmp # 
----+base_path: /home/nxingyu/data # /root/data # 
----+tmp_path: /home/nxingyu/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
-----    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----     unfrozen: 0
-----    maximum_unfrozen: 2
----+    maximum_unfrozen: 1
----     unfreeze_step: 1
----     # unfreeze_every: 3
----     punct_label_ids:
----@@ -58,10 +58,10 @@ model:
----         - "—"
----         - "…"
---- 
-----    punct_class_weights: true
----+    punct_class_weights: false
----     
----     dataset:
-----        data_dir: /home/nxingyu2/data # /root/data # 
----+        data_dir: /home/nxingyu/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -106,6 +106,12 @@ model:
----         config: null
----         # unfrozen_layers: 1
---- 
----+    mlp:
----+        num_fc_layers: 2
----+        fc_dropout: 0.1
----+        log_softmax: false
----+        activation: 'relu'
----+        
----     punct_head:
----         punct_num_fc_layers: 1
----         fc_dropout: 0.1
----@@ -122,17 +128,19 @@ model:
----         use_transformer_init: true
----         loss: 'cel'
----         gamma: 0.1 # coefficient of gradient reversal
----+        pooling: 'token'
----+        idx_conditioned_on: 0
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 3
----+        alpha: 4
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: novograd
----+        name: adamw
----         lr: 1e-3
----         weight_decay: 0.00
----         sched:
----diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
----index d4ff927..cafc57d 100644
------- a/experiment/core/layers/sequence_classifier.py
----+++ b/experiment/core/layers/sequence_classifier.py
----@@ -1,3 +1,4 @@
----+import torch
---- from torch import nn
---- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
---- from core.utils import transformer_weights_init
----@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
----         log_softmax: bool = True,
----         dropout: float = 0.0,
----         use_transformer_init: bool = True,
-----        idx_conditioned_on: int = 0,
----+        pooling: str = 'mean', # mean, max, mean_max, token
----+        idx_conditioned_on: int = None,
----     ):
----         """
----         Initializes the SequenceClassifier module.
----@@ -37,6 +39,7 @@ class SequenceClassifier(nn.Module):
----         super().__init__()
----         self.log_softmax = log_softmax
----         self._idx_conditioned_on = idx_conditioned_on
----+        self.pooling = pooling
----         self.mlp = MultiLayerPerceptron(
----             hidden_size=hidden_size,
----             num_classes=num_classes,
----@@ -48,7 +51,24 @@ class SequenceClassifier(nn.Module):
----         if use_transformer_init:
----             self.apply(lambda module: transformer_weights_init(module, xavier=False))
---- 
-----    def forward(self, hidden_states):
----+    def forward(self, hidden_states, subtoken_mask=None):
----         hidden_states = self.dropout(hidden_states)
-----        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
----+        pp(self.pooling)
----+        if self.pooling=='token':
----+            pooled = hidden_states[:, self._idx_conditioned_on]
----+        else:
----+            if subtoken_mask==None:
----+                ct=hidden_states.shape[1] # Seq len
----+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
----+            else:
----+                ct = torch.sum(subtoken_mask,axis=1)
----+            pooled_sum = torch.sum(hidden_states,axis=1)
----+            if self.pooling=='mean' or self.pooling == 'mean_max':
----+                pooled_mean = pooled_sum//ct
----+            if self.pooling=='max' or self.pooling=='mean_max':
----+                pooled_max = torch.max(hidden_states,axis=1)[0]
----+            pooled=pooled_mean if self.pooling=='mean' else \
----+                pooled_max if self.pooling=='max' else \
----+                    torch.cat([pooled_mean,pooled_max],axis=-1)
----+        logits = self.mlp(pooled)
----         return logits
----diff --git a/experiment/info.log b/experiment/info.log
----index 2471fe9..e69de29 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,300 +0,0 @@
-----[INFO] - GPU available: True, used: False
-----[INFO] - TPU available: None, using: 0 TPU cores
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        184
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.23       0.34       0.53        594
------ (label_id: 3)                                          3.06      25.42       5.46         59
-----. (label_id: 4)                                         47.22      12.98      20.36        524
-----: (label_id: 5)                                          0.00       0.00       0.00         18
-----; (label_id: 6)                                          0.00       0.00       0.00         13
-----? (label_id: 7)                                          8.45       6.32       7.23         95
-----— (label_id: 8)                                          0.00       0.00       0.00         12
-----… (label_id: 9)                                          0.00       0.00       0.00          0
------------------------
-----micro avg                                                6.05       6.05       6.05       1503
-----macro avg                                                6.66       5.01       3.73       1503
-----weighted avg                                            17.61       6.05       7.98       1503
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00         92
------------------------
-----micro avg                                              100.00     100.00     100.00         92
-----macro avg                                              100.00     100.00     100.00         92
-----weighted avg                                           100.00     100.00     100.00         92
-----
-----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 1.5848931924611143e-08
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
-----will be used during training (effective maximum steps = 3192) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 3192
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        202
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.62       0.45       0.70        669
------ (label_id: 3)                                          3.48      27.27       6.17         66
-----. (label_id: 4)                                         45.06      13.01      20.19        561
-----: (label_id: 5)                                          1.52       6.67       2.47         15
-----; (label_id: 6)                                          0.00       0.00       0.00         15
-----? (label_id: 7)                                          8.70       7.32       7.95         82
-----— (label_id: 8)                                          0.00       0.00       0.00         13
-----… (label_id: 9)                                          0.00       0.00       0.00          1
------------------------
-----micro avg                                                6.20       6.20       6.20       1628
-----macro avg                                                6.04       5.47       3.75       1628
-----weighted avg                                            16.79       6.20       7.92       1628
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        101
------------------------
-----micro avg                                              100.00     100.00     100.00        101
-----macro avg                                              100.00     100.00     100.00        101
-----weighted avg                                           100.00     100.00     100.00        101
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.34       0.73       0.46       4402
-----! (label_id: 1)                                          0.42      13.95       0.82        129
-----, (label_id: 2)                                          2.53       0.64       1.03      15243
------ (label_id: 3)                                          2.45      21.03       4.38       1322
-----. (label_id: 4)                                         44.00      11.40      18.11      12542
-----: (label_id: 5)                                          0.43       1.41       0.65        354
-----; (label_id: 6)                                          0.00       0.00       0.00        163
-----? (label_id: 7)                                          4.16       6.27       5.00       1117
-----— (label_id: 8)                                          3.00       0.61       1.02        488
-----… (label_id: 9)                                          0.97       6.17       1.68         81
------------------------
-----micro avg                                                5.41       5.41       5.41      35841
-----macro avg                                                5.83       6.22       3.32      35841
-----weighted avg                                            16.78       5.41       7.18      35841
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2201
------------------------
-----micro avg                                              100.00     100.00     100.00       2201
-----macro avg                                              100.00     100.00     100.00       2201
-----weighted avg                                           100.00     100.00     100.00       2201
-----
-----[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.20       0.43       0.27       4226
-----! (label_id: 1)                                          0.44      14.17       0.86        127
-----, (label_id: 2)                                          1.93       0.49       0.78      14611
------ (label_id: 3)                                          2.23      19.56       4.01       1237
-----. (label_id: 4)                                         43.37      11.25      17.86      11977
-----: (label_id: 5)                                          0.68       2.34       1.05        342
-----; (label_id: 6)                                          0.00       0.00       0.00        129
-----? (label_id: 7)                                          5.16       7.47       6.10       1058
-----— (label_id: 8)                                          2.15       0.49       0.80        409
-----… (label_id: 9)                                          0.69       4.23       1.19         71
------------------------
-----micro avg                                                5.23       5.23       5.23      34187
-----macro avg                                                5.68       6.04       3.29      34187
-----weighted avg                                            16.32       5.23       6.98      34187
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2113
------------------------
-----micro avg                                              100.00     100.00     100.00       2113
-----macro avg                                              100.00     100.00     100.00       2113
-----weighted avg                                           100.00     100.00     100.00       2113
-----
-----[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.61       0.39       4228
-----! (label_id: 1)                                          0.30       8.28       0.58        145
-----, (label_id: 2)                                          2.27       0.58       0.92      14495
------ (label_id: 3)                                          2.64      21.78       4.70       1327
-----. (label_id: 4)                                         44.87      11.66      18.51      12193
-----: (label_id: 5)                                          0.60       1.93       0.91        362
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.19       6.40       5.07       1078
-----— (label_id: 8)                                          1.16       0.22       0.37        459
-----… (label_id: 9)                                          0.85       4.17       1.41         96
------------------------
-----micro avg                                                5.54       5.54       5.54      34547
-----macro avg                                                5.72       5.56       3.29      34547
-----weighted avg                                            17.08       5.54       7.33      34547
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2114
------------------------
-----micro avg                                              100.00     100.00     100.00       2114
-----macro avg                                              100.00     100.00     100.00       2114
-----weighted avg                                           100.00     100.00     100.00       2114
-----
-----[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.29       0.63       0.40       4444
-----! (label_id: 1)                                          0.38      10.67       0.74        150
-----, (label_id: 2)                                          2.32       0.59       0.94      15290
------ (label_id: 3)                                          2.34      20.28       4.19       1292
-----. (label_id: 4)                                         43.85      11.68      18.44      12599
-----: (label_id: 5)                                          0.41       1.28       0.62        392
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.24       6.30       5.07       1111
-----— (label_id: 8)                                          0.00       0.00       0.00        456
-----… (label_id: 9)                                          0.38       2.41       0.65         83
------------------------
-----micro avg                                                5.40       5.40       5.40      35981
-----macro avg                                                5.42       5.38       3.11      35981
-----weighted avg                                            16.59       5.40       7.22      35981
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2222
------------------------
-----micro avg                                              100.00     100.00     100.00       2222
-----macro avg                                              100.00     100.00     100.00       2222
-----weighted avg                                           100.00     100.00     100.00       2222
-----
-----[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.35       0.73       0.48       3844
-----! (label_id: 1)                                          0.54      14.62       1.04        130
-----, (label_id: 2)                                          2.32       0.59       0.94      13056
------ (label_id: 3)                                          2.67      22.28       4.77       1194
-----. (label_id: 4)                                         44.45      11.95      18.84      10791
-----: (label_id: 5)                                          0.84       3.21       1.33        280
-----; (label_id: 6)                                          0.00       0.00       0.00        140
-----? (label_id: 7)                                          4.17       6.56       5.10        914
-----— (label_id: 8)                                          0.00       0.00       0.00        401
-----… (label_id: 9)                                          0.48       2.63       0.81         76
------------------------
-----micro avg                                                5.68       5.68       5.68      30826
-----macro avg                                                5.58       6.26       3.33      30826
-----weighted avg                                            16.82       5.68       7.41      30826
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1922
------------------------
-----micro avg                                              100.00     100.00     100.00       1922
-----macro avg                                              100.00     100.00     100.00       1922
-----weighted avg                                           100.00     100.00     100.00       1922
-----
-----[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.60       0.39       3970
-----! (label_id: 1)                                          0.35      10.66       0.68        122
-----, (label_id: 2)                                          2.09       0.53       0.85      13469
------ (label_id: 3)                                          2.29      19.32       4.10       1201
-----. (label_id: 4)                                         43.43      11.24      17.86      11227
-----: (label_id: 5)                                          0.63       2.30       0.99        304
-----; (label_id: 6)                                          0.00       0.00       0.00        141
-----? (label_id: 7)                                          4.52       6.86       5.45       1006
-----— (label_id: 8)                                          1.15       0.23       0.38        444
-----… (label_id: 9)                                          0.45       2.67       0.78         75
------------------------
-----micro avg                                                5.26       5.26       5.26      31959
-----macro avg                                                5.52       5.44       3.15      31959
-----weighted avg                                            16.42       5.26       7.02      31959
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1985
------------------------
-----micro avg                                              100.00     100.00     100.00       1985
-----macro avg                                              100.00     100.00     100.00       1985
-----weighted avg                                           100.00     100.00     100.00       1985
-----
-----[INFO] - Epoch 5, step 2394: val_loss was not in top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.23       0.48       0.31       4126
-----! (label_id: 1)                                          0.29       9.40       0.56        117
-----, (label_id: 2)                                          1.91       0.49       0.77      14019
------ (label_id: 3)                                          2.52      22.59       4.53       1164
-----. (label_id: 4)                                         44.15      11.65      18.44      11789
-----: (label_id: 5)                                          0.72       2.41       1.11        332
-----; (label_id: 6)                                          0.56       0.61       0.58        165
-----? (label_id: 7)                                          3.89       6.53       4.88        980
-----— (label_id: 8)                                          2.30       0.47       0.77        430
-----… (label_id: 9)                                          1.18       8.33       2.07         60
------------------------
-----micro avg                                                5.47       5.47       5.47      33182
-----macro avg                                                5.77       6.30       3.40      33182
-----weighted avg                                            16.77       5.47       7.25      33182
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2063
------------------------
-----micro avg                                              100.00     100.00     100.00       2063
-----macro avg                                              100.00     100.00     100.00       2063
-----weighted avg                                           100.00     100.00     100.00       2063
-----
-----[INFO] - Epoch 6, step 2793: val_loss was not in top 3
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index fa37b4c..6b5ff89 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             self.hparams.model.punct_class_weights=None
---- 
----+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
----+        self.mlp = MultiLayerPerceptron(
----+            self.transformer.config.hidden_size,
----+            self.transformer.config.hidden_size,
----+            num_layers=self.hparams.model.mlp.num_fc_layers, 
----+            activation=self.hparams.model.mlp.activation, 
----+            log_softmax=self.hparams.model.mlp.log_softmax
----+        )
----+
----         self.punct_classifier = TokenClassifier(
----             hidden_size=self.transformer.config.hidden_size,
----             num_classes=len(self.labels_to_ids),
----@@ -80,6 +89,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----             log_softmax=self.hparams.model.domain_head.log_softmax,
----             dropout=self.hparams.model.domain_head.fc_dropout,
----             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
----+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
----         )
---- 
----         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
----@@ -119,14 +129,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
----         self.freeze()
---- 
-----    def forward(self, input_ids, attention_mask, domain_ids=None):
----+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
----         hidden_states = self.transformer(
----             input_ids=input_ids, attention_mask=attention_mask
----         )[0]
----+        hidden_states = self.dropout(hidden_states)
----+        hidden_states = self.mlp(hidden_states)
----         punct_logits = self.punct_classifier(hidden_states=hidden_states)
----         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
----         domain_logits = self.domain_classifier(
-----            hidden_states=reverse_grad_hidden_states)
----+            hidden_states=reverse_grad_hidden_states,
----+            subtoken_mask=subtoken_mask)
----         return punct_logits, domain_logits
---- 
----     def _make_step(self, batch):
----@@ -136,7 +149,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         punct_labels = batch['labels']
----         domain_labels = batch['domain']
----         punct_logits, domain_logits = self(
-----            input_ids=input_ids, attention_mask=attention_mask
----+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
----         )
----         punctuation_loss = self.punctuation_loss(
----             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/lightning_logs.txt
---deleted file mode 100644
---index 9fa336d..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/lightning_logs.txt
---+++ /dev/null
---@@ -1,24 +0,0 @@
----Global seed set to 42
----GPU available: True, used: True
----TPU available: None, using: 0 TPU cores
----LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
----Using native 16bit precision.
----Global seed set to 42
----initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | dropout             | Dropout              | 0     
----2 | mlp                 | MultiLayerPerceptron | 131 K 
----3 | punct_classifier    | TokenClassifier      | 2.6 K 
----4 | domain_classifier   | SequenceClassifier   | 257   
----5 | punctuation_loss    | FocalDiceLoss        | 0     
----6 | domain_loss         | CrossEntropyLoss     | 0     
----7 | agg_loss            | AggregatorLoss       | 0     
----8 | punct_class_report  | ClassificationReport | 0     
----9 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----167 K     Trainable params
----13.4 M    Non-trainable params
----13.6 M    Total params
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/nemo_error_log.txt
---deleted file mode 100644
---index 0e115e9..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/nemo_error_log.txt
---+++ /dev/null
---@@ -1,7 +0,0 @@
----[NeMo W 2021-02-08 10:53:14 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-08 10:53:23 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 10:53:23 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index b137c51..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,9 +0,0 @@
----[NeMo I 2021-02-08 10:53:14 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-53-14
----[NeMo I 2021-02-08 10:53:14 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-08 10:53:14 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-08 10:53:23 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 10:53:23 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/events.out.tfevents.1612753145.Titan.17415.0 b/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/events.out.tfevents.1612753145.Titan.17415.0
---deleted file mode 100644
---index 388ea94..0000000
---Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/events.out.tfevents.1612753145.Titan.17415.0 and /dev/null differ
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/git-info.log
---deleted file mode 100644
---index a7bf865..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/git-info.log
---+++ /dev/null
---@@ -1,625 +0,0 @@
----commit hash: 939a671c8c117db6975316767ced5d95449e2b27
----diff --git a/README.md b/README.md
----index d52dba5..0b403d3 100644
------- a/README.md
----+++ b/README.md
----@@ -333,4 +333,71 @@ label                                                precision    recall       f
----  'punct_f1': 0.5858508944511414,
----  'punct_precision': 0.30176490545272827,
----  'punct_recall': 10.0,
----- 'test_loss': 0.8140875697135925}
----\ No newline at end of file
----+ 'test_loss': 0.8140875697135925}
----+
----+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
----+
----+ (label_id: 0)                                          62.15     100.00      76.66       5154
----+! (label_id: 1)                                          0.00       0.00       0.00        108
----+, (label_id: 2)                                          0.00       0.00       0.00      18022
----+- (label_id: 3)                                          0.00       0.00       0.00       1557
----+. (label_id: 4)                                         41.74      94.01      57.81      15164
----+: (label_id: 5)                                          0.00       0.00       0.00        319
----+; (label_id: 6)                                          0.00       0.00       0.00         88
----+? (label_id: 7)                                          0.00       0.00       0.00       1217
----+ (label_id: 8)                                          0.00       0.00       0.00        752
----+… (label_id: 9)                                          0.00       0.00       0.00         67
----+-------------------
----+micro avg                                               45.72      45.72      45.72      42448
----+macro avg                                               10.39      19.40      13.45      42448
----+weighted avg                                            22.46      45.72      29.96      42448
----+{ 'punct_f1': 13.446383476257324,
----+ 'punct_precision': 10.388500213623047,
----+ 'punct_recall': 19.400554656982422,
----+ 'test_loss': 0.44148480892181396}
----+
----+
----+ ## Log for 8/2/2021
----+
----+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
----+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
----+
----+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
----+End frozen 
----+micro avg                                               41.42      41.42      41.42      33406
----+macro avg                                               11.01      13.54      11.07      33406
----+weighted avg                                            34.88      41.42      34.20      33406
----+
----+1st layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               36.65      36.65      36.65      33463
----+macro avg                                               10.71       9.91       8.26      33463
----+weighted avg                                            34.32      36.65      31.08      33463
----+
----+2nd layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               35.72      35.72      35.72      42448
----+macro avg                                                3.57      10.00       5.26      42448
----+weighted avg                                            12.76      35.72      18.81      42448
----+
----+{'punct_f1': 5.264181137084961,
----+ 'punct_precision': 3.572371006011963,
----+ 'punct_recall': 10.0,
----+ 'test_loss': 18.49854850769043}
----+
----+
----+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
----+alpha from 3->4 seems to reduce convergence rate.
----+
----+micro avg                                               50.98      50.98      50.98      33463
----+macro avg                                               25.99      25.38      23.38      33463
----+weighted avg                                            50.31      50.98      48.27      33463
----+
----+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
----+micro avg                                               58.55      58.55      58.55      39340
----+macro avg                                               30.02      29.74      29.52      39340
----+weighted avg                                            57.91      58.55      57.51      39340
----+
----+still increasing?!
----+{'punct_f1': 29.523975372314453,
----+ 'punct_precision': 30.015613555908203,
----+ 'punct_recall': 29.738296508789062,
----+ 'test_loss': 0.3690211772918701}
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index fe58670..26bf21c 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,49 +1,49 @@
---- seed: 42
---- trainer:
-----    # gpus: 1 # the number of gpus, 0 for CPU
-----    # num_nodes: 1
-----    # max_epochs: 2
-----    # max_steps: null # precedence over max_epochs
-----    # accumulate_grad_batches: 4 # accumulates grads every k batches
-----    # gradient_clip_val: 0
-----    # amp_level: O1 # O1/O2 for mixed precision
-----    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
-----    # checkpoint_callback: false  # Provided by exp_manager
-----    # logger: false #false  # Provided by exp_manager
-----    # log_every_n_steps: 1  # Interval of logging.
-----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # resume_from_checkpoint: null
-----
-----    gpus: 0 # the number of gpus, 0 for CPU
----+    gpus: 1 # the number of gpus, 0 for CPU
----     num_nodes: 1
-----    max_epochs: 8
----+    max_epochs: 10
----     max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 1 # accumulates grads every k batches
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----     gradient_clip_val: 0
-----    amp_level: O0 # O1/O2 for mixed precision
-----    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
----+    amp_level: O1 # O1/O2 for mixed precision
----+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    accelerator: ddp
----     checkpoint_callback: false  # Provided by exp_manager
----     logger: false #false  # Provided by exp_manager
----     log_every_n_steps: 1  # Interval of logging.
----     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    reload_dataloaders_every_epoch: true
----     resume_from_checkpoint: null
---- 
----+    # gpus: 0 # the number of gpus, 0 for CPU
----+    # num_nodes: 1
----+    # max_epochs: 8
----+    # max_steps: null # precedence over max_epochs
----+    # accumulate_grad_batches: 1 # accumulates grads every k batches
----+    # gradient_clip_val: 0
----+    # amp_level: O0 # O1/O2 for mixed precision
----+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # # accelerator: ddp
----+    # checkpoint_callback: false  # Provided by exp_manager
----+    # logger: false #false  # Provided by exp_manager
----+    # log_every_n_steps: 1  # Interval of logging.
----+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    # reload_dataloaders_every_epoch: true
----+    # resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /home/nxingyu2/data # /root/data # 
-----tmp_path: /home/nxingyu2/data/tmp # /tmp # 
----+base_path: /home/nxingyu/data # /root/data # 
----+tmp_path: /home/nxingyu/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
-----    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----     unfrozen: 0
-----    maximum_unfrozen: 2
----+    maximum_unfrozen: 1
----     unfreeze_step: 1
----     # unfreeze_every: 3
----     punct_label_ids:
----@@ -58,10 +58,10 @@ model:
----         - "—"
----         - "…"
---- 
-----    punct_class_weights: true
----+    punct_class_weights: false
----     
----     dataset:
-----        data_dir: /home/nxingyu2/data # /root/data # 
----+        data_dir: /home/nxingyu/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -106,6 +106,12 @@ model:
----         config: null
----         # unfrozen_layers: 1
---- 
----+    mlp:
----+        num_fc_layers: 2
----+        fc_dropout: 0.1
----+        log_softmax: false
----+        activation: 'relu'
----+        
----     punct_head:
----         punct_num_fc_layers: 1
----         fc_dropout: 0.1
----@@ -122,17 +128,19 @@ model:
----         use_transformer_init: true
----         loss: 'cel'
----         gamma: 0.1 # coefficient of gradient reversal
----+        pooling: 'token'
----+        idx_conditioned_on: 0
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 3
----+        alpha: 4
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: novograd
----+        name: adamw
----         lr: 1e-3
----         weight_decay: 0.00
----         sched:
----diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
----index d4ff927..cafc57d 100644
------- a/experiment/core/layers/sequence_classifier.py
----+++ b/experiment/core/layers/sequence_classifier.py
----@@ -1,3 +1,4 @@
----+import torch
---- from torch import nn
---- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
---- from core.utils import transformer_weights_init
----@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
----         log_softmax: bool = True,
----         dropout: float = 0.0,
----         use_transformer_init: bool = True,
-----        idx_conditioned_on: int = 0,
----+        pooling: str = 'mean', # mean, max, mean_max, token
----+        idx_conditioned_on: int = None,
----     ):
----         """
----         Initializes the SequenceClassifier module.
----@@ -37,6 +39,7 @@ class SequenceClassifier(nn.Module):
----         super().__init__()
----         self.log_softmax = log_softmax
----         self._idx_conditioned_on = idx_conditioned_on
----+        self.pooling = pooling
----         self.mlp = MultiLayerPerceptron(
----             hidden_size=hidden_size,
----             num_classes=num_classes,
----@@ -48,7 +51,24 @@ class SequenceClassifier(nn.Module):
----         if use_transformer_init:
----             self.apply(lambda module: transformer_weights_init(module, xavier=False))
---- 
-----    def forward(self, hidden_states):
----+    def forward(self, hidden_states, subtoken_mask=None):
----         hidden_states = self.dropout(hidden_states)
-----        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
----+        pp(self.pooling)
----+        if self.pooling=='token':
----+            pooled = hidden_states[:, self._idx_conditioned_on]
----+        else:
----+            if subtoken_mask==None:
----+                ct=hidden_states.shape[1] # Seq len
----+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
----+            else:
----+                ct = torch.sum(subtoken_mask,axis=1)
----+            pooled_sum = torch.sum(hidden_states,axis=1)
----+            if self.pooling=='mean' or self.pooling == 'mean_max':
----+                pooled_mean = pooled_sum//ct
----+            if self.pooling=='max' or self.pooling=='mean_max':
----+                pooled_max = torch.max(hidden_states,axis=1)[0]
----+            pooled=pooled_mean if self.pooling=='mean' else \
----+                pooled_max if self.pooling=='max' else \
----+                    torch.cat([pooled_mean,pooled_max],axis=-1)
----+        logits = self.mlp(pooled)
----         return logits
----diff --git a/experiment/info.log b/experiment/info.log
----index 2471fe9..e69de29 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,300 +0,0 @@
-----[INFO] - GPU available: True, used: False
-----[INFO] - TPU available: None, using: 0 TPU cores
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        184
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.23       0.34       0.53        594
------ (label_id: 3)                                          3.06      25.42       5.46         59
-----. (label_id: 4)                                         47.22      12.98      20.36        524
-----: (label_id: 5)                                          0.00       0.00       0.00         18
-----; (label_id: 6)                                          0.00       0.00       0.00         13
-----? (label_id: 7)                                          8.45       6.32       7.23         95
-----— (label_id: 8)                                          0.00       0.00       0.00         12
-----… (label_id: 9)                                          0.00       0.00       0.00          0
------------------------
-----micro avg                                                6.05       6.05       6.05       1503
-----macro avg                                                6.66       5.01       3.73       1503
-----weighted avg                                            17.61       6.05       7.98       1503
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00         92
------------------------
-----micro avg                                              100.00     100.00     100.00         92
-----macro avg                                              100.00     100.00     100.00         92
-----weighted avg                                           100.00     100.00     100.00         92
-----
-----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 1.5848931924611143e-08
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
-----will be used during training (effective maximum steps = 3192) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 3192
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        202
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.62       0.45       0.70        669
------ (label_id: 3)                                          3.48      27.27       6.17         66
-----. (label_id: 4)                                         45.06      13.01      20.19        561
-----: (label_id: 5)                                          1.52       6.67       2.47         15
-----; (label_id: 6)                                          0.00       0.00       0.00         15
-----? (label_id: 7)                                          8.70       7.32       7.95         82
-----— (label_id: 8)                                          0.00       0.00       0.00         13
-----… (label_id: 9)                                          0.00       0.00       0.00          1
------------------------
-----micro avg                                                6.20       6.20       6.20       1628
-----macro avg                                                6.04       5.47       3.75       1628
-----weighted avg                                            16.79       6.20       7.92       1628
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        101
------------------------
-----micro avg                                              100.00     100.00     100.00        101
-----macro avg                                              100.00     100.00     100.00        101
-----weighted avg                                           100.00     100.00     100.00        101
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.34       0.73       0.46       4402
-----! (label_id: 1)                                          0.42      13.95       0.82        129
-----, (label_id: 2)                                          2.53       0.64       1.03      15243
------ (label_id: 3)                                          2.45      21.03       4.38       1322
-----. (label_id: 4)                                         44.00      11.40      18.11      12542
-----: (label_id: 5)                                          0.43       1.41       0.65        354
-----; (label_id: 6)                                          0.00       0.00       0.00        163
-----? (label_id: 7)                                          4.16       6.27       5.00       1117
-----— (label_id: 8)                                          3.00       0.61       1.02        488
-----… (label_id: 9)                                          0.97       6.17       1.68         81
------------------------
-----micro avg                                                5.41       5.41       5.41      35841
-----macro avg                                                5.83       6.22       3.32      35841
-----weighted avg                                            16.78       5.41       7.18      35841
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2201
------------------------
-----micro avg                                              100.00     100.00     100.00       2201
-----macro avg                                              100.00     100.00     100.00       2201
-----weighted avg                                           100.00     100.00     100.00       2201
-----
-----[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.20       0.43       0.27       4226
-----! (label_id: 1)                                          0.44      14.17       0.86        127
-----, (label_id: 2)                                          1.93       0.49       0.78      14611
------ (label_id: 3)                                          2.23      19.56       4.01       1237
-----. (label_id: 4)                                         43.37      11.25      17.86      11977
-----: (label_id: 5)                                          0.68       2.34       1.05        342
-----; (label_id: 6)                                          0.00       0.00       0.00        129
-----? (label_id: 7)                                          5.16       7.47       6.10       1058
-----— (label_id: 8)                                          2.15       0.49       0.80        409
-----… (label_id: 9)                                          0.69       4.23       1.19         71
------------------------
-----micro avg                                                5.23       5.23       5.23      34187
-----macro avg                                                5.68       6.04       3.29      34187
-----weighted avg                                            16.32       5.23       6.98      34187
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2113
------------------------
-----micro avg                                              100.00     100.00     100.00       2113
-----macro avg                                              100.00     100.00     100.00       2113
-----weighted avg                                           100.00     100.00     100.00       2113
-----
-----[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.61       0.39       4228
-----! (label_id: 1)                                          0.30       8.28       0.58        145
-----, (label_id: 2)                                          2.27       0.58       0.92      14495
------ (label_id: 3)                                          2.64      21.78       4.70       1327
-----. (label_id: 4)                                         44.87      11.66      18.51      12193
-----: (label_id: 5)                                          0.60       1.93       0.91        362
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.19       6.40       5.07       1078
-----— (label_id: 8)                                          1.16       0.22       0.37        459
-----… (label_id: 9)                                          0.85       4.17       1.41         96
------------------------
-----micro avg                                                5.54       5.54       5.54      34547
-----macro avg                                                5.72       5.56       3.29      34547
-----weighted avg                                            17.08       5.54       7.33      34547
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2114
------------------------
-----micro avg                                              100.00     100.00     100.00       2114
-----macro avg                                              100.00     100.00     100.00       2114
-----weighted avg                                           100.00     100.00     100.00       2114
-----
-----[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.29       0.63       0.40       4444
-----! (label_id: 1)                                          0.38      10.67       0.74        150
-----, (label_id: 2)                                          2.32       0.59       0.94      15290
------ (label_id: 3)                                          2.34      20.28       4.19       1292
-----. (label_id: 4)                                         43.85      11.68      18.44      12599
-----: (label_id: 5)                                          0.41       1.28       0.62        392
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.24       6.30       5.07       1111
-----— (label_id: 8)                                          0.00       0.00       0.00        456
-----… (label_id: 9)                                          0.38       2.41       0.65         83
------------------------
-----micro avg                                                5.40       5.40       5.40      35981
-----macro avg                                                5.42       5.38       3.11      35981
-----weighted avg                                            16.59       5.40       7.22      35981
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2222
------------------------
-----micro avg                                              100.00     100.00     100.00       2222
-----macro avg                                              100.00     100.00     100.00       2222
-----weighted avg                                           100.00     100.00     100.00       2222
-----
-----[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.35       0.73       0.48       3844
-----! (label_id: 1)                                          0.54      14.62       1.04        130
-----, (label_id: 2)                                          2.32       0.59       0.94      13056
------ (label_id: 3)                                          2.67      22.28       4.77       1194
-----. (label_id: 4)                                         44.45      11.95      18.84      10791
-----: (label_id: 5)                                          0.84       3.21       1.33        280
-----; (label_id: 6)                                          0.00       0.00       0.00        140
-----? (label_id: 7)                                          4.17       6.56       5.10        914
-----— (label_id: 8)                                          0.00       0.00       0.00        401
-----… (label_id: 9)                                          0.48       2.63       0.81         76
------------------------
-----micro avg                                                5.68       5.68       5.68      30826
-----macro avg                                                5.58       6.26       3.33      30826
-----weighted avg                                            16.82       5.68       7.41      30826
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1922
------------------------
-----micro avg                                              100.00     100.00     100.00       1922
-----macro avg                                              100.00     100.00     100.00       1922
-----weighted avg                                           100.00     100.00     100.00       1922
-----
-----[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.60       0.39       3970
-----! (label_id: 1)                                          0.35      10.66       0.68        122
-----, (label_id: 2)                                          2.09       0.53       0.85      13469
------ (label_id: 3)                                          2.29      19.32       4.10       1201
-----. (label_id: 4)                                         43.43      11.24      17.86      11227
-----: (label_id: 5)                                          0.63       2.30       0.99        304
-----; (label_id: 6)                                          0.00       0.00       0.00        141
-----? (label_id: 7)                                          4.52       6.86       5.45       1006
-----— (label_id: 8)                                          1.15       0.23       0.38        444
-----… (label_id: 9)                                          0.45       2.67       0.78         75
------------------------
-----micro avg                                                5.26       5.26       5.26      31959
-----macro avg                                                5.52       5.44       3.15      31959
-----weighted avg                                            16.42       5.26       7.02      31959
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1985
------------------------
-----micro avg                                              100.00     100.00     100.00       1985
-----macro avg                                              100.00     100.00     100.00       1985
-----weighted avg                                           100.00     100.00     100.00       1985
-----
-----[INFO] - Epoch 5, step 2394: val_loss was not in top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.23       0.48       0.31       4126
-----! (label_id: 1)                                          0.29       9.40       0.56        117
-----, (label_id: 2)                                          1.91       0.49       0.77      14019
------ (label_id: 3)                                          2.52      22.59       4.53       1164
-----. (label_id: 4)                                         44.15      11.65      18.44      11789
-----: (label_id: 5)                                          0.72       2.41       1.11        332
-----; (label_id: 6)                                          0.56       0.61       0.58        165
-----? (label_id: 7)                                          3.89       6.53       4.88        980
-----— (label_id: 8)                                          2.30       0.47       0.77        430
-----… (label_id: 9)                                          1.18       8.33       2.07         60
------------------------
-----micro avg                                                5.47       5.47       5.47      33182
-----macro avg                                                5.77       6.30       3.40      33182
-----weighted avg                                            16.77       5.47       7.25      33182
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2063
------------------------
-----micro avg                                              100.00     100.00     100.00       2063
-----macro avg                                              100.00     100.00     100.00       2063
-----weighted avg                                           100.00     100.00     100.00       2063
-----
-----[INFO] - Epoch 6, step 2793: val_loss was not in top 3
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index fa37b4c..43fc93d 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             self.hparams.model.punct_class_weights=None
---- 
----+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
----+        self.mlp = MultiLayerPerceptron(
----+            self.transformer.config.hidden_size,
----+            self.transformer.config.hidden_size,
----+            num_layers=self.hparams.model.mlp.num_fc_layers, 
----+            activation=self.hparams.model.mlp.activation, 
----+            log_softmax=self.hparams.model.mlp.log_softmax
----+        )
----+
----         self.punct_classifier = TokenClassifier(
----             hidden_size=self.transformer.config.hidden_size,
----             num_classes=len(self.labels_to_ids),
----@@ -80,6 +89,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----             log_softmax=self.hparams.model.domain_head.log_softmax,
----             dropout=self.hparams.model.domain_head.fc_dropout,
----             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
----+            pooling=self.hparams.model.domain_head.pooling,
----+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
----         )
---- 
----         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
----@@ -119,14 +130,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
----         self.freeze()
---- 
-----    def forward(self, input_ids, attention_mask, domain_ids=None):
----+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
----         hidden_states = self.transformer(
----             input_ids=input_ids, attention_mask=attention_mask
----         )[0]
----+        hidden_states = self.dropout(hidden_states)
----+        hidden_states = self.mlp(hidden_states)
----         punct_logits = self.punct_classifier(hidden_states=hidden_states)
----         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
----         domain_logits = self.domain_classifier(
-----            hidden_states=reverse_grad_hidden_states)
----+            hidden_states=reverse_grad_hidden_states,
----+            subtoken_mask=subtoken_mask)
----         return punct_logits, domain_logits
---- 
----     def _make_step(self, batch):
----@@ -136,7 +150,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         punct_labels = batch['labels']
----         domain_labels = batch['domain']
----         punct_logits, domain_logits = self(
-----            input_ids=input_ids, attention_mask=attention_mask
----+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
----         )
----         punctuation_loss = self.punctuation_loss(
----             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/hparams.yaml
---deleted file mode 100644
---index 37788ae..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/hparams.yaml
---+++ /dev/null
---@@ -1,114 +0,0 @@
----seed: 42
----trainer:
----  gpus: 1
----  num_nodes: 1
----  max_epochs: 10
----  max_steps: null
----  accumulate_grad_batches: 4
----  gradient_clip_val: 0
----  amp_level: O1
----  precision: 16
----  accelerator: ddp
----  checkpoint_callback: false
----  logger: false
----  log_every_n_steps: 1
----  val_check_interval: 1.0
----  resume_from_checkpoint: null
----exp_manager:
----  exp_dir: /home/nxingyu/project/
----  name: Punctuation_with_Domain_discriminator
----  create_tensorboard_logger: true
----  create_checkpoint_callback: true
----base_path: /home/nxingyu/data
----tmp_path: /home/nxingyu/data/tmp
----model:
----  nemo_path: null
----  transformer_path: google/electra-small-discriminator
----  unfrozen: 0
----  maximum_unfrozen: 1
----  unfreeze_step: 1
----  punct_label_ids:
----  - ''
----  - '!'
----  - ','
----  - '-'
----  - .
----  - ':'
----  - ;
----  - '?'
----  - —
----  - …
----  punct_class_weights: false
----  dataset:
----    data_dir: /home/nxingyu/data
----    labelled:
----    - /home/nxingyu/data/ted_talks_processed
----    unlabelled: null
----    max_seq_length: 128
----    pad_label: ''
----    ignore_extra_tokens: false
----    ignore_start_end: false
----    use_cache: false
----    num_workers: 0
----    pin_memory: true
----    drop_last: false
----    num_labels: 10
----    num_domains: 1
----    test_unlabelled: true
----    train_ds:
----      shuffle: true
----      num_samples: -1
----      batch_size: 8
----    validation_ds:
----      shuffle: true
----      num_samples: -1
----      batch_size: 8
----  tokenizer:
----    tokenizer_name: google/electra-small-discriminator
----    vocab_file: null
----    tokenizer_model: null
----    special_tokens: null
----  language_model:
----    pretrained_model_name: google/electra-small-discriminator
----    lm_checkpoint: null
----    config_file: null
----    config: null
----  mlp:
----    num_fc_layers: 2
----    fc_dropout: 0.1
----    log_softmax: false
----    activation: relu
----  punct_head:
----    punct_num_fc_layers: 1
----    fc_dropout: 0.1
----    activation: relu
----    log_softmax: false
----    use_transformer_init: true
----    loss: dice
----  domain_head:
----    domain_num_fc_layers: 1
----    fc_dropout: 0.1
----    activation: relu
----    log_softmax: false
----    use_transformer_init: true
----    loss: cel
----    gamma: 0.1
----    pooling: token
----    idx_conditioned_on: 0
----  dice_loss:
----    epsilon: 0.01
----    alpha: 4
----    macro_average: true
----  focal_loss:
----    gamma: 5
----  optim:
----    name: adamw
----    lr: 0.001
----    weight_decay: 0.0
----    sched:
----      name: WarmupAnnealing
----      warmup_steps: null
----      warmup_ratio: 0.1
----      last_epoch: -1
----      monitor: val_loss
----      reduce_on_plateau: false
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/lightning_logs.txt
---deleted file mode 100644
---index 30fbbb4..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/lightning_logs.txt
---+++ /dev/null
---@@ -1,52 +0,0 @@
----Global seed set to 42
----GPU available: True, used: True
----TPU available: None, using: 0 TPU cores
----LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
----Using native 16bit precision.
----Global seed set to 42
----initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | dropout             | Dropout              | 0     
----2 | mlp                 | MultiLayerPerceptron | 131 K 
----3 | punct_classifier    | TokenClassifier      | 2.6 K 
----4 | domain_classifier   | SequenceClassifier   | 257   
----5 | punctuation_loss    | FocalDiceLoss        | 0     
----6 | domain_loss         | CrossEntropyLoss     | 0     
----7 | agg_loss            | AggregatorLoss       | 0     
----8 | punct_class_report  | ClassificationReport | 0     
----9 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----167 K     Trainable params
----13.4 M    Non-trainable params
----13.6 M    Total params
----LR finder stopped early due to diverging loss.
----Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/lr_find_temp_model.ckpt
----Failed to compute suggesting for `lr`. There might not be enough points.
----Traceback (most recent call last):
----  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 355, in suggestion
----    min_grad = np.gradient(loss).argmin()
----  File "<__array_function__ internals>", line 5, in gradient
----  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
----    raise ValueError(
----ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
----Global seed set to 42
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | dropout             | Dropout              | 0     
----2 | mlp                 | MultiLayerPerceptron | 131 K 
----3 | punct_classifier    | TokenClassifier      | 2.6 K 
----4 | domain_classifier   | SequenceClassifier   | 257   
----5 | punctuation_loss    | FocalDiceLoss        | 0     
----6 | domain_loss         | CrossEntropyLoss     | 0     
----7 | agg_loss            | AggregatorLoss       | 0     
----8 | punct_class_report  | ClassificationReport | 0     
----9 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----167 K     Trainable params
----13.4 M    Non-trainable params
----13.6 M    Total params
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/nemo_error_log.txt
---deleted file mode 100644
---index 7e0fd53..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/nemo_error_log.txt
---+++ /dev/null
---@@ -1,27 +0,0 @@
----[NeMo W 2021-02-08 10:58:45 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-08 10:58:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 10:58:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 10:58:57 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 10:59:03 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
----      warnings.warn(*args, **kwargs)
----    
----Failed to compute suggesting for `lr`. There might not be enough points.
----Traceback (most recent call last):
----  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 355, in suggestion
----    min_grad = np.gradient(loss).argmin()
----  File "<__array_function__ internals>", line 5, in gradient
----  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
----    raise ValueError(
----ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
----[NeMo W 2021-02-08 10:59:05 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 10:59:05 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index b17022d..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,21 +0,0 @@
----[NeMo I 2021-02-08 10:58:45 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-58-45
----[NeMo I 2021-02-08 10:58:45 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-08 10:58:45 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-08 10:58:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 10:58:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 10:58:57 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 10:59:03 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 10:59:05 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 10:59:05 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/git-info.log
---deleted file mode 100644
---index 9310747..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/git-info.log
---+++ /dev/null
---@@ -1,625 +0,0 @@
----commit hash: 939a671c8c117db6975316767ced5d95449e2b27
----diff --git a/README.md b/README.md
----index d52dba5..0b403d3 100644
------- a/README.md
----+++ b/README.md
----@@ -333,4 +333,71 @@ label                                                precision    recall       f
----  'punct_f1': 0.5858508944511414,
----  'punct_precision': 0.30176490545272827,
----  'punct_recall': 10.0,
----- 'test_loss': 0.8140875697135925}
----\ No newline at end of file
----+ 'test_loss': 0.8140875697135925}
----+
----+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
----+
----+ (label_id: 0)                                          62.15     100.00      76.66       5154
----+! (label_id: 1)                                          0.00       0.00       0.00        108
----+, (label_id: 2)                                          0.00       0.00       0.00      18022
----+- (label_id: 3)                                          0.00       0.00       0.00       1557
----+. (label_id: 4)                                         41.74      94.01      57.81      15164
----+: (label_id: 5)                                          0.00       0.00       0.00        319
----+; (label_id: 6)                                          0.00       0.00       0.00         88
----+? (label_id: 7)                                          0.00       0.00       0.00       1217
----+ (label_id: 8)                                          0.00       0.00       0.00        752
----+… (label_id: 9)                                          0.00       0.00       0.00         67
----+-------------------
----+micro avg                                               45.72      45.72      45.72      42448
----+macro avg                                               10.39      19.40      13.45      42448
----+weighted avg                                            22.46      45.72      29.96      42448
----+{ 'punct_f1': 13.446383476257324,
----+ 'punct_precision': 10.388500213623047,
----+ 'punct_recall': 19.400554656982422,
----+ 'test_loss': 0.44148480892181396}
----+
----+
----+ ## Log for 8/2/2021
----+
----+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
----+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
----+
----+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
----+End frozen 
----+micro avg                                               41.42      41.42      41.42      33406
----+macro avg                                               11.01      13.54      11.07      33406
----+weighted avg                                            34.88      41.42      34.20      33406
----+
----+1st layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               36.65      36.65      36.65      33463
----+macro avg                                               10.71       9.91       8.26      33463
----+weighted avg                                            34.32      36.65      31.08      33463
----+
----+2nd layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               35.72      35.72      35.72      42448
----+macro avg                                                3.57      10.00       5.26      42448
----+weighted avg                                            12.76      35.72      18.81      42448
----+
----+{'punct_f1': 5.264181137084961,
----+ 'punct_precision': 3.572371006011963,
----+ 'punct_recall': 10.0,
----+ 'test_loss': 18.49854850769043}
----+
----+
----+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
----+alpha from 3->4 seems to reduce convergence rate.
----+
----+micro avg                                               50.98      50.98      50.98      33463
----+macro avg                                               25.99      25.38      23.38      33463
----+weighted avg                                            50.31      50.98      48.27      33463
----+
----+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
----+micro avg                                               58.55      58.55      58.55      39340
----+macro avg                                               30.02      29.74      29.52      39340
----+weighted avg                                            57.91      58.55      57.51      39340
----+
----+still increasing?!
----+{'punct_f1': 29.523975372314453,
----+ 'punct_precision': 30.015613555908203,
----+ 'punct_recall': 29.738296508789062,
----+ 'test_loss': 0.3690211772918701}
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index fe58670..a4a012a 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,49 +1,49 @@
---- seed: 42
---- trainer:
-----    # gpus: 1 # the number of gpus, 0 for CPU
-----    # num_nodes: 1
-----    # max_epochs: 2
-----    # max_steps: null # precedence over max_epochs
-----    # accumulate_grad_batches: 4 # accumulates grads every k batches
-----    # gradient_clip_val: 0
-----    # amp_level: O1 # O1/O2 for mixed precision
-----    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
-----    # checkpoint_callback: false  # Provided by exp_manager
-----    # logger: false #false  # Provided by exp_manager
-----    # log_every_n_steps: 1  # Interval of logging.
-----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # resume_from_checkpoint: null
-----
-----    gpus: 0 # the number of gpus, 0 for CPU
----+    gpus: 1 # the number of gpus, 0 for CPU
----     num_nodes: 1
-----    max_epochs: 8
----+    max_epochs: 10
----     max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 1 # accumulates grads every k batches
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----     gradient_clip_val: 0
-----    amp_level: O0 # O1/O2 for mixed precision
-----    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
----+    amp_level: O1 # O1/O2 for mixed precision
----+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    accelerator: ddp
----     checkpoint_callback: false  # Provided by exp_manager
----     logger: false #false  # Provided by exp_manager
----     log_every_n_steps: 1  # Interval of logging.
----     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    reload_dataloaders_every_epoch: true
----     resume_from_checkpoint: null
---- 
----+    # gpus: 0 # the number of gpus, 0 for CPU
----+    # num_nodes: 1
----+    # max_epochs: 8
----+    # max_steps: null # precedence over max_epochs
----+    # accumulate_grad_batches: 1 # accumulates grads every k batches
----+    # gradient_clip_val: 0
----+    # amp_level: O0 # O1/O2 for mixed precision
----+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # # accelerator: ddp
----+    # checkpoint_callback: false  # Provided by exp_manager
----+    # logger: false #false  # Provided by exp_manager
----+    # log_every_n_steps: 1  # Interval of logging.
----+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    # reload_dataloaders_every_epoch: true
----+    # resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /home/nxingyu2/data # /root/data # 
-----tmp_path: /home/nxingyu2/data/tmp # /tmp # 
----+base_path: /home/nxingyu/data # /root/data # 
----+tmp_path: /home/nxingyu/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
-----    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----     unfrozen: 0
-----    maximum_unfrozen: 2
----+    maximum_unfrozen: 1
----     unfreeze_step: 1
----     # unfreeze_every: 3
----     punct_label_ids:
----@@ -58,10 +58,10 @@ model:
----         - "—"
----         - "…"
---- 
-----    punct_class_weights: true
----+    punct_class_weights: false
----     
----     dataset:
-----        data_dir: /home/nxingyu2/data # /root/data # 
----+        data_dir: /home/nxingyu/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -106,6 +106,12 @@ model:
----         config: null
----         # unfrozen_layers: 1
---- 
----+    mlp:
----+        num_fc_layers: 2
----+        fc_dropout: 0.1
----+        log_softmax: false
----+        activation: 'relu'
----+        
----     punct_head:
----         punct_num_fc_layers: 1
----         fc_dropout: 0.1
----@@ -122,17 +128,19 @@ model:
----         use_transformer_init: true
----         loss: 'cel'
----         gamma: 0.1 # coefficient of gradient reversal
----+        pooling: 'mean'
----+        idx_conditioned_on: 0
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 3
----+        alpha: 4
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: novograd
----+        name: adamw
----         lr: 1e-3
----         weight_decay: 0.00
----         sched:
----diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
----index d4ff927..cafc57d 100644
------- a/experiment/core/layers/sequence_classifier.py
----+++ b/experiment/core/layers/sequence_classifier.py
----@@ -1,3 +1,4 @@
----+import torch
---- from torch import nn
---- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
---- from core.utils import transformer_weights_init
----@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
----         log_softmax: bool = True,
----         dropout: float = 0.0,
----         use_transformer_init: bool = True,
-----        idx_conditioned_on: int = 0,
----+        pooling: str = 'mean', # mean, max, mean_max, token
----+        idx_conditioned_on: int = None,
----     ):
----         """
----         Initializes the SequenceClassifier module.
----@@ -37,6 +39,7 @@ class SequenceClassifier(nn.Module):
----         super().__init__()
----         self.log_softmax = log_softmax
----         self._idx_conditioned_on = idx_conditioned_on
----+        self.pooling = pooling
----         self.mlp = MultiLayerPerceptron(
----             hidden_size=hidden_size,
----             num_classes=num_classes,
----@@ -48,7 +51,24 @@ class SequenceClassifier(nn.Module):
----         if use_transformer_init:
----             self.apply(lambda module: transformer_weights_init(module, xavier=False))
---- 
-----    def forward(self, hidden_states):
----+    def forward(self, hidden_states, subtoken_mask=None):
----         hidden_states = self.dropout(hidden_states)
-----        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
----+        pp(self.pooling)
----+        if self.pooling=='token':
----+            pooled = hidden_states[:, self._idx_conditioned_on]
----+        else:
----+            if subtoken_mask==None:
----+                ct=hidden_states.shape[1] # Seq len
----+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
----+            else:
----+                ct = torch.sum(subtoken_mask,axis=1)
----+            pooled_sum = torch.sum(hidden_states,axis=1)
----+            if self.pooling=='mean' or self.pooling == 'mean_max':
----+                pooled_mean = pooled_sum//ct
----+            if self.pooling=='max' or self.pooling=='mean_max':
----+                pooled_max = torch.max(hidden_states,axis=1)[0]
----+            pooled=pooled_mean if self.pooling=='mean' else \
----+                pooled_max if self.pooling=='max' else \
----+                    torch.cat([pooled_mean,pooled_max],axis=-1)
----+        logits = self.mlp(pooled)
----         return logits
----diff --git a/experiment/info.log b/experiment/info.log
----index 2471fe9..e69de29 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,300 +0,0 @@
-----[INFO] - GPU available: True, used: False
-----[INFO] - TPU available: None, using: 0 TPU cores
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        184
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.23       0.34       0.53        594
------ (label_id: 3)                                          3.06      25.42       5.46         59
-----. (label_id: 4)                                         47.22      12.98      20.36        524
-----: (label_id: 5)                                          0.00       0.00       0.00         18
-----; (label_id: 6)                                          0.00       0.00       0.00         13
-----? (label_id: 7)                                          8.45       6.32       7.23         95
-----— (label_id: 8)                                          0.00       0.00       0.00         12
-----… (label_id: 9)                                          0.00       0.00       0.00          0
------------------------
-----micro avg                                                6.05       6.05       6.05       1503
-----macro avg                                                6.66       5.01       3.73       1503
-----weighted avg                                            17.61       6.05       7.98       1503
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00         92
------------------------
-----micro avg                                              100.00     100.00     100.00         92
-----macro avg                                              100.00     100.00     100.00         92
-----weighted avg                                           100.00     100.00     100.00         92
-----
-----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 1.5848931924611143e-08
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
-----will be used during training (effective maximum steps = 3192) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 3192
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        202
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.62       0.45       0.70        669
------ (label_id: 3)                                          3.48      27.27       6.17         66
-----. (label_id: 4)                                         45.06      13.01      20.19        561
-----: (label_id: 5)                                          1.52       6.67       2.47         15
-----; (label_id: 6)                                          0.00       0.00       0.00         15
-----? (label_id: 7)                                          8.70       7.32       7.95         82
-----— (label_id: 8)                                          0.00       0.00       0.00         13
-----… (label_id: 9)                                          0.00       0.00       0.00          1
------------------------
-----micro avg                                                6.20       6.20       6.20       1628
-----macro avg                                                6.04       5.47       3.75       1628
-----weighted avg                                            16.79       6.20       7.92       1628
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        101
------------------------
-----micro avg                                              100.00     100.00     100.00        101
-----macro avg                                              100.00     100.00     100.00        101
-----weighted avg                                           100.00     100.00     100.00        101
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.34       0.73       0.46       4402
-----! (label_id: 1)                                          0.42      13.95       0.82        129
-----, (label_id: 2)                                          2.53       0.64       1.03      15243
------ (label_id: 3)                                          2.45      21.03       4.38       1322
-----. (label_id: 4)                                         44.00      11.40      18.11      12542
-----: (label_id: 5)                                          0.43       1.41       0.65        354
-----; (label_id: 6)                                          0.00       0.00       0.00        163
-----? (label_id: 7)                                          4.16       6.27       5.00       1117
-----— (label_id: 8)                                          3.00       0.61       1.02        488
-----… (label_id: 9)                                          0.97       6.17       1.68         81
------------------------
-----micro avg                                                5.41       5.41       5.41      35841
-----macro avg                                                5.83       6.22       3.32      35841
-----weighted avg                                            16.78       5.41       7.18      35841
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2201
------------------------
-----micro avg                                              100.00     100.00     100.00       2201
-----macro avg                                              100.00     100.00     100.00       2201
-----weighted avg                                           100.00     100.00     100.00       2201
-----
-----[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.20       0.43       0.27       4226
-----! (label_id: 1)                                          0.44      14.17       0.86        127
-----, (label_id: 2)                                          1.93       0.49       0.78      14611
------ (label_id: 3)                                          2.23      19.56       4.01       1237
-----. (label_id: 4)                                         43.37      11.25      17.86      11977
-----: (label_id: 5)                                          0.68       2.34       1.05        342
-----; (label_id: 6)                                          0.00       0.00       0.00        129
-----? (label_id: 7)                                          5.16       7.47       6.10       1058
-----— (label_id: 8)                                          2.15       0.49       0.80        409
-----… (label_id: 9)                                          0.69       4.23       1.19         71
------------------------
-----micro avg                                                5.23       5.23       5.23      34187
-----macro avg                                                5.68       6.04       3.29      34187
-----weighted avg                                            16.32       5.23       6.98      34187
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2113
------------------------
-----micro avg                                              100.00     100.00     100.00       2113
-----macro avg                                              100.00     100.00     100.00       2113
-----weighted avg                                           100.00     100.00     100.00       2113
-----
-----[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.61       0.39       4228
-----! (label_id: 1)                                          0.30       8.28       0.58        145
-----, (label_id: 2)                                          2.27       0.58       0.92      14495
------ (label_id: 3)                                          2.64      21.78       4.70       1327
-----. (label_id: 4)                                         44.87      11.66      18.51      12193
-----: (label_id: 5)                                          0.60       1.93       0.91        362
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.19       6.40       5.07       1078
-----— (label_id: 8)                                          1.16       0.22       0.37        459
-----… (label_id: 9)                                          0.85       4.17       1.41         96
------------------------
-----micro avg                                                5.54       5.54       5.54      34547
-----macro avg                                                5.72       5.56       3.29      34547
-----weighted avg                                            17.08       5.54       7.33      34547
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2114
------------------------
-----micro avg                                              100.00     100.00     100.00       2114
-----macro avg                                              100.00     100.00     100.00       2114
-----weighted avg                                           100.00     100.00     100.00       2114
-----
-----[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.29       0.63       0.40       4444
-----! (label_id: 1)                                          0.38      10.67       0.74        150
-----, (label_id: 2)                                          2.32       0.59       0.94      15290
------ (label_id: 3)                                          2.34      20.28       4.19       1292
-----. (label_id: 4)                                         43.85      11.68      18.44      12599
-----: (label_id: 5)                                          0.41       1.28       0.62        392
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.24       6.30       5.07       1111
-----— (label_id: 8)                                          0.00       0.00       0.00        456
-----… (label_id: 9)                                          0.38       2.41       0.65         83
------------------------
-----micro avg                                                5.40       5.40       5.40      35981
-----macro avg                                                5.42       5.38       3.11      35981
-----weighted avg                                            16.59       5.40       7.22      35981
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2222
------------------------
-----micro avg                                              100.00     100.00     100.00       2222
-----macro avg                                              100.00     100.00     100.00       2222
-----weighted avg                                           100.00     100.00     100.00       2222
-----
-----[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.35       0.73       0.48       3844
-----! (label_id: 1)                                          0.54      14.62       1.04        130
-----, (label_id: 2)                                          2.32       0.59       0.94      13056
------ (label_id: 3)                                          2.67      22.28       4.77       1194
-----. (label_id: 4)                                         44.45      11.95      18.84      10791
-----: (label_id: 5)                                          0.84       3.21       1.33        280
-----; (label_id: 6)                                          0.00       0.00       0.00        140
-----? (label_id: 7)                                          4.17       6.56       5.10        914
-----— (label_id: 8)                                          0.00       0.00       0.00        401
-----… (label_id: 9)                                          0.48       2.63       0.81         76
------------------------
-----micro avg                                                5.68       5.68       5.68      30826
-----macro avg                                                5.58       6.26       3.33      30826
-----weighted avg                                            16.82       5.68       7.41      30826
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1922
------------------------
-----micro avg                                              100.00     100.00     100.00       1922
-----macro avg                                              100.00     100.00     100.00       1922
-----weighted avg                                           100.00     100.00     100.00       1922
-----
-----[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.60       0.39       3970
-----! (label_id: 1)                                          0.35      10.66       0.68        122
-----, (label_id: 2)                                          2.09       0.53       0.85      13469
------ (label_id: 3)                                          2.29      19.32       4.10       1201
-----. (label_id: 4)                                         43.43      11.24      17.86      11227
-----: (label_id: 5)                                          0.63       2.30       0.99        304
-----; (label_id: 6)                                          0.00       0.00       0.00        141
-----? (label_id: 7)                                          4.52       6.86       5.45       1006
-----— (label_id: 8)                                          1.15       0.23       0.38        444
-----… (label_id: 9)                                          0.45       2.67       0.78         75
------------------------
-----micro avg                                                5.26       5.26       5.26      31959
-----macro avg                                                5.52       5.44       3.15      31959
-----weighted avg                                            16.42       5.26       7.02      31959
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1985
------------------------
-----micro avg                                              100.00     100.00     100.00       1985
-----macro avg                                              100.00     100.00     100.00       1985
-----weighted avg                                           100.00     100.00     100.00       1985
-----
-----[INFO] - Epoch 5, step 2394: val_loss was not in top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.23       0.48       0.31       4126
-----! (label_id: 1)                                          0.29       9.40       0.56        117
-----, (label_id: 2)                                          1.91       0.49       0.77      14019
------ (label_id: 3)                                          2.52      22.59       4.53       1164
-----. (label_id: 4)                                         44.15      11.65      18.44      11789
-----: (label_id: 5)                                          0.72       2.41       1.11        332
-----; (label_id: 6)                                          0.56       0.61       0.58        165
-----? (label_id: 7)                                          3.89       6.53       4.88        980
-----— (label_id: 8)                                          2.30       0.47       0.77        430
-----… (label_id: 9)                                          1.18       8.33       2.07         60
------------------------
-----micro avg                                                5.47       5.47       5.47      33182
-----macro avg                                                5.77       6.30       3.40      33182
-----weighted avg                                            16.77       5.47       7.25      33182
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2063
------------------------
-----micro avg                                              100.00     100.00     100.00       2063
-----macro avg                                              100.00     100.00     100.00       2063
-----weighted avg                                           100.00     100.00     100.00       2063
-----
-----[INFO] - Epoch 6, step 2793: val_loss was not in top 3
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index fa37b4c..43fc93d 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             self.hparams.model.punct_class_weights=None
---- 
----+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
----+        self.mlp = MultiLayerPerceptron(
----+            self.transformer.config.hidden_size,
----+            self.transformer.config.hidden_size,
----+            num_layers=self.hparams.model.mlp.num_fc_layers, 
----+            activation=self.hparams.model.mlp.activation, 
----+            log_softmax=self.hparams.model.mlp.log_softmax
----+        )
----+
----         self.punct_classifier = TokenClassifier(
----             hidden_size=self.transformer.config.hidden_size,
----             num_classes=len(self.labels_to_ids),
----@@ -80,6 +89,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----             log_softmax=self.hparams.model.domain_head.log_softmax,
----             dropout=self.hparams.model.domain_head.fc_dropout,
----             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
----+            pooling=self.hparams.model.domain_head.pooling,
----+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
----         )
---- 
----         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
----@@ -119,14 +130,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
----         self.freeze()
---- 
-----    def forward(self, input_ids, attention_mask, domain_ids=None):
----+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
----         hidden_states = self.transformer(
----             input_ids=input_ids, attention_mask=attention_mask
----         )[0]
----+        hidden_states = self.dropout(hidden_states)
----+        hidden_states = self.mlp(hidden_states)
----         punct_logits = self.punct_classifier(hidden_states=hidden_states)
----         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
----         domain_logits = self.domain_classifier(
-----            hidden_states=reverse_grad_hidden_states)
----+            hidden_states=reverse_grad_hidden_states,
----+            subtoken_mask=subtoken_mask)
----         return punct_logits, domain_logits
---- 
----     def _make_step(self, batch):
----@@ -136,7 +150,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         punct_labels = batch['labels']
----         domain_labels = batch['domain']
----         punct_logits, domain_logits = self(
-----            input_ids=input_ids, attention_mask=attention_mask
----+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
----         )
----         punctuation_loss = self.punctuation_loss(
----             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/lightning_logs.txt
---deleted file mode 100644
---index 9fa336d..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/lightning_logs.txt
---+++ /dev/null
---@@ -1,24 +0,0 @@
----Global seed set to 42
----GPU available: True, used: True
----TPU available: None, using: 0 TPU cores
----LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
----Using native 16bit precision.
----Global seed set to 42
----initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | dropout             | Dropout              | 0     
----2 | mlp                 | MultiLayerPerceptron | 131 K 
----3 | punct_classifier    | TokenClassifier      | 2.6 K 
----4 | domain_classifier   | SequenceClassifier   | 257   
----5 | punctuation_loss    | FocalDiceLoss        | 0     
----6 | domain_loss         | CrossEntropyLoss     | 0     
----7 | agg_loss            | AggregatorLoss       | 0     
----8 | punct_class_report  | ClassificationReport | 0     
----9 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----167 K     Trainable params
----13.4 M    Non-trainable params
----13.6 M    Total params
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/nemo_error_log.txt
---deleted file mode 100644
---index 1ad0ccb..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/nemo_error_log.txt
---+++ /dev/null
---@@ -1,7 +0,0 @@
----[NeMo W 2021-02-08 10:59:31 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-08 10:59:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 10:59:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index 3b439bb..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,9 +0,0 @@
----[NeMo I 2021-02-08 10:59:31 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-59-31
----[NeMo I 2021-02-08 10:59:31 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-08 10:59:31 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-08 10:59:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 10:59:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/git-info.log
---deleted file mode 100644
---index 4bb1d67..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/git-info.log
---+++ /dev/null
---@@ -1,625 +0,0 @@
----commit hash: 939a671c8c117db6975316767ced5d95449e2b27
----diff --git a/README.md b/README.md
----index d52dba5..0b403d3 100644
------- a/README.md
----+++ b/README.md
----@@ -333,4 +333,71 @@ label                                                precision    recall       f
----  'punct_f1': 0.5858508944511414,
----  'punct_precision': 0.30176490545272827,
----  'punct_recall': 10.0,
----- 'test_loss': 0.8140875697135925}
----\ No newline at end of file
----+ 'test_loss': 0.8140875697135925}
----+
----+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
----+
----+ (label_id: 0)                                          62.15     100.00      76.66       5154
----+! (label_id: 1)                                          0.00       0.00       0.00        108
----+, (label_id: 2)                                          0.00       0.00       0.00      18022
----+- (label_id: 3)                                          0.00       0.00       0.00       1557
----+. (label_id: 4)                                         41.74      94.01      57.81      15164
----+: (label_id: 5)                                          0.00       0.00       0.00        319
----+; (label_id: 6)                                          0.00       0.00       0.00         88
----+? (label_id: 7)                                          0.00       0.00       0.00       1217
----+ (label_id: 8)                                          0.00       0.00       0.00        752
----+… (label_id: 9)                                          0.00       0.00       0.00         67
----+-------------------
----+micro avg                                               45.72      45.72      45.72      42448
----+macro avg                                               10.39      19.40      13.45      42448
----+weighted avg                                            22.46      45.72      29.96      42448
----+{ 'punct_f1': 13.446383476257324,
----+ 'punct_precision': 10.388500213623047,
----+ 'punct_recall': 19.400554656982422,
----+ 'test_loss': 0.44148480892181396}
----+
----+
----+ ## Log for 8/2/2021
----+
----+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
----+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
----+
----+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
----+End frozen 
----+micro avg                                               41.42      41.42      41.42      33406
----+macro avg                                               11.01      13.54      11.07      33406
----+weighted avg                                            34.88      41.42      34.20      33406
----+
----+1st layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               36.65      36.65      36.65      33463
----+macro avg                                               10.71       9.91       8.26      33463
----+weighted avg                                            34.32      36.65      31.08      33463
----+
----+2nd layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               35.72      35.72      35.72      42448
----+macro avg                                                3.57      10.00       5.26      42448
----+weighted avg                                            12.76      35.72      18.81      42448
----+
----+{'punct_f1': 5.264181137084961,
----+ 'punct_precision': 3.572371006011963,
----+ 'punct_recall': 10.0,
----+ 'test_loss': 18.49854850769043}
----+
----+
----+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
----+alpha from 3->4 seems to reduce convergence rate.
----+
----+micro avg                                               50.98      50.98      50.98      33463
----+macro avg                                               25.99      25.38      23.38      33463
----+weighted avg                                            50.31      50.98      48.27      33463
----+
----+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
----+micro avg                                               58.55      58.55      58.55      39340
----+macro avg                                               30.02      29.74      29.52      39340
----+weighted avg                                            57.91      58.55      57.51      39340
----+
----+still increasing?!
----+{'punct_f1': 29.523975372314453,
----+ 'punct_precision': 30.015613555908203,
----+ 'punct_recall': 29.738296508789062,
----+ 'test_loss': 0.3690211772918701}
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index fe58670..499ec58 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,49 +1,49 @@
---- seed: 42
---- trainer:
-----    # gpus: 1 # the number of gpus, 0 for CPU
-----    # num_nodes: 1
-----    # max_epochs: 2
-----    # max_steps: null # precedence over max_epochs
-----    # accumulate_grad_batches: 4 # accumulates grads every k batches
-----    # gradient_clip_val: 0
-----    # amp_level: O1 # O1/O2 for mixed precision
-----    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
-----    # checkpoint_callback: false  # Provided by exp_manager
-----    # logger: false #false  # Provided by exp_manager
-----    # log_every_n_steps: 1  # Interval of logging.
-----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # resume_from_checkpoint: null
-----
-----    gpus: 0 # the number of gpus, 0 for CPU
----+    gpus: 1 # the number of gpus, 0 for CPU
----     num_nodes: 1
-----    max_epochs: 8
----+    max_epochs: 10
----     max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 1 # accumulates grads every k batches
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----     gradient_clip_val: 0
-----    amp_level: O0 # O1/O2 for mixed precision
-----    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
----+    amp_level: O1 # O1/O2 for mixed precision
----+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    accelerator: ddp
----     checkpoint_callback: false  # Provided by exp_manager
----     logger: false #false  # Provided by exp_manager
----     log_every_n_steps: 1  # Interval of logging.
----     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    reload_dataloaders_every_epoch: true
----     resume_from_checkpoint: null
---- 
----+    # gpus: 0 # the number of gpus, 0 for CPU
----+    # num_nodes: 1
----+    # max_epochs: 8
----+    # max_steps: null # precedence over max_epochs
----+    # accumulate_grad_batches: 1 # accumulates grads every k batches
----+    # gradient_clip_val: 0
----+    # amp_level: O0 # O1/O2 for mixed precision
----+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # # accelerator: ddp
----+    # checkpoint_callback: false  # Provided by exp_manager
----+    # logger: false #false  # Provided by exp_manager
----+    # log_every_n_steps: 1  # Interval of logging.
----+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    # reload_dataloaders_every_epoch: true
----+    # resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /home/nxingyu2/data # /root/data # 
-----tmp_path: /home/nxingyu2/data/tmp # /tmp # 
----+base_path: /home/nxingyu/data # /root/data # 
----+tmp_path: /home/nxingyu/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
-----    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----     unfrozen: 0
-----    maximum_unfrozen: 2
----+    maximum_unfrozen: 1
----     unfreeze_step: 1
----     # unfreeze_every: 3
----     punct_label_ids:
----@@ -58,10 +58,10 @@ model:
----         - "—"
----         - "…"
---- 
-----    punct_class_weights: true
----+    punct_class_weights: false
----     
----     dataset:
-----        data_dir: /home/nxingyu2/data # /root/data # 
----+        data_dir: /home/nxingyu/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -106,6 +106,12 @@ model:
----         config: null
----         # unfrozen_layers: 1
---- 
----+    mlp:
----+        num_fc_layers: 2
----+        fc_dropout: 0.1
----+        log_softmax: false
----+        activation: 'relu'
----+        
----     punct_head:
----         punct_num_fc_layers: 1
----         fc_dropout: 0.1
----@@ -122,17 +128,19 @@ model:
----         use_transformer_init: true
----         loss: 'cel'
----         gamma: 0.1 # coefficient of gradient reversal
----+        pooling: 'max'
----+        idx_conditioned_on: 0
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 3
----+        alpha: 4
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: novograd
----+        name: adamw
----         lr: 1e-3
----         weight_decay: 0.00
----         sched:
----diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
----index d4ff927..cafc57d 100644
------- a/experiment/core/layers/sequence_classifier.py
----+++ b/experiment/core/layers/sequence_classifier.py
----@@ -1,3 +1,4 @@
----+import torch
---- from torch import nn
---- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
---- from core.utils import transformer_weights_init
----@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
----         log_softmax: bool = True,
----         dropout: float = 0.0,
----         use_transformer_init: bool = True,
-----        idx_conditioned_on: int = 0,
----+        pooling: str = 'mean', # mean, max, mean_max, token
----+        idx_conditioned_on: int = None,
----     ):
----         """
----         Initializes the SequenceClassifier module.
----@@ -37,6 +39,7 @@ class SequenceClassifier(nn.Module):
----         super().__init__()
----         self.log_softmax = log_softmax
----         self._idx_conditioned_on = idx_conditioned_on
----+        self.pooling = pooling
----         self.mlp = MultiLayerPerceptron(
----             hidden_size=hidden_size,
----             num_classes=num_classes,
----@@ -48,7 +51,24 @@ class SequenceClassifier(nn.Module):
----         if use_transformer_init:
----             self.apply(lambda module: transformer_weights_init(module, xavier=False))
---- 
-----    def forward(self, hidden_states):
----+    def forward(self, hidden_states, subtoken_mask=None):
----         hidden_states = self.dropout(hidden_states)
-----        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
----+        pp(self.pooling)
----+        if self.pooling=='token':
----+            pooled = hidden_states[:, self._idx_conditioned_on]
----+        else:
----+            if subtoken_mask==None:
----+                ct=hidden_states.shape[1] # Seq len
----+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
----+            else:
----+                ct = torch.sum(subtoken_mask,axis=1)
----+            pooled_sum = torch.sum(hidden_states,axis=1)
----+            if self.pooling=='mean' or self.pooling == 'mean_max':
----+                pooled_mean = pooled_sum//ct
----+            if self.pooling=='max' or self.pooling=='mean_max':
----+                pooled_max = torch.max(hidden_states,axis=1)[0]
----+            pooled=pooled_mean if self.pooling=='mean' else \
----+                pooled_max if self.pooling=='max' else \
----+                    torch.cat([pooled_mean,pooled_max],axis=-1)
----+        logits = self.mlp(pooled)
----         return logits
----diff --git a/experiment/info.log b/experiment/info.log
----index 2471fe9..e69de29 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,300 +0,0 @@
-----[INFO] - GPU available: True, used: False
-----[INFO] - TPU available: None, using: 0 TPU cores
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        184
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.23       0.34       0.53        594
------ (label_id: 3)                                          3.06      25.42       5.46         59
-----. (label_id: 4)                                         47.22      12.98      20.36        524
-----: (label_id: 5)                                          0.00       0.00       0.00         18
-----; (label_id: 6)                                          0.00       0.00       0.00         13
-----? (label_id: 7)                                          8.45       6.32       7.23         95
-----— (label_id: 8)                                          0.00       0.00       0.00         12
-----… (label_id: 9)                                          0.00       0.00       0.00          0
------------------------
-----micro avg                                                6.05       6.05       6.05       1503
-----macro avg                                                6.66       5.01       3.73       1503
-----weighted avg                                            17.61       6.05       7.98       1503
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00         92
------------------------
-----micro avg                                              100.00     100.00     100.00         92
-----macro avg                                              100.00     100.00     100.00         92
-----weighted avg                                           100.00     100.00     100.00         92
-----
-----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 1.5848931924611143e-08
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
-----will be used during training (effective maximum steps = 3192) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 3192
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        202
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.62       0.45       0.70        669
------ (label_id: 3)                                          3.48      27.27       6.17         66
-----. (label_id: 4)                                         45.06      13.01      20.19        561
-----: (label_id: 5)                                          1.52       6.67       2.47         15
-----; (label_id: 6)                                          0.00       0.00       0.00         15
-----? (label_id: 7)                                          8.70       7.32       7.95         82
-----— (label_id: 8)                                          0.00       0.00       0.00         13
-----… (label_id: 9)                                          0.00       0.00       0.00          1
------------------------
-----micro avg                                                6.20       6.20       6.20       1628
-----macro avg                                                6.04       5.47       3.75       1628
-----weighted avg                                            16.79       6.20       7.92       1628
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        101
------------------------
-----micro avg                                              100.00     100.00     100.00        101
-----macro avg                                              100.00     100.00     100.00        101
-----weighted avg                                           100.00     100.00     100.00        101
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.34       0.73       0.46       4402
-----! (label_id: 1)                                          0.42      13.95       0.82        129
-----, (label_id: 2)                                          2.53       0.64       1.03      15243
------ (label_id: 3)                                          2.45      21.03       4.38       1322
-----. (label_id: 4)                                         44.00      11.40      18.11      12542
-----: (label_id: 5)                                          0.43       1.41       0.65        354
-----; (label_id: 6)                                          0.00       0.00       0.00        163
-----? (label_id: 7)                                          4.16       6.27       5.00       1117
-----— (label_id: 8)                                          3.00       0.61       1.02        488
-----… (label_id: 9)                                          0.97       6.17       1.68         81
------------------------
-----micro avg                                                5.41       5.41       5.41      35841
-----macro avg                                                5.83       6.22       3.32      35841
-----weighted avg                                            16.78       5.41       7.18      35841
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2201
------------------------
-----micro avg                                              100.00     100.00     100.00       2201
-----macro avg                                              100.00     100.00     100.00       2201
-----weighted avg                                           100.00     100.00     100.00       2201
-----
-----[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.20       0.43       0.27       4226
-----! (label_id: 1)                                          0.44      14.17       0.86        127
-----, (label_id: 2)                                          1.93       0.49       0.78      14611
------ (label_id: 3)                                          2.23      19.56       4.01       1237
-----. (label_id: 4)                                         43.37      11.25      17.86      11977
-----: (label_id: 5)                                          0.68       2.34       1.05        342
-----; (label_id: 6)                                          0.00       0.00       0.00        129
-----? (label_id: 7)                                          5.16       7.47       6.10       1058
-----— (label_id: 8)                                          2.15       0.49       0.80        409
-----… (label_id: 9)                                          0.69       4.23       1.19         71
------------------------
-----micro avg                                                5.23       5.23       5.23      34187
-----macro avg                                                5.68       6.04       3.29      34187
-----weighted avg                                            16.32       5.23       6.98      34187
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2113
------------------------
-----micro avg                                              100.00     100.00     100.00       2113
-----macro avg                                              100.00     100.00     100.00       2113
-----weighted avg                                           100.00     100.00     100.00       2113
-----
-----[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.61       0.39       4228
-----! (label_id: 1)                                          0.30       8.28       0.58        145
-----, (label_id: 2)                                          2.27       0.58       0.92      14495
------ (label_id: 3)                                          2.64      21.78       4.70       1327
-----. (label_id: 4)                                         44.87      11.66      18.51      12193
-----: (label_id: 5)                                          0.60       1.93       0.91        362
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.19       6.40       5.07       1078
-----— (label_id: 8)                                          1.16       0.22       0.37        459
-----… (label_id: 9)                                          0.85       4.17       1.41         96
------------------------
-----micro avg                                                5.54       5.54       5.54      34547
-----macro avg                                                5.72       5.56       3.29      34547
-----weighted avg                                            17.08       5.54       7.33      34547
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2114
------------------------
-----micro avg                                              100.00     100.00     100.00       2114
-----macro avg                                              100.00     100.00     100.00       2114
-----weighted avg                                           100.00     100.00     100.00       2114
-----
-----[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.29       0.63       0.40       4444
-----! (label_id: 1)                                          0.38      10.67       0.74        150
-----, (label_id: 2)                                          2.32       0.59       0.94      15290
------ (label_id: 3)                                          2.34      20.28       4.19       1292
-----. (label_id: 4)                                         43.85      11.68      18.44      12599
-----: (label_id: 5)                                          0.41       1.28       0.62        392
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.24       6.30       5.07       1111
-----— (label_id: 8)                                          0.00       0.00       0.00        456
-----… (label_id: 9)                                          0.38       2.41       0.65         83
------------------------
-----micro avg                                                5.40       5.40       5.40      35981
-----macro avg                                                5.42       5.38       3.11      35981
-----weighted avg                                            16.59       5.40       7.22      35981
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2222
------------------------
-----micro avg                                              100.00     100.00     100.00       2222
-----macro avg                                              100.00     100.00     100.00       2222
-----weighted avg                                           100.00     100.00     100.00       2222
-----
-----[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.35       0.73       0.48       3844
-----! (label_id: 1)                                          0.54      14.62       1.04        130
-----, (label_id: 2)                                          2.32       0.59       0.94      13056
------ (label_id: 3)                                          2.67      22.28       4.77       1194
-----. (label_id: 4)                                         44.45      11.95      18.84      10791
-----: (label_id: 5)                                          0.84       3.21       1.33        280
-----; (label_id: 6)                                          0.00       0.00       0.00        140
-----? (label_id: 7)                                          4.17       6.56       5.10        914
-----— (label_id: 8)                                          0.00       0.00       0.00        401
-----… (label_id: 9)                                          0.48       2.63       0.81         76
------------------------
-----micro avg                                                5.68       5.68       5.68      30826
-----macro avg                                                5.58       6.26       3.33      30826
-----weighted avg                                            16.82       5.68       7.41      30826
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1922
------------------------
-----micro avg                                              100.00     100.00     100.00       1922
-----macro avg                                              100.00     100.00     100.00       1922
-----weighted avg                                           100.00     100.00     100.00       1922
-----
-----[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.60       0.39       3970
-----! (label_id: 1)                                          0.35      10.66       0.68        122
-----, (label_id: 2)                                          2.09       0.53       0.85      13469
------ (label_id: 3)                                          2.29      19.32       4.10       1201
-----. (label_id: 4)                                         43.43      11.24      17.86      11227
-----: (label_id: 5)                                          0.63       2.30       0.99        304
-----; (label_id: 6)                                          0.00       0.00       0.00        141
-----? (label_id: 7)                                          4.52       6.86       5.45       1006
-----— (label_id: 8)                                          1.15       0.23       0.38        444
-----… (label_id: 9)                                          0.45       2.67       0.78         75
------------------------
-----micro avg                                                5.26       5.26       5.26      31959
-----macro avg                                                5.52       5.44       3.15      31959
-----weighted avg                                            16.42       5.26       7.02      31959
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1985
------------------------
-----micro avg                                              100.00     100.00     100.00       1985
-----macro avg                                              100.00     100.00     100.00       1985
-----weighted avg                                           100.00     100.00     100.00       1985
-----
-----[INFO] - Epoch 5, step 2394: val_loss was not in top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.23       0.48       0.31       4126
-----! (label_id: 1)                                          0.29       9.40       0.56        117
-----, (label_id: 2)                                          1.91       0.49       0.77      14019
------ (label_id: 3)                                          2.52      22.59       4.53       1164
-----. (label_id: 4)                                         44.15      11.65      18.44      11789
-----: (label_id: 5)                                          0.72       2.41       1.11        332
-----; (label_id: 6)                                          0.56       0.61       0.58        165
-----? (label_id: 7)                                          3.89       6.53       4.88        980
-----— (label_id: 8)                                          2.30       0.47       0.77        430
-----… (label_id: 9)                                          1.18       8.33       2.07         60
------------------------
-----micro avg                                                5.47       5.47       5.47      33182
-----macro avg                                                5.77       6.30       3.40      33182
-----weighted avg                                            16.77       5.47       7.25      33182
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2063
------------------------
-----micro avg                                              100.00     100.00     100.00       2063
-----macro avg                                              100.00     100.00     100.00       2063
-----weighted avg                                           100.00     100.00     100.00       2063
-----
-----[INFO] - Epoch 6, step 2793: val_loss was not in top 3
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index fa37b4c..43fc93d 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             self.hparams.model.punct_class_weights=None
---- 
----+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
----+        self.mlp = MultiLayerPerceptron(
----+            self.transformer.config.hidden_size,
----+            self.transformer.config.hidden_size,
----+            num_layers=self.hparams.model.mlp.num_fc_layers, 
----+            activation=self.hparams.model.mlp.activation, 
----+            log_softmax=self.hparams.model.mlp.log_softmax
----+        )
----+
----         self.punct_classifier = TokenClassifier(
----             hidden_size=self.transformer.config.hidden_size,
----             num_classes=len(self.labels_to_ids),
----@@ -80,6 +89,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----             log_softmax=self.hparams.model.domain_head.log_softmax,
----             dropout=self.hparams.model.domain_head.fc_dropout,
----             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
----+            pooling=self.hparams.model.domain_head.pooling,
----+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
----         )
---- 
----         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
----@@ -119,14 +130,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
----         self.freeze()
---- 
-----    def forward(self, input_ids, attention_mask, domain_ids=None):
----+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
----         hidden_states = self.transformer(
----             input_ids=input_ids, attention_mask=attention_mask
----         )[0]
----+        hidden_states = self.dropout(hidden_states)
----+        hidden_states = self.mlp(hidden_states)
----         punct_logits = self.punct_classifier(hidden_states=hidden_states)
----         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
----         domain_logits = self.domain_classifier(
-----            hidden_states=reverse_grad_hidden_states)
----+            hidden_states=reverse_grad_hidden_states,
----+            subtoken_mask=subtoken_mask)
----         return punct_logits, domain_logits
---- 
----     def _make_step(self, batch):
----@@ -136,7 +150,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         punct_labels = batch['labels']
----         domain_labels = batch['domain']
----         punct_logits, domain_logits = self(
-----            input_ids=input_ids, attention_mask=attention_mask
----+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
----         )
----         punctuation_loss = self.punctuation_loss(
----             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/lightning_logs.txt
---deleted file mode 100644
---index fe58044..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/lightning_logs.txt
---+++ /dev/null
---@@ -1,35 +0,0 @@
----Global seed set to 42
----GPU available: True, used: True
----TPU available: None, using: 0 TPU cores
----LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
----Using native 16bit precision.
----Global seed set to 42
----initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | dropout             | Dropout              | 0     
----2 | mlp                 | MultiLayerPerceptron | 131 K 
----3 | punct_classifier    | TokenClassifier      | 2.6 K 
----4 | domain_classifier   | SequenceClassifier   | 257   
----5 | punctuation_loss    | FocalDiceLoss        | 0     
----6 | domain_loss         | CrossEntropyLoss     | 0     
----7 | agg_loss            | AggregatorLoss       | 0     
----8 | punct_class_report  | ClassificationReport | 0     
----9 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----167 K     Trainable params
----13.4 M    Non-trainable params
----13.6 M    Total params
----LR finder stopped early due to diverging loss.
----Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/lr_find_temp_model.ckpt
----Failed to compute suggesting for `lr`. There might not be enough points.
----Traceback (most recent call last):
----  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 355, in suggestion
----    min_grad = np.gradient(loss).argmin()
----  File "<__array_function__ internals>", line 5, in gradient
----  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
----    raise ValueError(
----ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
----Global seed set to 42
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/nemo_error_log.txt
---deleted file mode 100644
---index 620260e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/nemo_error_log.txt
---+++ /dev/null
---@@ -1,21 +0,0 @@
----[NeMo W 2021-02-08 10:59:58 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-08 11:00:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:00:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:00:09 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:00:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
----      warnings.warn(*args, **kwargs)
----    
----Failed to compute suggesting for `lr`. There might not be enough points.
----Traceback (most recent call last):
----  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 355, in suggestion
----    min_grad = np.gradient(loss).argmin()
----  File "<__array_function__ internals>", line 5, in gradient
----  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
----    raise ValueError(
----ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index bb424d8..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,15 +0,0 @@
----[NeMo I 2021-02-08 10:59:58 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_10-59-58
----[NeMo I 2021-02-08 10:59:58 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-08 10:59:58 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-08 11:00:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:00:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:00:09 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:00:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
----      warnings.warn(*args, **kwargs)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/git-info.log
---deleted file mode 100644
---index a12885f..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/git-info.log
---+++ /dev/null
---@@ -1,625 +0,0 @@
----commit hash: 939a671c8c117db6975316767ced5d95449e2b27
----diff --git a/README.md b/README.md
----index d52dba5..0b403d3 100644
------- a/README.md
----+++ b/README.md
----@@ -333,4 +333,71 @@ label                                                precision    recall       f
----  'punct_f1': 0.5858508944511414,
----  'punct_precision': 0.30176490545272827,
----  'punct_recall': 10.0,
----- 'test_loss': 0.8140875697135925}
----\ No newline at end of file
----+ 'test_loss': 0.8140875697135925}
----+
----+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
----+
----+ (label_id: 0)                                          62.15     100.00      76.66       5154
----+! (label_id: 1)                                          0.00       0.00       0.00        108
----+, (label_id: 2)                                          0.00       0.00       0.00      18022
----+- (label_id: 3)                                          0.00       0.00       0.00       1557
----+. (label_id: 4)                                         41.74      94.01      57.81      15164
----+: (label_id: 5)                                          0.00       0.00       0.00        319
----+; (label_id: 6)                                          0.00       0.00       0.00         88
----+? (label_id: 7)                                          0.00       0.00       0.00       1217
----+ (label_id: 8)                                          0.00       0.00       0.00        752
----+… (label_id: 9)                                          0.00       0.00       0.00         67
----+-------------------
----+micro avg                                               45.72      45.72      45.72      42448
----+macro avg                                               10.39      19.40      13.45      42448
----+weighted avg                                            22.46      45.72      29.96      42448
----+{ 'punct_f1': 13.446383476257324,
----+ 'punct_precision': 10.388500213623047,
----+ 'punct_recall': 19.400554656982422,
----+ 'test_loss': 0.44148480892181396}
----+
----+
----+ ## Log for 8/2/2021
----+
----+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
----+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
----+
----+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
----+End frozen 
----+micro avg                                               41.42      41.42      41.42      33406
----+macro avg                                               11.01      13.54      11.07      33406
----+weighted avg                                            34.88      41.42      34.20      33406
----+
----+1st layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               36.65      36.65      36.65      33463
----+macro avg                                               10.71       9.91       8.26      33463
----+weighted avg                                            34.32      36.65      31.08      33463
----+
----+2nd layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               35.72      35.72      35.72      42448
----+macro avg                                                3.57      10.00       5.26      42448
----+weighted avg                                            12.76      35.72      18.81      42448
----+
----+{'punct_f1': 5.264181137084961,
----+ 'punct_precision': 3.572371006011963,
----+ 'punct_recall': 10.0,
----+ 'test_loss': 18.49854850769043}
----+
----+
----+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
----+alpha from 3->4 seems to reduce convergence rate.
----+
----+micro avg                                               50.98      50.98      50.98      33463
----+macro avg                                               25.99      25.38      23.38      33463
----+weighted avg                                            50.31      50.98      48.27      33463
----+
----+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
----+micro avg                                               58.55      58.55      58.55      39340
----+macro avg                                               30.02      29.74      29.52      39340
----+weighted avg                                            57.91      58.55      57.51      39340
----+
----+still increasing?!
----+{'punct_f1': 29.523975372314453,
----+ 'punct_precision': 30.015613555908203,
----+ 'punct_recall': 29.738296508789062,
----+ 'test_loss': 0.3690211772918701}
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index fe58670..a4a012a 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,49 +1,49 @@
---- seed: 42
---- trainer:
-----    # gpus: 1 # the number of gpus, 0 for CPU
-----    # num_nodes: 1
-----    # max_epochs: 2
-----    # max_steps: null # precedence over max_epochs
-----    # accumulate_grad_batches: 4 # accumulates grads every k batches
-----    # gradient_clip_val: 0
-----    # amp_level: O1 # O1/O2 for mixed precision
-----    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
-----    # checkpoint_callback: false  # Provided by exp_manager
-----    # logger: false #false  # Provided by exp_manager
-----    # log_every_n_steps: 1  # Interval of logging.
-----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # resume_from_checkpoint: null
-----
-----    gpus: 0 # the number of gpus, 0 for CPU
----+    gpus: 1 # the number of gpus, 0 for CPU
----     num_nodes: 1
-----    max_epochs: 8
----+    max_epochs: 10
----     max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 1 # accumulates grads every k batches
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----     gradient_clip_val: 0
-----    amp_level: O0 # O1/O2 for mixed precision
-----    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
----+    amp_level: O1 # O1/O2 for mixed precision
----+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    accelerator: ddp
----     checkpoint_callback: false  # Provided by exp_manager
----     logger: false #false  # Provided by exp_manager
----     log_every_n_steps: 1  # Interval of logging.
----     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    reload_dataloaders_every_epoch: true
----     resume_from_checkpoint: null
---- 
----+    # gpus: 0 # the number of gpus, 0 for CPU
----+    # num_nodes: 1
----+    # max_epochs: 8
----+    # max_steps: null # precedence over max_epochs
----+    # accumulate_grad_batches: 1 # accumulates grads every k batches
----+    # gradient_clip_val: 0
----+    # amp_level: O0 # O1/O2 for mixed precision
----+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # # accelerator: ddp
----+    # checkpoint_callback: false  # Provided by exp_manager
----+    # logger: false #false  # Provided by exp_manager
----+    # log_every_n_steps: 1  # Interval of logging.
----+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    # reload_dataloaders_every_epoch: true
----+    # resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /home/nxingyu2/data # /root/data # 
-----tmp_path: /home/nxingyu2/data/tmp # /tmp # 
----+base_path: /home/nxingyu/data # /root/data # 
----+tmp_path: /home/nxingyu/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
-----    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----     unfrozen: 0
-----    maximum_unfrozen: 2
----+    maximum_unfrozen: 1
----     unfreeze_step: 1
----     # unfreeze_every: 3
----     punct_label_ids:
----@@ -58,10 +58,10 @@ model:
----         - "—"
----         - "…"
---- 
-----    punct_class_weights: true
----+    punct_class_weights: false
----     
----     dataset:
-----        data_dir: /home/nxingyu2/data # /root/data # 
----+        data_dir: /home/nxingyu/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -106,6 +106,12 @@ model:
----         config: null
----         # unfrozen_layers: 1
---- 
----+    mlp:
----+        num_fc_layers: 2
----+        fc_dropout: 0.1
----+        log_softmax: false
----+        activation: 'relu'
----+        
----     punct_head:
----         punct_num_fc_layers: 1
----         fc_dropout: 0.1
----@@ -122,17 +128,19 @@ model:
----         use_transformer_init: true
----         loss: 'cel'
----         gamma: 0.1 # coefficient of gradient reversal
----+        pooling: 'mean'
----+        idx_conditioned_on: 0
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 3
----+        alpha: 4
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: novograd
----+        name: adamw
----         lr: 1e-3
----         weight_decay: 0.00
----         sched:
----diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
----index d4ff927..3dfd622 100644
------- a/experiment/core/layers/sequence_classifier.py
----+++ b/experiment/core/layers/sequence_classifier.py
----@@ -1,3 +1,4 @@
----+import torch
---- from torch import nn
---- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
---- from core.utils import transformer_weights_init
----@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
----         log_softmax: bool = True,
----         dropout: float = 0.0,
----         use_transformer_init: bool = True,
-----        idx_conditioned_on: int = 0,
----+        pooling: str = 'mean', # mean, max, mean_max, token
----+        idx_conditioned_on: int = None,
----     ):
----         """
----         Initializes the SequenceClassifier module.
----@@ -37,6 +39,7 @@ class SequenceClassifier(nn.Module):
----         super().__init__()
----         self.log_softmax = log_softmax
----         self._idx_conditioned_on = idx_conditioned_on
----+        self.pooling = pooling
----         self.mlp = MultiLayerPerceptron(
----             hidden_size=hidden_size,
----             num_classes=num_classes,
----@@ -48,7 +51,24 @@ class SequenceClassifier(nn.Module):
----         if use_transformer_init:
----             self.apply(lambda module: transformer_weights_init(module, xavier=False))
---- 
-----    def forward(self, hidden_states):
----+    def forward(self, hidden_states, subtoken_mask=None):
----         hidden_states = self.dropout(hidden_states)
-----        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
----+        if self.pooling=='token':
----+            pooled = hidden_states[:, self._idx_conditioned_on]
----+        else:
----+            if subtoken_mask==None:
----+                ct=hidden_states.shape[1] # Seq len
----+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
----+            else:
----+                ct = torch.sum(subtoken_mask,axis=1)
----+            pooled_sum = torch.sum(hidden_states,axis=1)
----+            pp(ct,pooled_sum)
----+            if self.pooling=='mean' or self.pooling == 'mean_max':
----+                pooled_mean = pooled_sum//ct
----+            if self.pooling=='max' or self.pooling=='mean_max':
----+                pooled_max = torch.max(hidden_states,axis=1)[0]
----+            pooled=pooled_mean if self.pooling=='mean' else \
----+                pooled_max if self.pooling=='max' else \
----+                    torch.cat([pooled_mean,pooled_max],axis=-1)
----+        logits = self.mlp(pooled)
----         return logits
----diff --git a/experiment/info.log b/experiment/info.log
----index 2471fe9..e69de29 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,300 +0,0 @@
-----[INFO] - GPU available: True, used: False
-----[INFO] - TPU available: None, using: 0 TPU cores
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        184
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.23       0.34       0.53        594
------ (label_id: 3)                                          3.06      25.42       5.46         59
-----. (label_id: 4)                                         47.22      12.98      20.36        524
-----: (label_id: 5)                                          0.00       0.00       0.00         18
-----; (label_id: 6)                                          0.00       0.00       0.00         13
-----? (label_id: 7)                                          8.45       6.32       7.23         95
-----— (label_id: 8)                                          0.00       0.00       0.00         12
-----… (label_id: 9)                                          0.00       0.00       0.00          0
------------------------
-----micro avg                                                6.05       6.05       6.05       1503
-----macro avg                                                6.66       5.01       3.73       1503
-----weighted avg                                            17.61       6.05       7.98       1503
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00         92
------------------------
-----micro avg                                              100.00     100.00     100.00         92
-----macro avg                                              100.00     100.00     100.00         92
-----weighted avg                                           100.00     100.00     100.00         92
-----
-----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 1.5848931924611143e-08
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
-----will be used during training (effective maximum steps = 3192) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 3192
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        202
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.62       0.45       0.70        669
------ (label_id: 3)                                          3.48      27.27       6.17         66
-----. (label_id: 4)                                         45.06      13.01      20.19        561
-----: (label_id: 5)                                          1.52       6.67       2.47         15
-----; (label_id: 6)                                          0.00       0.00       0.00         15
-----? (label_id: 7)                                          8.70       7.32       7.95         82
-----— (label_id: 8)                                          0.00       0.00       0.00         13
-----… (label_id: 9)                                          0.00       0.00       0.00          1
------------------------
-----micro avg                                                6.20       6.20       6.20       1628
-----macro avg                                                6.04       5.47       3.75       1628
-----weighted avg                                            16.79       6.20       7.92       1628
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        101
------------------------
-----micro avg                                              100.00     100.00     100.00        101
-----macro avg                                              100.00     100.00     100.00        101
-----weighted avg                                           100.00     100.00     100.00        101
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.34       0.73       0.46       4402
-----! (label_id: 1)                                          0.42      13.95       0.82        129
-----, (label_id: 2)                                          2.53       0.64       1.03      15243
------ (label_id: 3)                                          2.45      21.03       4.38       1322
-----. (label_id: 4)                                         44.00      11.40      18.11      12542
-----: (label_id: 5)                                          0.43       1.41       0.65        354
-----; (label_id: 6)                                          0.00       0.00       0.00        163
-----? (label_id: 7)                                          4.16       6.27       5.00       1117
-----— (label_id: 8)                                          3.00       0.61       1.02        488
-----… (label_id: 9)                                          0.97       6.17       1.68         81
------------------------
-----micro avg                                                5.41       5.41       5.41      35841
-----macro avg                                                5.83       6.22       3.32      35841
-----weighted avg                                            16.78       5.41       7.18      35841
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2201
------------------------
-----micro avg                                              100.00     100.00     100.00       2201
-----macro avg                                              100.00     100.00     100.00       2201
-----weighted avg                                           100.00     100.00     100.00       2201
-----
-----[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.20       0.43       0.27       4226
-----! (label_id: 1)                                          0.44      14.17       0.86        127
-----, (label_id: 2)                                          1.93       0.49       0.78      14611
------ (label_id: 3)                                          2.23      19.56       4.01       1237
-----. (label_id: 4)                                         43.37      11.25      17.86      11977
-----: (label_id: 5)                                          0.68       2.34       1.05        342
-----; (label_id: 6)                                          0.00       0.00       0.00        129
-----? (label_id: 7)                                          5.16       7.47       6.10       1058
-----— (label_id: 8)                                          2.15       0.49       0.80        409
-----… (label_id: 9)                                          0.69       4.23       1.19         71
------------------------
-----micro avg                                                5.23       5.23       5.23      34187
-----macro avg                                                5.68       6.04       3.29      34187
-----weighted avg                                            16.32       5.23       6.98      34187
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2113
------------------------
-----micro avg                                              100.00     100.00     100.00       2113
-----macro avg                                              100.00     100.00     100.00       2113
-----weighted avg                                           100.00     100.00     100.00       2113
-----
-----[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.61       0.39       4228
-----! (label_id: 1)                                          0.30       8.28       0.58        145
-----, (label_id: 2)                                          2.27       0.58       0.92      14495
------ (label_id: 3)                                          2.64      21.78       4.70       1327
-----. (label_id: 4)                                         44.87      11.66      18.51      12193
-----: (label_id: 5)                                          0.60       1.93       0.91        362
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.19       6.40       5.07       1078
-----— (label_id: 8)                                          1.16       0.22       0.37        459
-----… (label_id: 9)                                          0.85       4.17       1.41         96
------------------------
-----micro avg                                                5.54       5.54       5.54      34547
-----macro avg                                                5.72       5.56       3.29      34547
-----weighted avg                                            17.08       5.54       7.33      34547
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2114
------------------------
-----micro avg                                              100.00     100.00     100.00       2114
-----macro avg                                              100.00     100.00     100.00       2114
-----weighted avg                                           100.00     100.00     100.00       2114
-----
-----[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.29       0.63       0.40       4444
-----! (label_id: 1)                                          0.38      10.67       0.74        150
-----, (label_id: 2)                                          2.32       0.59       0.94      15290
------ (label_id: 3)                                          2.34      20.28       4.19       1292
-----. (label_id: 4)                                         43.85      11.68      18.44      12599
-----: (label_id: 5)                                          0.41       1.28       0.62        392
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.24       6.30       5.07       1111
-----— (label_id: 8)                                          0.00       0.00       0.00        456
-----… (label_id: 9)                                          0.38       2.41       0.65         83
------------------------
-----micro avg                                                5.40       5.40       5.40      35981
-----macro avg                                                5.42       5.38       3.11      35981
-----weighted avg                                            16.59       5.40       7.22      35981
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2222
------------------------
-----micro avg                                              100.00     100.00     100.00       2222
-----macro avg                                              100.00     100.00     100.00       2222
-----weighted avg                                           100.00     100.00     100.00       2222
-----
-----[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.35       0.73       0.48       3844
-----! (label_id: 1)                                          0.54      14.62       1.04        130
-----, (label_id: 2)                                          2.32       0.59       0.94      13056
------ (label_id: 3)                                          2.67      22.28       4.77       1194
-----. (label_id: 4)                                         44.45      11.95      18.84      10791
-----: (label_id: 5)                                          0.84       3.21       1.33        280
-----; (label_id: 6)                                          0.00       0.00       0.00        140
-----? (label_id: 7)                                          4.17       6.56       5.10        914
-----— (label_id: 8)                                          0.00       0.00       0.00        401
-----… (label_id: 9)                                          0.48       2.63       0.81         76
------------------------
-----micro avg                                                5.68       5.68       5.68      30826
-----macro avg                                                5.58       6.26       3.33      30826
-----weighted avg                                            16.82       5.68       7.41      30826
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1922
------------------------
-----micro avg                                              100.00     100.00     100.00       1922
-----macro avg                                              100.00     100.00     100.00       1922
-----weighted avg                                           100.00     100.00     100.00       1922
-----
-----[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.60       0.39       3970
-----! (label_id: 1)                                          0.35      10.66       0.68        122
-----, (label_id: 2)                                          2.09       0.53       0.85      13469
------ (label_id: 3)                                          2.29      19.32       4.10       1201
-----. (label_id: 4)                                         43.43      11.24      17.86      11227
-----: (label_id: 5)                                          0.63       2.30       0.99        304
-----; (label_id: 6)                                          0.00       0.00       0.00        141
-----? (label_id: 7)                                          4.52       6.86       5.45       1006
-----— (label_id: 8)                                          1.15       0.23       0.38        444
-----… (label_id: 9)                                          0.45       2.67       0.78         75
------------------------
-----micro avg                                                5.26       5.26       5.26      31959
-----macro avg                                                5.52       5.44       3.15      31959
-----weighted avg                                            16.42       5.26       7.02      31959
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1985
------------------------
-----micro avg                                              100.00     100.00     100.00       1985
-----macro avg                                              100.00     100.00     100.00       1985
-----weighted avg                                           100.00     100.00     100.00       1985
-----
-----[INFO] - Epoch 5, step 2394: val_loss was not in top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.23       0.48       0.31       4126
-----! (label_id: 1)                                          0.29       9.40       0.56        117
-----, (label_id: 2)                                          1.91       0.49       0.77      14019
------ (label_id: 3)                                          2.52      22.59       4.53       1164
-----. (label_id: 4)                                         44.15      11.65      18.44      11789
-----: (label_id: 5)                                          0.72       2.41       1.11        332
-----; (label_id: 6)                                          0.56       0.61       0.58        165
-----? (label_id: 7)                                          3.89       6.53       4.88        980
-----— (label_id: 8)                                          2.30       0.47       0.77        430
-----… (label_id: 9)                                          1.18       8.33       2.07         60
------------------------
-----micro avg                                                5.47       5.47       5.47      33182
-----macro avg                                                5.77       6.30       3.40      33182
-----weighted avg                                            16.77       5.47       7.25      33182
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2063
------------------------
-----micro avg                                              100.00     100.00     100.00       2063
-----macro avg                                              100.00     100.00     100.00       2063
-----weighted avg                                           100.00     100.00     100.00       2063
-----
-----[INFO] - Epoch 6, step 2793: val_loss was not in top 3
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index fa37b4c..43fc93d 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             self.hparams.model.punct_class_weights=None
---- 
----+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
----+        self.mlp = MultiLayerPerceptron(
----+            self.transformer.config.hidden_size,
----+            self.transformer.config.hidden_size,
----+            num_layers=self.hparams.model.mlp.num_fc_layers, 
----+            activation=self.hparams.model.mlp.activation, 
----+            log_softmax=self.hparams.model.mlp.log_softmax
----+        )
----+
----         self.punct_classifier = TokenClassifier(
----             hidden_size=self.transformer.config.hidden_size,
----             num_classes=len(self.labels_to_ids),
----@@ -80,6 +89,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----             log_softmax=self.hparams.model.domain_head.log_softmax,
----             dropout=self.hparams.model.domain_head.fc_dropout,
----             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
----+            pooling=self.hparams.model.domain_head.pooling,
----+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
----         )
---- 
----         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
----@@ -119,14 +130,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
----         self.freeze()
---- 
-----    def forward(self, input_ids, attention_mask, domain_ids=None):
----+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
----         hidden_states = self.transformer(
----             input_ids=input_ids, attention_mask=attention_mask
----         )[0]
----+        hidden_states = self.dropout(hidden_states)
----+        hidden_states = self.mlp(hidden_states)
----         punct_logits = self.punct_classifier(hidden_states=hidden_states)
----         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
----         domain_logits = self.domain_classifier(
-----            hidden_states=reverse_grad_hidden_states)
----+            hidden_states=reverse_grad_hidden_states,
----+            subtoken_mask=subtoken_mask)
----         return punct_logits, domain_logits
---- 
----     def _make_step(self, batch):
----@@ -136,7 +150,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         punct_labels = batch['labels']
----         domain_labels = batch['domain']
----         punct_logits, domain_logits = self(
-----            input_ids=input_ids, attention_mask=attention_mask
----+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
----         )
----         punctuation_loss = self.punctuation_loss(
----             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/lightning_logs.txt
---deleted file mode 100644
---index 9fa336d..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/lightning_logs.txt
---+++ /dev/null
---@@ -1,24 +0,0 @@
----Global seed set to 42
----GPU available: True, used: True
----TPU available: None, using: 0 TPU cores
----LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
----Using native 16bit precision.
----Global seed set to 42
----initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | dropout             | Dropout              | 0     
----2 | mlp                 | MultiLayerPerceptron | 131 K 
----3 | punct_classifier    | TokenClassifier      | 2.6 K 
----4 | domain_classifier   | SequenceClassifier   | 257   
----5 | punctuation_loss    | FocalDiceLoss        | 0     
----6 | domain_loss         | CrossEntropyLoss     | 0     
----7 | agg_loss            | AggregatorLoss       | 0     
----8 | punct_class_report  | ClassificationReport | 0     
----9 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----167 K     Trainable params
----13.4 M    Non-trainable params
----13.6 M    Total params
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/nemo_error_log.txt
---deleted file mode 100644
---index 6028b56..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/nemo_error_log.txt
---+++ /dev/null
---@@ -1,7 +0,0 @@
----[NeMo W 2021-02-08 11:00:55 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-08 11:01:04 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:01:04 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index 703b70c..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,9 +0,0 @@
----[NeMo I 2021-02-08 11:00:55 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-00-55
----[NeMo I 2021-02-08 11:00:55 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-08 11:00:55 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-08 11:01:04 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:01:04 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/git-info.log
---deleted file mode 100644
---index c7188a9..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/git-info.log
---+++ /dev/null
---@@ -1,625 +0,0 @@
----commit hash: 939a671c8c117db6975316767ced5d95449e2b27
----diff --git a/README.md b/README.md
----index d52dba5..0b403d3 100644
------- a/README.md
----+++ b/README.md
----@@ -333,4 +333,71 @@ label                                                precision    recall       f
----  'punct_f1': 0.5858508944511414,
----  'punct_precision': 0.30176490545272827,
----  'punct_recall': 10.0,
----- 'test_loss': 0.8140875697135925}
----\ No newline at end of file
----+ 'test_loss': 0.8140875697135925}
----+
----+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
----+
----+ (label_id: 0)                                          62.15     100.00      76.66       5154
----+! (label_id: 1)                                          0.00       0.00       0.00        108
----+, (label_id: 2)                                          0.00       0.00       0.00      18022
----+- (label_id: 3)                                          0.00       0.00       0.00       1557
----+. (label_id: 4)                                         41.74      94.01      57.81      15164
----+: (label_id: 5)                                          0.00       0.00       0.00        319
----+; (label_id: 6)                                          0.00       0.00       0.00         88
----+? (label_id: 7)                                          0.00       0.00       0.00       1217
----+ (label_id: 8)                                          0.00       0.00       0.00        752
----+… (label_id: 9)                                          0.00       0.00       0.00         67
----+-------------------
----+micro avg                                               45.72      45.72      45.72      42448
----+macro avg                                               10.39      19.40      13.45      42448
----+weighted avg                                            22.46      45.72      29.96      42448
----+{ 'punct_f1': 13.446383476257324,
----+ 'punct_precision': 10.388500213623047,
----+ 'punct_recall': 19.400554656982422,
----+ 'test_loss': 0.44148480892181396}
----+
----+
----+ ## Log for 8/2/2021
----+
----+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
----+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
----+
----+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
----+End frozen 
----+micro avg                                               41.42      41.42      41.42      33406
----+macro avg                                               11.01      13.54      11.07      33406
----+weighted avg                                            34.88      41.42      34.20      33406
----+
----+1st layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               36.65      36.65      36.65      33463
----+macro avg                                               10.71       9.91       8.26      33463
----+weighted avg                                            34.32      36.65      31.08      33463
----+
----+2nd layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               35.72      35.72      35.72      42448
----+macro avg                                                3.57      10.00       5.26      42448
----+weighted avg                                            12.76      35.72      18.81      42448
----+
----+{'punct_f1': 5.264181137084961,
----+ 'punct_precision': 3.572371006011963,
----+ 'punct_recall': 10.0,
----+ 'test_loss': 18.49854850769043}
----+
----+
----+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
----+alpha from 3->4 seems to reduce convergence rate.
----+
----+micro avg                                               50.98      50.98      50.98      33463
----+macro avg                                               25.99      25.38      23.38      33463
----+weighted avg                                            50.31      50.98      48.27      33463
----+
----+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
----+micro avg                                               58.55      58.55      58.55      39340
----+macro avg                                               30.02      29.74      29.52      39340
----+weighted avg                                            57.91      58.55      57.51      39340
----+
----+still increasing?!
----+{'punct_f1': 29.523975372314453,
----+ 'punct_precision': 30.015613555908203,
----+ 'punct_recall': 29.738296508789062,
----+ 'test_loss': 0.3690211772918701}
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index fe58670..a4a012a 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,49 +1,49 @@
---- seed: 42
---- trainer:
-----    # gpus: 1 # the number of gpus, 0 for CPU
-----    # num_nodes: 1
-----    # max_epochs: 2
-----    # max_steps: null # precedence over max_epochs
-----    # accumulate_grad_batches: 4 # accumulates grads every k batches
-----    # gradient_clip_val: 0
-----    # amp_level: O1 # O1/O2 for mixed precision
-----    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
-----    # checkpoint_callback: false  # Provided by exp_manager
-----    # logger: false #false  # Provided by exp_manager
-----    # log_every_n_steps: 1  # Interval of logging.
-----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # resume_from_checkpoint: null
-----
-----    gpus: 0 # the number of gpus, 0 for CPU
----+    gpus: 1 # the number of gpus, 0 for CPU
----     num_nodes: 1
-----    max_epochs: 8
----+    max_epochs: 10
----     max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 1 # accumulates grads every k batches
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----     gradient_clip_val: 0
-----    amp_level: O0 # O1/O2 for mixed precision
-----    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
----+    amp_level: O1 # O1/O2 for mixed precision
----+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    accelerator: ddp
----     checkpoint_callback: false  # Provided by exp_manager
----     logger: false #false  # Provided by exp_manager
----     log_every_n_steps: 1  # Interval of logging.
----     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    reload_dataloaders_every_epoch: true
----     resume_from_checkpoint: null
---- 
----+    # gpus: 0 # the number of gpus, 0 for CPU
----+    # num_nodes: 1
----+    # max_epochs: 8
----+    # max_steps: null # precedence over max_epochs
----+    # accumulate_grad_batches: 1 # accumulates grads every k batches
----+    # gradient_clip_val: 0
----+    # amp_level: O0 # O1/O2 for mixed precision
----+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # # accelerator: ddp
----+    # checkpoint_callback: false  # Provided by exp_manager
----+    # logger: false #false  # Provided by exp_manager
----+    # log_every_n_steps: 1  # Interval of logging.
----+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    # reload_dataloaders_every_epoch: true
----+    # resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /home/nxingyu2/data # /root/data # 
-----tmp_path: /home/nxingyu2/data/tmp # /tmp # 
----+base_path: /home/nxingyu/data # /root/data # 
----+tmp_path: /home/nxingyu/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
-----    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----     unfrozen: 0
-----    maximum_unfrozen: 2
----+    maximum_unfrozen: 1
----     unfreeze_step: 1
----     # unfreeze_every: 3
----     punct_label_ids:
----@@ -58,10 +58,10 @@ model:
----         - "—"
----         - "…"
---- 
-----    punct_class_weights: true
----+    punct_class_weights: false
----     
----     dataset:
-----        data_dir: /home/nxingyu2/data # /root/data # 
----+        data_dir: /home/nxingyu/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -106,6 +106,12 @@ model:
----         config: null
----         # unfrozen_layers: 1
---- 
----+    mlp:
----+        num_fc_layers: 2
----+        fc_dropout: 0.1
----+        log_softmax: false
----+        activation: 'relu'
----+        
----     punct_head:
----         punct_num_fc_layers: 1
----         fc_dropout: 0.1
----@@ -122,17 +128,19 @@ model:
----         use_transformer_init: true
----         loss: 'cel'
----         gamma: 0.1 # coefficient of gradient reversal
----+        pooling: 'mean'
----+        idx_conditioned_on: 0
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 3
----+        alpha: 4
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: novograd
----+        name: adamw
----         lr: 1e-3
----         weight_decay: 0.00
----         sched:
----diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
----index d4ff927..618351a 100644
------- a/experiment/core/layers/sequence_classifier.py
----+++ b/experiment/core/layers/sequence_classifier.py
----@@ -1,3 +1,4 @@
----+import torch
---- from torch import nn
---- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
---- from core.utils import transformer_weights_init
----@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
----         log_softmax: bool = True,
----         dropout: float = 0.0,
----         use_transformer_init: bool = True,
-----        idx_conditioned_on: int = 0,
----+        pooling: str = 'mean', # mean, max, mean_max, token
----+        idx_conditioned_on: int = None,
----     ):
----         """
----         Initializes the SequenceClassifier module.
----@@ -37,6 +39,7 @@ class SequenceClassifier(nn.Module):
----         super().__init__()
----         self.log_softmax = log_softmax
----         self._idx_conditioned_on = idx_conditioned_on
----+        self.pooling = pooling
----         self.mlp = MultiLayerPerceptron(
----             hidden_size=hidden_size,
----             num_classes=num_classes,
----@@ -48,7 +51,24 @@ class SequenceClassifier(nn.Module):
----         if use_transformer_init:
----             self.apply(lambda module: transformer_weights_init(module, xavier=False))
---- 
-----    def forward(self, hidden_states):
----+    def forward(self, hidden_states, subtoken_mask=None):
----         hidden_states = self.dropout(hidden_states)
-----        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
----+        if self.pooling=='token':
----+            pooled = hidden_states[:, self._idx_conditioned_on]
----+        else:
----+            if subtoken_mask==None:
----+                ct=hidden_states.shape[1] # Seq len
----+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
----+            else:
----+                ct = torch.sum(subtoken_mask,axis=1)
----+            pooled_sum = torch.sum(hidden_states,axis=1)
----+            pp(ct.shape,pooled_sum.shape)
----+            if self.pooling=='mean' or self.pooling == 'mean_max':
----+                pooled_mean = pooled_sum//ct
----+            if self.pooling=='max' or self.pooling=='mean_max':
----+                pooled_max = torch.max(hidden_states,axis=1)[0]
----+            pooled=pooled_mean if self.pooling=='mean' else \
----+                pooled_max if self.pooling=='max' else \
----+                    torch.cat([pooled_mean,pooled_max],axis=-1)
----+        logits = self.mlp(pooled)
----         return logits
----diff --git a/experiment/info.log b/experiment/info.log
----index 2471fe9..e69de29 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,300 +0,0 @@
-----[INFO] - GPU available: True, used: False
-----[INFO] - TPU available: None, using: 0 TPU cores
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        184
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.23       0.34       0.53        594
------ (label_id: 3)                                          3.06      25.42       5.46         59
-----. (label_id: 4)                                         47.22      12.98      20.36        524
-----: (label_id: 5)                                          0.00       0.00       0.00         18
-----; (label_id: 6)                                          0.00       0.00       0.00         13
-----? (label_id: 7)                                          8.45       6.32       7.23         95
-----— (label_id: 8)                                          0.00       0.00       0.00         12
-----… (label_id: 9)                                          0.00       0.00       0.00          0
------------------------
-----micro avg                                                6.05       6.05       6.05       1503
-----macro avg                                                6.66       5.01       3.73       1503
-----weighted avg                                            17.61       6.05       7.98       1503
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00         92
------------------------
-----micro avg                                              100.00     100.00     100.00         92
-----macro avg                                              100.00     100.00     100.00         92
-----weighted avg                                           100.00     100.00     100.00         92
-----
-----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 1.5848931924611143e-08
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
-----will be used during training (effective maximum steps = 3192) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 3192
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        202
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.62       0.45       0.70        669
------ (label_id: 3)                                          3.48      27.27       6.17         66
-----. (label_id: 4)                                         45.06      13.01      20.19        561
-----: (label_id: 5)                                          1.52       6.67       2.47         15
-----; (label_id: 6)                                          0.00       0.00       0.00         15
-----? (label_id: 7)                                          8.70       7.32       7.95         82
-----— (label_id: 8)                                          0.00       0.00       0.00         13
-----… (label_id: 9)                                          0.00       0.00       0.00          1
------------------------
-----micro avg                                                6.20       6.20       6.20       1628
-----macro avg                                                6.04       5.47       3.75       1628
-----weighted avg                                            16.79       6.20       7.92       1628
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        101
------------------------
-----micro avg                                              100.00     100.00     100.00        101
-----macro avg                                              100.00     100.00     100.00        101
-----weighted avg                                           100.00     100.00     100.00        101
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.34       0.73       0.46       4402
-----! (label_id: 1)                                          0.42      13.95       0.82        129
-----, (label_id: 2)                                          2.53       0.64       1.03      15243
------ (label_id: 3)                                          2.45      21.03       4.38       1322
-----. (label_id: 4)                                         44.00      11.40      18.11      12542
-----: (label_id: 5)                                          0.43       1.41       0.65        354
-----; (label_id: 6)                                          0.00       0.00       0.00        163
-----? (label_id: 7)                                          4.16       6.27       5.00       1117
-----— (label_id: 8)                                          3.00       0.61       1.02        488
-----… (label_id: 9)                                          0.97       6.17       1.68         81
------------------------
-----micro avg                                                5.41       5.41       5.41      35841
-----macro avg                                                5.83       6.22       3.32      35841
-----weighted avg                                            16.78       5.41       7.18      35841
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2201
------------------------
-----micro avg                                              100.00     100.00     100.00       2201
-----macro avg                                              100.00     100.00     100.00       2201
-----weighted avg                                           100.00     100.00     100.00       2201
-----
-----[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.20       0.43       0.27       4226
-----! (label_id: 1)                                          0.44      14.17       0.86        127
-----, (label_id: 2)                                          1.93       0.49       0.78      14611
------ (label_id: 3)                                          2.23      19.56       4.01       1237
-----. (label_id: 4)                                         43.37      11.25      17.86      11977
-----: (label_id: 5)                                          0.68       2.34       1.05        342
-----; (label_id: 6)                                          0.00       0.00       0.00        129
-----? (label_id: 7)                                          5.16       7.47       6.10       1058
-----— (label_id: 8)                                          2.15       0.49       0.80        409
-----… (label_id: 9)                                          0.69       4.23       1.19         71
------------------------
-----micro avg                                                5.23       5.23       5.23      34187
-----macro avg                                                5.68       6.04       3.29      34187
-----weighted avg                                            16.32       5.23       6.98      34187
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2113
------------------------
-----micro avg                                              100.00     100.00     100.00       2113
-----macro avg                                              100.00     100.00     100.00       2113
-----weighted avg                                           100.00     100.00     100.00       2113
-----
-----[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.61       0.39       4228
-----! (label_id: 1)                                          0.30       8.28       0.58        145
-----, (label_id: 2)                                          2.27       0.58       0.92      14495
------ (label_id: 3)                                          2.64      21.78       4.70       1327
-----. (label_id: 4)                                         44.87      11.66      18.51      12193
-----: (label_id: 5)                                          0.60       1.93       0.91        362
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.19       6.40       5.07       1078
-----— (label_id: 8)                                          1.16       0.22       0.37        459
-----… (label_id: 9)                                          0.85       4.17       1.41         96
------------------------
-----micro avg                                                5.54       5.54       5.54      34547
-----macro avg                                                5.72       5.56       3.29      34547
-----weighted avg                                            17.08       5.54       7.33      34547
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2114
------------------------
-----micro avg                                              100.00     100.00     100.00       2114
-----macro avg                                              100.00     100.00     100.00       2114
-----weighted avg                                           100.00     100.00     100.00       2114
-----
-----[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.29       0.63       0.40       4444
-----! (label_id: 1)                                          0.38      10.67       0.74        150
-----, (label_id: 2)                                          2.32       0.59       0.94      15290
------ (label_id: 3)                                          2.34      20.28       4.19       1292
-----. (label_id: 4)                                         43.85      11.68      18.44      12599
-----: (label_id: 5)                                          0.41       1.28       0.62        392
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.24       6.30       5.07       1111
-----— (label_id: 8)                                          0.00       0.00       0.00        456
-----… (label_id: 9)                                          0.38       2.41       0.65         83
------------------------
-----micro avg                                                5.40       5.40       5.40      35981
-----macro avg                                                5.42       5.38       3.11      35981
-----weighted avg                                            16.59       5.40       7.22      35981
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2222
------------------------
-----micro avg                                              100.00     100.00     100.00       2222
-----macro avg                                              100.00     100.00     100.00       2222
-----weighted avg                                           100.00     100.00     100.00       2222
-----
-----[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.35       0.73       0.48       3844
-----! (label_id: 1)                                          0.54      14.62       1.04        130
-----, (label_id: 2)                                          2.32       0.59       0.94      13056
------ (label_id: 3)                                          2.67      22.28       4.77       1194
-----. (label_id: 4)                                         44.45      11.95      18.84      10791
-----: (label_id: 5)                                          0.84       3.21       1.33        280
-----; (label_id: 6)                                          0.00       0.00       0.00        140
-----? (label_id: 7)                                          4.17       6.56       5.10        914
-----— (label_id: 8)                                          0.00       0.00       0.00        401
-----… (label_id: 9)                                          0.48       2.63       0.81         76
------------------------
-----micro avg                                                5.68       5.68       5.68      30826
-----macro avg                                                5.58       6.26       3.33      30826
-----weighted avg                                            16.82       5.68       7.41      30826
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1922
------------------------
-----micro avg                                              100.00     100.00     100.00       1922
-----macro avg                                              100.00     100.00     100.00       1922
-----weighted avg                                           100.00     100.00     100.00       1922
-----
-----[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.60       0.39       3970
-----! (label_id: 1)                                          0.35      10.66       0.68        122
-----, (label_id: 2)                                          2.09       0.53       0.85      13469
------ (label_id: 3)                                          2.29      19.32       4.10       1201
-----. (label_id: 4)                                         43.43      11.24      17.86      11227
-----: (label_id: 5)                                          0.63       2.30       0.99        304
-----; (label_id: 6)                                          0.00       0.00       0.00        141
-----? (label_id: 7)                                          4.52       6.86       5.45       1006
-----— (label_id: 8)                                          1.15       0.23       0.38        444
-----… (label_id: 9)                                          0.45       2.67       0.78         75
------------------------
-----micro avg                                                5.26       5.26       5.26      31959
-----macro avg                                                5.52       5.44       3.15      31959
-----weighted avg                                            16.42       5.26       7.02      31959
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1985
------------------------
-----micro avg                                              100.00     100.00     100.00       1985
-----macro avg                                              100.00     100.00     100.00       1985
-----weighted avg                                           100.00     100.00     100.00       1985
-----
-----[INFO] - Epoch 5, step 2394: val_loss was not in top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.23       0.48       0.31       4126
-----! (label_id: 1)                                          0.29       9.40       0.56        117
-----, (label_id: 2)                                          1.91       0.49       0.77      14019
------ (label_id: 3)                                          2.52      22.59       4.53       1164
-----. (label_id: 4)                                         44.15      11.65      18.44      11789
-----: (label_id: 5)                                          0.72       2.41       1.11        332
-----; (label_id: 6)                                          0.56       0.61       0.58        165
-----? (label_id: 7)                                          3.89       6.53       4.88        980
-----— (label_id: 8)                                          2.30       0.47       0.77        430
-----… (label_id: 9)                                          1.18       8.33       2.07         60
------------------------
-----micro avg                                                5.47       5.47       5.47      33182
-----macro avg                                                5.77       6.30       3.40      33182
-----weighted avg                                            16.77       5.47       7.25      33182
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2063
------------------------
-----micro avg                                              100.00     100.00     100.00       2063
-----macro avg                                              100.00     100.00     100.00       2063
-----weighted avg                                           100.00     100.00     100.00       2063
-----
-----[INFO] - Epoch 6, step 2793: val_loss was not in top 3
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index fa37b4c..43fc93d 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             self.hparams.model.punct_class_weights=None
---- 
----+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
----+        self.mlp = MultiLayerPerceptron(
----+            self.transformer.config.hidden_size,
----+            self.transformer.config.hidden_size,
----+            num_layers=self.hparams.model.mlp.num_fc_layers, 
----+            activation=self.hparams.model.mlp.activation, 
----+            log_softmax=self.hparams.model.mlp.log_softmax
----+        )
----+
----         self.punct_classifier = TokenClassifier(
----             hidden_size=self.transformer.config.hidden_size,
----             num_classes=len(self.labels_to_ids),
----@@ -80,6 +89,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----             log_softmax=self.hparams.model.domain_head.log_softmax,
----             dropout=self.hparams.model.domain_head.fc_dropout,
----             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
----+            pooling=self.hparams.model.domain_head.pooling,
----+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
----         )
---- 
----         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
----@@ -119,14 +130,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
----         self.freeze()
---- 
-----    def forward(self, input_ids, attention_mask, domain_ids=None):
----+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
----         hidden_states = self.transformer(
----             input_ids=input_ids, attention_mask=attention_mask
----         )[0]
----+        hidden_states = self.dropout(hidden_states)
----+        hidden_states = self.mlp(hidden_states)
----         punct_logits = self.punct_classifier(hidden_states=hidden_states)
----         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
----         domain_logits = self.domain_classifier(
-----            hidden_states=reverse_grad_hidden_states)
----+            hidden_states=reverse_grad_hidden_states,
----+            subtoken_mask=subtoken_mask)
----         return punct_logits, domain_logits
---- 
----     def _make_step(self, batch):
----@@ -136,7 +150,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         punct_labels = batch['labels']
----         domain_labels = batch['domain']
----         punct_logits, domain_logits = self(
-----            input_ids=input_ids, attention_mask=attention_mask
----+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
----         )
----         punctuation_loss = self.punctuation_loss(
----             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/lightning_logs.txt
---deleted file mode 100644
---index 9fa336d..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/lightning_logs.txt
---+++ /dev/null
---@@ -1,24 +0,0 @@
----Global seed set to 42
----GPU available: True, used: True
----TPU available: None, using: 0 TPU cores
----LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
----Using native 16bit precision.
----Global seed set to 42
----initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | dropout             | Dropout              | 0     
----2 | mlp                 | MultiLayerPerceptron | 131 K 
----3 | punct_classifier    | TokenClassifier      | 2.6 K 
----4 | domain_classifier   | SequenceClassifier   | 257   
----5 | punctuation_loss    | FocalDiceLoss        | 0     
----6 | domain_loss         | CrossEntropyLoss     | 0     
----7 | agg_loss            | AggregatorLoss       | 0     
----8 | punct_class_report  | ClassificationReport | 0     
----9 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----167 K     Trainable params
----13.4 M    Non-trainable params
----13.6 M    Total params
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/nemo_error_log.txt
---deleted file mode 100644
---index d318902..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/nemo_error_log.txt
---+++ /dev/null
---@@ -1,7 +0,0 @@
----[NeMo W 2021-02-08 11:02:01 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-08 11:02:10 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:02:10 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index 4c89259..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,9 +0,0 @@
----[NeMo I 2021-02-08 11:02:01 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-02-01
----[NeMo I 2021-02-08 11:02:01 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-08 11:02:01 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-08 11:02:10 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:02:10 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/git-info.log
---deleted file mode 100644
---index 9aebfb8..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/git-info.log
---+++ /dev/null
---@@ -1,625 +0,0 @@
----commit hash: 939a671c8c117db6975316767ced5d95449e2b27
----diff --git a/README.md b/README.md
----index d52dba5..0b403d3 100644
------- a/README.md
----+++ b/README.md
----@@ -333,4 +333,71 @@ label                                                precision    recall       f
----  'punct_f1': 0.5858508944511414,
----  'punct_precision': 0.30176490545272827,
----  'punct_recall': 10.0,
----- 'test_loss': 0.8140875697135925}
----\ No newline at end of file
----+ 'test_loss': 0.8140875697135925}
----+
----+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
----+
----+ (label_id: 0)                                          62.15     100.00      76.66       5154
----+! (label_id: 1)                                          0.00       0.00       0.00        108
----+, (label_id: 2)                                          0.00       0.00       0.00      18022
----+- (label_id: 3)                                          0.00       0.00       0.00       1557
----+. (label_id: 4)                                         41.74      94.01      57.81      15164
----+: (label_id: 5)                                          0.00       0.00       0.00        319
----+; (label_id: 6)                                          0.00       0.00       0.00         88
----+? (label_id: 7)                                          0.00       0.00       0.00       1217
----+ (label_id: 8)                                          0.00       0.00       0.00        752
----+… (label_id: 9)                                          0.00       0.00       0.00         67
----+-------------------
----+micro avg                                               45.72      45.72      45.72      42448
----+macro avg                                               10.39      19.40      13.45      42448
----+weighted avg                                            22.46      45.72      29.96      42448
----+{ 'punct_f1': 13.446383476257324,
----+ 'punct_precision': 10.388500213623047,
----+ 'punct_recall': 19.400554656982422,
----+ 'test_loss': 0.44148480892181396}
----+
----+
----+ ## Log for 8/2/2021
----+
----+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
----+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
----+
----+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
----+End frozen 
----+micro avg                                               41.42      41.42      41.42      33406
----+macro avg                                               11.01      13.54      11.07      33406
----+weighted avg                                            34.88      41.42      34.20      33406
----+
----+1st layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               36.65      36.65      36.65      33463
----+macro avg                                               10.71       9.91       8.26      33463
----+weighted avg                                            34.32      36.65      31.08      33463
----+
----+2nd layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               35.72      35.72      35.72      42448
----+macro avg                                                3.57      10.00       5.26      42448
----+weighted avg                                            12.76      35.72      18.81      42448
----+
----+{'punct_f1': 5.264181137084961,
----+ 'punct_precision': 3.572371006011963,
----+ 'punct_recall': 10.0,
----+ 'test_loss': 18.49854850769043}
----+
----+
----+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
----+alpha from 3->4 seems to reduce convergence rate.
----+
----+micro avg                                               50.98      50.98      50.98      33463
----+macro avg                                               25.99      25.38      23.38      33463
----+weighted avg                                            50.31      50.98      48.27      33463
----+
----+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
----+micro avg                                               58.55      58.55      58.55      39340
----+macro avg                                               30.02      29.74      29.52      39340
----+weighted avg                                            57.91      58.55      57.51      39340
----+
----+still increasing?!
----+{'punct_f1': 29.523975372314453,
----+ 'punct_precision': 30.015613555908203,
----+ 'punct_recall': 29.738296508789062,
----+ 'test_loss': 0.3690211772918701}
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index fe58670..a4a012a 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,49 +1,49 @@
---- seed: 42
---- trainer:
-----    # gpus: 1 # the number of gpus, 0 for CPU
-----    # num_nodes: 1
-----    # max_epochs: 2
-----    # max_steps: null # precedence over max_epochs
-----    # accumulate_grad_batches: 4 # accumulates grads every k batches
-----    # gradient_clip_val: 0
-----    # amp_level: O1 # O1/O2 for mixed precision
-----    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
-----    # checkpoint_callback: false  # Provided by exp_manager
-----    # logger: false #false  # Provided by exp_manager
-----    # log_every_n_steps: 1  # Interval of logging.
-----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # resume_from_checkpoint: null
-----
-----    gpus: 0 # the number of gpus, 0 for CPU
----+    gpus: 1 # the number of gpus, 0 for CPU
----     num_nodes: 1
-----    max_epochs: 8
----+    max_epochs: 10
----     max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 1 # accumulates grads every k batches
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----     gradient_clip_val: 0
-----    amp_level: O0 # O1/O2 for mixed precision
-----    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
----+    amp_level: O1 # O1/O2 for mixed precision
----+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    accelerator: ddp
----     checkpoint_callback: false  # Provided by exp_manager
----     logger: false #false  # Provided by exp_manager
----     log_every_n_steps: 1  # Interval of logging.
----     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    reload_dataloaders_every_epoch: true
----     resume_from_checkpoint: null
---- 
----+    # gpus: 0 # the number of gpus, 0 for CPU
----+    # num_nodes: 1
----+    # max_epochs: 8
----+    # max_steps: null # precedence over max_epochs
----+    # accumulate_grad_batches: 1 # accumulates grads every k batches
----+    # gradient_clip_val: 0
----+    # amp_level: O0 # O1/O2 for mixed precision
----+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # # accelerator: ddp
----+    # checkpoint_callback: false  # Provided by exp_manager
----+    # logger: false #false  # Provided by exp_manager
----+    # log_every_n_steps: 1  # Interval of logging.
----+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    # reload_dataloaders_every_epoch: true
----+    # resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /home/nxingyu2/data # /root/data # 
-----tmp_path: /home/nxingyu2/data/tmp # /tmp # 
----+base_path: /home/nxingyu/data # /root/data # 
----+tmp_path: /home/nxingyu/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
-----    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----     unfrozen: 0
-----    maximum_unfrozen: 2
----+    maximum_unfrozen: 1
----     unfreeze_step: 1
----     # unfreeze_every: 3
----     punct_label_ids:
----@@ -58,10 +58,10 @@ model:
----         - "—"
----         - "…"
---- 
-----    punct_class_weights: true
----+    punct_class_weights: false
----     
----     dataset:
-----        data_dir: /home/nxingyu2/data # /root/data # 
----+        data_dir: /home/nxingyu/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -106,6 +106,12 @@ model:
----         config: null
----         # unfrozen_layers: 1
---- 
----+    mlp:
----+        num_fc_layers: 2
----+        fc_dropout: 0.1
----+        log_softmax: false
----+        activation: 'relu'
----+        
----     punct_head:
----         punct_num_fc_layers: 1
----         fc_dropout: 0.1
----@@ -122,17 +128,19 @@ model:
----         use_transformer_init: true
----         loss: 'cel'
----         gamma: 0.1 # coefficient of gradient reversal
----+        pooling: 'mean'
----+        idx_conditioned_on: 0
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 3
----+        alpha: 4
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: novograd
----+        name: adamw
----         lr: 1e-3
----         weight_decay: 0.00
----         sched:
----diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
----index d4ff927..f1c68d0 100644
------- a/experiment/core/layers/sequence_classifier.py
----+++ b/experiment/core/layers/sequence_classifier.py
----@@ -1,3 +1,4 @@
----+import torch
---- from torch import nn
---- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
---- from core.utils import transformer_weights_init
----@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
----         log_softmax: bool = True,
----         dropout: float = 0.0,
----         use_transformer_init: bool = True,
-----        idx_conditioned_on: int = 0,
----+        pooling: str = 'mean', # mean, max, mean_max, token
----+        idx_conditioned_on: int = None,
----     ):
----         """
----         Initializes the SequenceClassifier module.
----@@ -37,6 +39,7 @@ class SequenceClassifier(nn.Module):
----         super().__init__()
----         self.log_softmax = log_softmax
----         self._idx_conditioned_on = idx_conditioned_on
----+        self.pooling = pooling
----         self.mlp = MultiLayerPerceptron(
----             hidden_size=hidden_size,
----             num_classes=num_classes,
----@@ -48,7 +51,24 @@ class SequenceClassifier(nn.Module):
----         if use_transformer_init:
----             self.apply(lambda module: transformer_weights_init(module, xavier=False))
---- 
-----    def forward(self, hidden_states):
----+    def forward(self, hidden_states, subtoken_mask=None):
----         hidden_states = self.dropout(hidden_states)
-----        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
----+        if self.pooling=='token':
----+            pooled = hidden_states[:, self._idx_conditioned_on]
----+        else:
----+            if subtoken_mask==None:
----+                ct=hidden_states.shape[1] # Seq len
----+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
----+            else:
----+                ct = torch.sum(subtoken_mask,axis=1).unsqueeze(1)
----+            pooled_sum = torch.sum(hidden_states,axis=1)
----+            pp(ct.shape,pooled_sum.shape)
----+            if self.pooling=='mean' or self.pooling == 'mean_max':
----+                pooled_mean = pooled_sum//ct
----+            if self.pooling=='max' or self.pooling=='mean_max':
----+                pooled_max = torch.max(hidden_states,axis=1)[0]
----+            pooled=pooled_mean if self.pooling=='mean' else \
----+                pooled_max if self.pooling=='max' else \
----+                    torch.cat([pooled_mean,pooled_max],axis=-1)
----+        logits = self.mlp(pooled)
----         return logits
----diff --git a/experiment/info.log b/experiment/info.log
----index 2471fe9..e69de29 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,300 +0,0 @@
-----[INFO] - GPU available: True, used: False
-----[INFO] - TPU available: None, using: 0 TPU cores
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        184
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.23       0.34       0.53        594
------ (label_id: 3)                                          3.06      25.42       5.46         59
-----. (label_id: 4)                                         47.22      12.98      20.36        524
-----: (label_id: 5)                                          0.00       0.00       0.00         18
-----; (label_id: 6)                                          0.00       0.00       0.00         13
-----? (label_id: 7)                                          8.45       6.32       7.23         95
-----— (label_id: 8)                                          0.00       0.00       0.00         12
-----… (label_id: 9)                                          0.00       0.00       0.00          0
------------------------
-----micro avg                                                6.05       6.05       6.05       1503
-----macro avg                                                6.66       5.01       3.73       1503
-----weighted avg                                            17.61       6.05       7.98       1503
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00         92
------------------------
-----micro avg                                              100.00     100.00     100.00         92
-----macro avg                                              100.00     100.00     100.00         92
-----weighted avg                                           100.00     100.00     100.00         92
-----
-----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 1.5848931924611143e-08
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
-----will be used during training (effective maximum steps = 3192) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 3192
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        202
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.62       0.45       0.70        669
------ (label_id: 3)                                          3.48      27.27       6.17         66
-----. (label_id: 4)                                         45.06      13.01      20.19        561
-----: (label_id: 5)                                          1.52       6.67       2.47         15
-----; (label_id: 6)                                          0.00       0.00       0.00         15
-----? (label_id: 7)                                          8.70       7.32       7.95         82
-----— (label_id: 8)                                          0.00       0.00       0.00         13
-----… (label_id: 9)                                          0.00       0.00       0.00          1
------------------------
-----micro avg                                                6.20       6.20       6.20       1628
-----macro avg                                                6.04       5.47       3.75       1628
-----weighted avg                                            16.79       6.20       7.92       1628
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        101
------------------------
-----micro avg                                              100.00     100.00     100.00        101
-----macro avg                                              100.00     100.00     100.00        101
-----weighted avg                                           100.00     100.00     100.00        101
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.34       0.73       0.46       4402
-----! (label_id: 1)                                          0.42      13.95       0.82        129
-----, (label_id: 2)                                          2.53       0.64       1.03      15243
------ (label_id: 3)                                          2.45      21.03       4.38       1322
-----. (label_id: 4)                                         44.00      11.40      18.11      12542
-----: (label_id: 5)                                          0.43       1.41       0.65        354
-----; (label_id: 6)                                          0.00       0.00       0.00        163
-----? (label_id: 7)                                          4.16       6.27       5.00       1117
-----— (label_id: 8)                                          3.00       0.61       1.02        488
-----… (label_id: 9)                                          0.97       6.17       1.68         81
------------------------
-----micro avg                                                5.41       5.41       5.41      35841
-----macro avg                                                5.83       6.22       3.32      35841
-----weighted avg                                            16.78       5.41       7.18      35841
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2201
------------------------
-----micro avg                                              100.00     100.00     100.00       2201
-----macro avg                                              100.00     100.00     100.00       2201
-----weighted avg                                           100.00     100.00     100.00       2201
-----
-----[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.20       0.43       0.27       4226
-----! (label_id: 1)                                          0.44      14.17       0.86        127
-----, (label_id: 2)                                          1.93       0.49       0.78      14611
------ (label_id: 3)                                          2.23      19.56       4.01       1237
-----. (label_id: 4)                                         43.37      11.25      17.86      11977
-----: (label_id: 5)                                          0.68       2.34       1.05        342
-----; (label_id: 6)                                          0.00       0.00       0.00        129
-----? (label_id: 7)                                          5.16       7.47       6.10       1058
-----— (label_id: 8)                                          2.15       0.49       0.80        409
-----… (label_id: 9)                                          0.69       4.23       1.19         71
------------------------
-----micro avg                                                5.23       5.23       5.23      34187
-----macro avg                                                5.68       6.04       3.29      34187
-----weighted avg                                            16.32       5.23       6.98      34187
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2113
------------------------
-----micro avg                                              100.00     100.00     100.00       2113
-----macro avg                                              100.00     100.00     100.00       2113
-----weighted avg                                           100.00     100.00     100.00       2113
-----
-----[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.61       0.39       4228
-----! (label_id: 1)                                          0.30       8.28       0.58        145
-----, (label_id: 2)                                          2.27       0.58       0.92      14495
------ (label_id: 3)                                          2.64      21.78       4.70       1327
-----. (label_id: 4)                                         44.87      11.66      18.51      12193
-----: (label_id: 5)                                          0.60       1.93       0.91        362
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.19       6.40       5.07       1078
-----— (label_id: 8)                                          1.16       0.22       0.37        459
-----… (label_id: 9)                                          0.85       4.17       1.41         96
------------------------
-----micro avg                                                5.54       5.54       5.54      34547
-----macro avg                                                5.72       5.56       3.29      34547
-----weighted avg                                            17.08       5.54       7.33      34547
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2114
------------------------
-----micro avg                                              100.00     100.00     100.00       2114
-----macro avg                                              100.00     100.00     100.00       2114
-----weighted avg                                           100.00     100.00     100.00       2114
-----
-----[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.29       0.63       0.40       4444
-----! (label_id: 1)                                          0.38      10.67       0.74        150
-----, (label_id: 2)                                          2.32       0.59       0.94      15290
------ (label_id: 3)                                          2.34      20.28       4.19       1292
-----. (label_id: 4)                                         43.85      11.68      18.44      12599
-----: (label_id: 5)                                          0.41       1.28       0.62        392
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.24       6.30       5.07       1111
-----— (label_id: 8)                                          0.00       0.00       0.00        456
-----… (label_id: 9)                                          0.38       2.41       0.65         83
------------------------
-----micro avg                                                5.40       5.40       5.40      35981
-----macro avg                                                5.42       5.38       3.11      35981
-----weighted avg                                            16.59       5.40       7.22      35981
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2222
------------------------
-----micro avg                                              100.00     100.00     100.00       2222
-----macro avg                                              100.00     100.00     100.00       2222
-----weighted avg                                           100.00     100.00     100.00       2222
-----
-----[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.35       0.73       0.48       3844
-----! (label_id: 1)                                          0.54      14.62       1.04        130
-----, (label_id: 2)                                          2.32       0.59       0.94      13056
------ (label_id: 3)                                          2.67      22.28       4.77       1194
-----. (label_id: 4)                                         44.45      11.95      18.84      10791
-----: (label_id: 5)                                          0.84       3.21       1.33        280
-----; (label_id: 6)                                          0.00       0.00       0.00        140
-----? (label_id: 7)                                          4.17       6.56       5.10        914
-----— (label_id: 8)                                          0.00       0.00       0.00        401
-----… (label_id: 9)                                          0.48       2.63       0.81         76
------------------------
-----micro avg                                                5.68       5.68       5.68      30826
-----macro avg                                                5.58       6.26       3.33      30826
-----weighted avg                                            16.82       5.68       7.41      30826
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1922
------------------------
-----micro avg                                              100.00     100.00     100.00       1922
-----macro avg                                              100.00     100.00     100.00       1922
-----weighted avg                                           100.00     100.00     100.00       1922
-----
-----[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.60       0.39       3970
-----! (label_id: 1)                                          0.35      10.66       0.68        122
-----, (label_id: 2)                                          2.09       0.53       0.85      13469
------ (label_id: 3)                                          2.29      19.32       4.10       1201
-----. (label_id: 4)                                         43.43      11.24      17.86      11227
-----: (label_id: 5)                                          0.63       2.30       0.99        304
-----; (label_id: 6)                                          0.00       0.00       0.00        141
-----? (label_id: 7)                                          4.52       6.86       5.45       1006
-----— (label_id: 8)                                          1.15       0.23       0.38        444
-----… (label_id: 9)                                          0.45       2.67       0.78         75
------------------------
-----micro avg                                                5.26       5.26       5.26      31959
-----macro avg                                                5.52       5.44       3.15      31959
-----weighted avg                                            16.42       5.26       7.02      31959
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1985
------------------------
-----micro avg                                              100.00     100.00     100.00       1985
-----macro avg                                              100.00     100.00     100.00       1985
-----weighted avg                                           100.00     100.00     100.00       1985
-----
-----[INFO] - Epoch 5, step 2394: val_loss was not in top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.23       0.48       0.31       4126
-----! (label_id: 1)                                          0.29       9.40       0.56        117
-----, (label_id: 2)                                          1.91       0.49       0.77      14019
------ (label_id: 3)                                          2.52      22.59       4.53       1164
-----. (label_id: 4)                                         44.15      11.65      18.44      11789
-----: (label_id: 5)                                          0.72       2.41       1.11        332
-----; (label_id: 6)                                          0.56       0.61       0.58        165
-----? (label_id: 7)                                          3.89       6.53       4.88        980
-----— (label_id: 8)                                          2.30       0.47       0.77        430
-----… (label_id: 9)                                          1.18       8.33       2.07         60
------------------------
-----micro avg                                                5.47       5.47       5.47      33182
-----macro avg                                                5.77       6.30       3.40      33182
-----weighted avg                                            16.77       5.47       7.25      33182
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2063
------------------------
-----micro avg                                              100.00     100.00     100.00       2063
-----macro avg                                              100.00     100.00     100.00       2063
-----weighted avg                                           100.00     100.00     100.00       2063
-----
-----[INFO] - Epoch 6, step 2793: val_loss was not in top 3
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index fa37b4c..43fc93d 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             self.hparams.model.punct_class_weights=None
---- 
----+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
----+        self.mlp = MultiLayerPerceptron(
----+            self.transformer.config.hidden_size,
----+            self.transformer.config.hidden_size,
----+            num_layers=self.hparams.model.mlp.num_fc_layers, 
----+            activation=self.hparams.model.mlp.activation, 
----+            log_softmax=self.hparams.model.mlp.log_softmax
----+        )
----+
----         self.punct_classifier = TokenClassifier(
----             hidden_size=self.transformer.config.hidden_size,
----             num_classes=len(self.labels_to_ids),
----@@ -80,6 +89,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----             log_softmax=self.hparams.model.domain_head.log_softmax,
----             dropout=self.hparams.model.domain_head.fc_dropout,
----             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
----+            pooling=self.hparams.model.domain_head.pooling,
----+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
----         )
---- 
----         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
----@@ -119,14 +130,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
----         self.freeze()
---- 
-----    def forward(self, input_ids, attention_mask, domain_ids=None):
----+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
----         hidden_states = self.transformer(
----             input_ids=input_ids, attention_mask=attention_mask
----         )[0]
----+        hidden_states = self.dropout(hidden_states)
----+        hidden_states = self.mlp(hidden_states)
----         punct_logits = self.punct_classifier(hidden_states=hidden_states)
----         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
----         domain_logits = self.domain_classifier(
-----            hidden_states=reverse_grad_hidden_states)
----+            hidden_states=reverse_grad_hidden_states,
----+            subtoken_mask=subtoken_mask)
----         return punct_logits, domain_logits
---- 
----     def _make_step(self, batch):
----@@ -136,7 +150,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         punct_labels = batch['labels']
----         domain_labels = batch['domain']
----         punct_logits, domain_logits = self(
-----            input_ids=input_ids, attention_mask=attention_mask
----+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
----         )
----         punctuation_loss = self.punctuation_loss(
----             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/lightning_logs.txt
---deleted file mode 100644
---index 9fa336d..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/lightning_logs.txt
---+++ /dev/null
---@@ -1,24 +0,0 @@
----Global seed set to 42
----GPU available: True, used: True
----TPU available: None, using: 0 TPU cores
----LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
----Using native 16bit precision.
----Global seed set to 42
----initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | dropout             | Dropout              | 0     
----2 | mlp                 | MultiLayerPerceptron | 131 K 
----3 | punct_classifier    | TokenClassifier      | 2.6 K 
----4 | domain_classifier   | SequenceClassifier   | 257   
----5 | punctuation_loss    | FocalDiceLoss        | 0     
----6 | domain_loss         | CrossEntropyLoss     | 0     
----7 | agg_loss            | AggregatorLoss       | 0     
----8 | punct_class_report  | ClassificationReport | 0     
----9 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----167 K     Trainable params
----13.4 M    Non-trainable params
----13.6 M    Total params
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/nemo_error_log.txt
---deleted file mode 100644
---index 93e0364..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/nemo_error_log.txt
---+++ /dev/null
---@@ -1,10 +0,0 @@
----[NeMo W 2021-02-08 11:03:15 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-08 11:03:24 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:03:24 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:03:26 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index 6c8beed..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,12 +0,0 @@
----[NeMo I 2021-02-08 11:03:15 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-03-15
----[NeMo I 2021-02-08 11:03:15 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-08 11:03:15 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-08 11:03:24 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:03:24 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:03:26 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/git-info.log
---deleted file mode 100644
---index 4010e76..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/git-info.log
---+++ /dev/null
---@@ -1,624 +0,0 @@
----commit hash: 939a671c8c117db6975316767ced5d95449e2b27
----diff --git a/README.md b/README.md
----index d52dba5..0b403d3 100644
------- a/README.md
----+++ b/README.md
----@@ -333,4 +333,71 @@ label                                                precision    recall       f
----  'punct_f1': 0.5858508944511414,
----  'punct_precision': 0.30176490545272827,
----  'punct_recall': 10.0,
----- 'test_loss': 0.8140875697135925}
----\ No newline at end of file
----+ 'test_loss': 0.8140875697135925}
----+
----+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
----+
----+ (label_id: 0)                                          62.15     100.00      76.66       5154
----+! (label_id: 1)                                          0.00       0.00       0.00        108
----+, (label_id: 2)                                          0.00       0.00       0.00      18022
----+- (label_id: 3)                                          0.00       0.00       0.00       1557
----+. (label_id: 4)                                         41.74      94.01      57.81      15164
----+: (label_id: 5)                                          0.00       0.00       0.00        319
----+; (label_id: 6)                                          0.00       0.00       0.00         88
----+? (label_id: 7)                                          0.00       0.00       0.00       1217
----+ (label_id: 8)                                          0.00       0.00       0.00        752
----+… (label_id: 9)                                          0.00       0.00       0.00         67
----+-------------------
----+micro avg                                               45.72      45.72      45.72      42448
----+macro avg                                               10.39      19.40      13.45      42448
----+weighted avg                                            22.46      45.72      29.96      42448
----+{ 'punct_f1': 13.446383476257324,
----+ 'punct_precision': 10.388500213623047,
----+ 'punct_recall': 19.400554656982422,
----+ 'test_loss': 0.44148480892181396}
----+
----+
----+ ## Log for 8/2/2021
----+
----+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
----+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
----+
----+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
----+End frozen 
----+micro avg                                               41.42      41.42      41.42      33406
----+macro avg                                               11.01      13.54      11.07      33406
----+weighted avg                                            34.88      41.42      34.20      33406
----+
----+1st layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               36.65      36.65      36.65      33463
----+macro avg                                               10.71       9.91       8.26      33463
----+weighted avg                                            34.32      36.65      31.08      33463
----+
----+2nd layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               35.72      35.72      35.72      42448
----+macro avg                                                3.57      10.00       5.26      42448
----+weighted avg                                            12.76      35.72      18.81      42448
----+
----+{'punct_f1': 5.264181137084961,
----+ 'punct_precision': 3.572371006011963,
----+ 'punct_recall': 10.0,
----+ 'test_loss': 18.49854850769043}
----+
----+
----+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
----+alpha from 3->4 seems to reduce convergence rate.
----+
----+micro avg                                               50.98      50.98      50.98      33463
----+macro avg                                               25.99      25.38      23.38      33463
----+weighted avg                                            50.31      50.98      48.27      33463
----+
----+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
----+micro avg                                               58.55      58.55      58.55      39340
----+macro avg                                               30.02      29.74      29.52      39340
----+weighted avg                                            57.91      58.55      57.51      39340
----+
----+still increasing?!
----+{'punct_f1': 29.523975372314453,
----+ 'punct_precision': 30.015613555908203,
----+ 'punct_recall': 29.738296508789062,
----+ 'test_loss': 0.3690211772918701}
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index fe58670..a4a012a 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,49 +1,49 @@
---- seed: 42
---- trainer:
-----    # gpus: 1 # the number of gpus, 0 for CPU
-----    # num_nodes: 1
-----    # max_epochs: 2
-----    # max_steps: null # precedence over max_epochs
-----    # accumulate_grad_batches: 4 # accumulates grads every k batches
-----    # gradient_clip_val: 0
-----    # amp_level: O1 # O1/O2 for mixed precision
-----    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
-----    # checkpoint_callback: false  # Provided by exp_manager
-----    # logger: false #false  # Provided by exp_manager
-----    # log_every_n_steps: 1  # Interval of logging.
-----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # resume_from_checkpoint: null
-----
-----    gpus: 0 # the number of gpus, 0 for CPU
----+    gpus: 1 # the number of gpus, 0 for CPU
----     num_nodes: 1
-----    max_epochs: 8
----+    max_epochs: 10
----     max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 1 # accumulates grads every k batches
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----     gradient_clip_val: 0
-----    amp_level: O0 # O1/O2 for mixed precision
-----    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
----+    amp_level: O1 # O1/O2 for mixed precision
----+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    accelerator: ddp
----     checkpoint_callback: false  # Provided by exp_manager
----     logger: false #false  # Provided by exp_manager
----     log_every_n_steps: 1  # Interval of logging.
----     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    reload_dataloaders_every_epoch: true
----     resume_from_checkpoint: null
---- 
----+    # gpus: 0 # the number of gpus, 0 for CPU
----+    # num_nodes: 1
----+    # max_epochs: 8
----+    # max_steps: null # precedence over max_epochs
----+    # accumulate_grad_batches: 1 # accumulates grads every k batches
----+    # gradient_clip_val: 0
----+    # amp_level: O0 # O1/O2 for mixed precision
----+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # # accelerator: ddp
----+    # checkpoint_callback: false  # Provided by exp_manager
----+    # logger: false #false  # Provided by exp_manager
----+    # log_every_n_steps: 1  # Interval of logging.
----+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    # reload_dataloaders_every_epoch: true
----+    # resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /home/nxingyu2/data # /root/data # 
-----tmp_path: /home/nxingyu2/data/tmp # /tmp # 
----+base_path: /home/nxingyu/data # /root/data # 
----+tmp_path: /home/nxingyu/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
-----    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----     unfrozen: 0
-----    maximum_unfrozen: 2
----+    maximum_unfrozen: 1
----     unfreeze_step: 1
----     # unfreeze_every: 3
----     punct_label_ids:
----@@ -58,10 +58,10 @@ model:
----         - "—"
----         - "…"
---- 
-----    punct_class_weights: true
----+    punct_class_weights: false
----     
----     dataset:
-----        data_dir: /home/nxingyu2/data # /root/data # 
----+        data_dir: /home/nxingyu/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -106,6 +106,12 @@ model:
----         config: null
----         # unfrozen_layers: 1
---- 
----+    mlp:
----+        num_fc_layers: 2
----+        fc_dropout: 0.1
----+        log_softmax: false
----+        activation: 'relu'
----+        
----     punct_head:
----         punct_num_fc_layers: 1
----         fc_dropout: 0.1
----@@ -122,17 +128,19 @@ model:
----         use_transformer_init: true
----         loss: 'cel'
----         gamma: 0.1 # coefficient of gradient reversal
----+        pooling: 'mean'
----+        idx_conditioned_on: 0
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 3
----+        alpha: 4
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: novograd
----+        name: adamw
----         lr: 1e-3
----         weight_decay: 0.00
----         sched:
----diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
----index d4ff927..f84c6e0 100644
------- a/experiment/core/layers/sequence_classifier.py
----+++ b/experiment/core/layers/sequence_classifier.py
----@@ -1,3 +1,4 @@
----+import torch
---- from torch import nn
---- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
---- from core.utils import transformer_weights_init
----@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
----         log_softmax: bool = True,
----         dropout: float = 0.0,
----         use_transformer_init: bool = True,
-----        idx_conditioned_on: int = 0,
----+        pooling: str = 'mean', # mean, max, mean_max, token
----+        idx_conditioned_on: int = None,
----     ):
----         """
----         Initializes the SequenceClassifier module.
----@@ -37,6 +39,7 @@ class SequenceClassifier(nn.Module):
----         super().__init__()
----         self.log_softmax = log_softmax
----         self._idx_conditioned_on = idx_conditioned_on
----+        self.pooling = pooling
----         self.mlp = MultiLayerPerceptron(
----             hidden_size=hidden_size,
----             num_classes=num_classes,
----@@ -48,7 +51,23 @@ class SequenceClassifier(nn.Module):
----         if use_transformer_init:
----             self.apply(lambda module: transformer_weights_init(module, xavier=False))
---- 
-----    def forward(self, hidden_states):
----+    def forward(self, hidden_states, subtoken_mask=None):
----         hidden_states = self.dropout(hidden_states)
-----        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
----+        if self.pooling=='token':
----+            pooled = hidden_states[:, self._idx_conditioned_on]
----+        else:
----+            if subtoken_mask==None:
----+                ct=hidden_states.shape[1] # Seq len
----+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
----+            else:
----+                ct = torch.sum(subtoken_mask,axis=1).unsqueeze(1)
----+            pooled_sum = torch.sum(hidden_states,axis=1)            
----+            if self.pooling=='mean' or self.pooling == 'mean_max':
----+                pooled_mean = torch.div(pooled_sum,ct)
----+            if self.pooling=='max' or self.pooling=='mean_max':
----+                pooled_max = torch.max(hidden_states,axis=1)[0]
----+            pooled=pooled_mean if self.pooling=='mean' else \
----+                pooled_max if self.pooling=='max' else \
----+                    torch.cat([pooled_mean,pooled_max],axis=-1)
----+        logits = self.mlp(pooled)
----         return logits
----diff --git a/experiment/info.log b/experiment/info.log
----index 2471fe9..e69de29 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,300 +0,0 @@
-----[INFO] - GPU available: True, used: False
-----[INFO] - TPU available: None, using: 0 TPU cores
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        184
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.23       0.34       0.53        594
------ (label_id: 3)                                          3.06      25.42       5.46         59
-----. (label_id: 4)                                         47.22      12.98      20.36        524
-----: (label_id: 5)                                          0.00       0.00       0.00         18
-----; (label_id: 6)                                          0.00       0.00       0.00         13
-----? (label_id: 7)                                          8.45       6.32       7.23         95
-----— (label_id: 8)                                          0.00       0.00       0.00         12
-----… (label_id: 9)                                          0.00       0.00       0.00          0
------------------------
-----micro avg                                                6.05       6.05       6.05       1503
-----macro avg                                                6.66       5.01       3.73       1503
-----weighted avg                                            17.61       6.05       7.98       1503
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00         92
------------------------
-----micro avg                                              100.00     100.00     100.00         92
-----macro avg                                              100.00     100.00     100.00         92
-----weighted avg                                           100.00     100.00     100.00         92
-----
-----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 1.5848931924611143e-08
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
-----will be used during training (effective maximum steps = 3192) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 3192
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        202
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.62       0.45       0.70        669
------ (label_id: 3)                                          3.48      27.27       6.17         66
-----. (label_id: 4)                                         45.06      13.01      20.19        561
-----: (label_id: 5)                                          1.52       6.67       2.47         15
-----; (label_id: 6)                                          0.00       0.00       0.00         15
-----? (label_id: 7)                                          8.70       7.32       7.95         82
-----— (label_id: 8)                                          0.00       0.00       0.00         13
-----… (label_id: 9)                                          0.00       0.00       0.00          1
------------------------
-----micro avg                                                6.20       6.20       6.20       1628
-----macro avg                                                6.04       5.47       3.75       1628
-----weighted avg                                            16.79       6.20       7.92       1628
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        101
------------------------
-----micro avg                                              100.00     100.00     100.00        101
-----macro avg                                              100.00     100.00     100.00        101
-----weighted avg                                           100.00     100.00     100.00        101
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.34       0.73       0.46       4402
-----! (label_id: 1)                                          0.42      13.95       0.82        129
-----, (label_id: 2)                                          2.53       0.64       1.03      15243
------ (label_id: 3)                                          2.45      21.03       4.38       1322
-----. (label_id: 4)                                         44.00      11.40      18.11      12542
-----: (label_id: 5)                                          0.43       1.41       0.65        354
-----; (label_id: 6)                                          0.00       0.00       0.00        163
-----? (label_id: 7)                                          4.16       6.27       5.00       1117
-----— (label_id: 8)                                          3.00       0.61       1.02        488
-----… (label_id: 9)                                          0.97       6.17       1.68         81
------------------------
-----micro avg                                                5.41       5.41       5.41      35841
-----macro avg                                                5.83       6.22       3.32      35841
-----weighted avg                                            16.78       5.41       7.18      35841
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2201
------------------------
-----micro avg                                              100.00     100.00     100.00       2201
-----macro avg                                              100.00     100.00     100.00       2201
-----weighted avg                                           100.00     100.00     100.00       2201
-----
-----[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.20       0.43       0.27       4226
-----! (label_id: 1)                                          0.44      14.17       0.86        127
-----, (label_id: 2)                                          1.93       0.49       0.78      14611
------ (label_id: 3)                                          2.23      19.56       4.01       1237
-----. (label_id: 4)                                         43.37      11.25      17.86      11977
-----: (label_id: 5)                                          0.68       2.34       1.05        342
-----; (label_id: 6)                                          0.00       0.00       0.00        129
-----? (label_id: 7)                                          5.16       7.47       6.10       1058
-----— (label_id: 8)                                          2.15       0.49       0.80        409
-----… (label_id: 9)                                          0.69       4.23       1.19         71
------------------------
-----micro avg                                                5.23       5.23       5.23      34187
-----macro avg                                                5.68       6.04       3.29      34187
-----weighted avg                                            16.32       5.23       6.98      34187
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2113
------------------------
-----micro avg                                              100.00     100.00     100.00       2113
-----macro avg                                              100.00     100.00     100.00       2113
-----weighted avg                                           100.00     100.00     100.00       2113
-----
-----[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.61       0.39       4228
-----! (label_id: 1)                                          0.30       8.28       0.58        145
-----, (label_id: 2)                                          2.27       0.58       0.92      14495
------ (label_id: 3)                                          2.64      21.78       4.70       1327
-----. (label_id: 4)                                         44.87      11.66      18.51      12193
-----: (label_id: 5)                                          0.60       1.93       0.91        362
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.19       6.40       5.07       1078
-----— (label_id: 8)                                          1.16       0.22       0.37        459
-----… (label_id: 9)                                          0.85       4.17       1.41         96
------------------------
-----micro avg                                                5.54       5.54       5.54      34547
-----macro avg                                                5.72       5.56       3.29      34547
-----weighted avg                                            17.08       5.54       7.33      34547
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2114
------------------------
-----micro avg                                              100.00     100.00     100.00       2114
-----macro avg                                              100.00     100.00     100.00       2114
-----weighted avg                                           100.00     100.00     100.00       2114
-----
-----[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.29       0.63       0.40       4444
-----! (label_id: 1)                                          0.38      10.67       0.74        150
-----, (label_id: 2)                                          2.32       0.59       0.94      15290
------ (label_id: 3)                                          2.34      20.28       4.19       1292
-----. (label_id: 4)                                         43.85      11.68      18.44      12599
-----: (label_id: 5)                                          0.41       1.28       0.62        392
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.24       6.30       5.07       1111
-----— (label_id: 8)                                          0.00       0.00       0.00        456
-----… (label_id: 9)                                          0.38       2.41       0.65         83
------------------------
-----micro avg                                                5.40       5.40       5.40      35981
-----macro avg                                                5.42       5.38       3.11      35981
-----weighted avg                                            16.59       5.40       7.22      35981
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2222
------------------------
-----micro avg                                              100.00     100.00     100.00       2222
-----macro avg                                              100.00     100.00     100.00       2222
-----weighted avg                                           100.00     100.00     100.00       2222
-----
-----[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.35       0.73       0.48       3844
-----! (label_id: 1)                                          0.54      14.62       1.04        130
-----, (label_id: 2)                                          2.32       0.59       0.94      13056
------ (label_id: 3)                                          2.67      22.28       4.77       1194
-----. (label_id: 4)                                         44.45      11.95      18.84      10791
-----: (label_id: 5)                                          0.84       3.21       1.33        280
-----; (label_id: 6)                                          0.00       0.00       0.00        140
-----? (label_id: 7)                                          4.17       6.56       5.10        914
-----— (label_id: 8)                                          0.00       0.00       0.00        401
-----… (label_id: 9)                                          0.48       2.63       0.81         76
------------------------
-----micro avg                                                5.68       5.68       5.68      30826
-----macro avg                                                5.58       6.26       3.33      30826
-----weighted avg                                            16.82       5.68       7.41      30826
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1922
------------------------
-----micro avg                                              100.00     100.00     100.00       1922
-----macro avg                                              100.00     100.00     100.00       1922
-----weighted avg                                           100.00     100.00     100.00       1922
-----
-----[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.60       0.39       3970
-----! (label_id: 1)                                          0.35      10.66       0.68        122
-----, (label_id: 2)                                          2.09       0.53       0.85      13469
------ (label_id: 3)                                          2.29      19.32       4.10       1201
-----. (label_id: 4)                                         43.43      11.24      17.86      11227
-----: (label_id: 5)                                          0.63       2.30       0.99        304
-----; (label_id: 6)                                          0.00       0.00       0.00        141
-----? (label_id: 7)                                          4.52       6.86       5.45       1006
-----— (label_id: 8)                                          1.15       0.23       0.38        444
-----… (label_id: 9)                                          0.45       2.67       0.78         75
------------------------
-----micro avg                                                5.26       5.26       5.26      31959
-----macro avg                                                5.52       5.44       3.15      31959
-----weighted avg                                            16.42       5.26       7.02      31959
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1985
------------------------
-----micro avg                                              100.00     100.00     100.00       1985
-----macro avg                                              100.00     100.00     100.00       1985
-----weighted avg                                           100.00     100.00     100.00       1985
-----
-----[INFO] - Epoch 5, step 2394: val_loss was not in top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.23       0.48       0.31       4126
-----! (label_id: 1)                                          0.29       9.40       0.56        117
-----, (label_id: 2)                                          1.91       0.49       0.77      14019
------ (label_id: 3)                                          2.52      22.59       4.53       1164
-----. (label_id: 4)                                         44.15      11.65      18.44      11789
-----: (label_id: 5)                                          0.72       2.41       1.11        332
-----; (label_id: 6)                                          0.56       0.61       0.58        165
-----? (label_id: 7)                                          3.89       6.53       4.88        980
-----— (label_id: 8)                                          2.30       0.47       0.77        430
-----… (label_id: 9)                                          1.18       8.33       2.07         60
------------------------
-----micro avg                                                5.47       5.47       5.47      33182
-----macro avg                                                5.77       6.30       3.40      33182
-----weighted avg                                            16.77       5.47       7.25      33182
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2063
------------------------
-----micro avg                                              100.00     100.00     100.00       2063
-----macro avg                                              100.00     100.00     100.00       2063
-----weighted avg                                           100.00     100.00     100.00       2063
-----
-----[INFO] - Epoch 6, step 2793: val_loss was not in top 3
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index fa37b4c..43fc93d 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             self.hparams.model.punct_class_weights=None
---- 
----+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
----+        self.mlp = MultiLayerPerceptron(
----+            self.transformer.config.hidden_size,
----+            self.transformer.config.hidden_size,
----+            num_layers=self.hparams.model.mlp.num_fc_layers, 
----+            activation=self.hparams.model.mlp.activation, 
----+            log_softmax=self.hparams.model.mlp.log_softmax
----+        )
----+
----         self.punct_classifier = TokenClassifier(
----             hidden_size=self.transformer.config.hidden_size,
----             num_classes=len(self.labels_to_ids),
----@@ -80,6 +89,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----             log_softmax=self.hparams.model.domain_head.log_softmax,
----             dropout=self.hparams.model.domain_head.fc_dropout,
----             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
----+            pooling=self.hparams.model.domain_head.pooling,
----+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
----         )
---- 
----         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
----@@ -119,14 +130,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
----         self.freeze()
---- 
-----    def forward(self, input_ids, attention_mask, domain_ids=None):
----+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
----         hidden_states = self.transformer(
----             input_ids=input_ids, attention_mask=attention_mask
----         )[0]
----+        hidden_states = self.dropout(hidden_states)
----+        hidden_states = self.mlp(hidden_states)
----         punct_logits = self.punct_classifier(hidden_states=hidden_states)
----         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
----         domain_logits = self.domain_classifier(
-----            hidden_states=reverse_grad_hidden_states)
----+            hidden_states=reverse_grad_hidden_states,
----+            subtoken_mask=subtoken_mask)
----         return punct_logits, domain_logits
---- 
----     def _make_step(self, batch):
----@@ -136,7 +150,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         punct_labels = batch['labels']
----         domain_labels = batch['domain']
----         punct_logits, domain_logits = self(
-----            input_ids=input_ids, attention_mask=attention_mask
----+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
----         )
----         punctuation_loss = self.punctuation_loss(
----             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/lightning_logs.txt
---deleted file mode 100644
---index 0037c7a..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/lightning_logs.txt
---+++ /dev/null
---@@ -1,25 +0,0 @@
----Global seed set to 42
----GPU available: True, used: True
----TPU available: None, using: 0 TPU cores
----LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
----Using native 16bit precision.
----Global seed set to 42
----initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | dropout             | Dropout              | 0     
----2 | mlp                 | MultiLayerPerceptron | 131 K 
----3 | punct_classifier    | TokenClassifier      | 2.6 K 
----4 | domain_classifier   | SequenceClassifier   | 257   
----5 | punctuation_loss    | FocalDiceLoss        | 0     
----6 | domain_loss         | CrossEntropyLoss     | 0     
----7 | agg_loss            | AggregatorLoss       | 0     
----8 | punct_class_report  | ClassificationReport | 0     
----9 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----167 K     Trainable params
----13.4 M    Non-trainable params
----13.6 M    Total params
----LR finder stopped early due to diverging loss.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/nemo_error_log.txt
---deleted file mode 100644
---index 0a0389f..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/nemo_error_log.txt
---+++ /dev/null
---@@ -1,13 +0,0 @@
----[NeMo W 2021-02-08 11:04:26 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-08 11:04:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:04:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:04:37 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:04:42 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
----      warnings.warn(*args, **kwargs)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index 312482b..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,15 +0,0 @@
----[NeMo I 2021-02-08 11:04:26 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-04-26
----[NeMo I 2021-02-08 11:04:26 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-08 11:04:26 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-08 11:04:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:04:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:04:37 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:04:42 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
----      warnings.warn(*args, **kwargs)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/git-info.log
---deleted file mode 100644
---index 13fff12..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/git-info.log
---+++ /dev/null
---@@ -1,627 +0,0 @@
----commit hash: 939a671c8c117db6975316767ced5d95449e2b27
----diff --git a/README.md b/README.md
----index d52dba5..0b403d3 100644
------- a/README.md
----+++ b/README.md
----@@ -333,4 +333,71 @@ label                                                precision    recall       f
----  'punct_f1': 0.5858508944511414,
----  'punct_precision': 0.30176490545272827,
----  'punct_recall': 10.0,
----- 'test_loss': 0.8140875697135925}
----\ No newline at end of file
----+ 'test_loss': 0.8140875697135925}
----+
----+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
----+
----+ (label_id: 0)                                          62.15     100.00      76.66       5154
----+! (label_id: 1)                                          0.00       0.00       0.00        108
----+, (label_id: 2)                                          0.00       0.00       0.00      18022
----+- (label_id: 3)                                          0.00       0.00       0.00       1557
----+. (label_id: 4)                                         41.74      94.01      57.81      15164
----+: (label_id: 5)                                          0.00       0.00       0.00        319
----+; (label_id: 6)                                          0.00       0.00       0.00         88
----+? (label_id: 7)                                          0.00       0.00       0.00       1217
----+ (label_id: 8)                                          0.00       0.00       0.00        752
----+… (label_id: 9)                                          0.00       0.00       0.00         67
----+-------------------
----+micro avg                                               45.72      45.72      45.72      42448
----+macro avg                                               10.39      19.40      13.45      42448
----+weighted avg                                            22.46      45.72      29.96      42448
----+{ 'punct_f1': 13.446383476257324,
----+ 'punct_precision': 10.388500213623047,
----+ 'punct_recall': 19.400554656982422,
----+ 'test_loss': 0.44148480892181396}
----+
----+
----+ ## Log for 8/2/2021
----+
----+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
----+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
----+
----+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
----+End frozen 
----+micro avg                                               41.42      41.42      41.42      33406
----+macro avg                                               11.01      13.54      11.07      33406
----+weighted avg                                            34.88      41.42      34.20      33406
----+
----+1st layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               36.65      36.65      36.65      33463
----+macro avg                                               10.71       9.91       8.26      33463
----+weighted avg                                            34.32      36.65      31.08      33463
----+
----+2nd layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               35.72      35.72      35.72      42448
----+macro avg                                                3.57      10.00       5.26      42448
----+weighted avg                                            12.76      35.72      18.81      42448
----+
----+{'punct_f1': 5.264181137084961,
----+ 'punct_precision': 3.572371006011963,
----+ 'punct_recall': 10.0,
----+ 'test_loss': 18.49854850769043}
----+
----+
----+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
----+alpha from 3->4 seems to reduce convergence rate.
----+
----+micro avg                                               50.98      50.98      50.98      33463
----+macro avg                                               25.99      25.38      23.38      33463
----+weighted avg                                            50.31      50.98      48.27      33463
----+
----+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
----+micro avg                                               58.55      58.55      58.55      39340
----+macro avg                                               30.02      29.74      29.52      39340
----+weighted avg                                            57.91      58.55      57.51      39340
----+
----+still increasing?!
----+{'punct_f1': 29.523975372314453,
----+ 'punct_precision': 30.015613555908203,
----+ 'punct_recall': 29.738296508789062,
----+ 'test_loss': 0.3690211772918701}
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index fe58670..91dbc8c 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,49 +1,49 @@
---- seed: 42
---- trainer:
-----    # gpus: 1 # the number of gpus, 0 for CPU
-----    # num_nodes: 1
-----    # max_epochs: 2
-----    # max_steps: null # precedence over max_epochs
-----    # accumulate_grad_batches: 4 # accumulates grads every k batches
-----    # gradient_clip_val: 0
-----    # amp_level: O1 # O1/O2 for mixed precision
-----    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
-----    # checkpoint_callback: false  # Provided by exp_manager
-----    # logger: false #false  # Provided by exp_manager
-----    # log_every_n_steps: 1  # Interval of logging.
-----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # resume_from_checkpoint: null
-----
-----    gpus: 0 # the number of gpus, 0 for CPU
----+    gpus: 1 # the number of gpus, 0 for CPU
----     num_nodes: 1
-----    max_epochs: 8
----+    max_epochs: 10
----     max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 1 # accumulates grads every k batches
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----     gradient_clip_val: 0
-----    amp_level: O0 # O1/O2 for mixed precision
-----    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
----+    amp_level: O1 # O1/O2 for mixed precision
----+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    accelerator: ddp
----     checkpoint_callback: false  # Provided by exp_manager
----     logger: false #false  # Provided by exp_manager
----     log_every_n_steps: 1  # Interval of logging.
----     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    reload_dataloaders_every_epoch: true
----     resume_from_checkpoint: null
---- 
----+    # gpus: 0 # the number of gpus, 0 for CPU
----+    # num_nodes: 1
----+    # max_epochs: 8
----+    # max_steps: null # precedence over max_epochs
----+    # accumulate_grad_batches: 1 # accumulates grads every k batches
----+    # gradient_clip_val: 0
----+    # amp_level: O0 # O1/O2 for mixed precision
----+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # # accelerator: ddp
----+    # checkpoint_callback: false  # Provided by exp_manager
----+    # logger: false #false  # Provided by exp_manager
----+    # log_every_n_steps: 1  # Interval of logging.
----+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    # reload_dataloaders_every_epoch: true
----+    # resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /home/nxingyu2/data # /root/data # 
-----tmp_path: /home/nxingyu2/data/tmp # /tmp # 
----+base_path: /home/nxingyu/data # /root/data # 
----+tmp_path: /home/nxingyu/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
-----    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----     unfrozen: 0
-----    maximum_unfrozen: 2
----+    maximum_unfrozen: 1
----     unfreeze_step: 1
----     # unfreeze_every: 3
----     punct_label_ids:
----@@ -58,10 +58,10 @@ model:
----         - "—"
----         - "…"
---- 
-----    punct_class_weights: true
----+    punct_class_weights: false
----     
----     dataset:
-----        data_dir: /home/nxingyu2/data # /root/data # 
----+        data_dir: /home/nxingyu/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -106,6 +106,12 @@ model:
----         config: null
----         # unfrozen_layers: 1
---- 
----+    mlp:
----+        num_fc_layers: 2
----+        fc_dropout: 0.1
----+        log_softmax: false
----+        activation: 'relu'
----+        
----     punct_head:
----         punct_num_fc_layers: 1
----         fc_dropout: 0.1
----@@ -122,17 +128,19 @@ model:
----         use_transformer_init: true
----         loss: 'cel'
----         gamma: 0.1 # coefficient of gradient reversal
----+        pooling: 'mean_max'
----+        idx_conditioned_on: 0
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 3
----+        alpha: 4
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: novograd
----+        name: adamw
----         lr: 1e-3
----         weight_decay: 0.00
----         sched:
----diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
----index d4ff927..f9927ac 100644
------- a/experiment/core/layers/sequence_classifier.py
----+++ b/experiment/core/layers/sequence_classifier.py
----@@ -1,3 +1,4 @@
----+import torch
---- from torch import nn
---- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
---- from core.utils import transformer_weights_init
----@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
----         log_softmax: bool = True,
----         dropout: float = 0.0,
----         use_transformer_init: bool = True,
-----        idx_conditioned_on: int = 0,
----+        pooling: str = 'mean', # mean, max, mean_max, token
----+        idx_conditioned_on: int = None,
----     ):
----         """
----         Initializes the SequenceClassifier module.
----@@ -37,8 +39,9 @@ class SequenceClassifier(nn.Module):
----         super().__init__()
----         self.log_softmax = log_softmax
----         self._idx_conditioned_on = idx_conditioned_on
----+        self.pooling = pooling
----         self.mlp = MultiLayerPerceptron(
-----            hidden_size=hidden_size,
----+            hidden_size=(hidden_size*2 if pooling=='mean_max' else hidden_size),
----             num_classes=num_classes,
----             num_layers=num_layers,
----             activation=activation,
----@@ -48,7 +51,23 @@ class SequenceClassifier(nn.Module):
----         if use_transformer_init:
----             self.apply(lambda module: transformer_weights_init(module, xavier=False))
---- 
-----    def forward(self, hidden_states):
----+    def forward(self, hidden_states, subtoken_mask=None):
----         hidden_states = self.dropout(hidden_states)
-----        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
----+        if self.pooling=='token':
----+            pooled = hidden_states[:, self._idx_conditioned_on]
----+        else:
----+            if subtoken_mask==None:
----+                ct=hidden_states.shape[1] # Seq len
----+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
----+            else:
----+                ct = torch.sum(subtoken_mask,axis=1).unsqueeze(1)
----+            pooled_sum = torch.sum(hidden_states,axis=1)            
----+            if self.pooling=='mean' or self.pooling == 'mean_max':
----+                pooled_mean = torch.div(pooled_sum,ct)
----+            if self.pooling=='max' or self.pooling=='mean_max':
----+                pooled_max = torch.max(hidden_states,axis=1)[0]
----+            pooled=pooled_mean if self.pooling=='mean' else \
----+                pooled_max if self.pooling=='max' else \
----+                    torch.cat([pooled_mean,pooled_max],axis=-1)
----+        logits = self.mlp(pooled)
----         return logits
----diff --git a/experiment/info.log b/experiment/info.log
----index 2471fe9..e69de29 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,300 +0,0 @@
-----[INFO] - GPU available: True, used: False
-----[INFO] - TPU available: None, using: 0 TPU cores
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        184
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.23       0.34       0.53        594
------ (label_id: 3)                                          3.06      25.42       5.46         59
-----. (label_id: 4)                                         47.22      12.98      20.36        524
-----: (label_id: 5)                                          0.00       0.00       0.00         18
-----; (label_id: 6)                                          0.00       0.00       0.00         13
-----? (label_id: 7)                                          8.45       6.32       7.23         95
-----— (label_id: 8)                                          0.00       0.00       0.00         12
-----… (label_id: 9)                                          0.00       0.00       0.00          0
------------------------
-----micro avg                                                6.05       6.05       6.05       1503
-----macro avg                                                6.66       5.01       3.73       1503
-----weighted avg                                            17.61       6.05       7.98       1503
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00         92
------------------------
-----micro avg                                              100.00     100.00     100.00         92
-----macro avg                                              100.00     100.00     100.00         92
-----weighted avg                                           100.00     100.00     100.00         92
-----
-----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 1.5848931924611143e-08
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
-----will be used during training (effective maximum steps = 3192) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 3192
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        202
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.62       0.45       0.70        669
------ (label_id: 3)                                          3.48      27.27       6.17         66
-----. (label_id: 4)                                         45.06      13.01      20.19        561
-----: (label_id: 5)                                          1.52       6.67       2.47         15
-----; (label_id: 6)                                          0.00       0.00       0.00         15
-----? (label_id: 7)                                          8.70       7.32       7.95         82
-----— (label_id: 8)                                          0.00       0.00       0.00         13
-----… (label_id: 9)                                          0.00       0.00       0.00          1
------------------------
-----micro avg                                                6.20       6.20       6.20       1628
-----macro avg                                                6.04       5.47       3.75       1628
-----weighted avg                                            16.79       6.20       7.92       1628
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        101
------------------------
-----micro avg                                              100.00     100.00     100.00        101
-----macro avg                                              100.00     100.00     100.00        101
-----weighted avg                                           100.00     100.00     100.00        101
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.34       0.73       0.46       4402
-----! (label_id: 1)                                          0.42      13.95       0.82        129
-----, (label_id: 2)                                          2.53       0.64       1.03      15243
------ (label_id: 3)                                          2.45      21.03       4.38       1322
-----. (label_id: 4)                                         44.00      11.40      18.11      12542
-----: (label_id: 5)                                          0.43       1.41       0.65        354
-----; (label_id: 6)                                          0.00       0.00       0.00        163
-----? (label_id: 7)                                          4.16       6.27       5.00       1117
-----— (label_id: 8)                                          3.00       0.61       1.02        488
-----… (label_id: 9)                                          0.97       6.17       1.68         81
------------------------
-----micro avg                                                5.41       5.41       5.41      35841
-----macro avg                                                5.83       6.22       3.32      35841
-----weighted avg                                            16.78       5.41       7.18      35841
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2201
------------------------
-----micro avg                                              100.00     100.00     100.00       2201
-----macro avg                                              100.00     100.00     100.00       2201
-----weighted avg                                           100.00     100.00     100.00       2201
-----
-----[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.20       0.43       0.27       4226
-----! (label_id: 1)                                          0.44      14.17       0.86        127
-----, (label_id: 2)                                          1.93       0.49       0.78      14611
------ (label_id: 3)                                          2.23      19.56       4.01       1237
-----. (label_id: 4)                                         43.37      11.25      17.86      11977
-----: (label_id: 5)                                          0.68       2.34       1.05        342
-----; (label_id: 6)                                          0.00       0.00       0.00        129
-----? (label_id: 7)                                          5.16       7.47       6.10       1058
-----— (label_id: 8)                                          2.15       0.49       0.80        409
-----… (label_id: 9)                                          0.69       4.23       1.19         71
------------------------
-----micro avg                                                5.23       5.23       5.23      34187
-----macro avg                                                5.68       6.04       3.29      34187
-----weighted avg                                            16.32       5.23       6.98      34187
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2113
------------------------
-----micro avg                                              100.00     100.00     100.00       2113
-----macro avg                                              100.00     100.00     100.00       2113
-----weighted avg                                           100.00     100.00     100.00       2113
-----
-----[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.61       0.39       4228
-----! (label_id: 1)                                          0.30       8.28       0.58        145
-----, (label_id: 2)                                          2.27       0.58       0.92      14495
------ (label_id: 3)                                          2.64      21.78       4.70       1327
-----. (label_id: 4)                                         44.87      11.66      18.51      12193
-----: (label_id: 5)                                          0.60       1.93       0.91        362
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.19       6.40       5.07       1078
-----— (label_id: 8)                                          1.16       0.22       0.37        459
-----… (label_id: 9)                                          0.85       4.17       1.41         96
------------------------
-----micro avg                                                5.54       5.54       5.54      34547
-----macro avg                                                5.72       5.56       3.29      34547
-----weighted avg                                            17.08       5.54       7.33      34547
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2114
------------------------
-----micro avg                                              100.00     100.00     100.00       2114
-----macro avg                                              100.00     100.00     100.00       2114
-----weighted avg                                           100.00     100.00     100.00       2114
-----
-----[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.29       0.63       0.40       4444
-----! (label_id: 1)                                          0.38      10.67       0.74        150
-----, (label_id: 2)                                          2.32       0.59       0.94      15290
------ (label_id: 3)                                          2.34      20.28       4.19       1292
-----. (label_id: 4)                                         43.85      11.68      18.44      12599
-----: (label_id: 5)                                          0.41       1.28       0.62        392
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.24       6.30       5.07       1111
-----— (label_id: 8)                                          0.00       0.00       0.00        456
-----… (label_id: 9)                                          0.38       2.41       0.65         83
------------------------
-----micro avg                                                5.40       5.40       5.40      35981
-----macro avg                                                5.42       5.38       3.11      35981
-----weighted avg                                            16.59       5.40       7.22      35981
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2222
------------------------
-----micro avg                                              100.00     100.00     100.00       2222
-----macro avg                                              100.00     100.00     100.00       2222
-----weighted avg                                           100.00     100.00     100.00       2222
-----
-----[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.35       0.73       0.48       3844
-----! (label_id: 1)                                          0.54      14.62       1.04        130
-----, (label_id: 2)                                          2.32       0.59       0.94      13056
------ (label_id: 3)                                          2.67      22.28       4.77       1194
-----. (label_id: 4)                                         44.45      11.95      18.84      10791
-----: (label_id: 5)                                          0.84       3.21       1.33        280
-----; (label_id: 6)                                          0.00       0.00       0.00        140
-----? (label_id: 7)                                          4.17       6.56       5.10        914
-----— (label_id: 8)                                          0.00       0.00       0.00        401
-----… (label_id: 9)                                          0.48       2.63       0.81         76
------------------------
-----micro avg                                                5.68       5.68       5.68      30826
-----macro avg                                                5.58       6.26       3.33      30826
-----weighted avg                                            16.82       5.68       7.41      30826
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1922
------------------------
-----micro avg                                              100.00     100.00     100.00       1922
-----macro avg                                              100.00     100.00     100.00       1922
-----weighted avg                                           100.00     100.00     100.00       1922
-----
-----[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.60       0.39       3970
-----! (label_id: 1)                                          0.35      10.66       0.68        122
-----, (label_id: 2)                                          2.09       0.53       0.85      13469
------ (label_id: 3)                                          2.29      19.32       4.10       1201
-----. (label_id: 4)                                         43.43      11.24      17.86      11227
-----: (label_id: 5)                                          0.63       2.30       0.99        304
-----; (label_id: 6)                                          0.00       0.00       0.00        141
-----? (label_id: 7)                                          4.52       6.86       5.45       1006
-----— (label_id: 8)                                          1.15       0.23       0.38        444
-----… (label_id: 9)                                          0.45       2.67       0.78         75
------------------------
-----micro avg                                                5.26       5.26       5.26      31959
-----macro avg                                                5.52       5.44       3.15      31959
-----weighted avg                                            16.42       5.26       7.02      31959
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1985
------------------------
-----micro avg                                              100.00     100.00     100.00       1985
-----macro avg                                              100.00     100.00     100.00       1985
-----weighted avg                                           100.00     100.00     100.00       1985
-----
-----[INFO] - Epoch 5, step 2394: val_loss was not in top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.23       0.48       0.31       4126
-----! (label_id: 1)                                          0.29       9.40       0.56        117
-----, (label_id: 2)                                          1.91       0.49       0.77      14019
------ (label_id: 3)                                          2.52      22.59       4.53       1164
-----. (label_id: 4)                                         44.15      11.65      18.44      11789
-----: (label_id: 5)                                          0.72       2.41       1.11        332
-----; (label_id: 6)                                          0.56       0.61       0.58        165
-----? (label_id: 7)                                          3.89       6.53       4.88        980
-----— (label_id: 8)                                          2.30       0.47       0.77        430
-----… (label_id: 9)                                          1.18       8.33       2.07         60
------------------------
-----micro avg                                                5.47       5.47       5.47      33182
-----macro avg                                                5.77       6.30       3.40      33182
-----weighted avg                                            16.77       5.47       7.25      33182
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2063
------------------------
-----micro avg                                              100.00     100.00     100.00       2063
-----macro avg                                              100.00     100.00     100.00       2063
-----weighted avg                                           100.00     100.00     100.00       2063
-----
-----[INFO] - Epoch 6, step 2793: val_loss was not in top 3
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index fa37b4c..43fc93d 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             self.hparams.model.punct_class_weights=None
---- 
----+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
----+        self.mlp = MultiLayerPerceptron(
----+            self.transformer.config.hidden_size,
----+            self.transformer.config.hidden_size,
----+            num_layers=self.hparams.model.mlp.num_fc_layers, 
----+            activation=self.hparams.model.mlp.activation, 
----+            log_softmax=self.hparams.model.mlp.log_softmax
----+        )
----+
----         self.punct_classifier = TokenClassifier(
----             hidden_size=self.transformer.config.hidden_size,
----             num_classes=len(self.labels_to_ids),
----@@ -80,6 +89,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----             log_softmax=self.hparams.model.domain_head.log_softmax,
----             dropout=self.hparams.model.domain_head.fc_dropout,
----             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
----+            pooling=self.hparams.model.domain_head.pooling,
----+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
----         )
---- 
----         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
----@@ -119,14 +130,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
----         self.freeze()
---- 
-----    def forward(self, input_ids, attention_mask, domain_ids=None):
----+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
----         hidden_states = self.transformer(
----             input_ids=input_ids, attention_mask=attention_mask
----         )[0]
----+        hidden_states = self.dropout(hidden_states)
----+        hidden_states = self.mlp(hidden_states)
----         punct_logits = self.punct_classifier(hidden_states=hidden_states)
----         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
----         domain_logits = self.domain_classifier(
-----            hidden_states=reverse_grad_hidden_states)
----+            hidden_states=reverse_grad_hidden_states,
----+            subtoken_mask=subtoken_mask)
----         return punct_logits, domain_logits
---- 
----     def _make_step(self, batch):
----@@ -136,7 +150,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         punct_labels = batch['labels']
----         domain_labels = batch['domain']
----         punct_logits, domain_logits = self(
-----            input_ids=input_ids, attention_mask=attention_mask
----+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
----         )
----         punctuation_loss = self.punctuation_loss(
----             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/lightning_logs.txt
---deleted file mode 100644
---index 031cd06..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/lightning_logs.txt
---+++ /dev/null
---@@ -1,25 +0,0 @@
----Global seed set to 42
----GPU available: True, used: True
----TPU available: None, using: 0 TPU cores
----LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
----Using native 16bit precision.
----Global seed set to 42
----initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | dropout             | Dropout              | 0     
----2 | mlp                 | MultiLayerPerceptron | 131 K 
----3 | punct_classifier    | TokenClassifier      | 2.6 K 
----4 | domain_classifier   | SequenceClassifier   | 513   
----5 | punctuation_loss    | FocalDiceLoss        | 0     
----6 | domain_loss         | CrossEntropyLoss     | 0     
----7 | agg_loss            | AggregatorLoss       | 0     
----8 | punct_class_report  | ClassificationReport | 0     
----9 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----167 K     Trainable params
----13.4 M    Non-trainable params
----13.6 M    Total params
----LR finder stopped early due to diverging loss.
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/nemo_error_log.txt
---deleted file mode 100644
---index a3259fc..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/nemo_error_log.txt
---+++ /dev/null
---@@ -1,13 +0,0 @@
----[NeMo W 2021-02-08 11:06:05 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-08 11:06:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:06:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:06:17 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:06:21 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
----      warnings.warn(*args, **kwargs)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index d641997..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,15 +0,0 @@
----[NeMo I 2021-02-08 11:06:05 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-06-05
----[NeMo I 2021-02-08 11:06:05 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-08 11:06:05 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-08 11:06:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:06:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:06:17 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:06:21 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
----      warnings.warn(*args, **kwargs)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/events.out.tfevents.1612753856.Titan.19011.0 b/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/events.out.tfevents.1612753856.Titan.19011.0
---deleted file mode 100644
---index 481eef2..0000000
---Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/events.out.tfevents.1612753856.Titan.19011.0 and /dev/null differ
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/git-info.log
---deleted file mode 100644
---index daa2b33..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/git-info.log
---+++ /dev/null
---@@ -1,627 +0,0 @@
----commit hash: 939a671c8c117db6975316767ced5d95449e2b27
----diff --git a/README.md b/README.md
----index d52dba5..0b403d3 100644
------- a/README.md
----+++ b/README.md
----@@ -333,4 +333,71 @@ label                                                precision    recall       f
----  'punct_f1': 0.5858508944511414,
----  'punct_precision': 0.30176490545272827,
----  'punct_recall': 10.0,
----- 'test_loss': 0.8140875697135925}
----\ No newline at end of file
----+ 'test_loss': 0.8140875697135925}
----+
----+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
----+
----+ (label_id: 0)                                          62.15     100.00      76.66       5154
----+! (label_id: 1)                                          0.00       0.00       0.00        108
----+, (label_id: 2)                                          0.00       0.00       0.00      18022
----+- (label_id: 3)                                          0.00       0.00       0.00       1557
----+. (label_id: 4)                                         41.74      94.01      57.81      15164
----+: (label_id: 5)                                          0.00       0.00       0.00        319
----+; (label_id: 6)                                          0.00       0.00       0.00         88
----+? (label_id: 7)                                          0.00       0.00       0.00       1217
----+ (label_id: 8)                                          0.00       0.00       0.00        752
----+… (label_id: 9)                                          0.00       0.00       0.00         67
----+-------------------
----+micro avg                                               45.72      45.72      45.72      42448
----+macro avg                                               10.39      19.40      13.45      42448
----+weighted avg                                            22.46      45.72      29.96      42448
----+{ 'punct_f1': 13.446383476257324,
----+ 'punct_precision': 10.388500213623047,
----+ 'punct_recall': 19.400554656982422,
----+ 'test_loss': 0.44148480892181396}
----+
----+
----+ ## Log for 8/2/2021
----+
----+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
----+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
----+
----+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
----+End frozen 
----+micro avg                                               41.42      41.42      41.42      33406
----+macro avg                                               11.01      13.54      11.07      33406
----+weighted avg                                            34.88      41.42      34.20      33406
----+
----+1st layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               36.65      36.65      36.65      33463
----+macro avg                                               10.71       9.91       8.26      33463
----+weighted avg                                            34.32      36.65      31.08      33463
----+
----+2nd layer best lr 1e-10, set to 0.007943282347242822
----+micro avg                                               35.72      35.72      35.72      42448
----+macro avg                                                3.57      10.00       5.26      42448
----+weighted avg                                            12.76      35.72      18.81      42448
----+
----+{'punct_f1': 5.264181137084961,
----+ 'punct_precision': 3.572371006011963,
----+ 'punct_recall': 10.0,
----+ 'test_loss': 18.49854850769043}
----+
----+
----+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
----+alpha from 3->4 seems to reduce convergence rate.
----+
----+micro avg                                               50.98      50.98      50.98      33463
----+macro avg                                               25.99      25.38      23.38      33463
----+weighted avg                                            50.31      50.98      48.27      33463
----+
----+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
----+micro avg                                               58.55      58.55      58.55      39340
----+macro avg                                               30.02      29.74      29.52      39340
----+weighted avg                                            57.91      58.55      57.51      39340
----+
----+still increasing?!
----+{'punct_f1': 29.523975372314453,
----+ 'punct_precision': 30.015613555908203,
----+ 'punct_recall': 29.738296508789062,
----+ 'test_loss': 0.3690211772918701}
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index fe58670..a4a012a 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,49 +1,49 @@
---- seed: 42
---- trainer:
-----    # gpus: 1 # the number of gpus, 0 for CPU
-----    # num_nodes: 1
-----    # max_epochs: 2
-----    # max_steps: null # precedence over max_epochs
-----    # accumulate_grad_batches: 4 # accumulates grads every k batches
-----    # gradient_clip_val: 0
-----    # amp_level: O1 # O1/O2 for mixed precision
-----    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
-----    # checkpoint_callback: false  # Provided by exp_manager
-----    # logger: false #false  # Provided by exp_manager
-----    # log_every_n_steps: 1  # Interval of logging.
-----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # resume_from_checkpoint: null
-----
-----    gpus: 0 # the number of gpus, 0 for CPU
----+    gpus: 1 # the number of gpus, 0 for CPU
----     num_nodes: 1
-----    max_epochs: 8
----+    max_epochs: 10
----     max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 1 # accumulates grads every k batches
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----     gradient_clip_val: 0
-----    amp_level: O0 # O1/O2 for mixed precision
-----    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
----+    amp_level: O1 # O1/O2 for mixed precision
----+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    accelerator: ddp
----     checkpoint_callback: false  # Provided by exp_manager
----     logger: false #false  # Provided by exp_manager
----     log_every_n_steps: 1  # Interval of logging.
----     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    reload_dataloaders_every_epoch: true
----     resume_from_checkpoint: null
---- 
----+    # gpus: 0 # the number of gpus, 0 for CPU
----+    # num_nodes: 1
----+    # max_epochs: 8
----+    # max_steps: null # precedence over max_epochs
----+    # accumulate_grad_batches: 1 # accumulates grads every k batches
----+    # gradient_clip_val: 0
----+    # amp_level: O0 # O1/O2 for mixed precision
----+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # # accelerator: ddp
----+    # checkpoint_callback: false  # Provided by exp_manager
----+    # logger: false #false  # Provided by exp_manager
----+    # log_every_n_steps: 1  # Interval of logging.
----+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    # reload_dataloaders_every_epoch: true
----+    # resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /home/nxingyu2/data # /root/data # 
-----tmp_path: /home/nxingyu2/data/tmp # /tmp # 
----+base_path: /home/nxingyu/data # /root/data # 
----+tmp_path: /home/nxingyu/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
-----    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----     unfrozen: 0
-----    maximum_unfrozen: 2
----+    maximum_unfrozen: 1
----     unfreeze_step: 1
----     # unfreeze_every: 3
----     punct_label_ids:
----@@ -58,10 +58,10 @@ model:
----         - "—"
----         - "…"
---- 
-----    punct_class_weights: true
----+    punct_class_weights: false
----     
----     dataset:
-----        data_dir: /home/nxingyu2/data # /root/data # 
----+        data_dir: /home/nxingyu/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -106,6 +106,12 @@ model:
----         config: null
----         # unfrozen_layers: 1
---- 
----+    mlp:
----+        num_fc_layers: 2
----+        fc_dropout: 0.1
----+        log_softmax: false
----+        activation: 'relu'
----+        
----     punct_head:
----         punct_num_fc_layers: 1
----         fc_dropout: 0.1
----@@ -122,17 +128,19 @@ model:
----         use_transformer_init: true
----         loss: 'cel'
----         gamma: 0.1 # coefficient of gradient reversal
----+        pooling: 'mean'
----+        idx_conditioned_on: 0
----     
----     dice_loss:
----         epsilon: 0.01
-----        alpha: 3
----+        alpha: 4
----         macro_average: true
---- 
----     focal_loss: 
----         gamma: 5
---- 
----     optim:
-----        name: novograd
----+        name: adamw
----         lr: 1e-3
----         weight_decay: 0.00
----         sched:
----diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
----index d4ff927..f9927ac 100644
------- a/experiment/core/layers/sequence_classifier.py
----+++ b/experiment/core/layers/sequence_classifier.py
----@@ -1,3 +1,4 @@
----+import torch
---- from torch import nn
---- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
---- from core.utils import transformer_weights_init
----@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
----         log_softmax: bool = True,
----         dropout: float = 0.0,
----         use_transformer_init: bool = True,
-----        idx_conditioned_on: int = 0,
----+        pooling: str = 'mean', # mean, max, mean_max, token
----+        idx_conditioned_on: int = None,
----     ):
----         """
----         Initializes the SequenceClassifier module.
----@@ -37,8 +39,9 @@ class SequenceClassifier(nn.Module):
----         super().__init__()
----         self.log_softmax = log_softmax
----         self._idx_conditioned_on = idx_conditioned_on
----+        self.pooling = pooling
----         self.mlp = MultiLayerPerceptron(
-----            hidden_size=hidden_size,
----+            hidden_size=(hidden_size*2 if pooling=='mean_max' else hidden_size),
----             num_classes=num_classes,
----             num_layers=num_layers,
----             activation=activation,
----@@ -48,7 +51,23 @@ class SequenceClassifier(nn.Module):
----         if use_transformer_init:
----             self.apply(lambda module: transformer_weights_init(module, xavier=False))
---- 
-----    def forward(self, hidden_states):
----+    def forward(self, hidden_states, subtoken_mask=None):
----         hidden_states = self.dropout(hidden_states)
-----        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
----+        if self.pooling=='token':
----+            pooled = hidden_states[:, self._idx_conditioned_on]
----+        else:
----+            if subtoken_mask==None:
----+                ct=hidden_states.shape[1] # Seq len
----+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
----+            else:
----+                ct = torch.sum(subtoken_mask,axis=1).unsqueeze(1)
----+            pooled_sum = torch.sum(hidden_states,axis=1)            
----+            if self.pooling=='mean' or self.pooling == 'mean_max':
----+                pooled_mean = torch.div(pooled_sum,ct)
----+            if self.pooling=='max' or self.pooling=='mean_max':
----+                pooled_max = torch.max(hidden_states,axis=1)[0]
----+            pooled=pooled_mean if self.pooling=='mean' else \
----+                pooled_max if self.pooling=='max' else \
----+                    torch.cat([pooled_mean,pooled_max],axis=-1)
----+        logits = self.mlp(pooled)
----         return logits
----diff --git a/experiment/info.log b/experiment/info.log
----index 2471fe9..e69de29 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,300 +0,0 @@
-----[INFO] - GPU available: True, used: False
-----[INFO] - TPU available: None, using: 0 TPU cores
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        184
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.23       0.34       0.53        594
------ (label_id: 3)                                          3.06      25.42       5.46         59
-----. (label_id: 4)                                         47.22      12.98      20.36        524
-----: (label_id: 5)                                          0.00       0.00       0.00         18
-----; (label_id: 6)                                          0.00       0.00       0.00         13
-----? (label_id: 7)                                          8.45       6.32       7.23         95
-----— (label_id: 8)                                          0.00       0.00       0.00         12
-----… (label_id: 9)                                          0.00       0.00       0.00          0
------------------------
-----micro avg                                                6.05       6.05       6.05       1503
-----macro avg                                                6.66       5.01       3.73       1503
-----weighted avg                                            17.61       6.05       7.98       1503
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00         92
------------------------
-----micro avg                                              100.00     100.00     100.00         92
-----macro avg                                              100.00     100.00     100.00         92
-----weighted avg                                           100.00     100.00     100.00         92
-----
-----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 1.5848931924611143e-08
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
-----will be used during training (effective maximum steps = 3192) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 3192
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        202
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.62       0.45       0.70        669
------ (label_id: 3)                                          3.48      27.27       6.17         66
-----. (label_id: 4)                                         45.06      13.01      20.19        561
-----: (label_id: 5)                                          1.52       6.67       2.47         15
-----; (label_id: 6)                                          0.00       0.00       0.00         15
-----? (label_id: 7)                                          8.70       7.32       7.95         82
-----— (label_id: 8)                                          0.00       0.00       0.00         13
-----… (label_id: 9)                                          0.00       0.00       0.00          1
------------------------
-----micro avg                                                6.20       6.20       6.20       1628
-----macro avg                                                6.04       5.47       3.75       1628
-----weighted avg                                            16.79       6.20       7.92       1628
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        101
------------------------
-----micro avg                                              100.00     100.00     100.00        101
-----macro avg                                              100.00     100.00     100.00        101
-----weighted avg                                           100.00     100.00     100.00        101
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.34       0.73       0.46       4402
-----! (label_id: 1)                                          0.42      13.95       0.82        129
-----, (label_id: 2)                                          2.53       0.64       1.03      15243
------ (label_id: 3)                                          2.45      21.03       4.38       1322
-----. (label_id: 4)                                         44.00      11.40      18.11      12542
-----: (label_id: 5)                                          0.43       1.41       0.65        354
-----; (label_id: 6)                                          0.00       0.00       0.00        163
-----? (label_id: 7)                                          4.16       6.27       5.00       1117
-----— (label_id: 8)                                          3.00       0.61       1.02        488
-----… (label_id: 9)                                          0.97       6.17       1.68         81
------------------------
-----micro avg                                                5.41       5.41       5.41      35841
-----macro avg                                                5.83       6.22       3.32      35841
-----weighted avg                                            16.78       5.41       7.18      35841
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2201
------------------------
-----micro avg                                              100.00     100.00     100.00       2201
-----macro avg                                              100.00     100.00     100.00       2201
-----weighted avg                                           100.00     100.00     100.00       2201
-----
-----[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.20       0.43       0.27       4226
-----! (label_id: 1)                                          0.44      14.17       0.86        127
-----, (label_id: 2)                                          1.93       0.49       0.78      14611
------ (label_id: 3)                                          2.23      19.56       4.01       1237
-----. (label_id: 4)                                         43.37      11.25      17.86      11977
-----: (label_id: 5)                                          0.68       2.34       1.05        342
-----; (label_id: 6)                                          0.00       0.00       0.00        129
-----? (label_id: 7)                                          5.16       7.47       6.10       1058
-----— (label_id: 8)                                          2.15       0.49       0.80        409
-----… (label_id: 9)                                          0.69       4.23       1.19         71
------------------------
-----micro avg                                                5.23       5.23       5.23      34187
-----macro avg                                                5.68       6.04       3.29      34187
-----weighted avg                                            16.32       5.23       6.98      34187
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2113
------------------------
-----micro avg                                              100.00     100.00     100.00       2113
-----macro avg                                              100.00     100.00     100.00       2113
-----weighted avg                                           100.00     100.00     100.00       2113
-----
-----[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.61       0.39       4228
-----! (label_id: 1)                                          0.30       8.28       0.58        145
-----, (label_id: 2)                                          2.27       0.58       0.92      14495
------ (label_id: 3)                                          2.64      21.78       4.70       1327
-----. (label_id: 4)                                         44.87      11.66      18.51      12193
-----: (label_id: 5)                                          0.60       1.93       0.91        362
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.19       6.40       5.07       1078
-----— (label_id: 8)                                          1.16       0.22       0.37        459
-----… (label_id: 9)                                          0.85       4.17       1.41         96
------------------------
-----micro avg                                                5.54       5.54       5.54      34547
-----macro avg                                                5.72       5.56       3.29      34547
-----weighted avg                                            17.08       5.54       7.33      34547
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2114
------------------------
-----micro avg                                              100.00     100.00     100.00       2114
-----macro avg                                              100.00     100.00     100.00       2114
-----weighted avg                                           100.00     100.00     100.00       2114
-----
-----[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.29       0.63       0.40       4444
-----! (label_id: 1)                                          0.38      10.67       0.74        150
-----, (label_id: 2)                                          2.32       0.59       0.94      15290
------ (label_id: 3)                                          2.34      20.28       4.19       1292
-----. (label_id: 4)                                         43.85      11.68      18.44      12599
-----: (label_id: 5)                                          0.41       1.28       0.62        392
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.24       6.30       5.07       1111
-----— (label_id: 8)                                          0.00       0.00       0.00        456
-----… (label_id: 9)                                          0.38       2.41       0.65         83
------------------------
-----micro avg                                                5.40       5.40       5.40      35981
-----macro avg                                                5.42       5.38       3.11      35981
-----weighted avg                                            16.59       5.40       7.22      35981
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2222
------------------------
-----micro avg                                              100.00     100.00     100.00       2222
-----macro avg                                              100.00     100.00     100.00       2222
-----weighted avg                                           100.00     100.00     100.00       2222
-----
-----[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.35       0.73       0.48       3844
-----! (label_id: 1)                                          0.54      14.62       1.04        130
-----, (label_id: 2)                                          2.32       0.59       0.94      13056
------ (label_id: 3)                                          2.67      22.28       4.77       1194
-----. (label_id: 4)                                         44.45      11.95      18.84      10791
-----: (label_id: 5)                                          0.84       3.21       1.33        280
-----; (label_id: 6)                                          0.00       0.00       0.00        140
-----? (label_id: 7)                                          4.17       6.56       5.10        914
-----— (label_id: 8)                                          0.00       0.00       0.00        401
-----… (label_id: 9)                                          0.48       2.63       0.81         76
------------------------
-----micro avg                                                5.68       5.68       5.68      30826
-----macro avg                                                5.58       6.26       3.33      30826
-----weighted avg                                            16.82       5.68       7.41      30826
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1922
------------------------
-----micro avg                                              100.00     100.00     100.00       1922
-----macro avg                                              100.00     100.00     100.00       1922
-----weighted avg                                           100.00     100.00     100.00       1922
-----
-----[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.60       0.39       3970
-----! (label_id: 1)                                          0.35      10.66       0.68        122
-----, (label_id: 2)                                          2.09       0.53       0.85      13469
------ (label_id: 3)                                          2.29      19.32       4.10       1201
-----. (label_id: 4)                                         43.43      11.24      17.86      11227
-----: (label_id: 5)                                          0.63       2.30       0.99        304
-----; (label_id: 6)                                          0.00       0.00       0.00        141
-----? (label_id: 7)                                          4.52       6.86       5.45       1006
-----— (label_id: 8)                                          1.15       0.23       0.38        444
-----… (label_id: 9)                                          0.45       2.67       0.78         75
------------------------
-----micro avg                                                5.26       5.26       5.26      31959
-----macro avg                                                5.52       5.44       3.15      31959
-----weighted avg                                            16.42       5.26       7.02      31959
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1985
------------------------
-----micro avg                                              100.00     100.00     100.00       1985
-----macro avg                                              100.00     100.00     100.00       1985
-----weighted avg                                           100.00     100.00     100.00       1985
-----
-----[INFO] - Epoch 5, step 2394: val_loss was not in top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.23       0.48       0.31       4126
-----! (label_id: 1)                                          0.29       9.40       0.56        117
-----, (label_id: 2)                                          1.91       0.49       0.77      14019
------ (label_id: 3)                                          2.52      22.59       4.53       1164
-----. (label_id: 4)                                         44.15      11.65      18.44      11789
-----: (label_id: 5)                                          0.72       2.41       1.11        332
-----; (label_id: 6)                                          0.56       0.61       0.58        165
-----? (label_id: 7)                                          3.89       6.53       4.88        980
-----— (label_id: 8)                                          2.30       0.47       0.77        430
-----… (label_id: 9)                                          1.18       8.33       2.07         60
------------------------
-----micro avg                                                5.47       5.47       5.47      33182
-----macro avg                                                5.77       6.30       3.40      33182
-----weighted avg                                            16.77       5.47       7.25      33182
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2063
------------------------
-----micro avg                                              100.00     100.00     100.00       2063
-----macro avg                                              100.00     100.00     100.00       2063
-----weighted avg                                           100.00     100.00     100.00       2063
-----
-----[INFO] - Epoch 6, step 2793: val_loss was not in top 3
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index fa37b4c..43fc93d 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             self.hparams.model.punct_class_weights=None
---- 
----+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
----+        self.mlp = MultiLayerPerceptron(
----+            self.transformer.config.hidden_size,
----+            self.transformer.config.hidden_size,
----+            num_layers=self.hparams.model.mlp.num_fc_layers, 
----+            activation=self.hparams.model.mlp.activation, 
----+            log_softmax=self.hparams.model.mlp.log_softmax
----+        )
----+
----         self.punct_classifier = TokenClassifier(
----             hidden_size=self.transformer.config.hidden_size,
----             num_classes=len(self.labels_to_ids),
----@@ -80,6 +89,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----             log_softmax=self.hparams.model.domain_head.log_softmax,
----             dropout=self.hparams.model.domain_head.fc_dropout,
----             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
----+            pooling=self.hparams.model.domain_head.pooling,
----+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
----         )
---- 
----         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
----@@ -119,14 +130,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
----         self.freeze()
---- 
-----    def forward(self, input_ids, attention_mask, domain_ids=None):
----+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
----         hidden_states = self.transformer(
----             input_ids=input_ids, attention_mask=attention_mask
----         )[0]
----+        hidden_states = self.dropout(hidden_states)
----+        hidden_states = self.mlp(hidden_states)
----         punct_logits = self.punct_classifier(hidden_states=hidden_states)
----         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
----         domain_logits = self.domain_classifier(
-----            hidden_states=reverse_grad_hidden_states)
----+            hidden_states=reverse_grad_hidden_states,
----+            subtoken_mask=subtoken_mask)
----         return punct_logits, domain_logits
---- 
----     def _make_step(self, batch):
----@@ -136,7 +150,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         punct_labels = batch['labels']
----         domain_labels = batch['domain']
----         punct_logits, domain_logits = self(
-----            input_ids=input_ids, attention_mask=attention_mask
----+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
----         )
----         punctuation_loss = self.punctuation_loss(
----             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/hparams.yaml
---deleted file mode 100644
---index ebcb726..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/hparams.yaml
---+++ /dev/null
---@@ -1,114 +0,0 @@
----seed: 42
----trainer:
----  gpus: 1
----  num_nodes: 1
----  max_epochs: 10
----  max_steps: null
----  accumulate_grad_batches: 4
----  gradient_clip_val: 0
----  amp_level: O1
----  precision: 16
----  accelerator: ddp
----  checkpoint_callback: false
----  logger: false
----  log_every_n_steps: 1
----  val_check_interval: 1.0
----  resume_from_checkpoint: null
----exp_manager:
----  exp_dir: /home/nxingyu/project/
----  name: Punctuation_with_Domain_discriminator
----  create_tensorboard_logger: true
----  create_checkpoint_callback: true
----base_path: /home/nxingyu/data
----tmp_path: /home/nxingyu/data/tmp
----model:
----  nemo_path: null
----  transformer_path: google/electra-small-discriminator
----  unfrozen: 0
----  maximum_unfrozen: 1
----  unfreeze_step: 1
----  punct_label_ids:
----  - ''
----  - '!'
----  - ','
----  - '-'
----  - .
----  - ':'
----  - ;
----  - '?'
----  - —
----  - …
----  punct_class_weights: false
----  dataset:
----    data_dir: /home/nxingyu/data
----    labelled:
----    - /home/nxingyu/data/ted_talks_processed
----    unlabelled: null
----    max_seq_length: 128
----    pad_label: ''
----    ignore_extra_tokens: false
----    ignore_start_end: false
----    use_cache: false
----    num_workers: 0
----    pin_memory: true
----    drop_last: false
----    num_labels: 10
----    num_domains: 1
----    test_unlabelled: true
----    train_ds:
----      shuffle: true
----      num_samples: -1
----      batch_size: 8
----    validation_ds:
----      shuffle: true
----      num_samples: -1
----      batch_size: 8
----  tokenizer:
----    tokenizer_name: google/electra-small-discriminator
----    vocab_file: null
----    tokenizer_model: null
----    special_tokens: null
----  language_model:
----    pretrained_model_name: google/electra-small-discriminator
----    lm_checkpoint: null
----    config_file: null
----    config: null
----  mlp:
----    num_fc_layers: 2
----    fc_dropout: 0.1
----    log_softmax: false
----    activation: relu
----  punct_head:
----    punct_num_fc_layers: 1
----    fc_dropout: 0.1
----    activation: relu
----    log_softmax: false
----    use_transformer_init: true
----    loss: dice
----  domain_head:
----    domain_num_fc_layers: 1
----    fc_dropout: 0.1
----    activation: relu
----    log_softmax: false
----    use_transformer_init: true
----    loss: cel
----    gamma: 0.1
----    pooling: mean
----    idx_conditioned_on: 0
----  dice_loss:
----    epsilon: 0.01
----    alpha: 4
----    macro_average: true
----  focal_loss:
----    gamma: 5
----  optim:
----    name: adamw
----    lr: 0.001
----    weight_decay: 0.0
----    sched:
----      name: WarmupAnnealing
----      warmup_steps: null
----      warmup_ratio: 0.1
----      last_epoch: -1
----      monitor: val_loss
----      reduce_on_plateau: false
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/lightning_logs.txt
---deleted file mode 100644
---index d3676e6..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/lightning_logs.txt
---+++ /dev/null
---@@ -1,46 +0,0 @@
----Global seed set to 42
----GPU available: True, used: True
----TPU available: None, using: 0 TPU cores
----LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
----Using native 16bit precision.
----Global seed set to 42
----initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | dropout             | Dropout              | 0     
----2 | mlp                 | MultiLayerPerceptron | 131 K 
----3 | punct_classifier    | TokenClassifier      | 2.6 K 
----4 | domain_classifier   | SequenceClassifier   | 257   
----5 | punctuation_loss    | FocalDiceLoss        | 0     
----6 | domain_loss         | CrossEntropyLoss     | 0     
----7 | agg_loss            | AggregatorLoss       | 0     
----8 | punct_class_report  | ClassificationReport | 0     
----9 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----167 K     Trainable params
----13.4 M    Non-trainable params
----13.6 M    Total params
----Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/lr_find_temp_model.ckpt
----Global seed set to 42
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | dropout             | Dropout              | 0     
----2 | mlp                 | MultiLayerPerceptron | 131 K 
----3 | punct_classifier    | TokenClassifier      | 2.6 K 
----4 | domain_classifier   | SequenceClassifier   | 257   
----5 | punctuation_loss    | FocalDiceLoss        | 0     
----6 | domain_loss         | CrossEntropyLoss     | 0     
----7 | agg_loss            | AggregatorLoss       | 0     
----8 | punct_class_report  | ClassificationReport | 0     
----9 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----167 K     Trainable params
----13.4 M    Non-trainable params
----13.6 M    Total params
----Epoch 0, global step 100: val_loss reached 0.87292 (best 0.87292), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=0.ckpt" as top 3
----Epoch 1, global step 200: val_loss reached 0.80471 (best 0.80471), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.80-epoch=1.ckpt" as top 3
----Epoch 2, global step 300: val_loss reached 0.80542 (best 0.80471), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.81-epoch=2.ckpt" as top 3
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/nemo_error_log.txt
---deleted file mode 100644
---index 5b76cfb..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/nemo_error_log.txt
---+++ /dev/null
---@@ -1,28 +0,0 @@
----[NeMo W 2021-02-08 11:07:07 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-08 11:07:16 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:07:16 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:07:18 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:10:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:10:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:10:57 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:11:01 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
----      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
----    
----[NeMo W 2021-02-08 11:16:19 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7fe326ec1700> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-08 11:16:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7fe326ea8040> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index 45ada6c..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,30 +0,0 @@
----[NeMo I 2021-02-08 11:07:07 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-07-07
----[NeMo I 2021-02-08 11:07:07 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-08 11:07:07 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-08 11:07:16 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:07:16 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:07:18 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:10:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:10:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:10:57 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:11:01 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
----      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
----    
----[NeMo W 2021-02-08 11:16:19 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7fe326ec1700> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
----[NeMo W 2021-02-08 11:16:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7fe326ea8040> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
----      warnings.warn(warn_msg)
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/cmd-args.log
---deleted file mode 100644
---index 11a5d8e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/cmd-args.log
---+++ /dev/null
---@@ -1 +0,0 @@
----main.py
---\ No newline at end of file
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/events.out.tfevents.1612754918.Titan.21528.0 b/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/events.out.tfevents.1612754918.Titan.21528.0
---deleted file mode 100644
---index 9596b1a..0000000
---Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/events.out.tfevents.1612754918.Titan.21528.0 and /dev/null differ
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/git-info.log
---deleted file mode 100644
---index e04e39e..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/git-info.log
---+++ /dev/null
---@@ -1,699 +0,0 @@
----commit hash: 939a671c8c117db6975316767ced5d95449e2b27
----diff --git a/README.md b/README.md
----index d52dba5..51ab30b 100644
------- a/README.md
----+++ b/README.md
----@@ -291,21 +291,22 @@ weighted avg         |  43.14   | 33.52 |  29.43  |  67486
----  0 layer not too much improvement, 1 layer pretty decent.
----  alpha 5 seems too high. to try full run 4 next.
---- layer 0 * 8 + layer 1 * 3
-----
-----  (label_id: 0)                                           0.00       0.00       0.00       5704
-----! (label_id: 1)                                          0.00       0.00       0.00        110
-----, (label_id: 2)                                          0.00       0.00       0.00      19711
------ (label_id: 3)                                          6.82      29.32      11.07       1702
-----. (label_id: 4)                                         37.30      83.82      51.62      18406
-----: (label_id: 5)                                          0.00       0.00       0.00        379
-----; (label_id: 6)                                          0.00       0.00       0.00        190
-----? (label_id: 7)                                          6.71       1.31       2.20       1446
-----— (label_id: 8)                                          0.00       0.00       0.00       1227
-----… (label_id: 9)                                          0.00       0.00       0.00         86
------------------------
-----micro avg                                               32.57      32.57      32.57      48961
-----macro avg                                                5.08      11.44       6.49      48961
-----weighted avg                                            14.46      32.57      19.86      48961
----+label                |  precision | recall |   f1   |    support
----+---|---|---|---|---
----+  (label_id: 0)        | 0.00     | 0.00    | 0.00  | 5704
----+! (label_id: 1)        | 0.00     | 0.00    | 0.00  | 110
----+, (label_id: 2)        | 0.00     | 0.00    | 0.00  | 19711
----+- (label_id: 3)        | 6.82     | 29.32   | 11.07 | 1702
----+. (label_id: 4)        | 37.30    | 83.82   | 51.62 | 18406
----+: (label_id: 5)        | 0.00     | 0.00    | 0.00  | 379
----+; (label_id: 6)        | 0.00     | 0.00    | 0.00  | 190
----+? (label_id: 7)        | 6.71     | 1.31    | 2.20  | 1446
----+— (label_id: 8)        | 0.00     | 0.00    | 0.00  | 1227
----+… (label_id: 9)        | 0.00     | 0.00    | 0.00  | 86
----+-------------------||||
----+micro avg              | 32.57    | 32.57   | 32.57 | 48961
----+macro avg              | 5.08     | 11.44   | 6.49  | 48961
----+weighted avg           | 14.46    | 32.57   | 19.86 | 48961
---- 
----  {'punct_f1': 6.104840278625488,
----  'punct_precision': 4.423948764801025,
----@@ -318,19 +319,96 @@ lr 0 : 0.008413951416451957
---- 1: 0.00031622776601683794 ** too high. to adjust the min to 1e-10?
---- 2: 0.00031622776601683794
---- 
-----label                                                precision    recall       f1           support
----- (label_id: 0)                                           0.00       0.00       0.00       7470
-----! (label_id: 1)                                          0.00       0.00       0.00        148
-----, (label_id: 2)                                          0.00       0.00       0.00      28513
------ (label_id: 3)                                          3.02     100.00       5.86       2074
-----. (label_id: 4)                                          0.00       0.00       0.00      25120
-----: (label_id: 5)                                          0.00       0.00       0.00        570
-----; (label_id: 6)                                          0.00       0.00       0.00        534
-----? (label_id: 7)                                          0.00       0.00       0.00       2085
-----— (label_id: 8)                                          0.00       0.00       0.00       2073
-----… (label_id: 9)                                          0.00       0.00       0.00        142
-----
----- 'punct_f1': 0.5858508944511414,
----+label                |  precision | recall |   f1   |    support
----+---|---|---|---|---
----+ (label_id: 0)       |   0.00  | 0.00     | 0.00  | 7470
----+! (label_id: 1)      |   0.00  | 0.00     | 0.00  | 148
----+, (label_id: 2)      |   0.00  | 0.00     | 0.00  | 28513
----+- (label_id: 3)      |   3.02  | 100.00   | 5.86  | 2074
----+. (label_id: 4)      |   0.00  | 0.00     | 0.00  | 25120
----+: (label_id: 5)      |   0.00  | 0.00     | 0.00  | 570
----+; (label_id: 6)      |   0.00  | 0.00     | 0.00  | 534
----+? (label_id: 7)      |   0.00  | 0.00     | 0.00  | 2085
----+— (label_id: 8)      |   0.00  | 0.00     | 0.00  | 2073
----+… (label_id: 9)      |   0.00  | 0.00     | 0.00  | 142
----+
----+ {'punct_f1': 0.5858508944511414,
----  'punct_precision': 0.30176490545272827,
----  'punct_recall': 10.0,
----- 'test_loss': 0.8140875697135925}
----\ No newline at end of file
----+ 'test_loss': 0.8140875697135925}
----+
----+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
----+
----+label                |  precision | recall |   f1   |    support
----+---|---|---|---|---
----+ (label_id: 0)      | 62.15  | 100.00   | 76.66   | 5154
----+! (label_id: 1)     | 0.00   | 0.00     | 0.00    | 108
----+, (label_id: 2)     | 0.00   | 0.00     | 0.00    | 18022
----+- (label_id: 3)     | 0.00   | 0.00     | 0.00    | 1557
----+. (label_id: 4)     | 41.74  | 94.01    | 57.81   | 15164
----+: (label_id: 5)     | 0.00   | 0.00     | 0.00    | 319
----+; (label_id: 6)     | 0.00   | 0.00     | 0.00    | 88
----+? (label_id: 7)     | 0.00   | 0.00     | 0.00    | 1217
----+ (label_id: 8)      | 0.00   | 0.00     | 0.00    | 752
----+… (label_id: 9)     | 0.00   | 0.00     | 0.00    | 67
----+-------------------||||
----+micro avg           | 45.72 | 45.72 | 45.72 | 42448
----+macro avg           | 10.39 | 19.40 | 13.45 | 42448
----+weighted avg        | 22.46 | 45.72 | 29.96 | 42448
----+
----+{ 'punct_f1': 13.446383476257324,
----+ 'punct_precision': 10.388500213623047,
----+ 'punct_recall': 19.400554656982422,
----+ 'test_loss': 0.44148480892181396}
----+
----+
----+ ## Log for 8/2/2021
----+
----+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
----+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
----+
----+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
----+End frozen 
----+micro avg      |  41.42 |   41.42  |    41.42   |   33406
----+macro avg      |  11.01 |   13.54  |    11.07   |   33406
----+weighted avg   |  34.88 |   41.42  |    34.20   |   33406
----+
----+1st layer best lr 1e-10, set to 0.007943282347242822
----+micro avg            |       36.65  |    36.65  |    36.65  |    33463
----+macro avg            |       10.71  |     9.91  |     8.26  |    33463
----+weighted avg         |       34.32  |    36.65  |    31.08  |    33463
----+
----+2nd layer best lr 1e-10, set to 0.007943282347242822
----+micro avg        |   35.72  |    35.72  |    35.72  |    42448
----+macro avg        |    3.57  |    10.00  |     5.26  |    42448
----+weighted avg     |   12.76  |    35.72  |    18.81  |    42448
----+
----+{'punct_f1': 5.264181137084961,
----+ 'punct_precision': 3.572371006011963,
----+ 'punct_recall': 10.0,
----+ 'test_loss': 18.49854850769043}
----+
----+
----+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
----+alpha from 3->4 seems to reduce convergence rate.
----+
----+micro avg        |   50.98  | 50.98  |  50.98    |  33463
----+macro avg        |   25.99  | 25.38  |  23.38    |  33463
----+weighted avg     |   50.31  | 50.98  |  48.27    |  33463
----+
----+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
----+micro avg     |  58.55  |  58.55 |  58.55 | 39340
----+macro avg     |  30.02  |  29.74 |  29.52 | 39340
----+weighted avg  |  57.91  |  58.55 |  57.51 | 39340
----+
----+still increasing?!
----+{'punct_f1': 29.523975372314453,
----+ 'punct_precision': 30.015613555908203,
----+ 'punct_recall': 29.738296508789062,
----+ 'test_loss': 0.3690211772918701}
----+
----+
----+### Implemented mlp 2 layer before classifier, 
----+
----+adamw mean 2 layer domain, dice, alpha 4 10 batch, accgrad 4 2021-02-08_11-07-07/
----+frozen lr 0.0025118864315095825 best: 0.01,
----diff --git a/experiment/config.yaml b/experiment/config.yaml
----index fe58670..b137ae8 100644
------- a/experiment/config.yaml
----+++ b/experiment/config.yaml
----@@ -1,49 +1,49 @@
---- seed: 42
---- trainer:
-----    # gpus: 1 # the number of gpus, 0 for CPU
-----    # num_nodes: 1
-----    # max_epochs: 2
-----    # max_steps: null # precedence over max_epochs
-----    # accumulate_grad_batches: 4 # accumulates grads every k batches
-----    # gradient_clip_val: 0
-----    # amp_level: O1 # O1/O2 for mixed precision
-----    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
-----    # checkpoint_callback: false  # Provided by exp_manager
-----    # logger: false #false  # Provided by exp_manager
-----    # log_every_n_steps: 1  # Interval of logging.
-----    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    # resume_from_checkpoint: null
-----
-----    gpus: 0 # the number of gpus, 0 for CPU
----+    gpus: 1 # the number of gpus, 0 for CPU
----     num_nodes: 1
-----    max_epochs: 8
----+    max_epochs: 10
----     max_steps: null # precedence over max_epochs
-----    accumulate_grad_batches: 1 # accumulates grads every k batches
----+    accumulate_grad_batches: 4 # accumulates grads every k batches
----     gradient_clip_val: 0
-----    amp_level: O0 # O1/O2 for mixed precision
-----    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-----    # accelerator: ddp
----+    amp_level: O1 # O1/O2 for mixed precision
----+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    accelerator: ddp
----     checkpoint_callback: false  # Provided by exp_manager
----     logger: false #false  # Provided by exp_manager
----     log_every_n_steps: 1  # Interval of logging.
----     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-----    reload_dataloaders_every_epoch: true
----     resume_from_checkpoint: null
---- 
----+    # gpus: 0 # the number of gpus, 0 for CPU
----+    # num_nodes: 1
----+    # max_epochs: 8
----+    # max_steps: null # precedence over max_epochs
----+    # accumulate_grad_batches: 1 # accumulates grads every k batches
----+    # gradient_clip_val: 0
----+    # amp_level: O0 # O1/O2 for mixed precision
----+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
----+    # # accelerator: ddp
----+    # checkpoint_callback: false  # Provided by exp_manager
----+    # logger: false #false  # Provided by exp_manager
----+    # log_every_n_steps: 1  # Interval of logging.
----+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
----+    # reload_dataloaders_every_epoch: true
----+    # resume_from_checkpoint: null
----+
---- exp_manager:
-----    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
----     name: Punctuation_with_Domain_discriminator  # The name of your model
----     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
----     create_checkpoint_callback: true 
-----base_path: /home/nxingyu2/data # /root/data # 
-----tmp_path: /home/nxingyu2/data/tmp # /tmp # 
----+base_path: /home/nxingyu/data # /root/data # 
----+tmp_path: /home/nxingyu/data/tmp # /tmp # 
---- 
---- model:
----     nemo_path: null
-----    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
----     unfrozen: 0
-----    maximum_unfrozen: 2
----+    maximum_unfrozen: 1
----     unfreeze_step: 1
----     # unfreeze_every: 3
----     punct_label_ids:
----@@ -58,10 +58,10 @@ model:
----         - "—"
----         - "…"
---- 
-----    punct_class_weights: true
----+    punct_class_weights: false
----     
----     dataset:
-----        data_dir: /home/nxingyu2/data # /root/data # 
----+        data_dir: /home/nxingyu/data # /root/data # 
----         labelled:
----             - ${base_path}/ted_talks_processed #
----         unlabelled:
----@@ -106,6 +106,12 @@ model:
----         config: null
----         # unfrozen_layers: 1
---- 
----+    mlp:
----+        num_fc_layers: 1
----+        fc_dropout: 0.1
----+        log_softmax: false
----+        activation: 'relu'
----+        
----     punct_head:
----         punct_num_fc_layers: 1
----         fc_dropout: 0.1
----@@ -122,6 +128,8 @@ model:
----         use_transformer_init: true
----         loss: 'cel'
----         gamma: 0.1 # coefficient of gradient reversal
----+        pooling: 'mean'
----+        idx_conditioned_on: 0
----     
----     dice_loss:
----         epsilon: 0.01
----@@ -132,7 +140,7 @@ model:
----         gamma: 5
---- 
----     optim:
-----        name: novograd
----+        name: adamw
----         lr: 1e-3
----         weight_decay: 0.00
----         sched:
----diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
----index d4ff927..f9927ac 100644
------- a/experiment/core/layers/sequence_classifier.py
----+++ b/experiment/core/layers/sequence_classifier.py
----@@ -1,3 +1,4 @@
----+import torch
---- from torch import nn
---- from core.layers.multi_layer_perceptron import MultiLayerPerceptron
---- from core.utils import transformer_weights_init
----@@ -20,7 +21,8 @@ class SequenceClassifier(nn.Module):
----         log_softmax: bool = True,
----         dropout: float = 0.0,
----         use_transformer_init: bool = True,
-----        idx_conditioned_on: int = 0,
----+        pooling: str = 'mean', # mean, max, mean_max, token
----+        idx_conditioned_on: int = None,
----     ):
----         """
----         Initializes the SequenceClassifier module.
----@@ -37,8 +39,9 @@ class SequenceClassifier(nn.Module):
----         super().__init__()
----         self.log_softmax = log_softmax
----         self._idx_conditioned_on = idx_conditioned_on
----+        self.pooling = pooling
----         self.mlp = MultiLayerPerceptron(
-----            hidden_size=hidden_size,
----+            hidden_size=(hidden_size*2 if pooling=='mean_max' else hidden_size),
----             num_classes=num_classes,
----             num_layers=num_layers,
----             activation=activation,
----@@ -48,7 +51,23 @@ class SequenceClassifier(nn.Module):
----         if use_transformer_init:
----             self.apply(lambda module: transformer_weights_init(module, xavier=False))
---- 
-----    def forward(self, hidden_states):
----+    def forward(self, hidden_states, subtoken_mask=None):
----         hidden_states = self.dropout(hidden_states)
-----        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
----+        if self.pooling=='token':
----+            pooled = hidden_states[:, self._idx_conditioned_on]
----+        else:
----+            if subtoken_mask==None:
----+                ct=hidden_states.shape[1] # Seq len
----+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
----+            else:
----+                ct = torch.sum(subtoken_mask,axis=1).unsqueeze(1)
----+            pooled_sum = torch.sum(hidden_states,axis=1)            
----+            if self.pooling=='mean' or self.pooling == 'mean_max':
----+                pooled_mean = torch.div(pooled_sum,ct)
----+            if self.pooling=='max' or self.pooling=='mean_max':
----+                pooled_max = torch.max(hidden_states,axis=1)[0]
----+            pooled=pooled_mean if self.pooling=='mean' else \
----+                pooled_max if self.pooling=='max' else \
----+                    torch.cat([pooled_mean,pooled_max],axis=-1)
----+        logits = self.mlp(pooled)
----         return logits
----diff --git a/experiment/info.log b/experiment/info.log
----index 2471fe9..e69de29 100644
------- a/experiment/info.log
----+++ b/experiment/info.log
----@@ -1,300 +0,0 @@
-----[INFO] - GPU available: True, used: False
-----[INFO] - TPU available: None, using: 0 TPU cores
-----[INFO] - shuffling train set
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 0.001
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
-----will be used during training (effective maximum steps = 80) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 80
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        184
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.23       0.34       0.53        594
------ (label_id: 3)                                          3.06      25.42       5.46         59
-----. (label_id: 4)                                         47.22      12.98      20.36        524
-----: (label_id: 5)                                          0.00       0.00       0.00         18
-----; (label_id: 6)                                          0.00       0.00       0.00         13
-----? (label_id: 7)                                          8.45       6.32       7.23         95
-----— (label_id: 8)                                          0.00       0.00       0.00         12
-----… (label_id: 9)                                          0.00       0.00       0.00          0
------------------------
-----micro avg                                                6.05       6.05       6.05       1503
-----macro avg                                                6.66       5.01       3.73       1503
-----weighted avg                                            17.61       6.05       7.98       1503
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00         92
------------------------
-----micro avg                                              100.00     100.00     100.00         92
-----macro avg                                              100.00     100.00     100.00         92
-----weighted avg                                           100.00     100.00     100.00         92
-----
-----[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
-----[INFO] - Optimizer config = Novograd (
-----Parameter Group 0
-----    amsgrad: False
-----    betas: (0.95, 0.98)
-----    eps: 1e-08
-----    grad_averaging: False
-----    lr: 1.5848931924611143e-08
-----    weight_decay: 0.0
-----)
-----[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
-----will be used during training (effective maximum steps = 3192) - 
-----Parameters : 
-----(warmup_steps: null
-----warmup_ratio: 0.1
-----last_epoch: -1
-----max_steps: 3192
-----)
-----[INFO] - 
-----  | Name                | Type                 | Params
------------------------------------------------------------------
-----0 | transformer         | ElectraModel         | 108 M 
-----1 | punct_classifier    | TokenClassifier      | 7.7 K 
-----2 | domain_classifier   | SequenceClassifier   | 769   
-----3 | punctuation_loss    | FocalDiceLoss        | 0     
-----4 | domain_loss         | CrossEntropyLoss     | 0     
-----5 | agg_loss            | AggregatorLoss       | 0     
-----6 | punct_class_report  | ClassificationReport | 0     
-----7 | domain_class_report | ClassificationReport | 0     
------------------------------------------------------------------
-----8.5 K     Trainable params
-----108 M     Non-trainable params
-----108 M     Total params
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.00       0.00       0.00        202
-----! (label_id: 1)                                          0.00       0.00       0.00          4
-----, (label_id: 2)                                          1.62       0.45       0.70        669
------ (label_id: 3)                                          3.48      27.27       6.17         66
-----. (label_id: 4)                                         45.06      13.01      20.19        561
-----: (label_id: 5)                                          1.52       6.67       2.47         15
-----; (label_id: 6)                                          0.00       0.00       0.00         15
-----? (label_id: 7)                                          8.70       7.32       7.95         82
-----— (label_id: 8)                                          0.00       0.00       0.00         13
-----… (label_id: 9)                                          0.00       0.00       0.00          1
------------------------
-----micro avg                                                6.20       6.20       6.20       1628
-----macro avg                                                6.04       5.47       3.75       1628
-----weighted avg                                            16.79       6.20       7.92       1628
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00        101
------------------------
-----micro avg                                              100.00     100.00     100.00        101
-----macro avg                                              100.00     100.00     100.00        101
-----weighted avg                                           100.00     100.00     100.00        101
-----
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.34       0.73       0.46       4402
-----! (label_id: 1)                                          0.42      13.95       0.82        129
-----, (label_id: 2)                                          2.53       0.64       1.03      15243
------ (label_id: 3)                                          2.45      21.03       4.38       1322
-----. (label_id: 4)                                         44.00      11.40      18.11      12542
-----: (label_id: 5)                                          0.43       1.41       0.65        354
-----; (label_id: 6)                                          0.00       0.00       0.00        163
-----? (label_id: 7)                                          4.16       6.27       5.00       1117
-----— (label_id: 8)                                          3.00       0.61       1.02        488
-----… (label_id: 9)                                          0.97       6.17       1.68         81
------------------------
-----micro avg                                                5.41       5.41       5.41      35841
-----macro avg                                                5.83       6.22       3.32      35841
-----weighted avg                                            16.78       5.41       7.18      35841
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2201
------------------------
-----micro avg                                              100.00     100.00     100.00       2201
-----macro avg                                              100.00     100.00     100.00       2201
-----weighted avg                                           100.00     100.00     100.00       2201
-----
-----[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.20       0.43       0.27       4226
-----! (label_id: 1)                                          0.44      14.17       0.86        127
-----, (label_id: 2)                                          1.93       0.49       0.78      14611
------ (label_id: 3)                                          2.23      19.56       4.01       1237
-----. (label_id: 4)                                         43.37      11.25      17.86      11977
-----: (label_id: 5)                                          0.68       2.34       1.05        342
-----; (label_id: 6)                                          0.00       0.00       0.00        129
-----? (label_id: 7)                                          5.16       7.47       6.10       1058
-----— (label_id: 8)                                          2.15       0.49       0.80        409
-----… (label_id: 9)                                          0.69       4.23       1.19         71
------------------------
-----micro avg                                                5.23       5.23       5.23      34187
-----macro avg                                                5.68       6.04       3.29      34187
-----weighted avg                                            16.32       5.23       6.98      34187
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2113
------------------------
-----micro avg                                              100.00     100.00     100.00       2113
-----macro avg                                              100.00     100.00     100.00       2113
-----weighted avg                                           100.00     100.00     100.00       2113
-----
-----[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.61       0.39       4228
-----! (label_id: 1)                                          0.30       8.28       0.58        145
-----, (label_id: 2)                                          2.27       0.58       0.92      14495
------ (label_id: 3)                                          2.64      21.78       4.70       1327
-----. (label_id: 4)                                         44.87      11.66      18.51      12193
-----: (label_id: 5)                                          0.60       1.93       0.91        362
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.19       6.40       5.07       1078
-----— (label_id: 8)                                          1.16       0.22       0.37        459
-----… (label_id: 9)                                          0.85       4.17       1.41         96
------------------------
-----micro avg                                                5.54       5.54       5.54      34547
-----macro avg                                                5.72       5.56       3.29      34547
-----weighted avg                                            17.08       5.54       7.33      34547
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2114
------------------------
-----micro avg                                              100.00     100.00     100.00       2114
-----macro avg                                              100.00     100.00     100.00       2114
-----weighted avg                                           100.00     100.00     100.00       2114
-----
-----[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.29       0.63       0.40       4444
-----! (label_id: 1)                                          0.38      10.67       0.74        150
-----, (label_id: 2)                                          2.32       0.59       0.94      15290
------ (label_id: 3)                                          2.34      20.28       4.19       1292
-----. (label_id: 4)                                         43.85      11.68      18.44      12599
-----: (label_id: 5)                                          0.41       1.28       0.62        392
-----; (label_id: 6)                                          0.00       0.00       0.00        164
-----? (label_id: 7)                                          4.24       6.30       5.07       1111
-----— (label_id: 8)                                          0.00       0.00       0.00        456
-----… (label_id: 9)                                          0.38       2.41       0.65         83
------------------------
-----micro avg                                                5.40       5.40       5.40      35981
-----macro avg                                                5.42       5.38       3.11      35981
-----weighted avg                                            16.59       5.40       7.22      35981
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2222
------------------------
-----micro avg                                              100.00     100.00     100.00       2222
-----macro avg                                              100.00     100.00     100.00       2222
-----weighted avg                                           100.00     100.00     100.00       2222
-----
-----[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.35       0.73       0.48       3844
-----! (label_id: 1)                                          0.54      14.62       1.04        130
-----, (label_id: 2)                                          2.32       0.59       0.94      13056
------ (label_id: 3)                                          2.67      22.28       4.77       1194
-----. (label_id: 4)                                         44.45      11.95      18.84      10791
-----: (label_id: 5)                                          0.84       3.21       1.33        280
-----; (label_id: 6)                                          0.00       0.00       0.00        140
-----? (label_id: 7)                                          4.17       6.56       5.10        914
-----— (label_id: 8)                                          0.00       0.00       0.00        401
-----… (label_id: 9)                                          0.48       2.63       0.81         76
------------------------
-----micro avg                                                5.68       5.68       5.68      30826
-----macro avg                                                5.58       6.26       3.33      30826
-----weighted avg                                            16.82       5.68       7.41      30826
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1922
------------------------
-----micro avg                                              100.00     100.00     100.00       1922
-----macro avg                                              100.00     100.00     100.00       1922
-----weighted avg                                           100.00     100.00     100.00       1922
-----
-----[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.28       0.60       0.39       3970
-----! (label_id: 1)                                          0.35      10.66       0.68        122
-----, (label_id: 2)                                          2.09       0.53       0.85      13469
------ (label_id: 3)                                          2.29      19.32       4.10       1201
-----. (label_id: 4)                                         43.43      11.24      17.86      11227
-----: (label_id: 5)                                          0.63       2.30       0.99        304
-----; (label_id: 6)                                          0.00       0.00       0.00        141
-----? (label_id: 7)                                          4.52       6.86       5.45       1006
-----— (label_id: 8)                                          1.15       0.23       0.38        444
-----… (label_id: 9)                                          0.45       2.67       0.78         75
------------------------
-----micro avg                                                5.26       5.26       5.26      31959
-----macro avg                                                5.52       5.44       3.15      31959
-----weighted avg                                            16.42       5.26       7.02      31959
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       1985
------------------------
-----micro avg                                              100.00     100.00     100.00       1985
-----macro avg                                              100.00     100.00     100.00       1985
-----weighted avg                                           100.00     100.00     100.00       1985
-----
-----[INFO] - Epoch 5, step 2394: val_loss was not in top 3
-----[INFO] - Punctuation report: 
-----label                                                precision    recall       f1           support   
----- (label_id: 0)                                           0.23       0.48       0.31       4126
-----! (label_id: 1)                                          0.29       9.40       0.56        117
-----, (label_id: 2)                                          1.91       0.49       0.77      14019
------ (label_id: 3)                                          2.52      22.59       4.53       1164
-----. (label_id: 4)                                         44.15      11.65      18.44      11789
-----: (label_id: 5)                                          0.72       2.41       1.11        332
-----; (label_id: 6)                                          0.56       0.61       0.58        165
-----? (label_id: 7)                                          3.89       6.53       4.88        980
-----— (label_id: 8)                                          2.30       0.47       0.77        430
-----… (label_id: 9)                                          1.18       8.33       2.07         60
------------------------
-----micro avg                                                5.47       5.47       5.47      33182
-----macro avg                                                5.77       6.30       3.40      33182
-----weighted avg                                            16.77       5.47       7.25      33182
-----
-----[INFO] - Domain report: 
-----label                                                precision    recall       f1           support   
-----0 (label_id: 0)                                        100.00     100.00     100.00       2063
------------------------
-----micro avg                                              100.00     100.00     100.00       2063
-----macro avg                                              100.00     100.00     100.00       2063
-----weighted avg                                           100.00     100.00     100.00       2063
-----
-----[INFO] - Epoch 6, step 2793: val_loss was not in top 3
----diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
----index fa37b4c..43fc93d 100644
------- a/experiment/models/punctuation_domain_model.py
----+++ b/experiment/models/punctuation_domain_model.py
----@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         else:
----             self.hparams.model.punct_class_weights=None
---- 
----+        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
----+        self.mlp = MultiLayerPerceptron(
----+            self.transformer.config.hidden_size,
----+            self.transformer.config.hidden_size,
----+            num_layers=self.hparams.model.mlp.num_fc_layers, 
----+            activation=self.hparams.model.mlp.activation, 
----+            log_softmax=self.hparams.model.mlp.log_softmax
----+        )
----+
----         self.punct_classifier = TokenClassifier(
----             hidden_size=self.transformer.config.hidden_size,
----             num_classes=len(self.labels_to_ids),
----@@ -80,6 +89,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----             log_softmax=self.hparams.model.domain_head.log_softmax,
----             dropout=self.hparams.model.domain_head.fc_dropout,
----             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
----+            pooling=self.hparams.model.domain_head.pooling,
----+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
----         )
---- 
----         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
----@@ -119,14 +130,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
----         self.freeze()
---- 
-----    def forward(self, input_ids, attention_mask, domain_ids=None):
----+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
----         hidden_states = self.transformer(
----             input_ids=input_ids, attention_mask=attention_mask
----         )[0]
----+        hidden_states = self.dropout(hidden_states)
----+        hidden_states = self.mlp(hidden_states)
----         punct_logits = self.punct_classifier(hidden_states=hidden_states)
----         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
----         domain_logits = self.domain_classifier(
-----            hidden_states=reverse_grad_hidden_states)
----+            hidden_states=reverse_grad_hidden_states,
----+            subtoken_mask=subtoken_mask)
----         return punct_logits, domain_logits
---- 
----     def _make_step(self, batch):
----@@ -136,7 +150,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
----         punct_labels = batch['labels']
----         domain_labels = batch['domain']
----         punct_logits, domain_logits = self(
-----            input_ids=input_ids, attention_mask=attention_mask
----+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
----         )
----         punctuation_loss = self.punctuation_loss(
----             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/hparams.yaml
---deleted file mode 100644
---index 8fe49f4..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/hparams.yaml
---+++ /dev/null
---@@ -1,114 +0,0 @@
----seed: 42
----trainer:
----  gpus: 1
----  num_nodes: 1
----  max_epochs: 10
----  max_steps: null
----  accumulate_grad_batches: 4
----  gradient_clip_val: 0
----  amp_level: O1
----  precision: 16
----  accelerator: ddp
----  checkpoint_callback: false
----  logger: false
----  log_every_n_steps: 1
----  val_check_interval: 1.0
----  resume_from_checkpoint: null
----exp_manager:
----  exp_dir: /home/nxingyu/project/
----  name: Punctuation_with_Domain_discriminator
----  create_tensorboard_logger: true
----  create_checkpoint_callback: true
----base_path: /home/nxingyu/data
----tmp_path: /home/nxingyu/data/tmp
----model:
----  nemo_path: null
----  transformer_path: google/electra-small-discriminator
----  unfrozen: 0
----  maximum_unfrozen: 1
----  unfreeze_step: 1
----  punct_label_ids:
----  - ''
----  - '!'
----  - ','
----  - '-'
----  - .
----  - ':'
----  - ;
----  - '?'
----  - —
----  - …
----  punct_class_weights: false
----  dataset:
----    data_dir: /home/nxingyu/data
----    labelled:
----    - /home/nxingyu/data/ted_talks_processed
----    unlabelled: null
----    max_seq_length: 128
----    pad_label: ''
----    ignore_extra_tokens: false
----    ignore_start_end: false
----    use_cache: false
----    num_workers: 0
----    pin_memory: true
----    drop_last: false
----    num_labels: 10
----    num_domains: 1
----    test_unlabelled: true
----    train_ds:
----      shuffle: true
----      num_samples: -1
----      batch_size: 8
----    validation_ds:
----      shuffle: true
----      num_samples: -1
----      batch_size: 8
----  tokenizer:
----    tokenizer_name: google/electra-small-discriminator
----    vocab_file: null
----    tokenizer_model: null
----    special_tokens: null
----  language_model:
----    pretrained_model_name: google/electra-small-discriminator
----    lm_checkpoint: null
----    config_file: null
----    config: null
----  mlp:
----    num_fc_layers: 1
----    fc_dropout: 0.1
----    log_softmax: false
----    activation: relu
----  punct_head:
----    punct_num_fc_layers: 1
----    fc_dropout: 0.1
----    activation: relu
----    log_softmax: false
----    use_transformer_init: true
----    loss: dice
----  domain_head:
----    domain_num_fc_layers: 1
----    fc_dropout: 0.1
----    activation: relu
----    log_softmax: false
----    use_transformer_init: true
----    loss: cel
----    gamma: 0.1
----    pooling: mean
----    idx_conditioned_on: 0
----  dice_loss:
----    epsilon: 0.01
----    alpha: 3
----    macro_average: true
----  focal_loss:
----    gamma: 5
----  optim:
----    name: adamw
----    lr: 0.001
----    weight_decay: 0.0
----    sched:
----      name: WarmupAnnealing
----      warmup_steps: null
----      warmup_ratio: 0.1
----      last_epoch: -1
----      monitor: val_loss
----      reduce_on_plateau: false
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/lightning_logs.txt
---deleted file mode 100644
---index 40113aa..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/lightning_logs.txt
---+++ /dev/null
---@@ -1,43 +0,0 @@
----Global seed set to 42
----GPU available: True, used: True
----TPU available: None, using: 0 TPU cores
----LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
----Using native 16bit precision.
----Global seed set to 42
----initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | dropout             | Dropout              | 0     
----2 | mlp                 | MultiLayerPerceptron | 65.8 K
----3 | punct_classifier    | TokenClassifier      | 2.6 K 
----4 | domain_classifier   | SequenceClassifier   | 257   
----5 | punctuation_loss    | FocalDiceLoss        | 0     
----6 | domain_loss         | CrossEntropyLoss     | 0     
----7 | agg_loss            | AggregatorLoss       | 0     
----8 | punct_class_report  | ClassificationReport | 0     
----9 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----101 K     Trainable params
----13.4 M    Non-trainable params
----13.6 M    Total params
----Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/lr_find_temp_model.ckpt
----Global seed set to 42
----
----  | Name                | Type                 | Params
-----------------------------------------------------------------
----0 | transformer         | ElectraModel         | 13.5 M
----1 | dropout             | Dropout              | 0     
----2 | mlp                 | MultiLayerPerceptron | 65.8 K
----3 | punct_classifier    | TokenClassifier      | 2.6 K 
----4 | domain_classifier   | SequenceClassifier   | 257   
----5 | punctuation_loss    | FocalDiceLoss        | 0     
----6 | domain_loss         | CrossEntropyLoss     | 0     
----7 | agg_loss            | AggregatorLoss       | 0     
----8 | punct_class_report  | ClassificationReport | 0     
----9 | domain_class_report | ClassificationReport | 0     
-----------------------------------------------------------------
----101 K     Trainable params
----13.4 M    Non-trainable params
----13.6 M    Total params
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/nemo_error_log.txt
---deleted file mode 100644
---index e026dd1..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/nemo_error_log.txt
---+++ /dev/null
---@@ -1,22 +0,0 @@
----[NeMo W 2021-02-08 11:24:45 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-08 11:24:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:24:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:24:58 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:28:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:28:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:28:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:28:44 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
----      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
----    
---diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/nemo_log_globalrank-0_localrank-0.txt
---deleted file mode 100644
---index b012129..0000000
------ a/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45/nemo_log_globalrank-0_localrank-0.txt
---+++ /dev/null
---@@ -1,24 +0,0 @@
----[NeMo I 2021-02-08 11:24:45 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-24-45
----[NeMo I 2021-02-08 11:24:45 exp_manager:519] TensorboardLogger has been set up
----[NeMo W 2021-02-08 11:24:45 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
----[NeMo W 2021-02-08 11:24:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:24:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:24:58 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:28:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:28:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:28:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
----      warnings.warn(*args, **kwargs)
----    
----[NeMo W 2021-02-08 11:28:44 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
----      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
----    
---diff --git a/README.md b/README.md
---index 51ab30b..beca3ef 100644
------ a/README.md
---+++ b/README.md
---@@ -167,17 +167,18 @@ label            | precision    | recall   | f1     | support
--- … (label_id: 9)  | 0.00         | 0.00     | 0.00   | 66
--- 
--- ###Focal DistilBERT gamma 3 0 unfrozen ted
----label                                                precision    recall       f1           support   
---- (label_id: 0)                                         100.00      51.29      67.80       4118
----! (label_id: 1)                                          0.00       0.00       0.00         91
----, (label_id: 2)                                          0.00       0.00       0.00      13953
----- (label_id: 3)                                         94.27      46.49      62.27       1310
----. (label_id: 4)                                         39.51      99.94      56.63      12142
----: (label_id: 5)                                          0.00       0.00       0.00        254
----; (label_id: 6)                                          0.00       0.00       0.00         79
----? (label_id: 7)                                          0.00       0.00       0.00        905
----— (label_id: 8)                                          0.00       0.00       0.00        566
----… (label_id: 9)                                          0.00       0.00       0.00         52
---+label               | precision   | recall | f1    | support   
---+---|---|---|---|---
---+ (label_id: 0)      | 100.00      | 51.29  | 67.80 | 4118
---+! (label_id: 1)     | 0.00        | 0.00   | 0.00  | 91
---+, (label_id: 2)     | 0.00        | 0.00   | 0.00  | 13953
---+\- (label_id: 3)    | 94.27       | 46.49  | 62.27 | 1310
---+. (label_id: 4)     | 39.51       | 99.94  | 56.63 | 12142
---+: (label_id: 5)     | 0.00        | 0.00   | 0.00  | 254
---+; (label_id: 6)     | 0.00        | 0.00   | 0.00  | 79
---+? (label_id: 7)     | 0.00        | 0.00   | 0.00  | 905
---+— (label_id: 8)     | 0.00        | 0.00   | 0.00  | 566
---+… (label_id: 9)     | 0.00        | 0.00   | 0.00  | 52
--- 
--- ## Observations
--- - CRF tends to perform better on higher proportion classes like blank, comma and period without class weights.
---@@ -195,7 +196,7 @@ label                 |   precision  |  recall |    f1    |      support
---  (label_id: 0)        |      79.50   |   29.94 |   43.50  |    5026
--- ! (label_id: 1)       |       6.84   |   20.59 |   10.27  |     102
--- , (label_id: 2)       |      50.70   |   60.09 |   55.00  |   17571
----- (label_id: 3)       |      64.45   |   82.11 |   72.22  |    1526
---+\- (label_id: 3)       |      64.45   |   82.11 |   72.22  |    1526
--- . (label_id: 4)       |      57.40   |   49.43 |   53.12  |   14767
--- : (label_id: 5)       |      17.86   |   31.83 |   22.89  |     289
--- ; (label_id: 6)       |       1.50   |    5.88 |    2.39  |      85
---@@ -220,7 +221,7 @@ label                 |   precision  |  recall |    f1    |      support
---  (label_id: 0)             |     0.00  |  0.00 |   0.00  |  5026
--- ! (label_id: 1)            |     0.00  |  0.00 |   0.00  |   102
--- , (label_id: 2)            |    42.79  | 47.54 |  45.04  | 17571
----- (label_id: 3)            |    73.63  | 80.87 |  77.08  |  1526
---+\- (label_id: 3)            |    73.63  | 80.87 |  77.08  |  1526
--- . (label_id: 4)            |    47.36  | 55.16 |  50.96  | 14767
--- : (label_id: 5)            |    10.88  | 27.68 |  15.62  |   289
--- ; (label_id: 6)            |     0.00  |  0.00 |   0.00  |    85
---@@ -245,7 +246,7 @@ label                  |  precision | recall |   f1   |     support
---  (label_id: 0)         |     59.35  |  52.35 |  55.63 |   7314
--- ! (label_id: 1)        |      0.00  |   0.00 |   0.00 |    154
--- , (label_id: 2)        |     44.15  |  82.80 |  57.59 |  28180
----- (label_id: 3)        |      3.91  |   2.02 |   2.66 |   1933
---+\- (label_id: 3)        |      3.91  |   2.02 |   2.66 |   1933
--- . (label_id: 4)        |     39.91  |  11.64 |  18.02 |  24624
--- : (label_id: 5)        |      0.00  |   0.00 |   0.00 |    522
--- ; (label_id: 6)        |      0.00  |   0.00 |   0.00 |    485
---@@ -268,7 +269,7 @@ label                |  precision | recall |   f1   |    support
---  (label_id: 0)       |  62.32   | 99.78 |  76.72  |   7314
--- ! (label_id: 1)      |   0.00   |  0.00 |   0.00  |    154
--- , (label_id: 2)      |  49.81   |  4.72 |   8.62  |  28180
----- (label_id: 3)      |   5.91   | 28.35 |   9.78  |   1933
---+\- (label_id: 3)      |   5.91   | 28.35 |   9.78  |   1933
--- . (label_id: 4)      |  41.80   | 52.40 |  46.50  |  24624
--- : (label_id: 5)      |   0.94   |  4.02 |   1.53  |    522
--- ; (label_id: 6)      |   0.00   |  0.00 |   0.00  |    485
---@@ -296,7 +297,7 @@ label                |  precision | recall |   f1   |    support
---   (label_id: 0)        | 0.00     | 0.00    | 0.00  | 5704
--- ! (label_id: 1)        | 0.00     | 0.00    | 0.00  | 110
--- , (label_id: 2)        | 0.00     | 0.00    | 0.00  | 19711
----- (label_id: 3)        | 6.82     | 29.32   | 11.07 | 1702
---+\- (label_id: 3)        | 6.82     | 29.32   | 11.07 | 1702
--- . (label_id: 4)        | 37.30    | 83.82   | 51.62 | 18406
--- : (label_id: 5)        | 0.00     | 0.00    | 0.00  | 379
--- ; (label_id: 6)        | 0.00     | 0.00    | 0.00  | 190
---@@ -324,7 +325,7 @@ label                |  precision | recall |   f1   |    support
---  (label_id: 0)       |   0.00  | 0.00     | 0.00  | 7470
--- ! (label_id: 1)      |   0.00  | 0.00     | 0.00  | 148
--- , (label_id: 2)      |   0.00  | 0.00     | 0.00  | 28513
----- (label_id: 3)      |   3.02  | 100.00   | 5.86  | 2074
---+\- (label_id: 3)      |   3.02  | 100.00   | 5.86  | 2074
--- . (label_id: 4)      |   0.00  | 0.00     | 0.00  | 25120
--- : (label_id: 5)      |   0.00  | 0.00     | 0.00  | 570
--- ; (label_id: 6)      |   0.00  | 0.00     | 0.00  | 534
---@@ -344,7 +345,7 @@ label                |  precision | recall |   f1   |    support
---  (label_id: 0)      | 62.15  | 100.00   | 76.66   | 5154
--- ! (label_id: 1)     | 0.00   | 0.00     | 0.00    | 108
--- , (label_id: 2)     | 0.00   | 0.00     | 0.00    | 18022
----- (label_id: 3)     | 0.00   | 0.00     | 0.00    | 1557
---+\- (label_id: 3)     | 0.00   | 0.00     | 0.00    | 1557
--- . (label_id: 4)     | 41.74  | 94.01    | 57.81   | 15164
--- : (label_id: 5)     | 0.00   | 0.00     | 0.00    | 319
--- ; (label_id: 6)     | 0.00   | 0.00     | 0.00    | 88
---@@ -412,3 +413,39 @@ still increasing?!
--- 
--- adamw mean 2 layer domain, dice, alpha 4 10 batch, accgrad 4 2021-02-08_11-07-07/
--- frozen lr 0.0025118864315095825 best: 0.01,
---+
---+unfreeze 0.07943282347242822 best lr 1e-10
---+
---+ep 6 
---+micro avg    | 64.21 | 64.21 | 64.21 | 33835
---+macro avg    | 36.55 | 37.56 | 36.71 | 33835
---+weighted avg | 63.77 | 64.21 | 63.91 | 33835
---+
---+{'punct_f1': 38.96394729614258,
---+ 'punct_precision': 38.412635803222656,
---+ 'punct_recall': 40.2258415222168,
---+ 'test_loss': 0.2748030722141266}
---+
---+
---+ ### CEL
---+
---+  (label_id: 0)                                         100.00     100.00     100.00       5564
---+! (label_id: 1)                                          0.00       0.00       0.00        148
---+, (label_id: 2)                                         69.27      76.77      72.83      19606
---+\- (label_id: 3)                                         87.16      75.17      80.72       1788
---+. (label_id: 4)                                         65.71      68.86      67.25      16090
---+: (label_id: 5)                                          0.00       0.00       0.00        368
---+; (label_id: 6)                                          0.00       0.00       0.00        202
---+? (label_id: 7)                                         47.76      17.08      25.16       1370
---+ (label_id: 8)                                          0.00       0.00       0.00        934
---+… (label_id: 9)                                          0.00       0.00       0.00        122
---+-------------------
---+micro avg                                               72.03      72.03      72.03      46192
---+macro avg                                               36.99      33.79      34.60      46192
---+weighted avg                                            69.12      72.03      70.25      46192
---+
---+{'punct_f1': 34.595890045166016,
---+ 'punct_precision': 36.98928451538086,
---+ 'punct_recall': 33.78831481933594,
---+ 'test_loss': 0.2638570964336395}
---+
---diff --git a/experiment/config.yaml b/experiment/config.yaml
---index b137ae8..070bc4f 100644
------ a/experiment/config.yaml
---+++ b/experiment/config.yaml
---@@ -63,8 +63,10 @@ model:
---     dataset:
---         data_dir: /home/nxingyu/data # /root/data # 
---         labelled:
----            - ${base_path}/ted_talks_processed #
---+            # - ${base_path}/ted_talks_processed #
---+            - ${base_path}/open_subtitles_processed #  
---         unlabelled:
---+            - ${base_path}/ted_talks_processed #
---             # - ${base_path}/open_subtitles_processed #  
---             # parameters for dataset preprocessing
---         max_seq_length: 128
---@@ -73,11 +75,11 @@ model:
---         ignore_start_end: false
---         use_cache: false
---         # shared among dataloaders
----        num_workers:  0
---+        num_workers:  4
---         pin_memory: true
---         drop_last: false
---         num_labels: 10
----        num_domains: 1
---+        num_domains: 2
---         test_unlabelled: true
--- 
---         train_ds:
---@@ -105,12 +107,6 @@ model:
---         config_file: null # json file, precedence over config
---         config: null
---         # unfrozen_layers: 1
----
----    mlp:
----        num_fc_layers: 1
----        fc_dropout: 0.1
----        log_softmax: false
----        activation: 'relu'
---         
---     punct_head:
---         punct_num_fc_layers: 1
---@@ -127,8 +123,8 @@ model:
---         log_softmax: false
---         use_transformer_init: true
---         loss: 'cel'
----        gamma: 0.1 # coefficient of gradient reversal
----        pooling: 'mean'
---+        gamma: 0 #0.1 # coefficient of gradient reversal
---+        pooling: 'mean_max' # 'mean' mean_max
---         idx_conditioned_on: 0
---     
---     dice_loss:
---@@ -137,7 +133,7 @@ model:
---         macro_average: true
--- 
---     focal_loss: 
----        gamma: 5
---+        gamma: 1
--- 
---     optim:
---         name: adamw
---diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
---index f9927ac..8053ecc 100644
------ a/experiment/core/layers/sequence_classifier.py
---+++ b/experiment/core/layers/sequence_classifier.py
---@@ -51,17 +51,18 @@ class SequenceClassifier(nn.Module):
---         if use_transformer_init:
---             self.apply(lambda module: transformer_weights_init(module, xavier=False))
--- 
----    def forward(self, hidden_states, subtoken_mask=None):
---+    def forward(self, hidden_states, attention_mask=None):
---         hidden_states = self.dropout(hidden_states)
---         if self.pooling=='token':
---             pooled = hidden_states[:, self._idx_conditioned_on]
---         else:
----            if subtoken_mask==None:
---+            if attention_mask is None:
---                 ct=hidden_states.shape[1] # Seq len
----                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
---             else:
----                ct = torch.sum(subtoken_mask,axis=1).unsqueeze(1)
----            pooled_sum = torch.sum(hidden_states,axis=1)            
---+                hidden_states=hidden_states*attention_mask.unsqueeze(2) # remove subtoken or padding contribution.
---+                ct = torch.sum(attention_mask,axis=1).unsqueeze(1)
---+            pooled_sum = torch.sum(hidden_states,axis=1)
---+
---             if self.pooling=='mean' or self.pooling == 'mean_max':
---                 pooled_mean = torch.div(pooled_sum,ct)
---             if self.pooling=='max' or self.pooling=='mean_max':
---diff --git a/experiment/data/__init__.py b/experiment/data/__init__.py
---index 9b0816f..818d8e3 100644
------ a/experiment/data/__init__.py
---+++ b/experiment/data/__init__.py
---@@ -1,3 +1,3 @@
----from data.punctuation_dataset import PunctuationDomainDataset, PunctuationDomainDatasets, PunctuationInferenceDataset
---+from data.punctuation_dataset_multi import PunctuationDomainDataset, PunctuationDomainDatasets, PunctuationInferenceDataset
--- from data.punctuation_datamodule import PunctuationDataModule
--- 
---diff --git a/experiment/data/punctuation_dataset.py b/experiment/data/punctuation_dataset.py
---index 8374d29..bfd015c 100644
------ a/experiment/data/punctuation_dataset.py
---+++ b/experiment/data/punctuation_dataset.py
---@@ -36,7 +36,9 @@ class PunctuationDomainDataset(IterableDataset):
---         labelled=True,
---         randomize=True,
---         target_file='',
----        tmp_path='~/data/tmp'
---+        tmp_path='~/data/tmp',
---+        start=0,
---+        end=-1,
---     ):
---         if not (os.path.exists(csv_file)):
---             raise FileNotFoundError(
---@@ -148,7 +150,9 @@ class PunctuationDomainDatasets(IterableDataset):
---                  tokenizer,
---                  randomize:bool=True,
---                  data_id='',
----                 tmp_path='~/data/tmp'):
---+                 tmp_path='~/data/tmp',
---+                 start=0,
---+                 end=-1,):
---         self.num_labelled=len(labelled)
---         self.datasets = []
---         self.iterables=[]
---diff --git a/experiment/info.log b/experiment/info.log
---index bd124fd..e69de29 100644
---Binary files a/experiment/info.log and b/experiment/info.log differ
---diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
---index 43fc93d..6a54496 100644
------ a/experiment/models/punctuation_domain_model.py
---+++ b/experiment/models/punctuation_domain_model.py
---@@ -62,15 +62,6 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---         else:
---             self.hparams.model.punct_class_weights=None
--- 
----        self.dropout=torch.nn.Dropout(self.hparams.model.mlp.fc_dropout)
----        self.mlp = MultiLayerPerceptron(
----            self.transformer.config.hidden_size,
----            self.transformer.config.hidden_size,
----            num_layers=self.hparams.model.mlp.num_fc_layers, 
----            activation=self.hparams.model.mlp.activation, 
----            log_softmax=self.hparams.model.mlp.log_softmax
----        )
----
---         self.punct_classifier = TokenClassifier(
---             hidden_size=self.transformer.config.hidden_size,
---             num_classes=len(self.labels_to_ids),
---@@ -134,13 +125,11 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
---         hidden_states = self.transformer(
---             input_ids=input_ids, attention_mask=attention_mask
---         )[0]
----        hidden_states = self.dropout(hidden_states)
----        hidden_states = self.mlp(hidden_states)
---         punct_logits = self.punct_classifier(hidden_states=hidden_states)
---         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
---         domain_logits = self.domain_classifier(
---             hidden_states=reverse_grad_hidden_states,
----            subtoken_mask=subtoken_mask)
---+            attention_mask=attention_mask)
---         return punct_logits, domain_logits
--- 
---     def _make_step(self, batch):
---@@ -669,6 +658,13 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
--- 
---         for layer in list(encoder.layer)[n:]:
---             set_requires_grad_for_module(layer, True)
---+        
---+        # Set output layer to true.
---+        last_iter=iter(encoder.layer[-1].children())
---+        last = next(last_iter)
---+        for last in last_iter:
---+            continue
---+        set_requires_grad_for_module(last, True)
--- 
---     def freeze(self) -> None:
---         try:
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/lightning_logs.txt
--deleted file mode 100644
--index 2a26724..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/lightning_logs.txt
--+++ /dev/null
--@@ -1,22 +0,0 @@
---Global seed set to 42
---GPU available: True, used: True
---TPU available: None, using: 0 TPU cores
---LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
---Using native 16bit precision.
---Global seed set to 42
---initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
---
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 1.0 K 
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---299 K     Trainable params
---13.2 M    Non-trainable params
---13.5 M    Total params
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_error_log.txt
--deleted file mode 100644
--index 4ddbe1b..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_error_log.txt
--+++ /dev/null
--@@ -1,4 +0,0 @@
---[NeMo W 2021-02-08 15:34:03 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-08 15:34:34 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_log_globalrank-0_localrank-0.txt
--deleted file mode 100644
--index dea36af..0000000
----- a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_log_globalrank-0_localrank-0.txt
--+++ /dev/null
--@@ -1,6 +0,0 @@
---[NeMo I 2021-02-08 15:34:03 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03
---[NeMo I 2021-02-08 15:34:03 exp_manager:519] TensorboardLogger has been set up
---[NeMo W 2021-02-08 15:34:03 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
---[NeMo W 2021-02-08 15:34:34 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
---      warnings.warn(*args, **kwargs)
---    
--diff --git a/README.md b/README.md
--index beca3ef..ef7d22c 100644
----- a/README.md
--+++ b/README.md
--@@ -449,3 +449,7 @@ weighted avg                                            69.12      72.03      70
--  'punct_recall': 33.78831481933594,
--  'test_loss': 0.2638570964336395}
-- 
--+
--+
--+### domain adversarial dice 3, open l ted ul 
--+initial_lr 0.007943282347242822
--diff --git a/experiment/config.yaml b/experiment/config.yaml
--index 070bc4f..e492246 100644
----- a/experiment/config.yaml
--+++ b/experiment/config.yaml
--@@ -66,7 +66,7 @@ model:
--             # - ${base_path}/ted_talks_processed #
--             - ${base_path}/open_subtitles_processed #  
--         unlabelled:
---            - ${base_path}/ted_talks_processed #
--+            # - ${base_path}/ted_talks_processed #
--             # - ${base_path}/open_subtitles_processed #  
--             # parameters for dataset preprocessing
--         max_seq_length: 128
--@@ -79,13 +79,13 @@ model:
--         pin_memory: true
--         drop_last: false
--         num_labels: 10
---        num_domains: 2
--+        num_domains: 1
--         test_unlabelled: true
-- 
--         train_ds:
--             shuffle: true
--             num_samples: -1
---            batch_size: 8
--+            batch_size: 4
-- 
--         validation_ds:
--             # if evaluation data is not in the model.dataset.data_dir as the training data or multiple datasets are used for
--@@ -93,7 +93,7 @@ model:
--             # ds_item: null # expected format: [PATH_TO_DEV1,PATH_TO_DEV2] (Note no space between the paths and square brackets)
--             shuffle: true
--             num_samples: -1
---            batch_size: 8
--+            batch_size: 4
-- 
--     tokenizer:
--         tokenizer_name: ${model.language_model.pretrained_model_name} # or sentencepiece
--@@ -123,7 +123,7 @@ model:
--         log_softmax: false
--         use_transformer_init: true
--         loss: 'cel'
---        gamma: 0 #0.1 # coefficient of gradient reversal
--+        gamma: 0.1 #0.1 # coefficient of gradient reversal
--         pooling: 'mean_max' # 'mean' mean_max
--         idx_conditioned_on: 0
--     
--diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
--index bc844cd..4027bb2 100644
----- a/experiment/data/punctuation_dataset_multi.py
--+++ b/experiment/data/punctuation_dataset_multi.py
--@@ -63,7 +63,8 @@ class PunctuationDomainDataset(IterableDataset):
--         self.randomize=randomize
--         self.target_file=target_file
--         self.tmp_path=tmp_path
---        os.system(f'cp {self.csv_file} {self.target_file}')
--+        if not (os.path.exists(self.target_file)):
--+            os.system(f'cp {self.csv_file} {self.target_file}')
-- 
--     def __iter__(self):
--         self.dataset=iter(pd.read_csv(
--diff --git a/experiment/info.log b/experiment/info.log
--index 481b8ff..e69de29 100644
----- a/experiment/info.log
--+++ b/experiment/info.log
--@@ -1,43 +0,0 @@
---[INFO] - shuffling train set
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.001
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fd3155e83d0>" 
---will be used during training (effective maximum steps = 80) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---last_epoch: -1
---max_steps: 80
---)
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                           0.00       0.00       0.00        168
---! (label_id: 1)                                         14.29       7.69      10.00        104
---, (label_id: 2)                                         23.21      27.23      25.06        584
---- (label_id: 3)                                          4.02      46.67       7.39         45
---. (label_id: 4)                                         54.17       1.19       2.33       1091
---: (label_id: 5)                                          0.00       0.00       0.00          0
---; (label_id: 6)                                          0.00       0.00       0.00          0
---? (label_id: 7)                                          6.42      22.75      10.01        189
---— (label_id: 8)                                          0.00       0.00       0.00          0
---… (label_id: 9)                                          9.09       2.38       3.77         84
----------------------
---micro avg                                               10.86      10.86      10.86       2265
---macro avg                                               15.88      15.42       8.37       2265
---weighted avg                                            33.68      10.86       9.17       2265
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                         50.00     100.00      66.67         84
---1 (label_id: 1)                                          0.00       0.00       0.00         84
----------------------
---micro avg                                               50.00      50.00      50.00        168
---macro avg                                               25.00      50.00      33.33        168
---weighted avg                                            25.00      50.00      33.33        168
---
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/hparams.yaml
-deleted file mode 100644
-index bbd15a0..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/hparams.yaml
-+++ /dev/null
-@@ -1,109 +0,0 @@
--seed: 42
--trainer:
--  gpus: 1
--  num_nodes: 1
--  max_epochs: 10
--  max_steps: null
--  accumulate_grad_batches: 4
--  gradient_clip_val: 0
--  amp_level: O1
--  precision: 16
--  accelerator: ddp
--  checkpoint_callback: false
--  logger: false
--  log_every_n_steps: 1
--  val_check_interval: 1.0
--  resume_from_checkpoint: null
--exp_manager:
--  exp_dir: /home/nxingyu/project/
--  name: Punctuation_with_Domain_discriminator
--  create_tensorboard_logger: true
--  create_checkpoint_callback: true
--base_path: /home/nxingyu/data
--tmp_path: /home/nxingyu/data/tmp
--model:
--  nemo_path: null
--  transformer_path: google/electra-small-discriminator
--  unfrozen: 0
--  maximum_unfrozen: 1
--  unfreeze_step: 1
--  punct_label_ids:
--  - ''
--  - '!'
--  - ','
--  - '-'
--  - .
--  - ':'
--  - ;
--  - '?'
--  - —
--  - …
--  punct_class_weights: false
--  dataset:
--    data_dir: /home/nxingyu/data
--    labelled:
--    - /home/nxingyu/data/open_subtitles_processed
--    unlabelled: null
--    max_seq_length: 128
--    pad_label: ''
--    ignore_extra_tokens: false
--    ignore_start_end: false
--    use_cache: false
--    num_workers: 4
--    pin_memory: true
--    drop_last: false
--    num_labels: 10
--    num_domains: 1
--    test_unlabelled: true
--    train_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--    validation_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--  tokenizer:
--    tokenizer_name: google/electra-small-discriminator
--    vocab_file: null
--    tokenizer_model: null
--    special_tokens: null
--  language_model:
--    pretrained_model_name: google/electra-small-discriminator
--    lm_checkpoint: null
--    config_file: null
--    config: null
--  punct_head:
--    punct_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: dice
--  domain_head:
--    domain_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: cel
--    gamma: 0.1
--    pooling: mean_max
--    idx_conditioned_on: 0
--  dice_loss:
--    epsilon: 0.01
--    alpha: 3
--    macro_average: true
--  focal_loss:
--    gamma: 1
--  optim:
--    name: adamw
--    lr: 0.001
--    weight_decay: 0.0
--    sched:
--      name: WarmupAnnealing
--      warmup_steps: null
--      warmup_ratio: 0.1
--      last_epoch: -1
--      monitor: val_loss
--      reduce_on_plateau: false
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/lightning_logs.txt
-deleted file mode 100644
-index 0f48e2a..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/lightning_logs.txt
-+++ /dev/null
-@@ -1,65 +0,0 @@
--Global seed set to 42
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
--Using native 16bit precision.
--Global seed set to 42
--initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 513   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--299 K     Trainable params
--13.2 M    Non-trainable params
--13.5 M    Total params
--Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/lr_find_temp_model.ckpt
--Global seed set to 42
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 513   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--299 K     Trainable params
--13.2 M    Non-trainable params
--13.5 M    Total params
--Epoch 0, global step 5305: val_loss reached 0.20905 (best 0.20905), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.21-epoch=0.ckpt" as top 3
--Epoch 1, global step 10610: val_loss reached 0.17265 (best 0.17265), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.17-epoch=1.ckpt" as top 3
--Epoch 2, global step 15915: val_loss reached 0.05470 (best 0.05470), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.05-epoch=2.ckpt" as top 3
--Epoch 3, global step 21220: val_loss reached 0.02875 (best 0.02875), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.03-epoch=3.ckpt" as top 3
--Epoch 4, global step 26525: val_loss reached -0.03932 (best -0.03932), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.04-epoch=4.ckpt" as top 3
--Epoch 5, global step 31830: val_loss reached -0.04410 (best -0.04410), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.04-epoch=5.ckpt" as top 3
--Epoch 6, global step 37135: val_loss reached -0.04524 (best -0.04524), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.05-epoch=6.ckpt" as top 3
--Epoch 7, global step 42440: val_loss reached -0.04689 (best -0.04689), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.05-epoch=7.ckpt" as top 3
--Epoch 8, global step 47745: val_loss reached -0.04978 (best -0.04978), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.05-epoch=8.ckpt" as top 3
--Epoch 9, global step 53050: val_loss reached -0.04850 (best -0.04978), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.05-epoch=9.ckpt" as top 3
--Global seed set to 42
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 513   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--825 K     Trainable params
--12.7 M    Non-trainable params
--13.5 M    Total params
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_error_log.txt
-deleted file mode 100644
-index ce38185..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_error_log.txt
-+++ /dev/null
-@@ -1,16 +0,0 @@
--[NeMo W 2021-02-08 16:23:25 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-08 16:23:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 16:25:00 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 16:25:22 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-08 17:13:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fbd320e2b50> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-08 17:20:28 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fbd3c4dd8e0> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_log_globalrank-0_localrank-0.txt
-deleted file mode 100644
-index 26f3ea7..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_log_globalrank-0_localrank-0.txt
-+++ /dev/null
-@@ -1,18 +0,0 @@
--[NeMo I 2021-02-08 16:23:25 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25
--[NeMo I 2021-02-08 16:23:25 exp_manager:519] TensorboardLogger has been set up
--[NeMo W 2021-02-08 16:23:25 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-08 16:23:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 16:25:00 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-08 16:25:22 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-08 17:13:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fbd320e2b50> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-08 17:20:28 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fbd3c4dd8e0> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-31-42/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-09_15-31-42/cmd-args.log
-deleted file mode 100644
-index 11a5d8e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-31-42/cmd-args.log
-+++ /dev/null
-@@ -1 +0,0 @@
--main.py
-\ No newline at end of file
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-31-42/events.out.tfevents.1612855975.Titan.15096.0 b/Punctuation_with_Domain_discriminator/2021-02-09_15-31-42/events.out.tfevents.1612855975.Titan.15096.0
-deleted file mode 100644
-index 773b3b5..0000000
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_15-31-42/events.out.tfevents.1612855975.Titan.15096.0 and /dev/null differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-31-42/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-09_15-31-42/git-info.log
-deleted file mode 100644
-index e11f014..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-31-42/git-info.log
-+++ /dev/null
-@@ -1,309 +0,0 @@
--commit hash: 836632f0fdebe90f93105efab295fe4cd83af4ca
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0 b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0
--index c3ff071..0e8637d 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0 differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt
--index 7f6eddf..a1b895b 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt
--@@ -39,3 +39,53 @@ Global seed set to 42
-- 66.4 M    Total params
-- Epoch 0, global step 200: val_loss reached 0.26631 (best 0.26631), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=0.ckpt" as top 3
-- Epoch 1, global step 400: val_loss reached 0.26336 (best 0.26336), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=1.ckpt" as top 3
--+Epoch 2, global step 600: val_loss reached 0.26109 (best 0.26109), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=2.ckpt" as top 3
--+Epoch 3, global step 800: val_loss reached 0.25805 (best 0.25805), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=3.ckpt" as top 3
--+Epoch 4, global step 1000: val_loss reached 0.25547 (best 0.25547), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=4.ckpt" as top 3
--+Epoch 5, global step 1200: val_loss reached 0.25339 (best 0.25339), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.25-epoch=5.ckpt" as top 3
--+Epoch 6, global step 1400: val_loss reached 0.24810 (best 0.24810), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.25-epoch=6.ckpt" as top 3
--+Epoch 7, global step 1600: val_loss reached 0.24127 (best 0.24127), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=7.ckpt" as top 3
--+Epoch 8, global step 1800: val_loss reached 0.23864 (best 0.23864), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=8.ckpt" as top 3
--+Epoch 9, global step 2000: val_loss reached 0.23646 (best 0.23646), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=9.ckpt" as top 3
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | DistilBertModel      | 66.4 M
--+1 | punct_classifier    | TokenClassifier      | 7.7 K 
--+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--+3 | punctuation_loss    | FocalDiceLoss        | 0     
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+7.1 M     Trainable params
--+59.3 M    Non-trainable params
--+66.4 M    Total params
--+LR finder stopped early due to diverging loss.
--+Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lr_find_temp_model.ckpt
--+Failed to compute suggesting for `lr`. There might not be enough points.
--+Traceback (most recent call last):
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
--+    min_grad = np.gradient(loss).argmin()
--+  File "<__array_function__ internals>", line 5, in gradient
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
--+    raise ValueError(
--+ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | DistilBertModel      | 66.4 M
--+1 | punct_classifier    | TokenClassifier      | 7.7 K 
--+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--+3 | punctuation_loss    | FocalDiceLoss        | 0     
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+7.1 M     Trainable params
--+59.3 M    Non-trainable params
--+66.4 M    Total params
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt
--index 2d9ccfb..a9e0895 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt
--@@ -14,3 +14,14 @@
-- [NeMo W 2021-02-09 15:11:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fa1136ae0a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 15:25:45 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--+      warnings.warn(*args, **kwargs)
--+    
--+Failed to compute suggesting for `lr`. There might not be enough points.
--+Traceback (most recent call last):
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
--+    min_grad = np.gradient(loss).argmin()
--+  File "<__array_function__ internals>", line 5, in gradient
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
--+    raise ValueError(
--+ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt
--index 80a7030..dfff301 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt
--@@ -16,3 +16,6 @@
-- [NeMo W 2021-02-09 15:11:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fa1136ae0a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 15:25:45 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--+      warnings.warn(*args, **kwargs)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0
--index bb4d846..e632425 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
--index 2869be1..a376c9f 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
--@@ -37,3 +37,7 @@ Global seed set to 42
-- 10.9 K    Trainable params
-- 66.4 M    Non-trainable params
-- 66.4 M    Total params
--+Epoch 0, global step 200: val_loss reached 108.49911 (best 108.49911), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=108.50-epoch=0.ckpt" as top 3
--+Epoch 1, global step 400: val_loss reached 155.59180 (best 108.49911), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=155.59-epoch=1.ckpt" as top 3
--+Epoch 2, global step 600: val_loss reached 46.96625 (best 46.96625), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=46.97-epoch=2.ckpt" as top 3
--+Epoch 3, global step 800: val_loss reached 63.33499 (best 46.96625), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=63.33-epoch=3.ckpt" as top 3
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
--index 568694f..6bd1322 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
--@@ -23,3 +23,9 @@
-- [NeMo W 2021-02-09 15:11:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--     
--+[NeMo W 2021-02-09 15:16:02 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce8610> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--+[NeMo W 2021-02-09 15:16:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce40a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
--index 7533c2c..e5eaea5 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
--@@ -25,3 +25,9 @@
-- [NeMo W 2021-02-09 15:11:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--     
--+[NeMo W 2021-02-09 15:16:02 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce8610> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--+[NeMo W 2021-02-09 15:16:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce40a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--diff --git a/experiment/config.yaml b/experiment/config.yaml
--index 0aeaa8b..25d70b6 100644
----- a/experiment/config.yaml
--+++ b/experiment/config.yaml
--@@ -2,7 +2,7 @@ seed: 42
-- trainer:
--     gpus: 1 # the number of gpus, 0 for CPU
--     num_nodes: 1
---    max_epochs: 10
--+    max_epochs: 2
--     max_steps: null # precedence over max_epochs
--     accumulate_grad_batches: 4 # accumulates grads every k batches
--     gradient_clip_val: 0
--@@ -114,7 +114,7 @@ model:
--         activation: 'relu'
--         log_softmax: false
--         use_transformer_init: true
---        loss: 'crf'
--+        loss: 'cel'
-- 
--     domain_head:
--         domain_num_fc_layers: 1
--diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
--index 88a268f..31923de 100644
----- a/experiment/data/punctuation_dataset_multi.py
--+++ b/experiment/data/punctuation_dataset_multi.py
--@@ -165,8 +165,8 @@ class PunctuationDomainDatasets(IterableDataset):
--         for path in labelled+unlabelled:
--             self.ds_lengths.append(int(subprocess.Popen(['wc', '-l', f'{path}.{split}.csv'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0]))
--         self.max_length=max(self.ds_lengths)
---        self.len=int(self.max_length/num_samples)
--         self.per_worker=int(self.max_length/self.num_workers)
--+        self.len=int(self.per_worker/num_samples)
--         self.class_weights=None
-- 
--         for i,path in enumerate(labelled):
--diff --git a/experiment/info.log b/experiment/info.log
--index ff2d606..e69de29 100644
----- a/experiment/info.log
--+++ b/experiment/info.log
--@@ -1,133 +0,0 @@
---[INFO] - shuffling train set
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.009261935523740748
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f596c78a3d0>" 
---will be used during training (effective maximum steps = 80) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---last_epoch: -1
---max_steps: 80
---)
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          80.85       4.94       9.30       1540
---! (label_id: 1)                                          0.00       0.00       0.00          2
---, (label_id: 2)                                          7.69       1.41       2.38        142
---- (label_id: 3)                                          1.67      20.00       3.08         20
---. (label_id: 4)                                          6.82      12.50       8.82         96
---: (label_id: 5)                                          0.00       0.00       0.00          0
---; (label_id: 6)                                          0.00       0.00       0.00          0
---? (label_id: 7)                                          0.00       0.00       0.00          4
---— (label_id: 8)                                          0.00       0.00       0.00          0
---… (label_id: 9)                                          0.00       0.00       0.00          0
----------------------
---micro avg                                                5.21       5.21       5.21       1804
---macro avg                                               16.17       6.47       3.93       1804
---weighted avg                                            70.01       5.21       8.63       1804
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00         20
----------------------
---micro avg                                              100.00     100.00     100.00         20
---macro avg                                              100.00     100.00     100.00         20
---weighted avg                                           100.00     100.00     100.00         20
---
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.0024506370946974477
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f596c570f40>" 
---will be used during training (effective maximum steps = 2000) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---last_epoch: -1
---max_steps: 2000
---)
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          80.85       4.94       9.30       1540
---! (label_id: 1)                                          0.00       0.00       0.00          2
---, (label_id: 2)                                          7.69       1.41       2.38        142
---- (label_id: 3)                                          1.67      20.00       3.08         20
---. (label_id: 4)                                          6.82      12.50       8.82         96
---: (label_id: 5)                                          0.00       0.00       0.00          0
---; (label_id: 6)                                          0.00       0.00       0.00          0
---? (label_id: 7)                                          0.00       0.00       0.00          4
---— (label_id: 8)                                          0.00       0.00       0.00          0
---… (label_id: 9)                                          0.00       0.00       0.00          0
----------------------
---micro avg                                                5.21       5.21       5.21       1804
---macro avg                                               16.17       6.47       3.93       1804
---weighted avg                                            70.01       5.21       8.63       1804
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00         20
----------------------
---micro avg                                              100.00     100.00     100.00         20
---macro avg                                              100.00     100.00     100.00         20
---weighted avg                                           100.00     100.00     100.00         20
---
---NFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          90.44      95.02      92.67     179900
---! (label_id: 1)                                          0.00       0.00       0.00         52
---, (label_id: 2)                                         21.92       9.58      13.34      13074
---- (label_id: 3)                                         44.94       6.69      11.64       1062
---. (label_id: 4)                                         28.64      30.78      29.67      11077
---: (label_id: 5)                                          0.00       0.00       0.00        330
---; (label_id: 6)                                          0.00       0.00       0.00        108
---? (label_id: 7)                                          0.00       0.00       0.00        899
---— (label_id: 8)                                          0.00       0.00       0.00        276
---… (label_id: 9)                                          0.00       0.00       0.00         28
----------------------
---micro avg                                               84.95      84.95      84.95     206806
---macro avg                                               18.59      14.21      14.73     206806
---weighted avg                                            81.82      84.95      83.11     206806
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1959
----------------------
---micro avg                                              100.00     100.00     100.00       1959
---macro avg                                              100.00     100.00     100.00       1959
---weighted avg                                           100.00     100.00     100.00       1959
---
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          90.62      94.69      92.61     179900
---! (label_id: 1)                                          0.00       0.00       0.00         52
---, (label_id: 2)                                         21.80      10.36      14.04      13074
---- (label_id: 3)                                         44.94       6.69      11.64       1062
---. (label_id: 4)                                         28.46      32.05      30.15      11077
---: (label_id: 5)                                          0.00       0.00       0.00        330
---; (label_id: 6)                                          0.00       0.00       0.00        108
---? (label_id: 7)                                          0.00       0.00       0.00        899
---— (label_id: 8)                                          0.00       0.00       0.00        276
---… (label_id: 9)                                          0.00       0.00       0.00         28
----------------------
---micro avg                                               84.77      84.77      84.77     206806
---macro avg                                               18.58      14.38      14.84     206806
---weighted avg                                            81.97      84.77      83.12     206806
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1959
----------------------
---micro avg                                              100.00     100.00     100.00       1959
---macro avg                                              100.00     100.00     100.00       1959
---weighted avg                                           100.00     100.00     100.00       1959
---
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-31-42/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-09_15-31-42/hparams.yaml
-deleted file mode 100644
-index aa6ca16..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-31-42/hparams.yaml
-+++ /dev/null
-@@ -1,109 +0,0 @@
--seed: 42
--trainer:
--  gpus: 1
--  num_nodes: 1
--  max_epochs: 2
--  max_steps: null
--  accumulate_grad_batches: 4
--  gradient_clip_val: 0
--  amp_level: O1
--  precision: 16
--  accelerator: ddp
--  checkpoint_callback: false
--  logger: false
--  log_every_n_steps: 1
--  val_check_interval: 1.0
--  resume_from_checkpoint: null
--exp_manager:
--  exp_dir: /home/nxingyu/project/
--  name: Punctuation_with_Domain_discriminator
--  create_tensorboard_logger: true
--  create_checkpoint_callback: true
--base_path: /home/nxingyu/data
--tmp_path: /home/nxingyu/data/tmp
--model:
--  nemo_path: null
--  transformer_path: distilbert-base-uncased
--  unfrozen: 0
--  maximum_unfrozen: 1
--  unfreeze_step: 1
--  punct_label_ids:
--  - ''
--  - '!'
--  - ','
--  - '-'
--  - .
--  - ':'
--  - ;
--  - '?'
--  - —
--  - …
--  punct_class_weights: false
--  dataset:
--    data_dir: /home/nxingyu/data
--    labelled:
--    - /home/nxingyu/data/ted_talks_processed
--    unlabelled: null
--    max_seq_length: 128
--    pad_label: ''
--    ignore_extra_tokens: false
--    ignore_start_end: false
--    use_cache: false
--    num_workers: 2
--    pin_memory: true
--    drop_last: true
--    num_labels: 10
--    num_domains: 1
--    test_unlabelled: true
--    train_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--    validation_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--  tokenizer:
--    tokenizer_name: distilbert-base-uncased
--    vocab_file: null
--    tokenizer_model: null
--    special_tokens: null
--  language_model:
--    pretrained_model_name: distilbert-base-uncased
--    lm_checkpoint: null
--    config_file: null
--    config: null
--  punct_head:
--    punct_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: cel
--  domain_head:
--    domain_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: cel
--    gamma: 0.1
--    pooling: mean_max
--    idx_conditioned_on: 0
--  dice_loss:
--    epsilon: 0.01
--    alpha: 3
--    macro_average: true
--  focal_loss:
--    gamma: 1
--  optim:
--    name: adamw
--    lr: 0.009261935523740748
--    weight_decay: 0.0
--    sched:
--      name: WarmupAnnealing
--      warmup_steps: null
--      warmup_ratio: 0.1
--      last_epoch: -1
--      monitor: val_loss
--      reduce_on_plateau: false
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-31-42/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-31-42/lightning_logs.txt
-deleted file mode 100644
-index 05e8283..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-31-42/lightning_logs.txt
-+++ /dev/null
-@@ -1,83 +0,0 @@
--Global seed set to 42
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
--Using native 16bit precision.
--Global seed set to 42
--initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | DistilBertModel      | 66.4 M
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--3 | punctuation_loss    | CrossEntropyLoss     | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--10.8 K    Trainable params
--66.4 M    Non-trainable params
--66.4 M    Total params
--Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-31-42/lr_find_temp_model.ckpt
--Global seed set to 42
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | DistilBertModel      | 66.4 M
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--3 | punctuation_loss    | CrossEntropyLoss     | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--10.8 K    Trainable params
--66.4 M    Non-trainable params
--66.4 M    Total params
--Epoch 0, global step 200: val_loss reached 19.33873 (best 19.33873), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-31-42/checkpoints/Punctuation_with_Domain_discriminator---val_loss=19.34-epoch=0.ckpt" as top 3
--Epoch 1, global step 400: val_loss reached 11.20076 (best 11.20076), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-31-42/checkpoints/Punctuation_with_Domain_discriminator---val_loss=11.20-epoch=1.ckpt" as top 3
--Global seed set to 42
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | DistilBertModel      | 66.4 M
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--3 | punctuation_loss    | CrossEntropyLoss     | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--7.1 M     Trainable params
--59.3 M    Non-trainable params
--66.4 M    Total params
--LR finder stopped early due to diverging loss.
--Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-31-42/lr_find_temp_model.ckpt
--Failed to compute suggesting for `lr`. There might not be enough points.
--Traceback (most recent call last):
--  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
--    min_grad = np.gradient(loss).argmin()
--  File "<__array_function__ internals>", line 5, in gradient
--  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
--    raise ValueError(
--ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
--Global seed set to 42
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | DistilBertModel      | 66.4 M
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--3 | punctuation_loss    | CrossEntropyLoss     | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--7.1 M     Trainable params
--59.3 M    Non-trainable params
--66.4 M    Total params
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-31-42/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-31-42/nemo_error_log.txt
-deleted file mode 100644
-index 55a7242..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-31-42/nemo_error_log.txt
-+++ /dev/null
-@@ -1,39 +0,0 @@
--[NeMo W 2021-02-09 15:31:43 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 15:31:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:31:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:31:58 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:32:55 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:32:55 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:32:57 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:32:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-09 15:35:19 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f56456635b0> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 15:35:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5645660040> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 15:38:33 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--      warnings.warn(*args, **kwargs)
--    
--Failed to compute suggesting for `lr`. There might not be enough points.
--Traceback (most recent call last):
--  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
--    min_grad = np.gradient(loss).argmin()
--  File "<__array_function__ internals>", line 5, in gradient
--  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
--    raise ValueError(
--ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-31-42/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-31-42/nemo_log_globalrank-0_localrank-0.txt
-deleted file mode 100644
-index 0c5268b..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_15-31-42/nemo_log_globalrank-0_localrank-0.txt
-+++ /dev/null
-@@ -1,33 +0,0 @@
--[NeMo I 2021-02-09 15:31:42 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-31-42
--[NeMo I 2021-02-09 15:31:42 exp_manager:519] TensorboardLogger has been set up
--[NeMo W 2021-02-09 15:31:43 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 15:31:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:31:56 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:31:58 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:32:55 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:32:55 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:32:57 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 15:32:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
--[NeMo W 2021-02-09 15:35:19 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f56456635b0> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 15:35:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5645660040> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-09 15:38:33 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--      warnings.warn(*args, **kwargs)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-01-34/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-09_16-01-34/cmd-args.log
-deleted file mode 100644
-index 11a5d8e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-01-34/cmd-args.log
-+++ /dev/null
-@@ -1 +0,0 @@
--main.py
-\ No newline at end of file
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-01-34/events.out.tfevents.1612857811.Titan.23786.0 b/Punctuation_with_Domain_discriminator/2021-02-09_16-01-34/events.out.tfevents.1612857811.Titan.23786.0
-deleted file mode 100644
-index 755de30..0000000
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_16-01-34/events.out.tfevents.1612857811.Titan.23786.0 and /dev/null differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-01-34/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-09_16-01-34/git-info.log
-deleted file mode 100644
-index 281ae2b..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-01-34/git-info.log
-+++ /dev/null
-@@ -1,436 +0,0 @@
--commit hash: 836632f0fdebe90f93105efab295fe4cd83af4ca
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0 b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0
--index c3ff071..0e8637d 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/events.out.tfevents.1612854583.Titan.1794.0 differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt
--index 7f6eddf..a1b895b 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lightning_logs.txt
--@@ -39,3 +39,53 @@ Global seed set to 42
-- 66.4 M    Total params
-- Epoch 0, global step 200: val_loss reached 0.26631 (best 0.26631), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=0.ckpt" as top 3
-- Epoch 1, global step 400: val_loss reached 0.26336 (best 0.26336), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=1.ckpt" as top 3
--+Epoch 2, global step 600: val_loss reached 0.26109 (best 0.26109), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=2.ckpt" as top 3
--+Epoch 3, global step 800: val_loss reached 0.25805 (best 0.25805), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=3.ckpt" as top 3
--+Epoch 4, global step 1000: val_loss reached 0.25547 (best 0.25547), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=4.ckpt" as top 3
--+Epoch 5, global step 1200: val_loss reached 0.25339 (best 0.25339), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.25-epoch=5.ckpt" as top 3
--+Epoch 6, global step 1400: val_loss reached 0.24810 (best 0.24810), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.25-epoch=6.ckpt" as top 3
--+Epoch 7, global step 1600: val_loss reached 0.24127 (best 0.24127), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=7.ckpt" as top 3
--+Epoch 8, global step 1800: val_loss reached 0.23864 (best 0.23864), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=8.ckpt" as top 3
--+Epoch 9, global step 2000: val_loss reached 0.23646 (best 0.23646), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.24-epoch=9.ckpt" as top 3
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | DistilBertModel      | 66.4 M
--+1 | punct_classifier    | TokenClassifier      | 7.7 K 
--+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--+3 | punctuation_loss    | FocalDiceLoss        | 0     
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+7.1 M     Trainable params
--+59.3 M    Non-trainable params
--+66.4 M    Total params
--+LR finder stopped early due to diverging loss.
--+Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/lr_find_temp_model.ckpt
--+Failed to compute suggesting for `lr`. There might not be enough points.
--+Traceback (most recent call last):
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
--+    min_grad = np.gradient(loss).argmin()
--+  File "<__array_function__ internals>", line 5, in gradient
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
--+    raise ValueError(
--+ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | DistilBertModel      | 66.4 M
--+1 | punct_classifier    | TokenClassifier      | 7.7 K 
--+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--+3 | punctuation_loss    | FocalDiceLoss        | 0     
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+7.1 M     Trainable params
--+59.3 M    Non-trainable params
--+66.4 M    Total params
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt
--index 2d9ccfb..a9e0895 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_error_log.txt
--@@ -14,3 +14,14 @@
-- [NeMo W 2021-02-09 15:11:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fa1136ae0a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 15:25:45 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--+      warnings.warn(*args, **kwargs)
--+    
--+Failed to compute suggesting for `lr`. There might not be enough points.
--+Traceback (most recent call last):
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
--+    min_grad = np.gradient(loss).argmin()
--+  File "<__array_function__ internals>", line 5, in gradient
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
--+    raise ValueError(
--+ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt
--index 80a7030..dfff301 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-08-53/nemo_log_globalrank-0_localrank-0.txt
--@@ -16,3 +16,6 @@
-- [NeMo W 2021-02-09 15:11:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fa1136ae0a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--       warnings.warn(warn_msg)
--     
--+[NeMo W 2021-02-09 15:25:45 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--+      warnings.warn(*args, **kwargs)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0
--index bb4d846..53377de 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/events.out.tfevents.1612854710.Titan.2739.0 differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
--index 2869be1..0f4c210 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lightning_logs.txt
--@@ -37,3 +37,55 @@ Global seed set to 42
-- 10.9 K    Trainable params
-- 66.4 M    Non-trainable params
-- 66.4 M    Total params
--+Epoch 0, global step 200: val_loss reached 108.49911 (best 108.49911), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=108.50-epoch=0.ckpt" as top 3
--+Epoch 1, global step 400: val_loss reached 155.59180 (best 108.49911), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=155.59-epoch=1.ckpt" as top 3
--+Epoch 2, global step 600: val_loss reached 46.96625 (best 46.96625), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=46.97-epoch=2.ckpt" as top 3
--+Epoch 3, global step 800: val_loss reached 63.33499 (best 46.96625), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=63.33-epoch=3.ckpt" as top 3
--+Epoch 4, global step 1000: val_loss reached 33.09097 (best 33.09097), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=33.09-epoch=4.ckpt" as top 3
--+Epoch 5, global step 1200: val_loss reached 30.02221 (best 30.02221), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=30.02-epoch=5.ckpt" as top 3
--+Epoch 6, global step 1400: val_loss reached 31.40887 (best 30.02221), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=31.41-epoch=6.ckpt" as top 3
--+Epoch 7, step 1600: val_loss was not in top 3
--+Epoch 8, global step 1800: val_loss reached 28.61345 (best 28.61345), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=28.61-epoch=8.ckpt" as top 3
--+Epoch 9, global step 2000: val_loss reached 26.27475 (best 26.27475), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/checkpoints/Punctuation_with_Domain_discriminator---val_loss=26.27-epoch=9.ckpt" as top 3
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | DistilBertModel      | 66.4 M
--+1 | punct_classifier    | TokenClassifier      | 7.7 K 
--+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--+3 | punctuation_loss    | LinearChainCRF       | 120   
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+7.1 M     Trainable params
--+59.3 M    Non-trainable params
--+66.4 M    Total params
--+LR finder stopped early due to diverging loss.
--+Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/lr_find_temp_model.ckpt
--+Failed to compute suggesting for `lr`. There might not be enough points.
--+Traceback (most recent call last):
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
--+    min_grad = np.gradient(loss).argmin()
--+  File "<__array_function__ internals>", line 5, in gradient
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
--+    raise ValueError(
--+ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
--+Global seed set to 42
--+
--+  | Name                | Type                 | Params
--+-------------------------------------------------------------
--+0 | transformer         | DistilBertModel      | 66.4 M
--+1 | punct_classifier    | TokenClassifier      | 7.7 K 
--+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--+3 | punctuation_loss    | LinearChainCRF       | 120   
--+4 | domain_loss         | CrossEntropyLoss     | 0     
--+5 | agg_loss            | AggregatorLoss       | 0     
--+6 | punct_class_report  | ClassificationReport | 0     
--+7 | domain_class_report | ClassificationReport | 0     
--+-------------------------------------------------------------
--+7.1 M     Trainable params
--+59.3 M    Non-trainable params
--+66.4 M    Total params
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
--index 568694f..cab9655 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_error_log.txt
--@@ -23,3 +23,20 @@
-- [NeMo W 2021-02-09 15:11:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--     
--+[NeMo W 2021-02-09 15:16:02 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce8610> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--+[NeMo W 2021-02-09 15:16:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce40a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--+[NeMo W 2021-02-09 15:57:31 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--+      warnings.warn(*args, **kwargs)
--+    
--+Failed to compute suggesting for `lr`. There might not be enough points.
--+Traceback (most recent call last):
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 356, in suggestion
--+    min_grad = np.gradient(loss).argmin()
--+  File "<__array_function__ internals>", line 5, in gradient
--+  File "/home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
--+    raise ValueError(
--+ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
--index 7533c2c..d19ea43 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_15-09-54/nemo_log_globalrank-0_localrank-0.txt
--@@ -25,3 +25,12 @@
-- [NeMo W 2021-02-09 15:11:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--     
--+[NeMo W 2021-02-09 15:16:02 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce8610> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--+[NeMo W 2021-02-09 15:16:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f5966ce40a0> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--+      warnings.warn(warn_msg)
--+    
--+[NeMo W 2021-02-09 15:57:31 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
--+      warnings.warn(*args, **kwargs)
--+    
--diff --git a/experiment/config.yaml b/experiment/config.yaml
--index 0aeaa8b..cb177ef 100644
----- a/experiment/config.yaml
--+++ b/experiment/config.yaml
--@@ -2,7 +2,7 @@ seed: 42
-- trainer:
--     gpus: 1 # the number of gpus, 0 for CPU
--     num_nodes: 1
---    max_epochs: 10
--+    max_epochs: 2
--     max_steps: null # precedence over max_epochs
--     accumulate_grad_batches: 4 # accumulates grads every k batches
--     gradient_clip_val: 0
--@@ -41,7 +41,7 @@ tmp_path: /home/nxingyu/data/tmp # /tmp #
-- 
-- model:
--     nemo_path: null
---    transformer_path: distilbert-base-uncased #google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
--+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
--     unfrozen: 0
--     maximum_unfrozen: 1
--     unfreeze_step: 1
--@@ -75,7 +75,7 @@ model:
--         ignore_start_end: false
--         use_cache: false
--         # shared among dataloaders
---        num_workers:  2
--+        num_workers:  4
--         pin_memory: true
--         drop_last: true
--         num_labels: 10
--@@ -114,7 +114,7 @@ model:
--         activation: 'relu'
--         log_softmax: false
--         use_transformer_init: true
---        loss: 'crf'
--+        loss: 'dice'
-- 
--     domain_head:
--         domain_num_fc_layers: 1
--diff --git a/experiment/data/punctuation_datamodule.py b/experiment/data/punctuation_datamodule.py
--index 20a4093..8711456 100644
----- a/experiment/data/punctuation_datamodule.py
--+++ b/experiment/data/punctuation_datamodule.py
--@@ -108,7 +108,8 @@ class PunctuationDataModule(LightningDataModule):
-- 
--         logging.info(f"shuffling train set")
--         # self.train_dataset.shuffle(randomize=False)
---        self.train_dataset.shuffle(randomize=True, seed=self.seed)
--+        if (self.train_shuffle):
--+            self.train_dataset.shuffle(randomize=True, seed=self.seed)
-- 
--     def train_dataloader(self):
--         return DataLoader(self.train_dataset,batch_size=None,num_workers=self.num_workers,pin_memory=self.pin_memory,drop_last=self.drop_last)
--diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
--index 88a268f..31923de 100644
----- a/experiment/data/punctuation_dataset_multi.py
--+++ b/experiment/data/punctuation_dataset_multi.py
--@@ -165,8 +165,8 @@ class PunctuationDomainDatasets(IterableDataset):
--         for path in labelled+unlabelled:
--             self.ds_lengths.append(int(subprocess.Popen(['wc', '-l', f'{path}.{split}.csv'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0]))
--         self.max_length=max(self.ds_lengths)
---        self.len=int(self.max_length/num_samples)
--         self.per_worker=int(self.max_length/self.num_workers)
--+        self.len=int(self.per_worker/num_samples)
--         self.class_weights=None
-- 
--         for i,path in enumerate(labelled):
--diff --git a/experiment/info.log b/experiment/info.log
--index ff2d606..e69de29 100644
----- a/experiment/info.log
--+++ b/experiment/info.log
--@@ -1,133 +0,0 @@
---[INFO] - shuffling train set
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.009261935523740748
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f596c78a3d0>" 
---will be used during training (effective maximum steps = 80) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---last_epoch: -1
---max_steps: 80
---)
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          80.85       4.94       9.30       1540
---! (label_id: 1)                                          0.00       0.00       0.00          2
---, (label_id: 2)                                          7.69       1.41       2.38        142
---- (label_id: 3)                                          1.67      20.00       3.08         20
---. (label_id: 4)                                          6.82      12.50       8.82         96
---: (label_id: 5)                                          0.00       0.00       0.00          0
---; (label_id: 6)                                          0.00       0.00       0.00          0
---? (label_id: 7)                                          0.00       0.00       0.00          4
---— (label_id: 8)                                          0.00       0.00       0.00          0
---… (label_id: 9)                                          0.00       0.00       0.00          0
----------------------
---micro avg                                                5.21       5.21       5.21       1804
---macro avg                                               16.17       6.47       3.93       1804
---weighted avg                                            70.01       5.21       8.63       1804
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00         20
----------------------
---micro avg                                              100.00     100.00     100.00         20
---macro avg                                              100.00     100.00     100.00         20
---weighted avg                                           100.00     100.00     100.00         20
---
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.0024506370946974477
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f596c570f40>" 
---will be used during training (effective maximum steps = 2000) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---last_epoch: -1
---max_steps: 2000
---)
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          80.85       4.94       9.30       1540
---! (label_id: 1)                                          0.00       0.00       0.00          2
---, (label_id: 2)                                          7.69       1.41       2.38        142
---- (label_id: 3)                                          1.67      20.00       3.08         20
---. (label_id: 4)                                          6.82      12.50       8.82         96
---: (label_id: 5)                                          0.00       0.00       0.00          0
---; (label_id: 6)                                          0.00       0.00       0.00          0
---? (label_id: 7)                                          0.00       0.00       0.00          4
---— (label_id: 8)                                          0.00       0.00       0.00          0
---… (label_id: 9)                                          0.00       0.00       0.00          0
----------------------
---micro avg                                                5.21       5.21       5.21       1804
---macro avg                                               16.17       6.47       3.93       1804
---weighted avg                                            70.01       5.21       8.63       1804
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00         20
----------------------
---micro avg                                              100.00     100.00     100.00         20
---macro avg                                              100.00     100.00     100.00         20
---weighted avg                                           100.00     100.00     100.00         20
---
---NFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          90.44      95.02      92.67     179900
---! (label_id: 1)                                          0.00       0.00       0.00         52
---, (label_id: 2)                                         21.92       9.58      13.34      13074
---- (label_id: 3)                                         44.94       6.69      11.64       1062
---. (label_id: 4)                                         28.64      30.78      29.67      11077
---: (label_id: 5)                                          0.00       0.00       0.00        330
---; (label_id: 6)                                          0.00       0.00       0.00        108
---? (label_id: 7)                                          0.00       0.00       0.00        899
---— (label_id: 8)                                          0.00       0.00       0.00        276
---… (label_id: 9)                                          0.00       0.00       0.00         28
----------------------
---micro avg                                               84.95      84.95      84.95     206806
---macro avg                                               18.59      14.21      14.73     206806
---weighted avg                                            81.82      84.95      83.11     206806
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1959
----------------------
---micro avg                                              100.00     100.00     100.00       1959
---macro avg                                              100.00     100.00     100.00       1959
---weighted avg                                           100.00     100.00     100.00       1959
---
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          90.62      94.69      92.61     179900
---! (label_id: 1)                                          0.00       0.00       0.00         52
---, (label_id: 2)                                         21.80      10.36      14.04      13074
---- (label_id: 3)                                         44.94       6.69      11.64       1062
---. (label_id: 4)                                         28.46      32.05      30.15      11077
---: (label_id: 5)                                          0.00       0.00       0.00        330
---; (label_id: 6)                                          0.00       0.00       0.00        108
---? (label_id: 7)                                          0.00       0.00       0.00        899
---— (label_id: 8)                                          0.00       0.00       0.00        276
---… (label_id: 9)                                          0.00       0.00       0.00         28
----------------------
---micro avg                                               84.77      84.77      84.77     206806
---macro avg                                               18.58      14.38      14.84     206806
---weighted avg                                            81.97      84.77      83.12     206806
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       1959
----------------------
---micro avg                                              100.00     100.00     100.00       1959
---macro avg                                              100.00     100.00     100.00       1959
---weighted avg                                           100.00     100.00     100.00       1959
---
--diff --git a/experiment/main.py b/experiment/main.py
--index 6b15e25..1dd39ff 100644
----- a/experiment/main.py
--+++ b/experiment/main.py
--@@ -37,9 +37,27 @@ def main(cfg: DictConfig)->None:
--     exp_manager(trainer, cfg.exp_manager)
--     model = PunctuationDomainModel(cfg=cfg, trainer=trainer, data_id = data_id)
--     
--+    lr_finder_dm=PunctuationDataModule(
--+            tokenizer= cfg.model.transformer_path,
--+            labelled= list(cfg.model.dataset.labelled),
--+            unlabelled= list(cfg.model.dataset.unlabelled),
--+            punct_label_ids= {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)},
--+            train_batch_size= cfg.model.dataset.train_ds.batch_size,
--+            max_seq_length= cfg.model.dataset.max_seq_length,
--+            val_batch_size= cfg.model.dataset.validation_ds.batch_size,
--+            num_workers= 1,
--+            pin_memory= False,
--+            train_shuffle= False,
--+            val_shuffle= False,
--+            seed=cfg.seed,
--+            data_id=data_id,
--+            tmp_path=cfg.tmp_path,
--+            test_unlabelled=False,
--+        )
--+
--     while(model.hparams.model.unfrozen<=cfg.model.maximum_unfrozen and model.hparams.model.unfrozen>=0):
--         trainer.current_epoch=0
---        lr_finder = trainer.tuner.lr_find(model,min_lr=1e-8, max_lr=0.5, num_training=80) #, early_stop_threshold=None
--+        lr_finder = trainer.tuner.lr_find(model,datamodule=lr_finder_dm,min_lr=1e-8, max_lr=0.5, num_training=80) #, early_stop_threshold=None
--         # Results can be found in
--         pp(lr_finder.results)
--         new_lr = lr_finder.suggestion()
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-01-34/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-09_16-01-34/hparams.yaml
-deleted file mode 100644
-index 4ff646f..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-01-34/hparams.yaml
-+++ /dev/null
-@@ -1,109 +0,0 @@
--seed: 42
--trainer:
--  gpus: 1
--  num_nodes: 1
--  max_epochs: 2
--  max_steps: null
--  accumulate_grad_batches: 4
--  gradient_clip_val: 0
--  amp_level: O1
--  precision: 16
--  accelerator: ddp
--  checkpoint_callback: false
--  logger: false
--  log_every_n_steps: 1
--  val_check_interval: 1.0
--  resume_from_checkpoint: null
--exp_manager:
--  exp_dir: /home/nxingyu/project/
--  name: Punctuation_with_Domain_discriminator
--  create_tensorboard_logger: true
--  create_checkpoint_callback: true
--base_path: /home/nxingyu/data
--tmp_path: /home/nxingyu/data/tmp
--model:
--  nemo_path: null
--  transformer_path: google/electra-small-discriminator
--  unfrozen: 0
--  maximum_unfrozen: 1
--  unfreeze_step: 1
--  punct_label_ids:
--  - ''
--  - '!'
--  - ','
--  - '-'
--  - .
--  - ':'
--  - ;
--  - '?'
--  - —
--  - …
--  punct_class_weights: false
--  dataset:
--    data_dir: /home/nxingyu/data
--    labelled:
--    - /home/nxingyu/data/ted_talks_processed
--    unlabelled: null
--    max_seq_length: 128
--    pad_label: ''
--    ignore_extra_tokens: false
--    ignore_start_end: false
--    use_cache: false
--    num_workers: 4
--    pin_memory: true
--    drop_last: true
--    num_labels: 10
--    num_domains: 1
--    test_unlabelled: true
--    train_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--    validation_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--  tokenizer:
--    tokenizer_name: google/electra-small-discriminator
--    vocab_file: null
--    tokenizer_model: null
--    special_tokens: null
--  language_model:
--    pretrained_model_name: google/electra-small-discriminator
--    lm_checkpoint: null
--    config_file: null
--    config: null
--  punct_head:
--    punct_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: dice
--  domain_head:
--    domain_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: cel
--    gamma: 0.1
--    pooling: mean_max
--    idx_conditioned_on: 0
--  dice_loss:
--    epsilon: 0.01
--    alpha: 3
--    macro_average: true
--  focal_loss:
--    gamma: 1
--  optim:
--    name: adamw
--    lr: 0.009261935523740748
--    weight_decay: 0.0
--    sched:
--      name: WarmupAnnealing
--      warmup_steps: null
--      warmup_ratio: 0.1
--      last_epoch: -1
--      monitor: val_loss
--      reduce_on_plateau: false
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-01-34/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-01-34/lightning_logs.txt
-deleted file mode 100644
-index ee72a83..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-01-34/lightning_logs.txt
-+++ /dev/null
-@@ -1,39 +0,0 @@
--Global seed set to 42
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
--Using native 16bit precision.
--Global seed set to 42
--initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 513   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--299 K     Trainable params
--13.2 M    Non-trainable params
--13.5 M    Total params
--Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-01-34/lr_find_temp_model.ckpt
--Global seed set to 42
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 513   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--299 K     Trainable params
--13.2 M    Non-trainable params
--13.5 M    Total params
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-01-34/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-01-34/nemo_error_log.txt
-deleted file mode 100644
-index efa9829..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-01-34/nemo_error_log.txt
-+++ /dev/null
-@@ -1,22 +0,0 @@
--[NeMo W 2021-02-09 16:01:34 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 16:01:48 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 16:01:48 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 16:01:49 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 16:03:31 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 16:03:31 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 16:03:32 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 16:03:34 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_16-01-34/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_16-01-34/nemo_log_globalrank-0_localrank-0.txt
-deleted file mode 100644
-index d093b07..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_16-01-34/nemo_log_globalrank-0_localrank-0.txt
-+++ /dev/null
-@@ -1,24 +0,0 @@
--[NeMo I 2021-02-09 16:01:34 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_16-01-34
--[NeMo I 2021-02-09 16:01:34 exp_manager:519] TensorboardLogger has been set up
--[NeMo W 2021-02-09 16:01:34 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 16:01:48 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 16:01:48 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 16:01:49 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 16:03:31 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 16:03:31 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 16:03:32 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 16:03:34 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
--      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/cmd-args.log
-deleted file mode 100644
-index 11a5d8e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/cmd-args.log
-+++ /dev/null
-@@ -1 +0,0 @@
--main.py
-\ No newline at end of file
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/events.out.tfevents.1612874369.intern-instance.13672.0 b/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/events.out.tfevents.1612874369.intern-instance.13672.0
-deleted file mode 100644
-index 100ff48..0000000
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/events.out.tfevents.1612874369.intern-instance.13672.0 and /dev/null differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/git-info.log
-deleted file mode 100644
-index 8e57986..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/git-info.log
-+++ /dev/null
-@@ -1,510 +0,0 @@
--commit hash: 5bbbaef9beee82324f41576fa8eca0ad2c24e5ae
--diff --git a/experiment/Untitled.ipynb b/experiment/Untitled.ipynb
--deleted file mode 100644
--index f5bb089..0000000
----- a/experiment/Untitled.ipynb
--+++ /dev/null
--@@ -1,136 +0,0 @@
---{
--- "cells": [
---  {
---   "cell_type": "code",
---   "execution_count": 1,
---   "id": "modern-amplifier",
---   "metadata": {},
---   "outputs": [
---    {
---     "name": "stderr",
---     "output_type": "stream",
---     "text": [
---      "14:59:48.20 LOG:\n"
---     ]
---    },
---    {
---     "name": "stdout",
---     "output_type": "stream",
---     "text": [
---      "shuffling <data.punctuation_dataset_multi.PunctuationDomainDataset object at 0x7f44ea05e5b0>\n"
---     ]
---    },
---    {
---     "name": "stderr",
---     "output_type": "stream",
---     "text": [
---      "14:59:48.28 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
---      "14:59:48.34 LOG:\n",
---      "14:59:48.63 .... f\"1st {n} encoder layers of transformer frozen\" = '1st 11 encoder layers of transformer frozen'\n",
---      "GPU available: True, used: False\n",
---      "TPU available: None, using: 0 TPU cores\n",
---      "[NeMo W 2021-02-09 14:59:48 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
---      "      warnings.warn(*args, **kwargs)\n",
---      "    \n"
---     ]
---    }
---   ],
---   "source": [
---    "import hydra\n",
---    "import numpy as np\n",
---    "import pytorch_lightning as pl\n",
---    "import torch\n",
---    "from omegaconf import DictConfig, OmegaConf\n",
---    "from transformers import AutoTokenizer\n",
---    "\n",
---    "from data import PunctuationDataModule, PunctuationInferenceDataset\n",
---    "import os\n",
---    "from models import PunctuationDomainModel\n",
---    "\n",
---    "from nemo.utils.exp_manager import exp_manager\n",
---    "from time import time\n",
---    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
---    "\n",
---    "import atexit\n",
---    "from copy import deepcopy\n",
---    "import snoop\n",
---    "snoop.install()\n",
---    "\n",
---    "from hydra.experimental import initialize, initialize_config_module, initialize_config_dir, compose\n",
---    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
---    "initialize()\n",
---    "cfg=compose(\n",
---    "    config_name=\"test_config.yaml\", \n",
---    ")\n",
---    "cfg.model.punct_label_ids=OmegaConf.create(sorted(cfg.model.punct_label_ids))\n",
---    "labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
---    "ids_to_labels = {_[0]:_[1] for _ in enumerate(cfg.model.punct_label_ids)}\n",
---    "\n",
---    "model = PunctuationDomainModel.load_from_checkpoint(\n",
---    "    checkpoint_path=\"/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_14-05-14/checkpoints/Punctuation_with_Domain_discriminator-last.ckpt\")\n",
---    "\n",
---    "trainer = pl.Trainer(**cfg.trainer)"
---   ]
---  },
---  {
---   "cell_type": "code",
---   "execution_count": 8,
---   "id": "hairy-proxy",
---   "metadata": {},
---   "outputs": [
---    {
---     "data": {
---      "text/plain": [
---       "[' we bought four shirts, one pen, and a mug from the nvidia gear store in santa clara                                                                                                            ',\n",
---       " ' what can i do for you today?                                                                                                                        ',\n",
---       " ' , how are you? ,                                                                                                                           ',\n",
---       " ' good morning. everyone? how have your weekends been? its a really great day?                                                                                                                  ']"
---      ]
---     },
---     "execution_count": 8,
---     "metadata": {},
---     "output_type": "execute_result"
---    }
---   ],
---   "source": [
---    "queries = [\n",
---    "    'we bought four shirts one pen and a mug from the nvidia gear store in santa clara',\n",
---    "    'what can i do for you today',\n",
---    "    'how are you',\n",
---    "    'good morning everyone how have your weekends been its a really great day'\n",
---    "]\n",
---    "inference_results = model.add_punctuation(queries)\n",
---    "inference_results"
---   ]
---  },
---  {
---   "cell_type": "code",
---   "execution_count": null,
---   "id": "employed-station",
---   "metadata": {},
---   "outputs": [],
---   "source": []
---  }
--- ],
--- "metadata": {
---  "kernelspec": {
---   "display_name": "Python 3",
---   "language": "python",
---   "name": "python3"
---  },
---  "language_info": {
---   "codemirror_mode": {
---    "name": "ipython",
---    "version": 3
---   },
---   "file_extension": ".py",
---   "mimetype": "text/x-python",
---   "name": "python",
---   "nbconvert_exporter": "python",
---   "pygments_lexer": "ipython3",
---   "version": "3.8.5"
---  }
--- },
--- "nbformat": 4,
--- "nbformat_minor": 5
---}
--diff --git a/experiment/config.yaml b/experiment/config.yaml
--index 8226922..6b576c7 100644
----- a/experiment/config.yaml
--+++ b/experiment/config.yaml
--@@ -1,43 +1,43 @@
-- seed: 42
-- trainer:
---    gpus: 1 # the number of gpus, 0 for CPU
--+    # gpus: 1 # the number of gpus, 0 for CPU
--+    # num_nodes: 1
--+    # max_epochs: 15
--+    # max_steps: null # precedence over max_epochs
--+    # accumulate_grad_batches: 4 # accumulates grads every k batches
--+    # gradient_clip_val: 0
--+    # amp_level: O1 # O1/O2 for mixed precision
--+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--+    # accelerator: ddp
--+    # checkpoint_callback: false  # Provided by exp_manager
--+    # logger: false #false  # Provided by exp_manager
--+    # log_every_n_steps: 1  # Interval of logging.
--+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--+    # resume_from_checkpoint: null
--+
--+    gpus: 0 # the number of gpus, 0 for CPU
--     num_nodes: 1
--     max_epochs: 15
--     max_steps: null # precedence over max_epochs
--     accumulate_grad_batches: 4 # accumulates grads every k batches
--     gradient_clip_val: 0
---    amp_level: O1 # O1/O2 for mixed precision
---    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
---    accelerator: ddp
--+    amp_level: O0 # O1/O2 for mixed precision
--+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--+    # accelerator: ddp
--     checkpoint_callback: false  # Provided by exp_manager
--     logger: false #false  # Provided by exp_manager
--     log_every_n_steps: 1  # Interval of logging.
--     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--+    reload_dataloaders_every_epoch: true
--     resume_from_checkpoint: null
-- 
---    # gpus: 0 # the number of gpus, 0 for CPU
---    # num_nodes: 1
---    # max_epochs: 8
---    # max_steps: null # precedence over max_epochs
---    # accumulate_grad_batches: 1 # accumulates grads every k batches
---    # gradient_clip_val: 0
---    # amp_level: O0 # O1/O2 for mixed precision
---    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
---    # # accelerator: ddp
---    # checkpoint_callback: false  # Provided by exp_manager
---    # logger: false #false  # Provided by exp_manager
---    # log_every_n_steps: 1  # Interval of logging.
---    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
---    # reload_dataloaders_every_epoch: true
---    # resume_from_checkpoint: null
---
-- exp_manager:
---    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
--+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
--     name: Punctuation_with_Domain_discriminator  # The name of your model
--     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
--     create_checkpoint_callback: true 
---base_path: /home/nxingyu/data # /root/data # 
---tmp_path: /home/nxingyu/data/tmp # /tmp # 
--+base_path: /home/nxingyu2/data # /root/data # 
--+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
-- 
-- model:
--     nemo_path: null
--@@ -61,7 +61,7 @@ model:
--     punct_class_weights: false
--     
--     dataset:
---        data_dir: /home/nxingyu/data # /root/data # 
--+        data_dir: /home/nxingyu2/data # /root/data # 
--         labelled:
--             # - ${base_path}/ted_talks_processed #
--             - ${base_path}/open_subtitles_processed #  
--@@ -81,6 +81,7 @@ model:
--         num_labels: 10
--         num_domains: 1
--         test_unlabelled: true
--+        attach_label_to_end: false # false if attach to start
-- 
--         train_ds:
--             shuffle: true
--diff --git a/experiment/core/utils.py b/experiment/core/utils.py
--index ce7436b..a2a708d 100644
----- a/experiment/core/utils.py
--+++ b/experiment/core/utils.py
--@@ -92,19 +92,19 @@ def subword_tokenize(tokenizer,tokens):
--     subwords = list(map(tokenizer.tokenize, tokens))
--     subword_lengths = list(map(len, subwords))
--     subwords = list(flatten(subwords))
---    # token_start_idxs = np.cumsum([0]+subword_lengths[:-1])
--+    token_start_idxs = np.cumsum([0]+subword_lengths[:-1])
--     token_end_idxs = np.cumsum([0]+subword_lengths[:-1])+np.array(subword_lengths)-1
---    return subwords, token_end_idxs
--+    return subwords, token_start_idxs,token_end_idxs
-- 
---def chunk_to_len(max_seq_length,tokenizer,tokens,labels=None):
---    subwords,token_end_idxs = subword_tokenize(tokenizer,tokens)
---    teim=token_end_idxs%(max_seq_length-2)
--+def chunk_to_len(max_seq_length,tokenizer,attach_label_to_end,tokens,labels=None):
--+    subwords,token_start_idxs,token_end_idxs = subword_tokenize(tokenizer,tokens)
--+    teim=token_end_idxs%(max_seq_length-2) if attach_label_to_end else token_start_idxs%(max_seq_length-2)
--     breakpoints=(np.argwhere(teim[1:]<teim[:-1]).flatten()+1).tolist()
---    split_token_end_idxs=np.array_split(token_end_idxs,breakpoints)
--+    split_token_idxs=np.array_split(token_end_idxs,breakpoints) if attach_label_to_end else np.array_split(token_start_idxs,breakpoints)
--     split_subwords=np.array_split(subwords,np.arange(max_seq_length-2,len(subwords),max_seq_length-2))
--     ids=[pad_to_len(max_seq_length,tokenizer.convert_tokens_to_ids(['[CLS]']+list(_)+['[SEP]'])) for _ in split_subwords]
--     masks=[]
---    for _ in split_token_end_idxs:
--+    for _ in split_token_idxs:
--         masks.append(position_to_mask(max_seq_length,_).copy())
--     padded_labels=None
--     if labels!=None:
--@@ -112,12 +112,12 @@ def chunk_to_len(max_seq_length,tokenizer,tokens,labels=None):
--         padded_labels=[pad_to_len(max_seq_length,align_labels_to_mask(*_)) for _ in zip(masks,split_labels)]
--     return ids,masks,padded_labels
--     
---def chunk_to_len_batch(max_seq_length,tokenizer,tokens,labels=None,labelled=True,ignore_index=-100):
--+def chunk_to_len_batch(max_seq_length,tokenizer,tokens,labels=None,labelled=True,ignore_index=-100, attach_label_to_end=True):
--     batch_ids=[]
--     batch_masks=[]
--     batch_labels=[]
--     for i,_ in enumerate(zip(tokens,tokens) if labels==None else zip(tokens,labels)):
---        a,b,c=chunk_to_len(max_seq_length,tokenizer,*_) if labels else chunk_to_len(max_seq_length,tokenizer,_[0])
--+        a,b,c=chunk_to_len(max_seq_length,tokenizer,attach_label_to_end,*_) if labels else chunk_to_len(max_seq_length,tokenizer,attach_label_to_end,_[0])
--         batch_ids.extend(a)
--         batch_masks.extend(b)
--         if labelled==True:
--diff --git a/experiment/data/punctuation_datamodule.py b/experiment/data/punctuation_datamodule.py
--index 8711456..fb69299 100644
----- a/experiment/data/punctuation_datamodule.py
--+++ b/experiment/data/punctuation_datamodule.py
--@@ -24,7 +24,8 @@ class PunctuationDataModule(LightningDataModule):
--             seed: int = 42,
--             data_id: str = '',
--             tmp_path:str = '~/data/tmp',
---            test_unlabelled:bool = True
--+            test_unlabelled:bool = True,
--+            attach_label_to_end:bool = True,
--             ):
--         #unlabelled=[], batch_size = 256, max_seq_length = 256, num_workers=1):
--         super().__init__()
--@@ -52,6 +53,7 @@ class PunctuationDataModule(LightningDataModule):
--         self.data_id=data_id
--         self.tmp_path=tmp_path
--         self.test_unlabelled=test_unlabelled
--+        self.attach_label_to_end=attach_label_to_end
--     
--     def reset(self):
--         # self.setup('fit')
--@@ -70,7 +72,8 @@ class PunctuationDataModule(LightningDataModule):
--                     tokenizer=self.tokenizer,
--                     randomize=self.train_shuffle,
--                     data_id=self.data_id,
---                    tmp_path=self.tmp_path)
--+                    tmp_path=self.tmp_path,
--+                    attach_label_to_end=self.attach_label_to_end)
--             self.val_dataset = PunctuationDomainDatasets(split='dev',
--                     num_samples=self.val_batch_size,
--                     max_seq_length=self.max_seq_length,
--@@ -80,7 +83,8 @@ class PunctuationDataModule(LightningDataModule):
--                     tokenizer=self.tokenizer,
--                     randomize=self.val_shuffle,
--                     data_id=self.data_id,
---                    tmp_path=self.tmp_path)
--+                    tmp_path=self.tmp_path,
--+                    attach_label_to_end=self.attach_label_to_end)
--         if stage=='test' or stage is None:
--             if (len(self.unlabelled)>0) and self.test_unlabelled:
--                 self.test_dataset = PunctuationDomainDatasets(split='test',
--@@ -92,7 +96,8 @@ class PunctuationDataModule(LightningDataModule):
--                     tokenizer=self.tokenizer,
--                     randomize=self.val_shuffle,
--                     data_id=self.data_id,
---                    tmp_path=self.tmp_path
--+                    tmp_path=self.tmp_path,
--+                    attach_label_to_end=self.attach_label_to_end
--                     )
--             else: self.test_dataset = PunctuationDomainDatasets(split='test',
--                     num_samples=self.val_batch_size,
--@@ -103,7 +108,8 @@ class PunctuationDataModule(LightningDataModule):
--                     tokenizer=self.tokenizer,
--                     randomize=self.val_shuffle,
--                     data_id=self.data_id,
---                    tmp_path=self.tmp_path
--+                    tmp_path=self.tmp_path,
--+                    attach_label_to_end=self.attach_label_to_end
--                     )
-- 
--         logging.info(f"shuffling train set")
--diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
--index 31923de..6a0452f 100644
----- a/experiment/data/punctuation_dataset_multi.py
--+++ b/experiment/data/punctuation_dataset_multi.py
--@@ -39,6 +39,7 @@ class PunctuationDomainDataset(IterableDataset):
--         tmp_path='~/data/tmp',
--         start=0,
--         end=-1,
--+        attach_label_to_end=True,
--     ):
--         if not (os.path.exists(csv_file)):
--             raise FileNotFoundError(
--@@ -63,6 +64,7 @@ class PunctuationDomainDataset(IterableDataset):
--         self.randomize=randomize
--         self.target_file=target_file
--         self.tmp_path=tmp_path
--+        self.attach_label_to_end=attach_label_to_end
--         if not (os.path.exists(self.target_file)):
--             os.system(f'cp {self.csv_file} {self.target_file}')
-- 
--@@ -152,6 +154,7 @@ class PunctuationDomainDatasets(IterableDataset):
--                  randomize:bool=True,
--                  data_id='',
--                  tmp_path='~/data/tmp',
--+                 attach_label_to_end=True,
--                  ):
--         worker_info = get_worker_info()
--         self.num_workers=1 if worker_info is None else worker_info.num_workers
--@@ -177,7 +180,8 @@ class PunctuationDomainDatasets(IterableDataset):
--                     punct_label_ids=punct_label_ids,domain=i,labelled=True,
--                     randomize=randomize,
--                     target_file=f'{target}.{split}.{data_id}.csv',
---                    tmp_path=tmp_path)
--+                    tmp_path=tmp_path,
--+                    attach_label_to_end=attach_label_to_end)
--             self.datasets.append(dataset)
--             self.iterables.append(cycle(dataset))
--             
--@@ -189,7 +193,8 @@ class PunctuationDomainDatasets(IterableDataset):
--                     punct_label_ids=punct_label_ids,domain=len(labelled)+i,labelled=False,
--                     randomize=randomize,
--                     target_file=f'{target}.{split}.{data_id}.csv',
---                    tmp_path=tmp_path)
--+                    tmp_path=tmp_path,
--+                    attach_label_to_end=attach_label_to_end)
--             self.datasets.append(dataset)
--             self.iterables.append(cycle(dataset))
-- 
--@@ -261,12 +266,13 @@ class PunctuationInferenceDataset(Dataset):
--             "labels": NeuralType(('B', 'T'), ChannelType()),
--         }
-- 
---    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], num_samples:int=256, degree:int = 0):
--+    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], num_samples:int=256, degree:int = 0, attach_label_to_end:bool=True):
--         """ Initializes BertPunctuationInferDataset. """
--         self.degree=degree
--         self.punct_label_ids=punct_label_ids
--         chunked=chunk_examples_with_degree(self.degree, self.punct_label_ids)(queries)
--         self.features = chunk_to_len_batch(max_seq_length, tokenizer,chunked['texts'],chunked['tags'],True)
--+        self.attach_label_to_end=attach_label_to_end
--         # self.all_input_ids = features['input_ids']
--         # self.all_attention_mask = features['attention_mask']
--         # self.all_subtoken_mask = features['subtoken_mask']
--diff --git a/experiment/info.log b/experiment/info.log
--index a9aa9db..5024c4f 100644
----- a/experiment/info.log
--+++ b/experiment/info.log
--@@ -1,43 +1,2 @@
---[INFO] - shuffling train set
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.01
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.CosineAnnealing object at 0x7f68053f0940>" 
---will be used during training (effective maximum steps = 79575) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---min_lr: 1.0e-10
---last_epoch: -1
---max_steps: 79575
---)
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          80.97      20.28      32.44       9732
---! (label_id: 1)                                          1.45       2.63       1.87        152
---, (label_id: 2)                                          5.44      27.46       9.08        772
---- (label_id: 3)                                          1.70      53.57       3.30         56
---. (label_id: 4)                                          3.12       0.85       1.34       1642
---: (label_id: 5)                                          0.00       0.00       0.00          0
---; (label_id: 6)                                          0.00       0.00       0.00          0
---? (label_id: 7)                                          1.42      22.94       2.67        218
---— (label_id: 8)                                          0.00       0.00       0.00          0
---… (label_id: 9)                                          0.93       2.04       1.28         98
----------------------
---micro avg                                               18.04      18.04      18.04      12670
---macro avg                                               13.58      18.54       7.42      12670
---weighted avg                                            62.99      18.04      25.74      12670
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00        116
----------------------
---micro avg                                              100.00     100.00     100.00        116
---macro avg                                              100.00     100.00     100.00        116
---weighted avg                                           100.00     100.00     100.00        116
---
--+[INFO] - GPU available: True, used: False
--+[INFO] - TPU available: None, using: 0 TPU cores
--diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
--index 5b5d668..09eb9d5 100644
----- a/experiment/models/punctuation_domain_model.py
--+++ b/experiment/models/punctuation_domain_model.py
--@@ -514,7 +514,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
--             seed=self._cfg.seed,
--             data_id=self.data_id,
--             tmp_path=self.hparams.tmp_path,
---            test_unlabelled=data_config.test_unlabelled
--+            test_unlabelled=data_config.test_unlabelled,
--+            attach_label_to_end=data_config.attach_label_to_end,
--         )
--         self.dm.setup()
--         self._train_dl=self.dm.train_dataloader
--@@ -722,7 +723,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
--             tokenizer= self.tokenizer,
--             queries=queries, 
--             max_seq_length=self.hparams.model.dataset.max_seq_length,
---            punct_label_ids=self._cfg.model.punct_label_ids)
--+            punct_label_ids=self._cfg.model.punct_label_ids,
--+            attach_label_to_end=self._cfg.model.dataset.attach_label_to_end)
--         batch=ds[0]
--         attention_mask = batch['attention_mask']
--         subtoken_mask = batch['subtoken_mask']
--diff --git a/experiment/test_config.yaml b/experiment/test_config.yaml
--index a179869..0cc65e7 100644
----- a/experiment/test_config.yaml
--+++ b/experiment/test_config.yaml
--@@ -32,15 +32,16 @@ trainer:
--     resume_from_checkpoint: null
-- 
-- exp_manager:
---    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
--+    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
--     name: Punctuation_with_Domain_discriminator  # The name of your model
--     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
--     create_checkpoint_callback: true 
---    restore_path: /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/
---    override_config_path: /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/hparams.yaml
--+    restore_path: /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/
--+    # 2021-02-08_11-57-58
--+    override_config_path: /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/hparams.yaml
-- 
---base_path: /home/nxingyu/data # /root/data # 
---tmp_path: /home/nxingyu/data/tmp # /tmp # 
--+base_path: /home/nxingyu2/data # /root/data # 
--+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
-- 
-- model:
--     nemo_path: null
--@@ -84,6 +85,7 @@ model:
--         num_labels: 10
--         num_domains: 2
--         test_unlabelled: true
--+        attach_label_to_end: false
-- 
--         train_ds:
--             shuffle: true
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/hparams.yaml
-deleted file mode 100644
-index 1248677..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/hparams.yaml
-+++ /dev/null
-@@ -1,111 +0,0 @@
--seed: 42
--trainer:
--  gpus: 0
--  num_nodes: 1
--  max_epochs: 15
--  max_steps: null
--  accumulate_grad_batches: 4
--  gradient_clip_val: 0
--  amp_level: O0
--  precision: 32
--  checkpoint_callback: false
--  logger: false
--  log_every_n_steps: 1
--  val_check_interval: 1.0
--  reload_dataloaders_every_epoch: true
--  resume_from_checkpoint: null
--exp_manager:
--  exp_dir: /home/nxingyu2/project/
--  name: Punctuation_with_Domain_discriminator
--  create_tensorboard_logger: true
--  create_checkpoint_callback: true
--base_path: /home/nxingyu2/data
--tmp_path: /home/nxingyu2/data/tmp
--model:
--  nemo_path: null
--  transformer_path: google/electra-small-discriminator
--  unfrozen: 0
--  maximum_unfrozen: 1
--  unfreeze_step: 1
--  punct_label_ids:
--  - ''
--  - '!'
--  - ','
--  - '-'
--  - .
--  - ':'
--  - ;
--  - '?'
--  - —
--  - …
--  punct_class_weights: false
--  dataset:
--    data_dir: /home/nxingyu2/data
--    labelled:
--    - /home/nxingyu2/data/open_subtitles_processed
--    unlabelled: null
--    max_seq_length: 128
--    pad_label: ''
--    ignore_extra_tokens: false
--    ignore_start_end: false
--    use_cache: false
--    num_workers: 4
--    pin_memory: true
--    drop_last: true
--    num_labels: 10
--    num_domains: 1
--    test_unlabelled: true
--    attach_label_to_end: false
--    train_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--    validation_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--  tokenizer:
--    tokenizer_name: google/electra-small-discriminator
--    vocab_file: null
--    tokenizer_model: null
--    special_tokens: null
--  language_model:
--    pretrained_model_name: google/electra-small-discriminator
--    lm_checkpoint: null
--    config_file: null
--    config: null
--  punct_head:
--    punct_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: dice
--  domain_head:
--    domain_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: cel
--    gamma: 0
--    pooling: mean_max
--    idx_conditioned_on: 0
--  dice_loss:
--    epsilon: 0.01
--    alpha: 3
--    macro_average: true
--  focal_loss:
--    gamma: 1
--  optim:
--    name: adamw
--    lr: 0.01
--    weight_decay: 0.0
--    sched:
--      name: CosineAnnealing
--      warmup_steps: null
--      warmup_ratio: 0.1
--      min_lr: 1.0e-10
--      last_epoch: -1
--      monitor: val_loss
--      reduce_on_plateau: false
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/lightning_logs.txt
-deleted file mode 100644
-index ccedce8..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/lightning_logs.txt
-+++ /dev/null
-@@ -1,18 +0,0 @@
--GPU available: True, used: False
--TPU available: None, using: 0 TPU cores
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 513   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--299 K     Trainable params
--13.2 M    Non-trainable params
--13.5 M    Total params
--Saving latest checkpoint...
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/nemo_error_log.txt
-deleted file mode 100644
-index 0eabc81..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/nemo_error_log.txt
-+++ /dev/null
-@@ -1,10 +0,0 @@
--[NeMo W 2021-02-09 20:39:07 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 20:39:07 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 20:39:29 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 20:57:17 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
--      warnings.warn(*args, **kwargs)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/nemo_log_globalrank-0_localrank-0.txt
-deleted file mode 100644
-index 31c2f88..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/nemo_log_globalrank-0_localrank-0.txt
-+++ /dev/null
-@@ -1,12 +0,0 @@
--[NeMo W 2021-02-09 20:39:07 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo I 2021-02-09 20:39:07 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07
--[NeMo I 2021-02-09 20:39:07 exp_manager:519] TensorboardLogger has been set up
--[NeMo W 2021-02-09 20:39:07 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 20:39:29 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 20:57:17 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
--      warnings.warn(*args, **kwargs)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_21-18-09/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-09_21-18-09/cmd-args.log
-deleted file mode 100644
-index 11a5d8e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_21-18-09/cmd-args.log
-+++ /dev/null
-@@ -1 +0,0 @@
--main.py
-\ No newline at end of file
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_21-18-09/events.out.tfevents.1612876711.intern-instance.19148.0 b/Punctuation_with_Domain_discriminator/2021-02-09_21-18-09/events.out.tfevents.1612876711.intern-instance.19148.0
-index 6d97ad2..4737dd0 100644
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_21-18-09/events.out.tfevents.1612876711.intern-instance.19148.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_21-18-09/events.out.tfevents.1612876711.intern-instance.19148.0 differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_21-18-09/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-09_21-18-09/git-info.log
-deleted file mode 100644
-index 7e00cfa..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_21-18-09/git-info.log
-+++ /dev/null
-@@ -1,266 +0,0 @@
--commit hash: bda9e1c0f8f25984dc81e533fc76c3b6370a5da5
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/events.out.tfevents.1612874369.intern-instance.13672.0 b/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/events.out.tfevents.1612874369.intern-instance.13672.0
--index 27ae516..100ff48 100644
--Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/events.out.tfevents.1612874369.intern-instance.13672.0 and b/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/events.out.tfevents.1612874369.intern-instance.13672.0 differ
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/lightning_logs.txt
--index fbc2948..ccedce8 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/lightning_logs.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/lightning_logs.txt
--@@ -15,3 +15,4 @@ TPU available: None, using: 0 TPU cores
-- 299 K     Trainable params
-- 13.2 M    Non-trainable params
-- 13.5 M    Total params
--+Saving latest checkpoint...
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/nemo_error_log.txt
--index 8ba301e..0eabc81 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/nemo_error_log.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/nemo_error_log.txt
--@@ -5,3 +5,6 @@
-- [NeMo W 2021-02-09 20:39:29 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--       warnings.warn(*args, **kwargs)
--     
--+[NeMo W 2021-02-09 20:57:17 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
--+      warnings.warn(*args, **kwargs)
--+    
--diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/nemo_log_globalrank-0_localrank-0.txt
--index 597f1fb..31c2f88 100644
----- a/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/nemo_log_globalrank-0_localrank-0.txt
--+++ b/Punctuation_with_Domain_discriminator/2021-02-09_20-39-07/nemo_log_globalrank-0_localrank-0.txt
--@@ -7,3 +7,6 @@
-- [NeMo W 2021-02-09 20:39:29 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--       warnings.warn(*args, **kwargs)
--     
--+[NeMo W 2021-02-09 20:57:17 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
--+      warnings.warn(*args, **kwargs)
--+    
--diff --git a/experiment/Inference.ipynb b/experiment/Inference.ipynb
--index 57ad8dc..99790f8 100644
----- a/experiment/Inference.ipynb
--+++ b/experiment/Inference.ipynb
--@@ -2,7 +2,7 @@
--  "cells": [
--   {
--    "cell_type": "code",
---   "execution_count": 2,
--+   "execution_count": 1,
--    "id": "modern-amplifier",
--    "metadata": {},
--    "outputs": [
--@@ -10,27 +10,7 @@
--      "output_type": "stream",
--      "name": "stdout",
--      "text": [
---      "/home/nxingyu2/project/experiment\n",
---      "shuffling <data.punctuation_dataset_multi.PunctuationDomainDataset object at 0x7f3fca1e4c40>\n",
---      "20:36:28.84 LOG:\n",
---      "20:36:28.90 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
---      "20:36:29.05 LOG:\n",
---      "20:36:29.16 .... f\"1st {n} encoder layers of transformer frozen\" = '1st 12 encoder layers of transformer frozen'\n"
---     ]
---    },
---    {
---     "output_type": "error",
---     "ename": "RuntimeError",
---     "evalue": "Error(s) in loading state_dict for PunctuationDomainModel:\n\tUnexpected key(s) in state_dict: \"punctuation_loss.weight\". ",
---     "traceback": [
---      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
---      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
---      "\u001b[0;32m<ipython-input-2-2cd77a8adcc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mids_to_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunct_label_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m model = PunctuationDomainModel.load_from_checkpoint(\n\u001b[0m\u001b[1;32m     34\u001b[0m     checkpoint_path=\"/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator-last.ckpt\")\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
---      "\u001b[0;32m~/project/experiment/models/punctuation_domain_model.py\u001b[0m in \u001b[0;36mload_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_model_restore_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_being_restored\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             checkpoint = super().load_from_checkpoint(\n\u001b[0m\u001b[1;32m    632\u001b[0m                 \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
---      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/core/saving.py\u001b[0m in \u001b[0;36mload_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHECKPOINT_HYPER_PARAMS_KEY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_model_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
---      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/core/saving.py\u001b[0m in \u001b[0;36m_load_model_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# load the state_dict on the model automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
---      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1052\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1053\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
---      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PunctuationDomainModel:\n\tUnexpected key(s) in state_dict: \"punctuation_loss.weight\". "
--+      "/home/nxingyu2/project/experiment\n"
--      ]
--     }
--    ],
--@@ -67,10 +47,81 @@
--     "labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
--     "ids_to_labels = {_[0]:_[1] for _ in enumerate(cfg.model.punct_label_ids)}\n",
--     "\n",
---    "model = PunctuationDomainModel.load_from_checkpoint(\n",
---    "    checkpoint_path=\"/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator-last.ckpt\")\n",
--+    "# model = PunctuationDomainModel.load_from_checkpoint(\n",
--+    "#     checkpoint_path=\"/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator-last.ckpt\")\n",
--     "\n",
---    "trainer = pl.Trainer(**cfg.trainer)"
--+    "# trainer = pl.Trainer(**cfg.trainer)"
--+   ]
--+  },
--+  {
--+   "cell_type": "code",
--+   "execution_count": 11,
--+   "metadata": {},
--+   "outputs": [
--+    {
--+     "output_type": "execute_result",
--+     "data": {
--+      "text/plain": [
--+       "{'input_ids': tensor([[ 101, 7570, 9541, 9541, 9541,  102,    0,    0,    0,    0,    0,    0,\n",
--+       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
--+       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
--+       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
--+       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
--+       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
--+       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
--+       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
--+       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
--+       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
--+       "             0,    0,    0,    0,    0,    0,    0,    0]]),\n",
--+       " 'attention_mask': tensor([[ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
--+       "          False, False, False, False, False, False, False, False, False, False,\n",
--+       "          False, False, False, False, False, False, False, False, False, False,\n",
--+       "          False, False, False, False, False, False, False, False, False, False,\n",
--+       "          False, False, False, False, False, False, False, False, False, False,\n",
--+       "          False, False, False, False, False, False, False, False, False, False,\n",
--+       "          False, False, False, False, False, False, False, False, False, False,\n",
--+       "          False, False, False, False, False, False, False, False, False, False,\n",
--+       "          False, False, False, False, False, False, False, False, False, False,\n",
--+       "          False, False, False, False, False, False, False, False, False, False,\n",
--+       "          False, False, False, False, False, False, False, False, False, False,\n",
--+       "          False, False, False, False, False, False, False, False, False, False,\n",
--+       "          False, False, False, False, False, False, False, False]]),\n",
--+       " 'subtoken_mask': tensor([[ True, False, False, False,  True,  True, False, False, False, False,\n",
--+       "          False, False, False, False, False, False, False, False, False, False,\n",
--+       "          False, False, False, False, False, False, False, False, False, False,\n",
--+       "          False, False, False, False, False, False, False, False, False, False,\n",
--+       "          False, False, False, False, False, False, False, False, False, False,\n",
--+       "          False, False, False, False, False, False, False, False, False, False,\n",
--+       "          False, False, False, False, False, False, False, False, False, False,\n",
--+       "          False, False, False, False, False, False, False, False, False, False,\n",
--+       "          False, False, False, False, False, False, False, False, False, False,\n",
--+       "          False, False, False, False, False, False, False, False, False, False,\n",
--+       "          False, False, False, False, False, False, False, False, False, False,\n",
--+       "          False, False, False, False, False, False, False, False, False, False,\n",
--+       "          False, False, False, False, False, False, False, False]]),\n",
--+       " 'labels': tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--+       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--+       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--+       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--+       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--+       "          0, 0, 0, 0, 0, 0, 0, 0]])}"
--+      ]
--+     },
--+     "metadata": {},
--+     "execution_count": 11
--+    }
--+   ],
--+   "source": [
--+    "queries = [\n",
--+    "    'Hooooooo!',\n",
--+    "    # 'what can i do for you today',\n",
--+    "    # 'how are you',\n",
--+    "    # 'good morning everyone how have your weekends been its a really great day'\n",
--+    "]\n",
--+    "cfg.model.punct_label_ids=OmegaConf.create(sorted(cfg.model.punct_label_ids))\n",
--+    "labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
--+    "ds=PunctuationInferenceDataset(tokenizer=AutoTokenizer.from_pretrained('google/electra-small-discriminator'), queries=queries, max_seq_length=128, punct_label_ids=labels_to_ids, degree=0, attach_label_to_end=True)\n",
--+    "ds[0]"
--    ]
--   },
--   {
--@@ -94,12 +145,7 @@
--     }
--    ],
--    "source": [
---    "queries = [\n",
---    "    'we bought four shirts one pen and a mug from the nvidia gear store in santa clara',\n",
---    "    'what can i do for you today',\n",
---    "    'how are you',\n",
---    "    'good morning everyone how have your weekends been its a really great day'\n",
---    "]\n",
--+    "\n",
--     "inference_results = model.add_punctuation(queries)\n",
--     "inference_results"
--    ]
--diff --git a/experiment/core/utils.py b/experiment/core/utils.py
--index a2a708d..c9470b8 100644
----- a/experiment/core/utils.py
--+++ b/experiment/core/utils.py
--@@ -99,6 +99,7 @@ def subword_tokenize(tokenizer,tokens):
-- def chunk_to_len(max_seq_length,tokenizer,attach_label_to_end,tokens,labels=None):
--     subwords,token_start_idxs,token_end_idxs = subword_tokenize(tokenizer,tokens)
--     teim=token_end_idxs%(max_seq_length-2) if attach_label_to_end else token_start_idxs%(max_seq_length-2)
--+
--     breakpoints=(np.argwhere(teim[1:]<teim[:-1]).flatten()+1).tolist()
--     split_token_idxs=np.array_split(token_end_idxs,breakpoints) if attach_label_to_end else np.array_split(token_start_idxs,breakpoints)
--     split_subwords=np.array_split(subwords,np.arange(max_seq_length-2,len(subwords),max_seq_length-2))
--diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
--index 6a0452f..d919d9c 100644
----- a/experiment/data/punctuation_dataset_multi.py
--+++ b/experiment/data/punctuation_dataset_multi.py
--@@ -271,7 +271,7 @@ class PunctuationInferenceDataset(Dataset):
--         self.degree=degree
--         self.punct_label_ids=punct_label_ids
--         chunked=chunk_examples_with_degree(self.degree, self.punct_label_ids)(queries)
---        self.features = chunk_to_len_batch(max_seq_length, tokenizer,chunked['texts'],chunked['tags'],True)
--+        self.features = chunk_to_len_batch(max_seq_length, tokenizer,chunked['texts'],chunked['tags'],attach_label_to_end=attach_label_to_end)
--         self.attach_label_to_end=attach_label_to_end
--         # self.all_input_ids = features['input_ids']
--         # self.all_attention_mask = features['attention_mask']
--diff --git a/experiment/info.log b/experiment/info.log
--index 695cb09..5024c4f 100644
----- a/experiment/info.log
--+++ b/experiment/info.log
--@@ -1,60 +1,2 @@
-- [INFO] - GPU available: True, used: False
-- [INFO] - TPU available: None, using: 0 TPU cores
---[INFO] - shuffling train set
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.01
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.CosineAnnealing object at 0x7fb7159543d0>" 
---will be used during training (effective maximum steps = 79575) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---min_lr: 1.0e-10
---last_epoch: -1
---max_steps: 79575
---)
---[INFO] - 
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 513   
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---299 K     Trainable params
---13.2 M    Non-trainable params
---13.5 M    Total params
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          80.99      20.22      32.36       9732
---! (label_id: 1)                                          1.45       2.63       1.87        152
---, (label_id: 2)                                          5.42      27.46       9.05        772
---- (label_id: 3)                                          1.70      53.57       3.30         56
---. (label_id: 4)                                          3.12       0.85       1.34       1642
---: (label_id: 5)                                          0.00       0.00       0.00          0
---; (label_id: 6)                                          0.00       0.00       0.00          0
---? (label_id: 7)                                          1.42      22.94       2.67        218
---— (label_id: 8)                                          0.00       0.00       0.00          0
---… (label_id: 9)                                          0.93       2.04       1.28         98
----------------------
---micro avg                                               18.00      18.00      18.00      12670
---macro avg                                               13.58      18.53       7.41      12670
---weighted avg                                            63.00      18.00      25.68      12670
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00        116
----------------------
---micro avg                                              100.00     100.00     100.00        116
---macro avg                                              100.00     100.00     100.00        116
---weighted avg                                           100.00     100.00     100.00        116
---
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_21-18-09/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_21-18-09/lightning_logs.txt
-deleted file mode 100644
-index fbc2948..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_21-18-09/lightning_logs.txt
-+++ /dev/null
-@@ -1,17 +0,0 @@
--GPU available: True, used: False
--TPU available: None, using: 0 TPU cores
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 13.5 M
--1 | punct_classifier    | TokenClassifier      | 2.6 K 
--2 | domain_classifier   | SequenceClassifier   | 513   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--299 K     Trainable params
--13.2 M    Non-trainable params
--13.5 M    Total params
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_21-18-09/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_21-18-09/nemo_error_log.txt
-deleted file mode 100644
-index 3885e21..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_21-18-09/nemo_error_log.txt
-+++ /dev/null
-@@ -1,7 +0,0 @@
--[NeMo W 2021-02-09 21:18:09 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-09 21:18:09 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 21:18:31 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_21-18-09/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_21-18-09/nemo_log_globalrank-0_localrank-0.txt
-deleted file mode 100644
-index 7726bf3..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-09_21-18-09/nemo_log_globalrank-0_localrank-0.txt
-+++ /dev/null
-@@ -1,9 +0,0 @@
--[NeMo W 2021-02-09 21:18:09 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo I 2021-02-09 21:18:09 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-09_21-18-09
--[NeMo I 2021-02-09 21:18:09 exp_manager:519] TensorboardLogger has been set up
--[NeMo W 2021-02-09 21:18:09 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-09 21:18:31 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index e9cfd19..2aa798f 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,43 +1,43 @@
- seed: 42
- trainer:
--    gpus: 1 # the number of gpus, 0 for CPU
--    num_nodes: 1
--    max_epochs: 15
--    max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 4 # accumulates grads every k batches
--    gradient_clip_val: 0
--    amp_level: O1 # O1/O2 for mixed precision
--    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    accelerator: ddp
--    checkpoint_callback: false  # Provided by exp_manager
--    logger: false #false  # Provided by exp_manager
--    log_every_n_steps: 1  # Interval of logging.
--    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    resume_from_checkpoint: null
--
--    # gpus: 0 # the number of gpus, 0 for CPU
-+    # gpus: 1 # the number of gpus, 0 for CPU
-     # num_nodes: 1
-     # max_epochs: 15
-     # max_steps: null # precedence over max_epochs
-     # accumulate_grad_batches: 4 # accumulates grads every k batches
-     # gradient_clip_val: 0
--    # amp_level: O0 # O1/O2 for mixed precision
--    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # # accelerator: ddp
-+    # amp_level: O1 # O1/O2 for mixed precision
-+    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-     # checkpoint_callback: false  # Provided by exp_manager
-     # logger: false #false  # Provided by exp_manager
-     # log_every_n_steps: 1  # Interval of logging.
-     # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # reload_dataloaders_every_epoch: true
-     # resume_from_checkpoint: null
- 
-+    gpus: 0 # the number of gpus, 0 for CPU
-+    num_nodes: 1
-+    max_epochs: 8
-+    max_steps: null # precedence over max_epochs
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-+    gradient_clip_val: 0
-+    amp_level: O0 # O1/O2 for mixed precision
-+    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # accelerator: ddp
-+    checkpoint_callback: false  # Provided by exp_manager
-+    logger: false #false  # Provided by exp_manager
-+    log_every_n_steps: 1  # Interval of logging.
-+    val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    reload_dataloaders_every_epoch: true
-+    resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /root/project # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu2/project/ # /root/project # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /root/data # /home/nxingyu2/data # 
--tmp_path: /tmp # /home/nxingyu2/data/tmp # 
-+base_path: /home/nxingyu2/data # /root/data # 
-+tmp_path: /home/nxingyu2/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
-@@ -61,10 +61,10 @@ model:
-     punct_class_weights: false
-     
-     dataset:
--        data_dir: /root/data # /home/nxingyu2/data # 
-+        data_dir: /home/nxingyu2/data # /root/data # 
-         labelled:
--            # - ${base_path}/ted_talks_processed #
--            - ${base_path}/open_subtitles_processed #  
-+            - ${base_path}/ted_talks_processed #
-+            # - ${base_path}/open_subtitles_processed #  
-         unlabelled:
-             # - ${base_path}/ted_talks_processed #
-             # - ${base_path}/open_subtitles_processed #  
-@@ -75,7 +75,7 @@ model:
-         ignore_start_end: false
-         use_cache: false
-         # shared among dataloaders
--        num_workers:  2
-+        num_workers:  4
-         pin_memory: true
-         drop_last: true
-         num_labels: 10
-@@ -166,16 +166,16 @@ hydra:
-             simple:
-                 format: '[%(levelname)s] - %(message)s'
-         handlers:
--          console:
--            class: logging.StreamHandler
--            formatter: simple
--            stream: ext://sys.stdout
--          file:
--            class: logging.FileHandler
--            level: INFO
--            formatter: simple
--            filename: info.log
--            encoding: utf8
--            mode: w
-+            console:
-+                class: logging.StreamHandler
-+                formatter: simple
-+                stream: ext://sys.stdout
-+            file:
-+                class: logging.FileHandler
-+                level: INFO
-+                formatter: simple
-+                filename: info.log
-+                encoding: utf8
-+                mode: w
-         root:
--          handlers: [console, file]
-+            handlers: [console, file]
-diff --git a/experiment/info.log b/experiment/info.log
-index b47e0d7..5024c4f 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,51 +1,2 @@
--[INFO] - Lock 139721997549824 acquired on /root/.cache/huggingface/transformers/ca13c16218c6780ec76753d3afa19fcb7cc759e3f63ee87e441562d374762b3d.0824000443bf1ea1b536f53cf8bb9bfaac5e0143ee58dd462bceafeab8f45020.lock
--[INFO] - Lock 139721997549824 released on /root/.cache/huggingface/transformers/ca13c16218c6780ec76753d3afa19fcb7cc759e3f63ee87e441562d374762b3d.0824000443bf1ea1b536f53cf8bb9bfaac5e0143ee58dd462bceafeab8f45020.lock
--[INFO] - Lock 139721997851520 acquired on /root/.cache/huggingface/transformers/1ebdea26ed1a6268cdf5d1fe36450e89c70e306c97d39e62ede8a31f1c43f9ad.baa63624f08a59503441bce3d427225c61fe79bfa9f6d4c30cde7d072d863e0c.lock
--[INFO] - Lock 139721997851520 released on /root/.cache/huggingface/transformers/1ebdea26ed1a6268cdf5d1fe36450e89c70e306c97d39e62ede8a31f1c43f9ad.baa63624f08a59503441bce3d427225c61fe79bfa9f6d4c30cde7d072d863e0c.lock
--[INFO] - Lock 139721996674480 acquired on /root/.cache/huggingface/transformers/ece45ade3e01224cf31fed8e183b306d17b84e8abd415363474cfe72274f7814.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock
--[INFO] - Lock 139721996674480 released on /root/.cache/huggingface/transformers/ece45ade3e01224cf31fed8e183b306d17b84e8abd415363474cfe72274f7814.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock
--[INFO] - Lock 139721996674672 acquired on /root/.cache/huggingface/transformers/92992b36de47dee64b1d5a31c05d8d51e3075b918a218f5ba4f6e306c4b81b8c.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock
--[INFO] - Lock 139721996674672 released on /root/.cache/huggingface/transformers/92992b36de47dee64b1d5a31c05d8d51e3075b918a218f5ba4f6e306c4b81b8c.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock
--[INFO] - shuffling train set
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.01
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.CosineAnnealing object at 0x7f13833a2a90>" 
--will be used during training (effective maximum steps = 79575) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--min_lr: 1.0e-10
--last_epoch: -1
--max_steps: 79575
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          80.91      20.20      32.33       9732
--! (label_id: 1)                                          1.45       2.63       1.87        152
--, (label_id: 2)                                          5.38      27.20       8.98        772
--- (label_id: 3)                                          1.70      53.57       3.30         56
--. (label_id: 4)                                          3.12       0.85       1.34       1642
--: (label_id: 5)                                          0.00       0.00       0.00          0
--; (label_id: 6)                                          0.00       0.00       0.00          0
--? (label_id: 7)                                          1.42      22.94       2.67        218
--— (label_id: 8)                                          0.00       0.00       0.00          0
--… (label_id: 9)                                          0.93       2.04       1.28         98
---------------------
--micro avg                                               17.96      17.96      17.96      12670
--macro avg                                               13.56      18.49       7.39      12670
--weighted avg                                            62.93      17.96      25.65      12670
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        116
---------------------
--micro avg                                              100.00     100.00     100.00        116
--macro avg                                              100.00     100.00     100.00        116
--weighted avg                                           100.00     100.00     100.00        116
--
-+[INFO] - GPU available: True, used: False
-+[INFO] - TPU available: None, using: 0 TPU cores
diff --git a/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/lightning_logs.txt
deleted file mode 100644
index b0e4d0d..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/lightning_logs.txt
+++ /dev/null
@@ -1,42 +0,0 @@
-GPU available: True, used: False
-TPU available: None, using: 0 TPU cores
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 513   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-299 K     Trainable params
-13.2 M    Non-trainable params
-13.5 M    Total params
-Epoch 0, global step 199: val_loss reached 0.27337 (best 0.27337), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=0.ckpt" as top 3
-Epoch 1, global step 399: val_loss reached 0.26732 (best 0.26732), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=1.ckpt" as top 3
-Epoch 2, global step 599: val_loss reached 0.26594 (best 0.26594), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=2.ckpt" as top 3
-Epoch 3, global step 799: val_loss reached 0.25942 (best 0.25942), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=3.ckpt" as top 3
-Epoch 4, global step 999: val_loss reached 0.26189 (best 0.25942), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=4.ckpt" as top 3
-Epoch 5, global step 1199: val_loss reached 0.26193 (best 0.25942), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=5.ckpt" as top 3
-Saving latest checkpoint...
-Epoch 6, step 1200: val_loss was not in top 3
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 513   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-825 K     Trainable params
-12.7 M    Non-trainable params
-13.5 M    Total params
-GPU available: True, used: False
-TPU available: None, using: 0 TPU cores
diff --git a/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 68288a9..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,24 +0,0 @@
-[NeMo W 2021-02-10 14:21:43 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo I 2021-02-10 14:21:43 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43
-[NeMo I 2021-02-10 14:21:43 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-10 14:21:43 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-10 14:21:46 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-10 15:02:34 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f13eec2d490> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-10 15:04:49 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f13eed13130> was reported to be 99 (when accessing len(dataloader)), but 100 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-10 18:42:59 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-10 18:43:29 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-10 18:46:01 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f13eed13610> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/events.out.tfevents.1613128521.intern-instance.30071.0 b/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/events.out.tfevents.1613128521.intern-instance.30071.0
deleted file mode 100644
index dcb35a4..0000000
Binary files a/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/events.out.tfevents.1613128521.intern-instance.30071.0 and /dev/null differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/git-info.log
deleted file mode 100644
index d978dac..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/git-info.log
+++ /dev/null
@@ -1,1434 +0,0 @@
-commit hash: 68c7776c6ae820211b0383957eb39d9ce7d7d7a7
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/events.out.tfevents.1613010499.intern-instance.11236.0 b/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/events.out.tfevents.1613010499.intern-instance.11236.0
-index 243ac86..7ff3150 100644
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/events.out.tfevents.1613010499.intern-instance.11236.0 and b/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/events.out.tfevents.1613010499.intern-instance.11236.0 differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/lightning_logs.txt
-index 3a982ac..83d2823 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/lightning_logs.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/lightning_logs.txt
-@@ -21,3 +21,50 @@ initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
- Epoch 0, global step 5254: val_loss reached 0.01068 (best 0.01068), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.01-epoch=0.ckpt" as top 3
- Epoch 1, global step 10509: val_loss reached 0.01570 (best 0.01068), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.02-epoch=1.ckpt" as top 3
- Epoch 2, global step 15764: val_loss reached 0.01145 (best 0.01068), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.01-epoch=2.ckpt" as top 3
-+Epoch 3, global step 21019: val_loss reached 0.00733 (best 0.00733), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.01-epoch=3.ckpt" as top 3
-+Epoch 4, global step 26274: val_loss reached 0.00962 (best 0.00733), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.01-epoch=4.ckpt" as top 3
-+Epoch 5, global step 31529: val_loss reached 0.01048 (best 0.00733), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.01-epoch=5.ckpt" as top 3
-+Epoch 6, step 36784: val_loss was not in top 3
-+Epoch 7, global step 42039: val_loss reached 0.00450 (best 0.00450), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.00-epoch=7.ckpt" as top 3
-+Epoch 8, global step 47294: val_loss reached 0.00307 (best 0.00307), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.00-epoch=8.ckpt" as top 3
-+Epoch 9, global step 52549: val_loss reached -0.00113 (best -0.00113), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.00-epoch=9.ckpt" as top 3
-+Epoch 10, global step 57804: val_loss reached -0.00221 (best -0.00221), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.00-epoch=10.ckpt" as top 3
-+Epoch 11, global step 63059: val_loss reached -0.00430 (best -0.00430), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.00-epoch=11.ckpt" as top 3
-+Epoch 12, global step 68314: val_loss reached -0.00639 (best -0.00639), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.01-epoch=12.ckpt" as top 3
-+Epoch 13, global step 73569: val_loss reached -0.00726 (best -0.00726), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.01-epoch=13.ckpt" as top 3
-+Epoch 14, global step 78824: val_loss reached -0.00660 (best -0.00726), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.01-epoch=14.ckpt" as top 3
-+Saving latest checkpoint...
-+
-+  | Name                | Type                 | Params
-+-------------------------------------------------------------
-+0 | transformer         | DistilBertModel      | 66.4 M
-+1 | punct_classifier    | TokenClassifier      | 7.7 K 
-+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
-+3 | punctuation_loss    | FocalDiceLoss        | 0     
-+4 | domain_loss         | CrossEntropyLoss     | 0     
-+5 | agg_loss            | AggregatorLoss       | 0     
-+6 | punct_class_report  | ClassificationReport | 0     
-+7 | domain_class_report | ClassificationReport | 0     
-+-------------------------------------------------------------
-+7.1 M     Trainable params
-+59.3 M    Non-trainable params
-+66.4 M    Total params
-+Epoch 0, global step 84079: val_loss reached -0.01453 (best -0.01453), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.01-epoch=0.ckpt" as top 3
-+Epoch 1, global step 89334: val_loss reached -0.02339 (best -0.02339), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.02-epoch=1.ckpt" as top 3
-+Epoch 2, global step 94589: val_loss reached -0.02879 (best -0.02879), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.03-epoch=2.ckpt" as top 3
-+Epoch 3, global step 99844: val_loss reached -0.03249 (best -0.03249), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.03-epoch=3.ckpt" as top 3
-+Epoch 4, global step 105099: val_loss reached -0.03651 (best -0.03651), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.04-epoch=4.ckpt" as top 3
-+Epoch 5, global step 110354: val_loss reached -0.03725 (best -0.03725), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.04-epoch=5.ckpt" as top 3
-+Epoch 6, global step 115609: val_loss reached -0.04035 (best -0.04035), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.04-epoch=6.ckpt" as top 3
-+Epoch 7, global step 120864: val_loss reached -0.04093 (best -0.04093), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.04-epoch=7.ckpt" as top 3
-+Epoch 8, global step 126119: val_loss reached -0.04200 (best -0.04200), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.04-epoch=8.ckpt" as top 3
-+Epoch 9, global step 131374: val_loss reached -0.04282 (best -0.04282), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.04-epoch=9.ckpt" as top 3
-+Epoch 10, global step 136629: val_loss reached -0.04352 (best -0.04352), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.04-epoch=10.ckpt" as top 3
-+Epoch 11, global step 141884: val_loss reached -0.04396 (best -0.04396), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.04-epoch=11.ckpt" as top 3
-+Epoch 12, global step 147139: val_loss reached -0.04385 (best -0.04396), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.04-epoch=12.ckpt" as top 3
-+Epoch 13, global step 152394: val_loss reached -0.04389 (best -0.04396), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.04-epoch=13.ckpt" as top 3
-+Epoch 14, step 157649: val_loss was not in top 3
-+GPU available: True, used: True
-+TPU available: None, using: 0 TPU cores
-+Using environment variable NODE_RANK for node rank (0).
-+LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/nemo_error_log.txt
-index 864dfb4..83e5ef3 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/nemo_error_log.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/nemo_error_log.txt
-@@ -8,3 +8,6 @@
- [NeMo W 2021-02-11 11:12:14 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f0d466c9b50> was reported to be 2627 (when accessing len(dataloader)), but 2628 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-       warnings.warn(warn_msg)
-     
-+[NeMo W 2021-02-12 17:56:43 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f0d466c9a00> was reported to be 2627 (when accessing len(dataloader)), but 2628 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/nemo_log_globalrank-0_localrank-0.txt
-index fc03a49..5f25b7a 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/nemo_log_globalrank-0_localrank-0.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-11_10-27-44/nemo_log_globalrank-0_localrank-0.txt
-@@ -10,3 +10,6 @@
- [NeMo W 2021-02-11 11:12:14 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f0d466c9b50> was reported to be 2627 (when accessing len(dataloader)), but 2628 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-       warnings.warn(warn_msg)
-     
-+[NeMo W 2021-02-12 17:56:43 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f0d466c9a00> was reported to be 2627 (when accessing len(dataloader)), but 2628 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/cmd-args.log
-deleted file mode 100644
-index 11a5d8e..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/cmd-args.log
-+++ /dev/null
-@@ -1 +0,0 @@
--main.py
-\ No newline at end of file
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/events.out.tfevents.1613015290.intern-instance.24189.0 b/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/events.out.tfevents.1613015290.intern-instance.24189.0
-deleted file mode 100644
-index 237ef93..0000000
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/events.out.tfevents.1613015290.intern-instance.24189.0 and /dev/null differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/git-info.log
-deleted file mode 100644
-index de9e9e7..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/git-info.log
-+++ /dev/null
-@@ -1,962 +0,0 @@
--commit hash: e06c976eca0f0f1ece5dd302964c113374b09762
--diff --git a/README.md b/README.md
--index 8b23590..e1fff55 100644
----- a/README.md
--+++ b/README.md
--@@ -619,3 +619,22 @@ weighted avg                                            90.70      90.25      90
--  'punct_precision': tensor(33.9651),
--  'punct_recall': tensor(33.8733),
--  'test_loss': tensor(0.2265)}
--+
--+TED end
--+ 'punct_f1': tensor(32.2363, device='cuda:0'),
--+ 'punct_precision': tensor(30.6842, device='cuda:0'),
--+ 'punct_recall': tensor(36.3651, device='cuda:0'),
--+ 'test_loss': tensor(0.2000, device='cuda:0')}
--+
--+TED start
--+'punct_f1': tensor(32.0951, device='cuda:0'),
--+'punct_precision': tensor(30.3402, device='cuda:0'),
--+'punct_recall': tensor(36.4819, device='cuda:0'),
--+'test_loss': tensor(0.1911, device='cuda:0')}
--+
--+
--+TED None
--+{'punct_f1': tensor(35.3044, device='cuda:0'),
--+ 'punct_precision': tensor(34.8901, device='cuda:0'),
--+ 'punct_recall': tensor(35.7895, device='cuda:0'),
--+ 'test_loss': tensor(0.2175, device='cuda:0')}
--diff --git a/experiment/Inference.ipynb b/experiment/Inference.ipynb
--index 99790f8..3756810 100644
----- a/experiment/Inference.ipynb
--+++ b/experiment/Inference.ipynb
--@@ -2,15 +2,34 @@
--  "cells": [
--   {
--    "cell_type": "code",
---   "execution_count": 1,
--+   "execution_count": 10,
--    "id": "modern-amplifier",
--    "metadata": {},
--    "outputs": [
--     {
---     "output_type": "stream",
--      "name": "stdout",
--+     "output_type": "stream",
--+     "text": [
--+      "/home/nxingyu2/project/experiment\n",
--+      "shuffling <data.punctuation_dataset_multi.PunctuationDomainDataset object at 0x7fb2c71e8790>\n"
--+     ]
--+    },
--+    {
--+     "name": "stderr",
--+     "output_type": "stream",
--      "text": [
---      "/home/nxingyu2/project/experiment\n"
--+      "11:23:21.60 LOG:\n",
--+      "11:23:21.61 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
--+      "11:23:21.62 LOG:\n",
--+      "11:23:21.63 .... f\"1st {n} encoder layers of transformer frozen\" = '1st 11 encoder layers of transformer frozen'\n",
--+      "GPU available: True, used: True\n",
--+      "INFO:lightning:GPU available: True, used: True\n",
--+      "TPU available: None, using: 0 TPU cores\n",
--+      "INFO:lightning:TPU available: None, using: 0 TPU cores\n",
--+      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
--+      "INFO:lightning:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
--+      "Using native 16bit precision.\n",
--+      "INFO:lightning:Using native 16bit precision.\n"
--      ]
--     }
--    ],
--@@ -37,29 +56,191 @@
--     "\n",
--     "from hydra.experimental import initialize, initialize_config_module, initialize_config_dir, compose\n",
--     "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
---    "initialize(config_path=\"project/experiment\")\n",
--+    "# folder=\"TEDend2021-02-11_07-57-33\"\n",
--+    "folder=\"TEDstart2021-02-11_07-55-58\"\n",
--+    "# folder=\"TEDnone2021-02-11_10-14-34\"\n",
--+    "initialize(config_path=\"../Punctuation_with_Domain_discriminator/\"+folder) #config_path=\"project/experiment\"\n",
--     "!pwd\n",
---    "\n",
--     "cfg=compose(\n",
---    "    config_name=\"test_config.yaml\", \n",
--+    "    config_name=\"hparams.yaml\", \n",
--     ")\n",
--     "cfg.model.punct_label_ids=OmegaConf.create(sorted(cfg.model.punct_label_ids))\n",
---    "labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
---    "ids_to_labels = {_[0]:_[1] for _ in enumerate(cfg.model.punct_label_ids)}\n",
--+    "# labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
--+    "# ids_to_labels = {_[0]:_[1] for _ in enumerate(cfg.model.punct_label_ids)}\n",
--+    "model = PunctuationDomainModel.load_from_checkpoint( #TEDend2021-02-11_07-57-33  # TEDstart2021-02-11_07-55-58\n",
--+    "    checkpoint_path=f\"/home/nxingyu2/project/Punctuation_with_Domain_discriminator/{folder}/checkpoints/Punctuation_with_Domain_discriminator-last.ckpt\")\n",
--+    "trainer = pl.Trainer(**cfg.trainer)"
--+   ]
--+  },
--+  {
--+   "cell_type": "code",
--+   "execution_count": 9,
--+   "id": "hairy-proxy",
--+   "metadata": {},
--+   "outputs": [
--+    {
--+     "data": {
--+      "text/plain": [
--+       "[' hooooooo,                                                                                                                           ',\n",
--+       " ' what can i do for you today?                                                                                                                        ',\n",
--+       " ' ? how are you?                                                                                                                            ',\n",
--+       " ' in guadeloupe or marti nique, it also brin gs into ques tion budgetary po licy. because the europe, an union is after all, making a present of ecu 1, 9 billion to three multinationals. where are the financial interests of the european union— firstly, development policy— in africa. in any case, in the acp, countries employ ment policy in madeira, the canaries— guadel, oupe, martinique and crete, regional, pol icy in the ultra- peripheral areas— human rights, which mr barthet mayer mentioned earlier, since dollar bananas are after all, slavery. bana nas, the product of human exploitation by three multinat ionals, payments of ec ',\n",
--+       " ' u 50 per mon th. instead of ecu 50 per day                                                                                                                   ',\n",
--+       " ' plans for this weekend, include turning wine into water. the small white buoys marked the location of hundreds of crab pots. he said he was not there yesterday. however, many people saw him there. today. arrived with a crash of my car through the garage door. the lyrics of the song sounded like fingernails on a chalkboard. the guinea fowl flies through the air with all the grace of a turtle. they ran around the corner, to find that they had traveled back in time,                                      ',\n",
--+       " ' good morning. everyone? how have your weekends been? its a really great day? thank you.                                                                                                                ',\n",
--+       " ' first of all, i too agree that tourism- related action must include employment training and education. as you know, after the european conference on tourism and employment in luxemborg, we set up a high- level group, whose mission was to examine how best tourism could contribute towards employment— the first stage,                                                                          ']"
--+      ]
--+     },
--+     "execution_count": 9,
--+     "metadata": {},
--+     "output_type": "execute_result"
--+    }
--+   ],
--+   "source": [
--     "\n",
---    "# model = PunctuationDomainModel.load_from_checkpoint(\n",
---    "#     checkpoint_path=\"/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator-last.ckpt\")\n",
--+    "queries = [\n",
--+    "    'Hooooooo!',\n",
--+    "    'what can i do for you today',\n",
--+    "    'how are you',\n",
--+    "    ' in guadeloupe or marti nique it also brin gs into ques tion budgetary po licy because the europe an union is after all making a present of ecu 1.9 billion to three multinationals where are the financial interests of the european union firstly development policy in africa in any case in the acp countries employ ment policy in madeira the canaries guadel oupe martinique and crete regional pol icy in the ultra-peripheral areas human rights which mr barthet mayer mentioned earlier since dollar bananas are after all slavery bana nas the product of human exploitation by three multinat ionals payments of ecu 50 per mon th instead of ecu 50 per day',\n",
--+    "    '''Plans for this weekend include turning wine into water.\n",
--+    "The small white buoys marked the location of hundreds of crab pots.\n",
--+    "He said he was not there yesterday; however, many people saw him there.\n",
--+    "Today arrived with a crash of my car through the garage door.\n",
--+    "The lyrics of the song sounded like fingernails on a chalkboard.\n",
--+    "The Guinea fowl flies through the air with all the grace of a turtle.\n",
--+    "They ran around the corner to find that they had traveled back in time.''',\n",
--+    "    'good morning everyone how have your weekends been its a really great day thank you',\n",
--+    "    'first of all i too agree that tourism related action must include employment training and education as you know after the european conference on tourism and employment in luxemborg we set up a high-level group whose mission was to examine how best tourism could contribute towards employment the first stage'\n",
--+    "]\n",
--+    "inference_results = model.add_punctuation(queries)\n",
--+    "inference_results"
--+   ]
--+  },
--+  {
--+   "cell_type": "code",
--+   "execution_count": 20,
--+   "id": "magnetic-approach",
--+   "metadata": {},
--+   "outputs": [
--+    {
--+     "name": "stdout",
--+     "output_type": "stream",
--+     "text": [
--+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
--+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
--+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
--+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
--+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
--+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
--+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
--+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
--+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
--+      " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n"
--+     ]
--+    }
--+   ],
--+   "source": [
--+    "for i in torch.zeros((10,10)):\n",
--+    "    print(' '.join('{:5.2f}'.format(x) for x in i))"
--+   ]
--+  },
--+  {
--+   "cell_type": "code",
--+   "execution_count": 12,
--+   "id": "dietary-violin",
--+   "metadata": {},
--+   "outputs": [
--+    {
--+     "data": {
--+      "text/plain": [
--+       "[' hooooooo                                                                                                                           ',\n",
--+       " ' what can i do for you today?                                                                                                                        ',\n",
--+       " ' , how? are? you?                                                                                                                            ',\n",
--+       " ' in guadeloupe or marti nique it also br-in gs into ques tion budgetary po. licy because the europe, an union is after all, making a present of ecu 1, 9 billion to three multinationals where are the financial interests of the european union? firstly development policy in africa. in any case, in the ac.p countries, employ men-t policy in madeira. the canaries guadel ou.pe martinique and crete, regional, pol, icy in the ultra- peripheral areas— human rights, which mr barthet mayer mentioned earlier, since dollar bananas are after all. slavery. bana nas. the product of human exploitation, by three multi-nat iona.ls payments of ec. ',\n",
--+       " ' u 50 per mon, th. instead of ecu 50 per day                                                                                                                   ',\n",
--+       " ' plans for this weekend, include turning wine into water. the small white bu.oys marked the location of hundreds of crab pots. he said he was not there yesterday. however many people saw him there today, arrived with a crash of my car through the garage door. the lyrics of the song sounded like fingernails on a chalkboard the guinea f-owl flies through the air with all the grace of a turtle. they ran around the corner, to find that they had traveled back in time,                                      ',\n",
--+       " ' good morning? everyone? how have your weekends? been? its a really great day? thank you.                                                                                                                ',\n",
--+       " ' first of all, i too agree that tourism- related action must include employment, training and education. as you know, after the european conference on tourism and employment in luxemborg we set up a high- level group, whose mission was to examine how best tourism could contribute towards employment— the first stage,                                                                          ']"
--+      ]
--+     },
--+     "execution_count": 12,
--+     "metadata": {},
--+     "output_type": "execute_result"
--+    }
--+   ],
--+   "source": [
--+    "\n",
--+    "queries = [\n",
--+    "    'Hooooooo!',\n",
--+    "    'what can i do for you today',\n",
--+    "    'how are you',\n",
--+    "    ' in guadeloupe or marti nique it also brin gs into ques tion budgetary po licy because the europe an union is after all making a present of ecu 1.9 billion to three multinationals where are the financial interests of the european union firstly development policy in africa in any case in the acp countries employ ment policy in madeira the canaries guadel oupe martinique and crete regional pol icy in the ultra-peripheral areas human rights which mr barthet mayer mentioned earlier since dollar bananas are after all slavery bana nas the product of human exploitation by three multinat ionals payments of ecu 50 per mon th instead of ecu 50 per day',\n",
--+    "    '''Plans for this weekend include turning wine into water.\n",
--+    "The small white buoys marked the location of hundreds of crab pots.\n",
--+    "He said he was not there yesterday; however, many people saw him there.\n",
--+    "Today arrived with a crash of my car through the garage door.\n",
--+    "The lyrics of the song sounded like fingernails on a chalkboard.\n",
--+    "The Guinea fowl flies through the air with all the grace of a turtle.\n",
--+    "They ran around the corner to find that they had traveled back in time.''',\n",
--+    "    'good morning everyone how have your weekends been its a really great day thank you',\n",
--+    "    'first of all i too agree that tourism related action must include employment training and education as you know after the european conference on tourism and employment in luxemborg we set up a high-level group whose mission was to examine how best tourism could contribute towards employment the first stage'\n",
--+    "]\n",
--+    "inference_results = model.add_punctuation(queries)\n",
--+    "inference_results"
--+   ]
--+  },
--+  {
--+   "cell_type": "code",
--+   "execution_count": 9,
--+   "id": "residential-scene",
--+   "metadata": {},
--+   "outputs": [
--+    {
--+     "data": {
--+      "text/plain": [
--+       "[' hooooooo,                                                                                                                           ',\n",
--+       " ' what can i do for you today?                                                                                                                        ',\n",
--+       " ' ? how are you?                                                                                                                            ',\n",
--+       " ' firstly, development policy in africa. in any case, in the acp, countries, employment policy in madeira, the canaries— guadeloupe, martinique and crete, regional policy in the ultra- peripheral areas— human rights, which mr barthet mayer, mentioned earlier, since dollar bananas are after all, slavery. bananas, the product of human exploitation by three multinationals, payments of ecu 50 per month, instead of ecu 50 per day in guadeloupe or martinique. it also brings into question budgetary policy. because the european union is after all, making a present of ecu 1, 9 billion to three multinationals. where are the financial interests of the european union?         ',\n",
--+       " ' plans for this weekend, include turning wine into water. the small white buoys marked the location of hundreds of crab pots. he said he was not there yesterday. however, many people saw him there. today. arrived with a crash of my car through the garage door. the lyrics of the song sounded like fingernails on a chalkboard. the guinea fowl flies through the air with all the grace of a turtle. they ran around the corner, to find that they had traveled back in time,                                      ',\n",
--+       " ' good morning. everyone? how have your weekends been? its a really great day? thank you.                                                                                                                ',\n",
--+       " ' first of all, i too agree that tourism- related action must include employment training and education. as you know, after the european conference on tourism and employment in luxemborg, we set up a high- level group, whose mission was to examine how best tourism could contribute towards employment— the first stage,                                                                          ']"
--+      ]
--+     },
--+     "execution_count": 9,
--+     "metadata": {},
--+     "output_type": "execute_result"
--+    }
--+   ],
--+   "source": [
--     "\n",
---    "# trainer = pl.Trainer(**cfg.trainer)"
--+    "queries = [\n",
--+    "    'Hooooooo!',\n",
--+    "    'what can i do for you today',\n",
--+    "    'how are you',\n",
--+    "    'firstly development policy in africa in any case in the acp countries employment policy in madeira the canaries guadeloupe martinique and crete regional policy in the ultra-peripheral areas human rights which mr barthet-mayer mentioned earlier since dollar bananas are after all slavery bananas the product of human exploitation by three multinationals payments of ecu 50 per month instead of ecu 50 per day in guadeloupe or martinique it also brings into question budgetary policy because the european union is after all making a present of ecu 1.9 billion to three multinationals where are the financial interests of the european union',\n",
--+    "    '''Plans for this weekend include turning wine into water.\n",
--+    "The small white buoys marked the location of hundreds of crab pots.\n",
--+    "He said he was not there yesterday; however, many people saw him there.\n",
--+    "Today arrived with a crash of my car through the garage door.\n",
--+    "The lyrics of the song sounded like fingernails on a chalkboard.\n",
--+    "The Guinea fowl flies through the air with all the grace of a turtle.\n",
--+    "They ran around the corner to find that they had traveled back in time.''',\n",
--+    "    'good morning everyone how have your weekends been its a really great day thank you',\n",
--+    "    'first of all i too agree that tourism related action must include employment training and education as you know after the european conference on tourism and employment in luxemborg we set up a high-level group whose mission was to examine how best tourism could contribute towards employment the first stage'\n",
--+    "]\n",
--+    "inference_results = model.add_punctuation(queries)\n",
--+    "inference_results"
--    ]
--   },
--   {
--    "cell_type": "code",
---   "execution_count": 11,
--+   "execution_count": 3,
--+   "id": "loose-assignment",
--    "metadata": {},
--    "outputs": [
--     {
---     "output_type": "execute_result",
--      "data": {
--       "text/plain": [
--        "{'input_ids': tensor([[ 101, 7570, 9541, 9541, 9541,  102,    0,    0,    0,    0,    0,    0,\n",
--@@ -86,7 +267,7 @@
--        "          False, False, False, False, False, False, False, False, False, False,\n",
--        "          False, False, False, False, False, False, False, False, False, False,\n",
--        "          False, False, False, False, False, False, False, False]]),\n",
---       " 'subtoken_mask': tensor([[ True, False, False, False,  True,  True, False, False, False, False,\n",
--+       " 'subtoken_mask': tensor([[ True,  True, False, False, False, False, False, False, False, False,\n",
--        "          False, False, False, False, False, False, False, False, False, False,\n",
--        "          False, False, False, False, False, False, False, False, False, False,\n",
--        "          False, False, False, False, False, False, False, False, False, False,\n",
--@@ -99,7 +280,7 @@
--        "          False, False, False, False, False, False, False, False, False, False,\n",
--        "          False, False, False, False, False, False, False, False, False, False,\n",
--        "          False, False, False, False, False, False, False, False]]),\n",
---       " 'labels': tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--+       " 'labels': tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--        "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--        "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--        "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
--@@ -107,11 +288,15 @@
--        "          0, 0, 0, 0, 0, 0, 0, 0]])}"
--       ]
--      },
--+     "execution_count": 3,
--      "metadata": {},
---     "execution_count": 11
--+     "output_type": "execute_result"
--     }
--    ],
--    "source": [
--+    "# cfg.model.punct_label_ids=OmegaConf.create(sorted(cfg.model.punct_label_ids))\n",
--+    "# labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
--+    "\n",
--     "queries = [\n",
--     "    'Hooooooo!',\n",
--     "    # 'what can i do for you today',\n",
--@@ -120,36 +305,10 @@
--     "]\n",
--     "cfg.model.punct_label_ids=OmegaConf.create(sorted(cfg.model.punct_label_ids))\n",
--     "labels_to_ids = {_[1]:_[0] for _ in enumerate(cfg.model.punct_label_ids)}\n",
---    "ds=PunctuationInferenceDataset(tokenizer=AutoTokenizer.from_pretrained('google/electra-small-discriminator'), queries=queries, max_seq_length=128, punct_label_ids=labels_to_ids, degree=0, attach_label_to_end=True)\n",
--+    "ds=PunctuationInferenceDataset(tokenizer=AutoTokenizer.from_pretrained('google/electra-small-discriminator'), queries=queries, max_seq_length=128, punct_label_ids=labels_to_ids, degree=0, attach_label_to_end=False)\n",
--     "ds[0]"
--    ]
--   },
---  {
---   "cell_type": "code",
---   "execution_count": 8,
---   "id": "hairy-proxy",
---   "metadata": {},
---   "outputs": [
---    {
---     "data": {
---      "text/plain": [
---       "[' we bought four shirts, one pen, and a mug from the nvidia gear store in santa clara                                                                                                            ',\n",
---       " ' what can i do for you today?                                                                                                                        ',\n",
---       " ' , how are you? ,                                                                                                                           ',\n",
---       " ' good morning. everyone? how have your weekends been? its a really great day?                                                                                                                  ']"
---      ]
---     },
---     "execution_count": 8,
---     "metadata": {},
---     "output_type": "execute_result"
---    }
---   ],
---   "source": [
---    "\n",
---    "inference_results = model.add_punctuation(queries)\n",
---    "inference_results"
---   ]
---  },
--   {
--    "cell_type": "code",
--    "execution_count": null,
--@@ -175,9 +334,9 @@
--    "name": "python",
--    "nbconvert_exporter": "python",
--    "pygments_lexer": "ipython3",
---   "version": "3.8.5-final"
--+   "version": "3.8.5"
--   }
--  },
--  "nbformat": 4,
--  "nbformat_minor": 5
---}
--\ No newline at end of file
--+}
--diff --git a/experiment/config.yaml b/experiment/config.yaml
--index 2aa798f..9cfe485 100644
----- a/experiment/config.yaml
--+++ b/experiment/config.yaml
--@@ -1,36 +1,36 @@
-- seed: 42
-- trainer:
---    # gpus: 1 # the number of gpus, 0 for CPU
---    # num_nodes: 1
---    # max_epochs: 15
---    # max_steps: null # precedence over max_epochs
---    # accumulate_grad_batches: 4 # accumulates grads every k batches
---    # gradient_clip_val: 0
---    # amp_level: O1 # O1/O2 for mixed precision
---    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
---    # accelerator: ddp
---    # checkpoint_callback: false  # Provided by exp_manager
---    # logger: false #false  # Provided by exp_manager
---    # log_every_n_steps: 1  # Interval of logging.
---    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
---    # resume_from_checkpoint: null
---
---    gpus: 0 # the number of gpus, 0 for CPU
--+    gpus: 1 # the number of gpus, 0 for CPU
--     num_nodes: 1
---    max_epochs: 8
--+    max_epochs: 15
--     max_steps: null # precedence over max_epochs
--     accumulate_grad_batches: 4 # accumulates grads every k batches
--     gradient_clip_val: 0
---    amp_level: O0 # O1/O2 for mixed precision
---    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
---    # accelerator: ddp
--+    amp_level: O1 # O1/O2 for mixed precision
--+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--+    accelerator: ddp
--     checkpoint_callback: false  # Provided by exp_manager
--     logger: false #false  # Provided by exp_manager
--     log_every_n_steps: 1  # Interval of logging.
--     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
---    reload_dataloaders_every_epoch: true
--     resume_from_checkpoint: null
-- 
--+    # gpus: 0 # the number of gpus, 0 for CPU
--+    # num_nodes: 1
--+    # max_epochs: 8
--+    # max_steps: null # precedence over max_epochs
--+    # accumulate_grad_batches: 4 # accumulates grads every k batches
--+    # gradient_clip_val: 0
--+    # amp_level: O0 # O1/O2 for mixed precision
--+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--+    # # accelerator: ddp
--+    # checkpoint_callback: false  # Provided by exp_manager
--+    # logger: false #false  # Provided by exp_manager
--+    # log_every_n_steps: 1  # Interval of logging.
--+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--+    # reload_dataloaders_every_epoch: true
--+    # resume_from_checkpoint: null
--+
-- exp_manager:
--     exp_dir: /home/nxingyu2/project/ # /root/project # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
--     name: Punctuation_with_Domain_discriminator  # The name of your model
--@@ -41,7 +41,7 @@ tmp_path: /home/nxingyu2/data/tmp # /tmp #
-- 
-- model:
--     nemo_path: null
---    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
--+    transformer_path: distilbert-base-uncased # google/electra-small-discriminator #  filename to save the model and associated artifacts to .nemo file
--     unfrozen: 0
--     maximum_unfrozen: 1
--     unfreeze_step: 1
--@@ -81,7 +81,7 @@ model:
--         num_labels: 10
--         num_domains: 1
--         test_unlabelled: true
---        attach_label_to_end: false # false if attach to start
--+        attach_label_to_end: none # false if attach to start none if dont mask
-- 
--         train_ds:
--             shuffle: true
--diff --git a/experiment/core/classification_report.py b/experiment/core/classification_report.py
--index 7ff6282..ca758d1 100644
----- a/experiment/core/classification_report.py
--+++ b/experiment/core/classification_report.py
--@@ -15,6 +15,7 @@
-- from typing import Any, Dict, Optional
-- 
-- import torch
--+from torch.nn.functional import one_hot
-- from pytorch_lightning.metrics import Metric
-- from pytorch_lightning.metrics.utils import METRIC_EPS
-- 
--@@ -82,11 +83,14 @@ class ClassificationReport(Metric):
--         self.add_state(
--             "num_examples_per_class", default=torch.zeros(num_classes), dist_reduce_fx='sum', persistent=False
--         )
--+        self.add_state("cm", default=torch.zeros((num_classes,num_classes)), dist_reduce_fx="sum", persistent=False)
-- 
--     def update(self, predictions: torch.Tensor, labels: torch.Tensor):
--         TP = []
--         FN = []
--         FP = []
--+        CM = torch.zeros((self.num_classes,self.num_classes),dtype=torch.long).to(predictions.device)
--+        CM.index_add_(0, predictions, one_hot(labels,num_classes=self.num_classes))
--         for label_id in range(self.num_classes):
--             current_label = labels == label_id
--             label_predicted = predictions == label_id
--@@ -98,11 +102,13 @@ class ClassificationReport(Metric):
--         tp = torch.tensor(TP).to(predictions.device)
--         fn = torch.tensor(FN).to(predictions.device)
--         fp = torch.tensor(FP).to(predictions.device)
--+        # cm = torch.tensor(CM).to(predictions.device)
--         num_examples_per_class = tp + fn
-- 
--         self.tp += tp
--         self.fn += fn
--         self.fp += fp
--+        self.cm += CM
--         self.num_examples_per_class += num_examples_per_class
-- 
--     def compute(self):
--@@ -156,16 +162,20 @@ class ClassificationReport(Metric):
--             + '\n'
--         )
-- 
--+        report += "\n-------------------\n"
--+        for i in self.cm:
--+            report+=(' '.join('{:5.2f}'.format(x) for x in i)) + '\n'
--+        report += "\n-------------------\n"
--         self.total_examples = total_examples
-- 
--         if self.mode == 'macro':
---            return macro_precision, macro_recall, macro_f1, report
--+            return macro_precision, macro_recall, macro_f1, report, self.cm
--         elif self.mode == 'weighted':
---            return weighted_precision, weighted_recall, weighted_f1, report
--+            return weighted_precision, weighted_recall, weighted_f1, report, self.cm
--         elif self.mode == 'micro':
---            return micro_precision, micro_recall, micro_f1, report
--+            return micro_precision, micro_recall, micro_f1, report, self.cm
--         elif self.mode == 'all':
---            return precision, recall, f1, report
--+            return precision, recall, f1, report, self.cm
--         else:
--             raise ValueError(
--                 f'{self.mode} mode is not supported. Choose "macro" to get aggregated numbers \
--diff --git a/experiment/core/utils.py b/experiment/core/utils.py
--index c9470b8..7f288e1 100644
----- a/experiment/core/utils.py
--+++ b/experiment/core/utils.py
--@@ -38,7 +38,7 @@ def align_labels_to_mask(mask,labels):
--     return m1.tolist()
-- 
-- def view_aligned(texts,tags,tokenizer,labels_to_ids):
---        return [re.sub(r'( ?\[((PAD)|(CLS)|(SEP))\] ?)',' ',re.sub(' ##','',' '.join(
--+        return [re.sub(r'( ?\[((PAD)|(CLS)|(SEP))\] ?)',' ',re.sub(' +##','',' '.join( #[.?!,;:\-—… ]+
--             [_[0]+_[1] for _ in list(
--                 zip(tokenizer.convert_ids_to_tokens(_[0]),
--                     [labels_to_ids[id] for id in _[1].tolist()])
--@@ -113,7 +113,11 @@ def chunk_to_len(max_seq_length,tokenizer,attach_label_to_end,tokens,labels=None
--         padded_labels=[pad_to_len(max_seq_length,align_labels_to_mask(*_)) for _ in zip(masks,split_labels)]
--     return ids,masks,padded_labels
--     
---def chunk_to_len_batch(max_seq_length,tokenizer,tokens,labels=None,labelled=True,ignore_index=-100, attach_label_to_end=True):
--+def chunk_to_len_batch(max_seq_length,tokenizer,tokens,labels=None,labelled=True,ignore_index=-100, attach_label_to_end=None):
--+    no_mask=False
--+    if attach_label_to_end is None:
--+        no_mask=True
--+        attach_label_to_end=True
--     batch_ids=[]
--     batch_masks=[]
--     batch_labels=[]
--@@ -126,8 +130,11 @@ def chunk_to_len_batch(max_seq_length,tokenizer,tokens,labels=None,labelled=True
--     output = {'input_ids': torch.as_tensor(batch_ids, dtype=torch.long),
--               'attention_mask': torch.as_tensor(batch_ids, dtype=torch.bool),
--               'subtoken_mask': torch.as_tensor(batch_masks,dtype=torch.bool)}
---    output['subtoken_mask']|=((output['input_ids']==101)|(output['input_ids']==102))
---    output['subtoken_mask']&=labelled
--+    if no_mask:
--+        output['subtoken_mask']=output['attention_mask']&(output['input_ids']!=102)
--+    else:
--+        output['subtoken_mask']|=(output['input_ids']==101)  # dont want end token |(output['input_ids']==102)
--+        output['subtoken_mask']&=labelled
--     output['labels']=torch.as_tensor(batch_labels,dtype=torch.long) if labelled==True else torch.zeros_like(output['input_ids'],dtype=torch.long)
--     return output
-- 
--diff --git a/experiment/data/punctuation_datamodule.py b/experiment/data/punctuation_datamodule.py
--index fb69299..71a1f7f 100644
----- a/experiment/data/punctuation_datamodule.py
--+++ b/experiment/data/punctuation_datamodule.py
--@@ -25,7 +25,7 @@ class PunctuationDataModule(LightningDataModule):
--             data_id: str = '',
--             tmp_path:str = '~/data/tmp',
--             test_unlabelled:bool = True,
---            attach_label_to_end:bool = True,
--+            attach_label_to_end:bool = None,
--             ):
--         #unlabelled=[], batch_size = 256, max_seq_length = 256, num_workers=1):
--         super().__init__()
--diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
--index 97f2fb2..4c9f67a 100644
----- a/experiment/data/punctuation_dataset_multi.py
--+++ b/experiment/data/punctuation_dataset_multi.py
--@@ -39,7 +39,7 @@ class PunctuationDomainDataset(IterableDataset):
--         tmp_path='~/data/tmp',
--         start=0,
--         end=-1,
---        attach_label_to_end=True,
--+        attach_label_to_end=None,
--     ):
--         if not (os.path.exists(csv_file)):
--             raise FileNotFoundError(
--@@ -154,7 +154,7 @@ class PunctuationDomainDatasets(IterableDataset):
--                  randomize:bool=True,
--                  data_id='',
--                  tmp_path='~/data/tmp',
---                 attach_label_to_end=True,
--+                 attach_label_to_end=None,
--                  ):
--         worker_info = get_worker_info()
--         self.num_workers=1 if worker_info is None else worker_info.num_workers
--@@ -266,7 +266,7 @@ class PunctuationInferenceDataset(Dataset):
--             "labels": NeuralType(('B', 'T'), ChannelType()),
--         }
-- 
---    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], num_samples:int=256, degree:int = 0, attach_label_to_end:bool=True):
--+    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], num_samples:int=256, degree:int = 0, attach_label_to_end:bool=None):
--         """ Initializes BertPunctuationInferDataset. """
--         self.degree=degree
--         self.punct_label_ids=punct_label_ids
--diff --git a/experiment/info.log b/experiment/info.log
--index 85bc297..104c5dd 100644
----- a/experiment/info.log
--+++ b/experiment/info.log
--@@ -1,192 +1,4 @@
---[INFO] - GPU available: True, used: False
--+[INFO] - GPU available: True, used: True
-- [INFO] - TPU available: None, using: 0 TPU cores
---[INFO] - shuffling train set
---[INFO] - Global seed set to 42
---[INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
---[INFO] - Optimizer config = AdamW (
---Parameter Group 0
---    amsgrad: False
---    betas: (0.9, 0.999)
---    eps: 1e-08
---    lr: 0.01
---    weight_decay: 0.0
---)
---[INFO] - Scheduler "<core.optim.lr_scheduler.CosineAnnealing object at 0x7f13e0b714f0>" 
---will be used during training (effective maximum steps = 1600) - 
---Parameters : 
---(warmup_steps: null
---warmup_ratio: 0.1
---min_lr: 1.0e-10
---last_epoch: -1
---max_steps: 1600
---)
---[INFO] - 
---  | Name                | Type                 | Params
----------------------------------------------------------------
---0 | transformer         | ElectraModel         | 13.5 M
---1 | punct_classifier    | TokenClassifier      | 2.6 K 
---2 | domain_classifier   | SequenceClassifier   | 513   
---3 | punctuation_loss    | FocalDiceLoss        | 0     
---4 | domain_loss         | CrossEntropyLoss     | 0     
---5 | agg_loss            | AggregatorLoss       | 0     
---6 | punct_class_report  | ClassificationReport | 0     
---7 | domain_class_report | ClassificationReport | 0     
----------------------------------------------------------------
---299 K     Trainable params
---13.2 M    Non-trainable params
---13.5 M    Total params
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          91.10      19.21      31.73       3196
---! (label_id: 1)                                          0.00       0.00       0.00          0
---, (label_id: 2)                                          5.61      37.37       9.76        198
---- (label_id: 3)                                          0.79       9.09       1.46         22
---. (label_id: 4)                                          0.00       0.00       0.00        212
---: (label_id: 5)                                          0.00       0.00       0.00          4
---; (label_id: 6)                                          0.00       0.00       0.00          4
---? (label_id: 7)                                          0.96      62.50       1.88         16
---— (label_id: 8)                                          0.00       0.00       0.00         20
---… (label_id: 9)                                          0.00       0.00       0.00          0
----------------------
---micro avg                                               19.06      19.06      19.06       3672
---macro avg                                               12.31      16.02       5.60       3672
---weighted avg                                            79.60      19.06      28.16       3672
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00         34
----------------------
---micro avg                                              100.00     100.00     100.00         34
---macro avg                                              100.00     100.00     100.00         34
---weighted avg                                           100.00     100.00     100.00         34
---
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          97.76      93.83      95.75     303856
---! (label_id: 1)                                          0.00       0.00       0.00        155
---, (label_id: 2)                                         38.85      59.39      46.98      23143
---- (label_id: 3)                                         72.00      53.11      61.13       1830
---. (label_id: 4)                                         58.36      60.43      59.38      20164
---: (label_id: 5)                                          0.00       0.00       0.00        439
---; (label_id: 6)                                          0.00       0.00       0.00        176
---? (label_id: 7)                                         24.31      36.42      29.15       1590
---— (label_id: 8)                                          6.84       6.23       6.52       1509
---… (label_id: 9)                                          0.00       0.00       0.00        111
----------------------
---micro avg                                               88.58      88.58      88.58     352973
---macro avg                                               29.81      30.94      29.89     352973
---weighted avg                                            90.55      88.58      89.38     352973
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       3181
----------------------
---micro avg                                              100.00     100.00     100.00       3181
---macro avg                                              100.00     100.00     100.00       3181
---weighted avg                                           100.00     100.00     100.00       3181
---
---[INFO] - Epoch 0, global step 199: val_loss reached 0.27337 (best 0.27337), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=0.ckpt" as top 3
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          97.63      94.98      96.29     303856
---! (label_id: 1)                                          0.00       0.00       0.00        155
---, (label_id: 2)                                         43.70      57.68      49.73      23143
---- (label_id: 3)                                         75.76      51.58      61.38       1830
---. (label_id: 4)                                         58.49      63.93      61.09      20164
---: (label_id: 5)                                          0.00       0.00       0.00        439
---; (label_id: 6)                                          0.00       0.00       0.00        176
---? (label_id: 7)                                         35.78      37.42      36.58       1590
---— (label_id: 8)                                          5.88       7.22       6.48       1509
---… (label_id: 9)                                          0.00       0.00       0.00        111
----------------------
---micro avg                                               89.67      89.67      89.67     352973
---macro avg                                               31.72      31.28      31.15     352973
---weighted avg                                            90.83      89.67      90.15     352973
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       3181
----------------------
---micro avg                                              100.00     100.00     100.00       3181
---macro avg                                              100.00     100.00     100.00       3181
---weighted avg                                           100.00     100.00     100.00       3181
---
---[INFO] - Epoch 1, global step 399: val_loss reached 0.26732 (best 0.26732), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=1.ckpt" as top 3
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          97.23      95.39      96.30     303856
---! (label_id: 1)                                          0.00       0.00       0.00        155
---, (label_id: 2)                                         47.20      48.78      47.98      23143
---- (label_id: 3)                                         69.68      55.25      61.63       1830
---. (label_id: 4)                                         55.03      67.55      60.65      20164
---: (label_id: 5)                                          0.00       0.00       0.00        439
---; (label_id: 6)                                          0.00       0.00       0.00        176
---? (label_id: 7)                                         28.81      45.91      35.40       1590
---— (label_id: 8)                                          8.17      11.93       9.70       1509
---… (label_id: 9)                                          0.00       0.00       0.00        111
----------------------
---micro avg                                               89.72      89.72      89.72     352973
---macro avg                                               30.61      32.48      31.17     352973
---weighted avg                                            90.47      89.72      90.03     352973
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       3181
----------------------
---micro avg                                              100.00     100.00     100.00       3181
---macro avg                                              100.00     100.00     100.00       3181
---weighted avg                                           100.00     100.00     100.00       3181
---
---[INFO] - Epoch 2, global step 599: val_loss reached 0.26594 (best 0.26594), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.27-epoch=2.ckpt" as top 3
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          97.60      95.36      96.46     303856
---! (label_id: 1)                                          0.00       0.00       0.00        155
---, (label_id: 2)                                         45.38      58.10      50.96      23143
---- (label_id: 3)                                         70.17      59.40      64.34       1830
---. (label_id: 4)                                         60.79      63.42      62.08      20164
---: (label_id: 5)                                          0.00       0.00       0.00        439
---; (label_id: 6)                                          0.00       0.00       0.00        176
---? (label_id: 7)                                         30.77      51.57      38.54       1590
---— (label_id: 8)                                         10.69       8.48       9.46       1509
---… (label_id: 9)                                          0.00       0.00       0.00        111
----------------------
---micro avg                                               90.10      90.10      90.10     352973
---macro avg                                               31.54      33.63      32.18     352973
---weighted avg                                            91.01      90.10      90.48     352973
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       3181
----------------------
---micro avg                                              100.00     100.00     100.00       3181
---macro avg                                              100.00     100.00     100.00       3181
---weighted avg                                           100.00     100.00     100.00       3181
---
---[INFO] - Epoch 3, global step 799: val_loss reached 0.25942 (best 0.25942), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=3.ckpt" as top 3
---[INFO] - Punctuation report: 
---label                                                precision    recall       f1           support   
--- (label_id: 0)                                          97.63      95.55      96.58     303856
---! (label_id: 1)                                          0.00       0.00       0.00        155
---, (label_id: 2)                                         46.84      57.30      51.54      23143
---- (label_id: 3)                                         75.90      57.49      65.42       1830
---. (label_id: 4)                                         59.34      67.15      63.00      20164
---: (label_id: 5)                                          0.00       0.00       0.00        439
---; (label_id: 6)                                          0.00       0.00       0.00        176
---? (label_id: 7)                                         33.55      48.62      39.70       1590
---— (label_id: 8)                                         12.65       6.36       8.47       1509
---… (label_id: 9)                                          0.00       0.00       0.00        111
----------------------
---micro avg                                               90.39      90.39      90.39     352973
---macro avg                                               32.59      33.25      32.47     352973
---weighted avg                                            91.10      90.39      90.67     352973
---
---[INFO] - Domain report: 
---label                                                precision    recall       f1           support   
---0 (label_id: 0)                                        100.00     100.00     100.00       3181
----------------------
---micro avg                                              100.00     100.00     100.00       3181
---macro avg                                              100.00     100.00     100.00       3181
---weighted avg                                           100.00     100.00     100.00       3181
---
---[INFO] - Epoch 4, global step 999: val_loss reached 0.26189 (best 0.25942), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-10_14-21-43/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=4.ckpt" as top 3
--+[INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
--+[INFO] - Using native 16bit precision.
--diff --git a/experiment/main.py b/experiment/main.py
--index 4ae03e6..51abddd 100644
----- a/experiment/main.py
--+++ b/experiment/main.py
--@@ -23,6 +23,7 @@ snoop.install()
-- 
-- @hydra.main(config_name="config")
-- def main(cfg: DictConfig)->None:
--+    torch.set_printoptions(sci_mode=False)
--     data_id = str(int(time()))
--     def savecounter():
--         # pp(os.system(f'rm -r {cfg.model.dataset.data_dir}/*.{data_id}.csv'))
--diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
--index 20e9816..d893ed1 100644
----- a/experiment/models/punctuation_domain_model.py
--+++ b/experiment/models/punctuation_domain_model.py
--@@ -171,7 +171,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-- 
--         self.log('lr', lr, prog_bar=True)
--         self.log('train_loss', loss)
---        self.log('gamma', self.grad_reverse.scale)
--+        self.log('gamma', self.grad_reverse.scale,logger=True)
-- 
--         return {'loss': loss, 'lr': lr}
-- 
--@@ -262,11 +262,11 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
--         avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
-- 
--         # calculate metrics and log classification report for Punctuation task
---        punct_precision, punct_recall, punct_f1, punct_report = self.punct_class_report.compute()
--+        punct_precision, punct_recall, punct_f1, punct_report, punctuation_cm = self.punct_class_report.compute()
--         logging.info(f'Punctuation report: {punct_report}')
-- 
--         # calculate metrics and log classification report for domainalization task
---        domain_precision, domain_recall, domain_f1, domain_report = self.domain_class_report.compute()
--+        domain_precision, domain_recall, domain_f1, domain_report, domain_cm = self.domain_class_report.compute()
--         logging.info(f'Domain report: {domain_report}')
-- 
--         self.log('val_loss', avg_loss, prog_bar=True)
--@@ -276,6 +276,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
--         self.log('domain_precision', domain_precision)
--         self.log('domain_f1', domain_f1)
--         self.log('domain_recall', domain_recall)
--+        self.log('punctuation_cm', punctuation_cm.__str__())
--+        self.log('domain_cm', domain_cm.__str__())
-- 
--     def test_epoch_end(self, outputs):
--         if outputs is not None and len(outputs) == 0:
--@@ -298,11 +300,11 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
--         avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()
-- 
--         # calculate metrics and log classification report for Punctuation task
---        punct_precision, punct_recall, punct_f1, punct_report = self.punct_class_report.compute()
--+        punct_precision, punct_recall, punct_f1, punct_report, punct_cm = self.punct_class_report.compute()
--         logging.info(f'Punctuation report: {punct_report}')
-- 
--         # calculate metrics and log classification report for domainalization task
---        domain_precision, domain_recall, domain_f1, domain_report = self.domain_class_report.compute()
--+        domain_precision, domain_recall, domain_f1, domain_report, domain_cm = self.domain_class_report.compute()
--         logging.info(f'Domain report: {domain_report}')
-- 
--         self.log('test_loss', avg_loss, prog_bar=True)
--@@ -312,6 +314,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
--         self.log('domain_precision', domain_precision)
--         self.log('domain_f1', domain_f1)
--         self.log('domain_recall', domain_recall)
--+        self.log('punctuation_cm', punct_cm.__str__())
--+        self.log('domain_cm', domain_cm.__str__())
-- 
--     def setup_optimization(self, optim_config: Optional[Union[DictConfig, Dict]] = None):
--         """
--@@ -724,7 +728,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
--             tokenizer= self.tokenizer,
--             queries=queries, 
--             max_seq_length=self.hparams.model.dataset.max_seq_length,
---            punct_label_ids=self._cfg.model.punct_label_ids,
--+            punct_label_ids=self.labels_to_ids,
--             attach_label_to_end=self._cfg.model.dataset.attach_label_to_end)
--         batch=ds[0]
--         attention_mask = batch['attention_mask']
--diff --git a/experiment/testing.py b/experiment/testing.py
--index 667f776..272edb8 100644
----- a/experiment/testing.py
--+++ b/experiment/testing.py
--@@ -21,50 +21,9 @@ from copy import deepcopy
-- import snoop
-- snoop.install()
-- 
---# @hydra.main(config_name="config")
---# def main(cfg: DictConfig)->None:
---#     data_id = str(int(time()))
---#     def savecounter():
---#         # pp(os.system(f'rm -r {cfg.model.dataset.data_dir}/*.{data_id}.csv'))
---#         pp(os.system(f'rm -r {cfg.tmp_path}/*.{data_id}.csv'))
---#     atexit.register(savecounter)
---
---#     cfg.model.maximum_unfrozen=max(cfg.model.maximum_unfrozen,cfg.model.unfrozen)
---
---#     pp(cfg)
---#     pl.seed_everything(cfg.seed)
---#     trainer = pl.Trainer(**cfg.trainer)
---#     exp_manager(trainer, cfg.exp_manager)
---#     model = PunctuationDomainModel(cfg=cfg, trainer=trainer, data_id = data_id)
---    
---#     while(model.hparams.model.unfrozen<=cfg.model.maximum_unfrozen and model.hparams.model.unfrozen>=0):
---#         trainer.current_epoch=0
---#         lr_finder = trainer.tuner.lr_find(model,min_lr=1e-10, max_lr=1e-02, num_training=80, early_stop_threshold=None)
---#         # Results can be found in
---#         pp(lr_finder.results)
---#         new_lr = lr_finder.suggestion()
---#         model.hparams.model.optim.lr = new_lr
---#         model.dm.reset()
---#         trainer.current_epoch=0
---#         trainer.fit(model)
---#         try:
---#             model.unfreeze(cfg.model.unfreeze_step)
---#         except:
---#             pp('training complete.')
---#             break
---#     if cfg.model.nemo_path:
---#         model.save_to(cfg.model.nemo_path)
---
---    
---    
---#     gpu = 1 if cfg.trainer.gpus != 0 else 0
---#     # model.dm.setup('test')
---#     trainer = pl.Trainer(gpus=gpu)
---#     trainer.test(model,ckpt_path=None)
---
---
---@hydra.main(config_name="test_config")
--+@hydra.main(config_path="../Punctuation_with_Domain_discriminator/TEDstart2021-02-11_07-55-58/",config_name="hparams.yaml")
-- def main(cfg : DictConfig) -> None:
--+    torch.set_printoptions(sci_mode=False)
--     # trainer=pl.Trainer(**cfg.trainer)
--     # exp_manager(trainer, cfg.get("exp_manager", None))
--     # do_training = False
--@@ -74,19 +33,22 @@ def main(cfg : DictConfig) -> None:
--     #     if cfg.model.nemo_path:
--     #         model.save_to(cfg.model.nemo_path)
--     # gpu = 1 if cfg.trainer.gpus != 0 else 0
---    model = PunctuationDomainModel.restore_from(restore_path=cfg.exp_manager.restore_path, override_config_path=cfg.exp_manager.override_config_path, )
---    trainer = pl.Trainer(gpus=gpu)
---    model.set_trainer(trainer)
---    queries = [
---        'we bought four shirts one pen and a mug from the nvidia gear store in santa clara',
---        'what can i do for you today',
---        'how are you',
---    ]
---    inference_results = model.add_punctuation_capitalization(queries)
---
---    for query, result in zip(queries, inference_results):
---        logging.info(f'Query : {query}')
---        logging.info(f'Result: {result.strip()}\n')
--+    # model = PunctuationDomainModel.restore_from(restore_path=cfg.exp_manager.restore_path, override_config_path=cfg.exp_manager.override_config_path, )
--+    model = PunctuationDomainModel.load_from_checkpoint( #TEDend2021-02-11_07-57-33  # TEDstart2021-02-11_07-55-58
--+    checkpoint_path="/home/nxingyu2/project/Punctuation_with_Domain_discriminator/TEDstart2021-02-11_07-55-58/checkpoints/Punctuation_with_Domain_discriminator-last.ckpt")
--+    trainer = pl.Trainer(**cfg.trainer)
--+    # trainer = pl.Trainer(gpus=gpu)
--+    trainer.test(model,ckpt_path=None)
--+    # queries = [
--+    #     'we bought four shirts one pen and a mug from the nvidia gear store in santa clara',
--+    #     'what can i do for you today',
--+    #     'how are you',
--+    # ]
--+    # inference_results = model.add_punctuation_capitalization(queries)
--+
--+    # for query, result in zip(queries, inference_results):
--+    #     logging.info(f'Query : {query}')
--+    #     logging.info(f'Result: {result.strip()}\n')
-- 
-- if __name__ == "__main__":
--     main()
--\ No newline at end of file
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/hparams.yaml
-deleted file mode 100644
-index d14b3a5..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/hparams.yaml
-+++ /dev/null
-@@ -1,111 +0,0 @@
--seed: 42
--trainer:
--  gpus: 1
--  num_nodes: 1
--  max_epochs: 15
--  max_steps: null
--  accumulate_grad_batches: 4
--  gradient_clip_val: 0
--  amp_level: O1
--  precision: 16
--  accelerator: ddp
--  checkpoint_callback: false
--  logger: false
--  log_every_n_steps: 1
--  val_check_interval: 1.0
--  resume_from_checkpoint: null
--exp_manager:
--  exp_dir: /home/nxingyu2/project/
--  name: Punctuation_with_Domain_discriminator
--  create_tensorboard_logger: true
--  create_checkpoint_callback: true
--base_path: /home/nxingyu2/data
--tmp_path: /home/nxingyu2/data/tmp
--model:
--  nemo_path: null
--  transformer_path: distilbert-base-uncased
--  unfrozen: 0
--  maximum_unfrozen: 1
--  unfreeze_step: 1
--  punct_label_ids:
--  - ''
--  - '!'
--  - ','
--  - '-'
--  - .
--  - ':'
--  - ;
--  - '?'
--  - —
--  - …
--  punct_class_weights: false
--  dataset:
--    data_dir: /home/nxingyu2/data
--    labelled:
--    - /home/nxingyu2/data/ted_talks_processed
--    unlabelled: null
--    max_seq_length: 128
--    pad_label: ''
--    ignore_extra_tokens: false
--    ignore_start_end: false
--    use_cache: false
--    num_workers: 4
--    pin_memory: true
--    drop_last: true
--    num_labels: 10
--    num_domains: 1
--    test_unlabelled: true
--    attach_label_to_end: none
--    train_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--    validation_ds:
--      shuffle: true
--      num_samples: -1
--      batch_size: 4
--  tokenizer:
--    tokenizer_name: distilbert-base-uncased
--    vocab_file: null
--    tokenizer_model: null
--    special_tokens: null
--  language_model:
--    pretrained_model_name: distilbert-base-uncased
--    lm_checkpoint: null
--    config_file: null
--    config: null
--  punct_head:
--    punct_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: dice
--  domain_head:
--    domain_num_fc_layers: 1
--    fc_dropout: 0.1
--    activation: relu
--    log_softmax: false
--    use_transformer_init: true
--    loss: cel
--    gamma: 0
--    pooling: mean_max
--    idx_conditioned_on: 0
--  dice_loss:
--    epsilon: 0.01
--    alpha: 3
--    macro_average: true
--  focal_loss:
--    gamma: 1
--  optim:
--    name: adamw
--    lr: 0.01
--    weight_decay: 0.0
--    sched:
--      name: CosineAnnealing
--      warmup_steps: null
--      warmup_ratio: 0.1
--      min_lr: 1.0e-10
--      last_epoch: -1
--      monitor: val_loss
--      reduce_on_plateau: false
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/lightning_logs.txt
-deleted file mode 100644
-index 180b7db..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/lightning_logs.txt
-+++ /dev/null
-@@ -1,23 +0,0 @@
--GPU available: True, used: True
--TPU available: None, using: 0 TPU cores
--LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
--Using native 16bit precision.
--initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | DistilBertModel      | 66.4 M
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 1.5 K 
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--10.8 K    Trainable params
--66.4 M    Non-trainable params
--66.4 M    Total params
--Epoch 0, global step 199: val_loss reached 0.30744 (best 0.30744), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.31-epoch=0.ckpt" as top 3
--Saving latest checkpoint...
--Epoch 0, global step 198: val_loss reached 0.30744 (best 0.30744), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.31-epoch=0-v0.ckpt" as top 3
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/nemo_error_log.txt
-deleted file mode 100644
-index 13088e0..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/nemo_error_log.txt
-+++ /dev/null
-@@ -1,10 +0,0 @@
--[NeMo W 2021-02-11 11:48:02 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-11 11:48:10 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-11 11:49:21 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fa57cf4f3d0> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-11 11:49:30 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fa57cf4cac0> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/nemo_log_globalrank-0_localrank-0.txt
-deleted file mode 100644
-index d2dd215..0000000
---- a/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02/nemo_log_globalrank-0_localrank-0.txt
-+++ /dev/null
-@@ -1,12 +0,0 @@
--[NeMo I 2021-02-11 11:48:02 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_11-48-02
--[NeMo I 2021-02-11 11:48:02 exp_manager:519] TensorboardLogger has been set up
--[NeMo W 2021-02-11 11:48:02 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
--[NeMo W 2021-02-11 11:48:10 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
--      warnings.warn(*args, **kwargs)
--    
--[NeMo W 2021-02-11 11:49:21 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fa57cf4f3d0> was reported to be 798 (when accessing len(dataloader)), but 799 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
--[NeMo W 2021-02-11 11:49:30 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fa57cf4cac0> was reported to be 100 (when accessing len(dataloader)), but 101 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
--      warnings.warn(warn_msg)
--    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/events.out.tfevents.1613018787.intern-instance.26577.0 b/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/events.out.tfevents.1613018787.intern-instance.26577.0
-index 7733536..d29916d 100644
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/events.out.tfevents.1613018787.intern-instance.26577.0 and b/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/events.out.tfevents.1613018787.intern-instance.26577.0 differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/lightning_logs.txt
-index d732323..1982832 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/lightning_logs.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/lightning_logs.txt
-@@ -18,3 +18,10 @@ initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
- 2.4 M     Trainable params
- 106 M     Non-trainable params
- 108 M     Total params
-+Epoch 0, global step 5254: val_loss reached 0.12680 (best 0.12680), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.13-epoch=0.ckpt" as top 3
-+Epoch 1, global step 10509: val_loss reached 0.99380 (best 0.12680), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.99-epoch=1.ckpt" as top 3
-+Epoch 2, global step 15764: val_loss reached 0.48886 (best 0.12680), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.49-epoch=2.ckpt" as top 3
-+Epoch 3, global step 21019: val_loss reached 0.12802 (best 0.12680), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.13-epoch=3.ckpt" as top 3
-+Epoch 4, global step 26274: val_loss reached 0.18930 (best 0.12680), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.19-epoch=4.ckpt" as top 3
-+Epoch 5, step 31529: val_loss was not in top 3
-+Epoch 6, global step 36784: val_loss reached 0.17587 (best 0.12680), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.18-epoch=6.ckpt" as top 3
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/nemo_error_log.txt
-index c5d946d..03caccf 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/nemo_error_log.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/nemo_error_log.txt
-@@ -2,3 +2,9 @@
- [NeMo W 2021-02-11 12:46:27 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-       warnings.warn(*args, **kwargs)
-     
-+[NeMo W 2021-02-11 15:27:02 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f2ae415a5b0> was reported to be 21018 (when accessing len(dataloader)), but 21019 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-+[NeMo W 2021-02-11 15:49:15 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f2ae4116dc0> was reported to be 2627 (when accessing len(dataloader)), but 2628 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/nemo_log_globalrank-0_localrank-0.txt
-index c5ed369..e00bdaf 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/nemo_log_globalrank-0_localrank-0.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-11_12-45-53/nemo_log_globalrank-0_localrank-0.txt
-@@ -4,3 +4,9 @@
- [NeMo W 2021-02-11 12:46:27 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-       warnings.warn(*args, **kwargs)
-     
-+[NeMo W 2021-02-11 15:27:02 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f2ae415a5b0> was reported to be 21018 (when accessing len(dataloader)), but 21019 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-+[NeMo W 2021-02-11 15:49:15 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f2ae4116dc0> was reported to be 2627 (when accessing len(dataloader)), but 2628 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-diff --git a/README.md b/README.md
-index 9c03a29..6407311 100644
---- a/README.md
-+++ b/README.md
-@@ -648,4 +648,42 @@ electra-small can't train opensubtitles
-  'test_loss': tensor(0.2392, device='cuda:0')}
- 
- 
--114802 distilbert opensub
-\ No newline at end of file
-+114802 distilbert opensub
-+ (label_id: 0)                                          97.41      94.81      96.09   13849820
-+! (label_id: 1)                                         28.56      40.82      33.60     228399
-+, (label_id: 2)                                         45.53      48.81      47.12     981518
-+- (label_id: 3)                                         58.45      42.81      49.42      85993
-+. (label_id: 4)                                         58.33      66.02      61.94    1857888
-+: (label_id: 5)                                         92.71      54.91      68.97       3983
-+; (label_id: 6)                                          0.00       0.00       0.00       1353
-+? (label_id: 7)                                         58.06      55.83      56.93     474152
-+— (label_id: 8)                                          0.00       0.00       0.00        512
-+… (label_id: 9)                                         22.95      23.05      23.00     161832
-+-------------------
-+micro avg                                               86.54      86.54      86.54   17645450
-+macro avg                                               46.20      42.71      43.71   17645450
-+weighted avg                                            87.57      86.54      86.99   17645450
-+
-+-------------------
-+tensor([[ 13130599.,  11431.,   99043.,  18848.,   130364.,  332.,  164.,   45236.,    56.,  44072.],
-+        [    31464.,  93226.,   54383.,   2660.,   123020.,   96.,   40.,   13890.,    80.,   7584.],
-+        [   178120.,  39328.,  479117.,   4423.,   279274.,  308.,  152.,   44966.,    76.,  26535.],
-+        [    16123.,    980.,    2488.,  36816.,     4584.,   36.,    4.,     424.,     0.,   1536.],
-+        [   389548.,  69723.,  268286.,   9439.,  1226576.,  884.,  922.,   99381.,   204.,  37687.],
-+        [       20.,      0.,      36.,     56.,       56., 2187.,    0.,       4.,     0.,      0.],
-+        [        0.,      0.,       0.,      0.,        0.,    0.,    0.,       0.,     0.,      0.],
-+        [    67051.,  10711.,   36473.,   1870.,    67834.,   64.,   47.,  264740.,    72.,   7108.],
-+        [        0.,      0.,       0.,      0.,        0.,    0.,    0.,       0.,     0.,      0.],
-+        [    36895.,   3000.,   41692.,  11881.,    26180.,   76.,   24.,    5511.,    24.,  37310.]], device='cuda:0')
-+
-+
-+ 'punct_f1': tensor(43.7067, device='cuda:0'),
-+ 'punct_precision': tensor(46.1993, device='cuda:0'),
-+ 'punct_recall': tensor(42.7068, device='cuda:0'),
-+ 'test_loss': tensor(-0.0626, device='cuda:0')}
-+
-+
-+electra base domain adversarial gamma 0
-+
-+
-+electra base domain adversarial gamma 0.05
-\ No newline at end of file
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index ef10a04..3cf8809 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -124,7 +124,7 @@ model:
-         log_softmax: false
-         use_transformer_init: true
-         loss: 'cel'
--        gamma: 0 #0.1 # coefficient of gradient reversal
-+        gamma: 0.1 #0.1 # coefficient of gradient reversal
-         pooling: 'mean_max' # 'mean' mean_max
-         idx_conditioned_on: 0
-     
-diff --git a/experiment/info.log b/experiment/info.log
-index d45bd15..dc33065 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,78 +1,4 @@
- [INFO] - GPU available: True, used: True
- [INFO] - TPU available: None, using: 0 TPU cores
--[INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
-+[INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
- [INFO] - Using native 16bit precision.
--[INFO] - shuffling train set
--[INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.01
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.CosineAnnealing object at 0x7f2aadd1dd30>" 
--will be used during training (effective maximum steps = 78825) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--min_lr: 1.0e-10
--last_epoch: -1
--max_steps: 78825
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 3.1 K 
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--2.4 M     Trainable params
--106 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          97.31       1.05       2.09      17162
--! (label_id: 1)                                          0.28      10.34       0.54         58
--, (label_id: 2)                                          3.49      10.34       5.22        532
--- (label_id: 3)                                          0.00       0.00       0.00         27
--. (label_id: 4)                                          7.95       2.53       3.84       1028
--: (label_id: 5)                                          0.00       0.00       0.00          0
--; (label_id: 6)                                          0.03      50.00       0.06          4
--? (label_id: 7)                                          0.37       2.07       0.63        145
--— (label_id: 8)                                          0.00       0.00       0.00          0
--… (label_id: 9)                                          0.52      36.00       1.03         75
---------------------
--micro avg                                                1.58       1.58       1.58      19031
--macro avg                                               13.74      14.04       1.68      19031
--weighted avg                                            88.29       1.58       2.25      19031
--
---------------------
--181.00  0.00  0.00  1.00  4.00  0.00  0.00  0.00  0.00  0.00
--2048.00  6.00 37.00  4.00 59.00  0.00  1.00  9.00  0.00 10.00
--1435.00 10.00 55.00  0.00 68.00  0.00  0.00  5.00  0.00  1.00
--80.00  0.00  3.00  0.00  2.00  0.00  0.00  0.00  0.00  1.00
--273.00  1.00 17.00  0.00 26.00  0.00  0.00  8.00  0.00  2.00
--2232.00  7.00 48.00  6.00 81.00  0.00  1.00 12.00  0.00  1.00
--5515.00 19.00 217.00 10.00 431.00  0.00  2.00 72.00  0.00 30.00
--754.00  0.00 13.00  2.00 28.00  0.00  0.00  3.00  0.00  3.00
--15.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
--4629.00 15.00 142.00  4.00 329.00  0.00  0.00 36.00  0.00 27.00
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                          0.00       0.00       0.00         80
--1 (label_id: 1)                                         50.00     100.00      66.67         80
---------------------
--micro avg                                               50.00      50.00      50.00        160
--macro avg                                               25.00      50.00      33.33        160
--weighted avg                                            25.00      50.00      33.33        160
--
---------------------
-- 0.00  0.00
--80.00 80.00
diff --git a/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/hparams.yaml
deleted file mode 100644
index fb46ccf..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/hparams.yaml
+++ /dev/null
@@ -1,112 +0,0 @@
-seed: 42
-trainer:
-  gpus: 1
-  num_nodes: 1
-  max_epochs: 15
-  max_steps: null
-  accumulate_grad_batches: 4
-  gradient_clip_val: 0
-  amp_level: O1
-  precision: 16
-  accelerator: ddp
-  checkpoint_callback: false
-  logger: false
-  log_every_n_steps: 1
-  val_check_interval: 1.0
-  resume_from_checkpoint: null
-exp_manager:
-  exp_dir: /home/nxingyu2/project/
-  name: Punctuation_with_Domain_discriminator
-  create_tensorboard_logger: true
-  create_checkpoint_callback: true
-base_path: /home/nxingyu2/data
-tmp_path: /home/nxingyu2/data/tmp
-model:
-  nemo_path: null
-  transformer_path: google/electra-base-discriminator
-  unfrozen: 0
-  maximum_unfrozen: 1
-  unfreeze_step: 1
-  punct_label_ids:
-  - ''
-  - '!'
-  - ','
-  - '-'
-  - .
-  - ':'
-  - ;
-  - '?'
-  - —
-  - …
-  punct_class_weights: false
-  dataset:
-    data_dir: /home/nxingyu2/data
-    labelled:
-    - /home/nxingyu2/data/open_subtitles_processed
-    unlabelled:
-    - /home/nxingyu2/data/ted_talks_processed
-    max_seq_length: 128
-    pad_label: ''
-    ignore_extra_tokens: false
-    ignore_start_end: false
-    use_cache: false
-    num_workers: 4
-    pin_memory: true
-    drop_last: true
-    num_labels: 10
-    num_domains: 2
-    test_unlabelled: true
-    attach_label_to_end: none
-    train_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 4
-    validation_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 4
-  tokenizer:
-    tokenizer_name: google/electra-base-discriminator
-    vocab_file: null
-    tokenizer_model: null
-    special_tokens: null
-  language_model:
-    pretrained_model_name: google/electra-base-discriminator
-    lm_checkpoint: null
-    config_file: null
-    config: null
-  punct_head:
-    punct_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: dice
-  domain_head:
-    domain_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: cel
-    gamma: 0.1
-    pooling: mean_max
-    idx_conditioned_on: 0
-  dice_loss:
-    epsilon: 0.01
-    alpha: 3
-    macro_average: true
-  focal_loss:
-    gamma: 1
-  optim:
-    name: adamw
-    lr: 0.01
-    weight_decay: 0.0
-    sched:
-      name: CosineAnnealing
-      warmup_steps: null
-      warmup_ratio: 0.1
-      min_lr: 1.0e-10
-      last_epoch: -1
-      monitor: val_loss
-      reduce_on_plateau: false
diff --git a/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/lightning_logs.txt
deleted file mode 100644
index f7ccecc..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/lightning_logs.txt
+++ /dev/null
@@ -1,70 +0,0 @@
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
-Using native 16bit precision.
-initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 108 M 
-1 | punct_classifier    | TokenClassifier      | 7.7 K 
-2 | domain_classifier   | SequenceClassifier   | 3.1 K 
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-2.4 M     Trainable params
-106 M     Non-trainable params
-108 M     Total params
-Epoch 0, global step 5254: val_loss reached 115.21539 (best 115.21539), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=115.22-epoch=0.ckpt" as top 3
-Epoch 1, global step 10509: val_loss reached 22.29331 (best 22.29331), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=22.29-epoch=1.ckpt" as top 3
-Epoch 2, global step 15764: val_loss reached 13.03634 (best 13.03634), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=13.04-epoch=2.ckpt" as top 3
-Epoch 3, global step 21019: val_loss reached 15.18966 (best 13.03634), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=15.19-epoch=3.ckpt" as top 3
-Epoch 4, global step 26274: val_loss reached 10.95131 (best 10.95131), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=10.95-epoch=4.ckpt" as top 3
-Epoch 5, global step 31529: val_loss reached 11.56726 (best 10.95131), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=11.57-epoch=5.ckpt" as top 3
-Epoch 6, global step 36784: val_loss reached 4.74845 (best 4.74845), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=4.75-epoch=6.ckpt" as top 3
-Epoch 7, global step 42039: val_loss reached 10.55844 (best 4.74845), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=10.56-epoch=7.ckpt" as top 3
-Epoch 8, global step 47294: val_loss reached 10.89379 (best 4.74845), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=10.89-epoch=8.ckpt" as top 3
-Epoch 9, step 52549: val_loss was not in top 3
-Epoch 10, global step 57804: val_loss reached 1.42583 (best 1.42583), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=1.43-epoch=10.ckpt" as top 3
-Epoch 11, global step 63059: val_loss reached 2.41300 (best 1.42583), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=2.41-epoch=11.ckpt" as top 3
-Epoch 12, global step 68314: val_loss reached 1.46759 (best 1.42583), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/checkpoints/Punctuation_with_Domain_discriminator---val_loss=1.47-epoch=12.ckpt" as top 3
-Epoch 13, step 73569: val_loss was not in top 3
-Epoch 14, step 78824: val_loss was not in top 3
-Saving latest checkpoint...
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 108 M 
-1 | punct_classifier    | TokenClassifier      | 7.7 K 
-2 | domain_classifier   | SequenceClassifier   | 3.1 K 
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-7.1 M     Trainable params
-101 M     Non-trainable params
-108 M     Total params
-Epoch 0, step 84079: val_loss was not in top 3
-Epoch 1, step 89334: val_loss was not in top 3
-Epoch 2, step 94589: val_loss was not in top 3
-Epoch 3, step 99844: val_loss was not in top 3
-Epoch 4, step 105099: val_loss was not in top 3
-Epoch 5, step 110354: val_loss was not in top 3
-Epoch 6, step 115609: val_loss was not in top 3
-Epoch 7, step 120864: val_loss was not in top 3
-Epoch 8, step 126119: val_loss was not in top 3
-Epoch 9, step 131374: val_loss was not in top 3
-Epoch 10, step 136629: val_loss was not in top 3
-Epoch 11, step 141884: val_loss was not in top 3
-Epoch 12, step 147139: val_loss was not in top 3
-Epoch 13, step 152394: val_loss was not in top 3
-Epoch 14, step 157649: val_loss was not in top 3
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-Using environment variable NODE_RANK for node rank (0).
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
diff --git a/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/nemo_error_log.txt
deleted file mode 100644
index fb35582..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/nemo_error_log.txt
+++ /dev/null
@@ -1,13 +0,0 @@
-[NeMo W 2021-02-12 19:14:52 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-12 19:15:21 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-12 21:46:12 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f98a6783f70> was reported to be 21018 (when accessing len(dataloader)), but 21019 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-12 22:07:38 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f98a675ea60> was reported to be 2627 (when accessing len(dataloader)), but 2628 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-14 22:33:54 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 94f9355..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,15 +0,0 @@
-[NeMo I 2021-02-12 19:14:52 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-12_19-14-52
-[NeMo I 2021-02-12 19:14:52 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-12 19:14:52 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-12 19:15:21 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-12 21:46:12 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f98a6783f70> was reported to be 21018 (when accessing len(dataloader)), but 21019 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-12 22:07:38 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f98a675ea60> was reported to be 2627 (when accessing len(dataloader)), but 2628 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-14 22:33:54 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-22_14-06-14/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-22_14-06-14/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-22_14-06-14/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-22_14-06-14/events.out.tfevents.1613973984.intern-instance.18302.0 b/Punctuation_with_Domain_discriminator/2021-02-22_14-06-14/events.out.tfevents.1613973984.intern-instance.18302.0
deleted file mode 100644
index 709c72e..0000000
Binary files a/Punctuation_with_Domain_discriminator/2021-02-22_14-06-14/events.out.tfevents.1613973984.intern-instance.18302.0 and /dev/null differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-22_14-06-14/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-22_14-06-14/git-info.log
deleted file mode 100644
index 058ff93..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-22_14-06-14/git-info.log
+++ /dev/null
@@ -1,1396 +0,0 @@
-commit hash: c6a8b49d859383000afe2a283030f3990928c023
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index 529e975..5f6090f 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -97,7 +97,7 @@ model:
-         num_domains: 2
-         test_unlabelled: true
-         attach_label_to_end: #None # false if attach to start none if dont mask
--        pad_start: 32
-+        pad_start: 64
-         train_ds:
-             shuffle: true
-             num_samples: -1
-@@ -126,12 +126,12 @@ model:
-         # unfrozen_layers: 1
-     
-     punct_head:
--        punct_num_fc_layers: 3
-+        punct_num_fc_layers: 0
-         fc_dropout: 0.1
-         activation: 'gelu'
-         log_softmax: false
-         use_transformer_init: true
--        loss: 'dice'
-+        loss: 'crf'
-         bilstm: false
- 
-     domain_head:
-@@ -147,7 +147,7 @@ model:
-     
-     dice_loss:
-         epsilon: 0.01
--        alpha: 4
-+        alpha: 3
-         macro_average: true
- 
-     focal_loss: 
-diff --git a/experiment/core/utils.py b/experiment/core/utils.py
-index 3c83339..c88f0be 100644
---- a/experiment/core/utils.py
-+++ b/experiment/core/utils.py
-@@ -104,7 +104,7 @@ def subword_tokenize(tokenizer,tokens, pad_start):
- def chunk_to_len(max_seq_length,tokenizer,attach_label_to_end,pad_start:int,tokens, labels=None):
-     subwords,token_start_idxs,token_end_idxs = subword_tokenize(tokenizer,tokens, pad_start)
-     teim=token_end_idxs%(max_seq_length-2) if attach_label_to_end else token_start_idxs%(max_seq_length-2)
--    breakpoints=(np.argwhere(teim[1:]<teim[:-1]).flatten()+1).tolist(
-+    breakpoints=(np.argwhere(teim[1:]<teim[:-1]).flatten()+1).tolist()
-     split_token_idxs=np.array_split(token_end_idxs,breakpoints) if attach_label_to_end else np.array_split(token_start_idxs,breakpoints)
-     split_subwords=np.array_split(subwords,np.arange(max_seq_length-2,len(subwords),max_seq_length-2))
-     ids=[pad_to_len(max_seq_length,tokenizer.convert_tokens_to_ids(['[CLS]']+list(_)+['[SEP]'])) for _ in split_subwords]
-diff --git a/experiment/info.log b/experiment/info.log
-index 6af8f44..104c5dd 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -2,1340 +2,3 @@
- [INFO] - TPU available: None, using: 0 TPU cores
- [INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
- [INFO] - Using native 16bit precision.
--[INFO] - shuffling train set
--[INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.02
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.CosineAnnealing object at 0x7fc902716100>" 
--will be used during training (effective maximum steps = 104) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--min_lr: 1.0e-08
--last_epoch: -1
--max_steps: 104
--)
--[INFO] - 
--  | Name                       | Type                 | Params
----------------------------------------------------------------------
--0 | transformer                | ElectraModel         | 108 M 
--1 | punct_classifier           | TokenClassifier      | 1.2 M 
--2 | domain_classifier          | SequenceClassifier   | 4.7 M 
--3 | punctuation_loss           | FocalDiceLoss        | 0     
--4 | domain_loss                | CrossEntropyLoss     | 0     
--5 | agg_loss                   | AggregatorLoss       | 0     
--6 | punct_class_report         | ClassificationReport | 0     
--7 | chunked_punct_class_report | ClassificationReport | 0     
--8 | domain_class_report        | ClassificationReport | 0     
----------------------------------------------------------------------
--5.9 M     Trainable params
--108 M     Non-trainable params
--114 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          80.24      45.75      58.28      34856
--! (label_id: 1)                                          0.00       0.00       0.00         34
--# (label_id: 2)                                          4.58      15.42       7.06       3230
--, (label_id: 3)                                          2.19       0.34       0.58       2376
--- (label_id: 4)                                          0.21       0.83       0.34        240
--. (label_id: 5)                                          3.85       2.31       2.89       2074
--: (label_id: 6)                                          0.22      21.43       0.44         56
--? (label_id: 7)                                          0.00       0.00       0.00        170
--… (label_id: 8)                                          0.00       0.00       0.00         10
---------------------
--micro avg                                               38.37      38.37      38.37      43046
--macro avg                                               10.14       9.56       7.73      43046
--weighted avg                                            65.62      38.37      47.89      43046
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--    15948.00         8.00      1568.00      1102.00       124.00      1026.00        32.00        66.00         2.00
--     1612.00         0.00       128.00       170.00        20.00       112.00         2.00        12.00         0.00
--     9068.00        18.00       498.00       686.00        28.00       498.00         6.00        66.00         4.00
--      306.00         0.00        40.00         8.00         2.00         8.00         2.00         0.00         0.00
--      702.00         0.00       214.00        12.00         2.00        14.00         0.00         2.00         0.00
--     1046.00         0.00        50.00        90.00         6.00        48.00         2.00         6.00         0.00
--     4388.00         8.00       296.00       260.00        32.00       330.00        12.00        16.00         4.00
--       94.00         0.00        10.00         0.00         0.00         0.00         0.00         0.00         0.00
--     1692.00         0.00       426.00        48.00        26.00        38.00         0.00         2.00         0.00
---------------------
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        358
---------------------
--micro avg                                              100.00     100.00     100.00        358
--macro avg                                              100.00     100.00     100.00        358
--weighted avg                                           100.00     100.00     100.00        358
--
---------------------
--           0
--      358.00
---------------------
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          95.13      90.73      92.88     553281
--! (label_id: 1)                                          7.51      18.88      10.75        482
--# (label_id: 2)                                         63.58      90.63      74.73      58468
--, (label_id: 3)                                         42.58      53.23      47.32      43682
--- (label_id: 4)                                         61.03      47.06      53.14       3534
--. (label_id: 5)                                         55.27      39.20      45.87      35434
--: (label_id: 6)                                         18.22      25.00      21.08        844
--? (label_id: 7)                                         19.55      20.03      19.79       2961
--… (label_id: 8)                                          0.00       0.00       0.00        239
---------------------
--micro avg                                               85.09      85.09      85.09     698925
--macro avg                                               40.32      42.75      40.62     698925
--weighted avg                                            86.50      85.09      85.44     698925
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--   502005.00        49.00      5294.00     13328.00      1329.00      5153.00       191.00       287.00        91.00
--      146.00        91.00         0.00       680.00         0.00       207.00        10.00        69.00         8.00
--    29057.00         0.00     52988.00       426.00       349.00       489.00         1.00        25.00         0.00
--    15957.00       171.00        88.00     23253.00       141.00     13648.00       240.00      1047.00        60.00
--      698.00         8.00        33.00       216.00      1663.00        83.00         0.00        24.00         0.00
--     4684.00        90.00        49.00      5244.00        44.00     13890.00       175.00       891.00        64.00
--      255.00        16.00         0.00       216.00         0.00       427.00       211.00        25.00         8.00
--      479.00        57.00        16.00       319.00         8.00      1537.00        16.00       593.00         8.00
--        0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
---------------------
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       5707
---------------------
--micro avg                                              100.00     100.00     100.00       5707
--macro avg                                              100.00     100.00     100.00       5707
--weighted avg                                           100.00     100.00     100.00       5707
--
---------------------
--           0
--     5707.00
---------------------
--
--[INFO] - Epoch 0, global step 12: val_loss reached 0.25918 (best 0.25918), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-22_12-24-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.26-epoch=0.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          93.74      94.76      94.25     553281
--! (label_id: 1)                                         10.67       4.98       6.79        482
--# (label_id: 2)                                         80.27      76.90      78.55      58468
--, (label_id: 3)                                         45.91      56.82      50.78      43682
--- (label_id: 4)                                         75.97      52.07      61.79       3534
--. (label_id: 5)                                         67.34      46.28      54.86      35434
--: (label_id: 6)                                         34.03      15.52      21.32        844
--? (label_id: 7)                                         41.24      22.09      28.77       2961
--… (label_id: 8)                                          0.00       0.00       0.00        239
---------------------
--micro avg                                               87.72      87.72      87.72     698925
--macro avg                                               49.91      41.05      44.12     698925
--weighted avg                                            87.81      87.72      87.60     698925
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--   524271.00        57.00     13383.00     14262.00      1490.00      5272.00       191.00       295.00        68.00
--       40.00        24.00         1.00       139.00         0.00        20.00         0.00         1.00         0.00
--    10606.00         0.00     44962.00       120.00        57.00       247.00         1.00        17.00         0.00
--    14984.00       179.00        56.00     24820.00       126.00     12818.00       272.00       689.00       123.00
--      499.00         0.00        58.00        17.00      1840.00         8.00         0.00         0.00         0.00
--     2362.00       181.00         8.00      3808.00        18.00     16399.00       249.00      1288.00        40.00
--       82.00         0.00         0.00        73.00         0.00        91.00       131.00         8.00         0.00
--      234.00        33.00         0.00       242.00         0.00       415.00         0.00       654.00         8.00
--      203.00         8.00         0.00       201.00         3.00       164.00         0.00         9.00         0.00
---------------------
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       5707
---------------------
--micro avg                                              100.00     100.00     100.00       5707
--macro avg                                              100.00     100.00     100.00       5707
--weighted avg                                           100.00     100.00     100.00       5707
--
---------------------
--           0
--     5707.00
---------------------
--
--[INFO] - Epoch 1, global step 25: val_loss reached 0.27507 (best 0.25918), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-22_12-24-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.28-epoch=1.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          95.83      93.42      94.61     553281
--! (label_id: 1)                                         24.00       9.96      14.08        482
--# (label_id: 2)                                         82.23      85.39      83.78      58468
--, (label_id: 3)                                         43.56      67.22      52.86      43682
--- (label_id: 4)                                         72.09      58.46      64.56       3534
--. (label_id: 5)                                         68.25      45.52      54.61      35434
--: (label_id: 6)                                         15.61      20.38      17.68        844
--? (label_id: 7)                                         39.51      38.33      38.91       2961
--… (label_id: 8)                                          2.08       6.69       3.17        239
---------------------
--micro avg                                               88.10      88.10      88.10     698925
--macro avg                                               49.24      47.26      47.14     698925
--weighted avg                                            89.49      88.10      88.50     698925
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--   516879.00        58.00      8390.00      9471.00      1185.00      2948.00       133.00       240.00        60.00
--        8.00        48.00         0.00       115.00         0.00        21.00         0.00         0.00         8.00
--    10541.00         0.00     49924.00        58.00        82.00       100.00         0.00         8.00         0.00
--    21800.00       145.00        48.00     29363.00       182.00     14874.00       282.00       606.00       107.00
--      702.00         0.00        74.00        16.00      2066.00         8.00         0.00         0.00         0.00
--     2484.00       141.00        32.00      3695.00        19.00     16128.00       238.00       870.00        24.00
--      282.00         1.00         0.00       299.00         0.00       305.00       172.00        35.00         8.00
--      299.00        57.00         0.00       499.00         0.00       857.00        10.00      1135.00        16.00
--      286.00        32.00         0.00       166.00         0.00       193.00         9.00        67.00        16.00
---------------------
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       5707
---------------------
--micro avg                                              100.00     100.00     100.00       5707
--macro avg                                              100.00     100.00     100.00       5707
--weighted avg                                           100.00     100.00     100.00       5707
--
---------------------
--           0
--     5707.00
---------------------
--
--[INFO] - Epoch 2, global step 38: val_loss reached 0.22484 (best 0.22484), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-22_12-24-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.22-epoch=2.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          96.30      93.59      94.93     553281
--! (label_id: 1)                                         18.29       6.64       9.74        482
--# (label_id: 2)                                         80.30      90.56      85.12      58468
--, (label_id: 3)                                         50.27      57.16      53.49      43682
--- (label_id: 4)                                         77.93      56.54      65.53       3534
--. (label_id: 5)                                         57.91      62.69      60.21      35434
--: (label_id: 6)                                         27.46      15.52      19.83        844
--? (label_id: 7)                                         34.90      44.41      39.08       2961
--… (label_id: 8)                                          2.88       3.35       3.09        239
---------------------
--micro avg                                               88.91      88.91      88.91     698925
--macro avg                                               49.58      47.83      47.89     698925
--weighted avg                                            89.62      88.91      89.19     698925
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--   517815.00        49.00      5396.00     10057.00      1184.00      2772.00       144.00       221.00        51.00
--        8.00        32.00         0.00       107.00         0.00        20.00         0.00         0.00         8.00
--    12737.00         0.00     52950.00        18.00       173.00        61.00         0.00         0.00         0.00
--    14614.00        91.00        32.00     24970.00       134.00      9028.00       316.00       391.00        98.00
--      533.00         0.00        25.00         0.00      1998.00         8.00         0.00         0.00         0.00
--     6903.00       211.00        65.00      7605.00        44.00     22215.00       235.00      1033.00        50.00
--       66.00         1.00         0.00       106.00         0.00       164.00       131.00         1.00         8.00
--      492.00        98.00         0.00       752.00         0.00      1085.00        10.00      1315.00        16.00
--      113.00         0.00         0.00        67.00         1.00        81.00         8.00         0.00         8.00
---------------------
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       5707
---------------------
--micro avg                                              100.00     100.00     100.00       5707
--macro avg                                              100.00     100.00     100.00       5707
--weighted avg                                           100.00     100.00     100.00       5707
--
---------------------
--           0
--     5707.00
---------------------
--
--[INFO] - Epoch 3, global step 51: val_loss reached 0.22367 (best 0.22367), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-22_12-24-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.22-epoch=3.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          96.78      93.13      94.92     553281
--! (label_id: 1)                                          8.47       6.64       7.44        482
--# (label_id: 2)                                         81.04      90.56      85.53      58468
--, (label_id: 3)                                         51.15      58.38      54.52      43682
--- (label_id: 4)                                         70.84      62.22      66.25       3534
--. (label_id: 5)                                         57.03      71.25      63.35      35434
--: (label_id: 6)                                         22.77      19.31      20.90        844
--? (label_id: 7)                                         53.73      34.55      42.06       2961
--… (label_id: 8)                                          0.93       3.77       1.49        239
---------------------
--micro avg                                               89.05      89.05      89.05     698925
--macro avg                                               49.19      48.87      48.50     698925
--weighted avg                                            90.10      89.05      89.46     698925
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--   515263.00        34.00      5326.00      8126.00       996.00      2268.00       127.00       208.00        50.00
--      113.00        32.00         0.00       115.00         0.00       100.00         1.00         9.00         8.00
--    12200.00         8.00     52946.00         9.00       146.00        27.00         0.00         0.00         0.00
--    16209.00       146.00        73.00     25500.00       134.00      7067.00       269.00       368.00        91.00
--      815.00         0.00        74.00         8.00      2199.00         8.00         0.00         0.00         0.00
--     7857.00       196.00        49.00      9223.00        59.00     25245.00       284.00      1287.00        65.00
--      117.00         1.00         0.00       197.00         0.00       212.00       163.00        18.00         8.00
--      191.00        57.00         0.00       299.00         0.00       326.00         0.00      1023.00         8.00
--      516.00         8.00         0.00       205.00         0.00       181.00         0.00        48.00         9.00
---------------------
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       5707
---------------------
--micro avg                                              100.00     100.00     100.00       5707
--macro avg                                              100.00     100.00     100.00       5707
--weighted avg                                           100.00     100.00     100.00       5707
--
---------------------
--           0
--     5707.00
---------------------
--
--[INFO] - Epoch 4, global step 64: val_loss reached 0.21327 (best 0.21327), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-22_12-24-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.21-epoch=4.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          96.92      92.79      94.81     553281
--! (label_id: 1)                                          6.64      11.83       8.51        482
--# (label_id: 2)                                         79.76      91.67      85.30      58468
--, (label_id: 3)                                         50.66      56.17      53.27      43682
--- (label_id: 4)                                         70.69      61.97      66.04       3534
--. (label_id: 5)                                         57.46      72.47      64.10      35434
--: (label_id: 6)                                         19.46      21.21      20.29        844
--? (label_id: 7)                                         36.26      48.19      41.38       2961
--… (label_id: 8)                                          0.00       0.00       0.00        239
---------------------
--micro avg                                               88.85      88.85      88.85     698925
--macro avg                                               46.43      50.70      48.19     698925
--weighted avg                                            90.01      88.85      89.30     698925
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--   513363.00        41.00      4705.00      8094.00      1022.00      2066.00       127.00       199.00        83.00
--      227.00        57.00         0.00       300.00         0.00       247.00         1.00        26.00         0.00
--    13355.00        16.00     53599.00        41.00       147.00        44.00         0.00         0.00         0.00
--    17171.00        90.00        57.00     24534.00       124.00      5971.00       210.00       208.00        66.00
--      818.00         0.00        66.00        16.00      2190.00         8.00         0.00         0.00         0.00
--     7631.00       187.00        41.00      9668.00        51.00     25679.00       309.00      1056.00        66.00
--      164.00         1.00         0.00       249.00         0.00       282.00       179.00        37.00         8.00
--      519.00        90.00         0.00       754.00         0.00      1112.00        18.00      1427.00        16.00
--       33.00         0.00         0.00        26.00         0.00        25.00         0.00         8.00         0.00
---------------------
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       5707
---------------------
--micro avg                                              100.00     100.00     100.00       5707
--macro avg                                              100.00     100.00     100.00       5707
--weighted avg                                           100.00     100.00     100.00       5707
--
---------------------
--           0
--     5707.00
---------------------
--
--[INFO] - Epoch 5, global step 77: val_loss reached 0.21444 (best 0.21327), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-22_12-24-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.21-epoch=5.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          96.65      93.42      95.00     553281
--! (label_id: 1)                                         21.62       4.98       8.09        482
--# (label_id: 2)                                         80.51      91.16      85.50      58468
--, (label_id: 3)                                         51.27      60.60      55.55      43682
--- (label_id: 4)                                         74.39      59.17      65.91       3534
--. (label_id: 5)                                         60.76      69.93      65.02      35434
--: (label_id: 6)                                         40.81      15.52      22.49        844
--? (label_id: 7)                                         48.73      35.53      41.09       2961
--… (label_id: 8)                                          0.00       0.00       0.00        239
---------------------
--micro avg                                               89.38      89.38      89.38     698925
--macro avg                                               52.75      47.81      48.74     698925
--weighted avg                                            90.18      89.38      89.67     698925
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--   516850.00        42.00      5046.00      8949.00      1120.00      2354.00       127.00       208.00        75.00
--       16.00        24.00         0.00        50.00         0.00        21.00         0.00         0.00         0.00
--    12714.00         8.00     53299.00         8.00       147.00        27.00         0.00         0.00         0.00
--    16500.00       106.00        32.00     26472.00       125.00      7741.00       283.00       312.00        58.00
--      662.00         0.00        50.00         0.00      2091.00         8.00         0.00         0.00         0.00
--     6219.00       245.00        41.00      7690.00        51.00     24780.00       295.00      1365.00        98.00
--       25.00         0.00         0.00        49.00         0.00       108.00       131.00         8.00         0.00
--      221.00        57.00         0.00       437.00         0.00       376.00         8.00      1052.00         8.00
--       74.00         0.00         0.00        27.00         0.00        19.00         0.00        16.00         0.00
---------------------
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       5707
---------------------
--micro avg                                              100.00     100.00     100.00       5707
--macro avg                                              100.00     100.00     100.00       5707
--weighted avg                                           100.00     100.00     100.00       5707
--
---------------------
--           0
--     5707.00
---------------------
--
--[INFO] - Epoch 6, step 90: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          96.52      93.71      95.09     553281
--! (label_id: 1)                                         17.65       4.98       7.77        482
--# (label_id: 2)                                         80.53      91.13      85.50      58468
--, (label_id: 3)                                         52.91      58.36      55.50      43682
--- (label_id: 4)                                         74.02      59.42      65.92       3534
--. (label_id: 5)                                         60.49      70.58      65.14      35434
--: (label_id: 6)                                         30.28      16.47      21.34        844
--? (label_id: 7)                                         45.26      38.84      41.80       2961
--… (label_id: 8)                                          0.00       0.00       0.00        239
---------------------
--micro avg                                               89.52      89.52      89.52     698925
--macro avg                                               50.85      48.16      48.67     698925
--weighted avg                                            90.13      89.52      89.74     698925
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--   518462.00        42.00      5062.00      9466.00      1111.00      2572.00       135.00       216.00        75.00
--       33.00        24.00         0.00        50.00         0.00        29.00         0.00         0.00         0.00
--    12680.00         8.00     53283.00        16.00       147.00        35.00         0.00         0.00         0.00
--    14717.00       106.00        32.00     25493.00       125.00      7088.00       259.00       303.00        58.00
--      679.00         0.00        50.00         0.00      2100.00         8.00         0.00         0.00         0.00
--     6278.00       229.00        41.00      8078.00        51.00     25008.00       301.00      1268.00        90.00
--       82.00         0.00         0.00        82.00         0.00       140.00       139.00         8.00         8.00
--      286.00        73.00         0.00       471.00         0.00       543.00        10.00      1150.00         8.00
--       64.00         0.00         0.00        26.00         0.00        11.00         0.00        16.00         0.00
---------------------
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       5707
---------------------
--micro avg                                              100.00     100.00     100.00       5707
--macro avg                                              100.00     100.00     100.00       5707
--weighted avg                                           100.00     100.00     100.00       5707
--
---------------------
--           0
--     5707.00
---------------------
--
--[INFO] - Epoch 7, global step 103: val_loss reached 0.22231 (best 0.21327), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-22_12-24-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.22-epoch=7.ckpt" as top 3
--[INFO] - Saving latest checkpoint...
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.CosineAnnealing object at 0x7fc927199b50>" 
--will be used during training (effective maximum steps = 104) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--min_lr: 1.0e-08
--last_epoch: -1
--max_steps: 104
--)
--[INFO] - 
--  | Name                       | Type                 | Params
----------------------------------------------------------------------
--0 | transformer                | ElectraModel         | 108 M 
--1 | punct_classifier           | TokenClassifier      | 1.2 M 
--2 | domain_classifier          | SequenceClassifier   | 4.7 M 
--3 | punctuation_loss           | FocalDiceLoss        | 0     
--4 | domain_loss                | CrossEntropyLoss     | 0     
--5 | agg_loss                   | AggregatorLoss       | 0     
--6 | punct_class_report         | ClassificationReport | 0     
--7 | chunked_punct_class_report | ClassificationReport | 0     
--8 | domain_class_report        | ClassificationReport | 0     
----------------------------------------------------------------------
--13.0 M    Trainable params
--101 M     Non-trainable params
--114 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          97.16      94.16      95.64      34856
--! (label_id: 1)                                         50.00      11.76      19.05         34
--# (label_id: 2)                                         78.69      92.14      84.88       3230
--, (label_id: 3)                                         54.23      59.34      56.67       2376
--- (label_id: 4)                                         71.05      67.50      69.23        240
--. (label_id: 5)                                         61.97      75.12      67.92       2074
--: (label_id: 6)                                         46.15      21.43      29.27         56
--? (label_id: 7)                                         46.30      29.41      35.97        170
--… (label_id: 8)                                          0.00       0.00       0.00         10
---------------------
--micro avg                                               90.59      90.59      90.59      43046
--macro avg                                               56.17      50.10      50.96      43046
--weighted avg                                            91.24      90.59      90.79      43046
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--    32822.00         2.00       246.00       488.00        54.00       146.00        10.00         8.00         4.00
--        0.00         4.00         0.00         2.00         0.00         2.00         0.00         0.00         0.00
--      802.00         0.00      2976.00         0.00         4.00         0.00         0.00         0.00         0.00
--      804.00         6.00         0.00      1410.00        16.00       334.00        18.00        10.00         2.00
--       62.00         0.00         4.00         0.00       162.00         0.00         0.00         0.00         0.00
--      356.00        18.00         4.00       452.00         4.00      1558.00        16.00       102.00         4.00
--        4.00         0.00         0.00         4.00         0.00         6.00        12.00         0.00         0.00
--        6.00         4.00         0.00        20.00         0.00        28.00         0.00        50.00         0.00
--        0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
---------------------
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        358
---------------------
--micro avg                                              100.00     100.00     100.00        358
--macro avg                                              100.00     100.00     100.00        358
--weighted avg                                           100.00     100.00     100.00        358
--
---------------------
--           0
--      358.00
---------------------
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          95.73      93.26      94.48     553281
--! (label_id: 1)                                          7.43       8.30       7.84        482
--# (label_id: 2)                                         86.20      76.84      81.25      58468
--, (label_id: 3)                                         50.21      61.35      55.23      43682
--- (label_id: 4)                                         48.86      73.01      58.54       3534
--. (label_id: 5)                                         58.84      71.94      64.73      35434
--: (label_id: 6)                                         58.24      12.56      20.66        844
--? (label_id: 7)                                         41.32      67.34      51.21       2961
--… (label_id: 8)                                          2.96       3.35       3.14        239
---------------------
--micro avg                                               88.42      88.42      88.42     698925
--macro avg                                               49.98      51.99      48.57     698925
--weighted avg                                            89.61      88.42      88.87     698925
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--   516014.00        74.00     12955.00      7156.00       731.00      1701.00       128.00       183.00        75.00
--       96.00        40.00         0.00       221.00         2.00       171.00         0.00         8.00         0.00
--     7138.00         8.00     44924.00         8.00         0.00        33.00         8.00         0.00         0.00
--    18998.00        56.00        72.00     26799.00       141.00      6836.00       252.00       142.00        75.00
--     2243.00         0.00       388.00        60.00      2580.00         9.00         0.00         0.00         0.00
--     7806.00       165.00       129.00      8633.00        76.00     25490.00       332.00       634.00        57.00
--       17.00         0.00         0.00         8.00         0.00        51.00       106.00         0.00         0.00
--      825.00       131.00         0.00       690.00         2.00      1142.00        18.00      1994.00        24.00
--      144.00         8.00         0.00       107.00         2.00         1.00         0.00         0.00         8.00
---------------------
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       5707
---------------------
--micro avg                                              100.00     100.00     100.00       5707
--macro avg                                              100.00     100.00     100.00       5707
--weighted avg                                           100.00     100.00     100.00       5707
--
---------------------
--           0
--     5707.00
---------------------
--
--[INFO] - Epoch 0, global step 116: val_loss reached 0.21226 (best 0.21226), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-22_12-24-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.21-epoch=0.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          94.03      96.81      95.40     553281
--! (label_id: 1)                                          8.07      18.26      11.19        482
--# (label_id: 2)                                         87.40      83.72      85.52      58468
--, (label_id: 3)                                         69.06      30.53      42.34      43682
--- (label_id: 4)                                         76.42      61.26      68.01       3534
--. (label_id: 5)                                         60.31      68.97      64.35      35434
--: (label_id: 6)                                         13.73      38.27      20.21        844
--? (label_id: 7)                                         34.69      74.91      47.42       2961
--… (label_id: 8)                                          2.04       6.69       3.13        239
---------------------
--micro avg                                               89.73      89.73      89.73     698925
--macro avg                                               49.53      53.27      48.62     698925
--weighted avg                                            89.68      89.73      89.16     698925
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--   535615.00        72.00      9470.00     17748.00      1229.00      4901.00       162.00       319.00       108.00
--      234.00        88.00         0.00       369.00         1.00       340.00        17.00        34.00         8.00
--     6945.00         0.00     48949.00        10.00        18.00        83.00         0.00         0.00         0.00
--     3726.00        40.00         0.00     13338.00        70.00      1952.00       113.00        43.00        33.00
--      617.00         0.00        25.00        17.00      2165.00         9.00         0.00         0.00         0.00
--     5162.00       158.00        16.00     10144.00        51.00     24440.00       194.00       312.00        50.00
--      294.00         9.00         0.00       724.00         0.00       968.00       323.00        27.00         8.00
--      581.00        90.00         0.00      1038.00         0.00      2417.00        34.00      2218.00        16.00
--      107.00        25.00         8.00       294.00         0.00       324.00         1.00         8.00        16.00
---------------------
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       5707
---------------------
--micro avg                                              100.00     100.00     100.00       5707
--macro avg                                              100.00     100.00     100.00       5707
--weighted avg                                           100.00     100.00     100.00       5707
--
---------------------
--           0
--     5707.00
---------------------
--
--[INFO] - Epoch 1, global step 129: val_loss reached 0.19629 (best 0.19629), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-22_12-24-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.20-epoch=1.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          94.82      96.10      95.45     553281
--! (label_id: 1)                                          6.75      18.67       9.91        482
--# (label_id: 2)                                         90.82      75.63      82.54      58468
--, (label_id: 3)                                         48.46      69.17      56.99      43682
--- (label_id: 4)                                         75.88      61.97      68.22       3534
--. (label_id: 5)                                         82.29      44.26      57.56      35434
--: (label_id: 6)                                         31.14      19.08      23.66        844
--? (label_id: 7)                                         62.90      65.55      64.20       2961
--… (label_id: 8)                                          2.91       3.35       3.11        239
---------------------
--micro avg                                               89.59      89.59      89.59     698925
--macro avg                                               55.11      50.42      51.29     698925
--weighted avg                                            90.55      89.59      89.60     698925
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--   531681.00        48.00     14058.00     11051.00      1208.00      2252.00       127.00       233.00        67.00
--      491.00        90.00         0.00       350.00         0.00       337.00        16.00        42.00         8.00
--     4469.00         0.00     44222.00         0.00         0.00         0.00         0.00         0.00         0.00
--    14377.00       156.00       131.00     30215.00       127.00     16410.00       422.00       382.00       132.00
--      647.00         0.00        25.00        16.00      2190.00         8.00         0.00         0.00         0.00
--     1163.00       124.00        16.00      1591.00         9.00     15684.00       109.00       347.00        16.00
--       67.00         0.00         0.00        98.00         0.00       183.00       161.00         8.00         0.00
--      265.00        64.00         8.00       305.00         0.00       486.00         9.00      1941.00         8.00
--      121.00         0.00         8.00        56.00         0.00        74.00         0.00         8.00         8.00
---------------------
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       5707
---------------------
--micro avg                                              100.00     100.00     100.00       5707
--macro avg                                              100.00     100.00     100.00       5707
--weighted avg                                           100.00     100.00     100.00       5707
--
---------------------
--           0
--     5707.00
---------------------
--
--[INFO] - Epoch 2, global step 142: val_loss reached 0.19627 (best 0.19627), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-22_12-24-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.20-epoch=2.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          98.03      91.68      94.75     553281
--! (label_id: 1)                                         26.67      13.28      17.73        482
--# (label_id: 2)                                         87.89      91.55      89.68      58468
--, (label_id: 3)                                         45.25      68.24      54.42      43682
--- (label_id: 4)                                         75.65      64.26      69.49       3534
--. (label_id: 5)                                         63.30      83.33      71.94      35434
--: (label_id: 6)                                         24.47      24.64      24.56        844
--? (label_id: 7)                                         53.68      71.77      61.42       2961
--… (label_id: 8)                                          0.00       0.00       0.00        239
---------------------
--micro avg                                               89.39      89.39      89.39     698925
--macro avg                                               52.77      56.53      53.78     698925
--weighted avg                                            91.65      89.39      90.21     698925
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--   507261.00        32.00      4696.00      3537.00       941.00       771.00        67.00        92.00        43.00
--        0.00        64.00         0.00        91.00         0.00        53.00         8.00        16.00         8.00
--     7332.00         0.00     53527.00         0.00        24.00        17.00         0.00         0.00         0.00
--    30891.00        66.00       108.00     29809.00       238.00      4286.00       241.00       142.00        98.00
--      707.00         0.00        16.00         0.00      2271.00         8.00         0.00         0.00         0.00
--     6219.00       277.00       121.00      9480.00        60.00     29526.00       311.00       570.00        82.00
--      147.00         1.00         0.00       206.00         0.00       272.00       208.00        16.00         0.00
--      724.00        42.00         0.00       551.00         0.00       500.00         9.00      2125.00         8.00
--        0.00         0.00         0.00         8.00         0.00         1.00         0.00         0.00         0.00
---------------------
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       5707
---------------------
--micro avg                                              100.00     100.00     100.00       5707
--macro avg                                              100.00     100.00     100.00       5707
--weighted avg                                           100.00     100.00     100.00       5707
--
---------------------
--           0
--     5707.00
---------------------
--
--[INFO] - Epoch 3, global step 155: val_loss reached 0.20718 (best 0.19627), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-22_12-24-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.21-epoch=3.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          96.83      95.72      96.27     553281
--! (label_id: 1)                                         25.26       4.98       8.32        482
--# (label_id: 2)                                         87.29      93.17      90.14      58468
--, (label_id: 3)                                         59.95      63.68      61.76      43682
--- (label_id: 4)                                         79.47      61.88      69.58       3534
--. (label_id: 5)                                         72.34      75.73      74.00      35434
--: (label_id: 6)                                         29.97      23.22      26.17        844
--? (label_id: 7)                                         69.80      60.49      64.81       2961
--… (label_id: 8)                                          0.00       0.00       0.00        239
---------------------
--micro avg                                               91.99      91.99      91.99     698925
--macro avg                                               57.88      53.21      54.56     698925
--weighted avg                                            92.12      91.99      92.03     698925
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--   529621.00        74.00      3933.00      9507.00      1131.00      2172.00       141.00       292.00        75.00
--        0.00        24.00         0.00        27.00         0.00        36.00         0.00         0.00         8.00
--     7866.00         0.00     54477.00         0.00        40.00        17.00         0.00         8.00         0.00
--    11855.00       124.00        25.00     27818.00       141.00      5851.00       237.00       258.00        91.00
--      557.00         0.00         8.00         0.00      2187.00         0.00         0.00         0.00         0.00
--     3145.00       204.00        25.00      5930.00        35.00     26835.00       261.00       596.00        65.00
--       73.00         0.00         0.00       155.00         0.00       214.00       196.00        16.00         0.00
--      156.00        56.00         0.00       245.00         0.00       309.00         9.00      1791.00         0.00
--        8.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
---------------------
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       5707
---------------------
--micro avg                                              100.00     100.00     100.00       5707
--macro avg                                              100.00     100.00     100.00       5707
--weighted avg                                           100.00     100.00     100.00       5707
--
---------------------
--           0
--     5707.00
---------------------
--
--[INFO] - Epoch 4, step 168: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          97.47      94.74      96.08     553281
--! (label_id: 1)                                         25.53       4.98       8.33        482
--# (label_id: 2)                                         87.44      92.89      90.08      58468
--, (label_id: 3)                                         58.45      63.31      60.78      43682
--- (label_id: 4)                                         68.38      68.42      68.40       3534
--. (label_id: 5)                                         66.11      83.86      73.93      35434
--: (label_id: 6)                                         40.83      17.42      24.42        844
--? (label_id: 7)                                         68.64      64.61      66.56       2961
--… (label_id: 8)                                          0.00       0.00       0.00        239
---------------------
--micro avg                                               91.62      91.62      91.62     698925
--macro avg                                               56.98      54.47      54.29     698925
--weighted avg                                            92.18      91.62      91.81     698925
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--   524164.00        40.00      3962.00      7020.00       890.00      1336.00       117.00       175.00        67.00
--        8.00        24.00         0.00        26.00         0.00        20.00         0.00         8.00         8.00
--     7742.00         0.00     54311.00         0.00        32.00        17.00         0.00         8.00         0.00
--    14912.00       108.00        65.00     27657.00       143.00      3958.00       196.00       199.00        82.00
--     1061.00         0.00        49.00         0.00      2418.00         8.00         0.00         0.00         0.00
--     5121.00       254.00        81.00      8603.00        51.00     29714.00       383.00       658.00        82.00
--       58.00         0.00         0.00        74.00         0.00        81.00       147.00         0.00         0.00
--      215.00        56.00         0.00       302.00         0.00       300.00         1.00      1913.00         0.00
--        0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
---------------------
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       5707
---------------------
--micro avg                                              100.00     100.00     100.00       5707
--macro avg                                              100.00     100.00     100.00       5707
--weighted avg                                           100.00     100.00     100.00       5707
--
---------------------
--           0
--     5707.00
---------------------
--
--[INFO] - Epoch 5, step 181: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          97.39      94.90      96.13     553281
--! (label_id: 1)                                         21.62       4.98       8.09        482
--# (label_id: 2)                                         87.50      92.75      90.05      58468
--, (label_id: 3)                                         58.27      65.73      61.77      43682
--- (label_id: 4)                                         70.17      67.69      68.90       3534
--. (label_id: 5)                                         68.82      80.86      74.36      35434
--: (label_id: 6)                                         39.22      21.33      27.63        844
--? (label_id: 7)                                         67.36      65.59      66.46       2961
--… (label_id: 8)                                          0.00       0.00       0.00        239
---------------------
--micro avg                                               91.74      91.74      91.74     698925
--macro avg                                               56.70      54.87      54.82     698925
--weighted avg                                            92.25      91.74      91.93     698925
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--   525089.00        40.00      4069.00      7275.00       916.00      1419.00       117.00       175.00        75.00
--        8.00        24.00         0.00        51.00         0.00        20.00         0.00         0.00         8.00
--     7692.00         0.00     54229.00         0.00        32.00        17.00         0.00         8.00         0.00
--    14858.00       116.00        65.00     28712.00       143.00      4860.00       231.00       216.00        75.00
--      969.00         0.00        40.00         0.00      2392.00         8.00         0.00         0.00         0.00
--     4383.00       254.00        65.00      7228.00        51.00     28653.00       307.00       612.00        81.00
--       58.00         0.00         0.00        82.00         0.00       131.00       180.00         8.00         0.00
--      224.00        48.00         0.00       334.00         0.00       326.00         9.00      1942.00         0.00
--        0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
---------------------
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       5707
---------------------
--micro avg                                              100.00     100.00     100.00       5707
--macro avg                                              100.00     100.00     100.00       5707
--weighted avg                                           100.00     100.00     100.00       5707
--
---------------------
--           0
--     5707.00
---------------------
--
--[INFO] - Epoch 6, step 194: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          97.34      94.99      96.15     553281
--! (label_id: 1)                                         23.30       4.98       8.21        482
--# (label_id: 2)                                         87.51      92.68      90.02      58468
--, (label_id: 3)                                         58.59      66.14      62.14      43682
--- (label_id: 4)                                         71.11      67.20      69.10       3534
--. (label_id: 5)                                         69.52      80.41      74.57      35434
--: (label_id: 6)                                         37.82      21.33      27.27        844
--? (label_id: 7)                                         67.58      65.62      66.59       2961
--… (label_id: 8)                                          0.00       0.00       0.00        239
---------------------
--micro avg                                               91.81      91.81      91.81     698925
--macro avg                                               56.97      54.82      54.89     698925
--weighted avg                                            92.27      91.81      91.98     698925
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--   525563.00        40.00      4118.00      7422.00       941.00      1468.00       117.00       175.00        75.00
--        8.00        24.00         0.00        43.00         0.00        20.00         0.00         0.00         8.00
--     7684.00         0.00     54188.00         0.00        24.00        17.00         0.00         8.00         0.00
--    14591.00       124.00        65.00     28891.00       151.00      4964.00       231.00       216.00        75.00
--      925.00         0.00        32.00         0.00      2375.00         8.00         0.00         0.00         0.00
--     4229.00       246.00        65.00      6910.00        43.00     28491.00       307.00       611.00        81.00
--       66.00         0.00         0.00        82.00         0.00       140.00       180.00         8.00         0.00
--      215.00        48.00         0.00       334.00         0.00       326.00         9.00      1943.00         0.00
--        0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
---------------------
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       5707
---------------------
--micro avg                                              100.00     100.00     100.00       5707
--macro avg                                              100.00     100.00     100.00       5707
--weighted avg                                           100.00     100.00     100.00       5707
--
---------------------
--           0
--     5707.00
---------------------
--
--[INFO] - Epoch 7, step 207: val_loss was not in top 3
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.0004
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.CosineAnnealing object at 0x7fc9271733d0>" 
--will be used during training (effective maximum steps = 104) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--min_lr: 1.0e-08
--last_epoch: -1
--max_steps: 104
--)
--[INFO] - 
--  | Name                       | Type                 | Params
----------------------------------------------------------------------
--0 | transformer                | ElectraModel         | 108 M 
--1 | punct_classifier           | TokenClassifier      | 1.2 M 
--2 | domain_classifier          | SequenceClassifier   | 4.7 M 
--3 | punctuation_loss           | FocalDiceLoss        | 0     
--4 | domain_loss                | CrossEntropyLoss     | 0     
--5 | agg_loss                   | AggregatorLoss       | 0     
--6 | punct_class_report         | ClassificationReport | 0     
--7 | chunked_punct_class_report | ClassificationReport | 0     
--8 | domain_class_report        | ClassificationReport | 0     
----------------------------------------------------------------------
--20.1 M    Trainable params
--94.7 M    Non-trainable params
--114 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          98.00      95.64      96.81      34856
--! (label_id: 1)                                         50.00      11.76      19.05         34
--# (label_id: 2)                                         86.28      93.87      89.92       3230
--, (label_id: 3)                                         61.47      69.02      65.03       2376
--- (label_id: 4)                                         70.87      75.00      72.87        240
--. (label_id: 5)                                         71.87      83.03      77.05       2074
--: (label_id: 6)                                         52.94      32.14      40.00         56
--? (label_id: 7)                                         72.73      65.88      69.14        170
--… (label_id: 8)                                          0.00       0.00       0.00         10
---------------------
--micro avg                                               93.03      93.03      93.03      43046
--macro avg                                               62.68      58.48      58.87      43046
--weighted avg                                            93.48      93.03      93.18      43046
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--    33338.00         2.00       190.00       350.00        38.00        82.00         8.00         6.00         4.00
--        0.00         4.00         0.00         2.00         0.00         2.00         0.00         0.00         0.00
--      480.00         0.00      3032.00         0.00         0.00         0.00         0.00         2.00         0.00
--      744.00         8.00         2.00      1640.00        16.00       234.00        14.00         8.00         2.00
--       72.00         0.00         2.00         0.00       180.00         0.00         0.00         0.00         0.00
--      212.00        16.00         4.00       374.00         6.00      1722.00        16.00        42.00         4.00
--        4.00         0.00         0.00         2.00         0.00        10.00        18.00         0.00         0.00
--        6.00         4.00         0.00         8.00         0.00        24.00         0.00       112.00         0.00
--        0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
---------------------
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        358
---------------------
--micro avg                                              100.00     100.00     100.00        358
--macro avg                                              100.00     100.00     100.00        358
--weighted avg                                           100.00     100.00     100.00        358
--
---------------------
--           0
--      358.00
---------------------
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          96.41      96.17      96.29     553281
--! (label_id: 1)                                         20.89       6.85      10.31        482
--# (label_id: 2)                                         87.35      92.79      89.99      58468
--, (label_id: 3)                                         64.14      57.89      60.86      43682
--- (label_id: 4)                                         82.28      61.23      70.21       3534
--. (label_id: 5)                                         70.67      77.63      73.99      35434
--: (label_id: 6)                                         35.17      18.13      23.92        844
--? (label_id: 7)                                         61.98      69.60      65.57       2961
--… (label_id: 8)                                          0.00       0.00       0.00        239
---------------------
--micro avg                                               92.08      92.08      92.08     698925
--macro avg                                               57.66      53.37      54.57     698925
--weighted avg                                            91.95      92.08      91.98     698925
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--   532098.00        74.00      4136.00     11835.00      1178.00      2050.00       178.00       264.00       107.00
--        0.00        33.00         0.00        59.00         0.00        41.00         0.00        17.00         8.00
--     7788.00         0.00     54251.00         0.00        49.00        17.00         0.00         0.00         0.00
--     8394.00       100.00         0.00     25288.00       101.00      5077.00       222.00       199.00        43.00
--      450.00         0.00         0.00        16.00      2164.00         0.00         0.00         0.00         0.00
--     4245.00       227.00        81.00      6049.00        42.00     27509.00       282.00       412.00        81.00
--       17.00         0.00         0.00        83.00         0.00       174.00       153.00         8.00         0.00
--      289.00        48.00         0.00       352.00         0.00       566.00         9.00      2061.00         0.00
--        0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
---------------------
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       5707
---------------------
--micro avg                                              100.00     100.00     100.00       5707
--macro avg                                              100.00     100.00     100.00       5707
--weighted avg                                           100.00     100.00     100.00       5707
--
---------------------
--           0
--     5707.00
---------------------
--
--[INFO] - Epoch 0, step 220: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          97.85      93.01      95.36     553281
--! (label_id: 1)                                          7.02       4.98       5.83        482
--# (label_id: 2)                                         84.77      95.08      89.63      58468
--, (label_id: 3)                                         50.12      73.41      59.57      43682
--- (label_id: 4)                                         91.30      41.26      56.83       3534
--. (label_id: 5)                                         68.66      74.52      71.47      35434
--: (label_id: 6)                                         25.39      21.33      23.18        844
--? (label_id: 7)                                         72.76      54.85      62.55       2961
--… (label_id: 8)                                          0.00       0.00       0.00        239
---------------------
--micro avg                                               90.41      90.41      90.41     698925
--macro avg                                               55.32      50.94      51.60     698925
--weighted avg                                            91.97      90.41      90.92     698925
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--   514582.00        40.00      2704.00      5464.00      1360.00      1368.00       101.00       222.00        67.00
--       84.00        24.00         0.00        91.00         0.00       126.00         0.00         9.00         8.00
--     9864.00         0.00     55592.00         0.00       105.00        17.00         0.00         0.00         0.00
--    23201.00       197.00        99.00     32067.00       538.00      7020.00       331.00       434.00        99.00
--      139.00         0.00         0.00         0.00      1458.00         0.00         0.00         0.00         0.00
--     5056.00       164.00        73.00      5759.00        72.00     26405.00       223.00       639.00        65.00
--      116.00         0.00         0.00       138.00         0.00       250.00       180.00        25.00         0.00
--      198.00        49.00         0.00       145.00         0.00       207.00         9.00      1624.00         0.00
--       41.00         8.00         0.00        18.00         1.00        41.00         0.00         8.00         0.00
---------------------
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       5707
---------------------
--micro avg                                              100.00     100.00     100.00       5707
--macro avg                                              100.00     100.00     100.00       5707
--weighted avg                                           100.00     100.00     100.00       5707
--
---------------------
--           0
--     5707.00
---------------------
--
--[INFO] - Epoch 1, step 233: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          97.95      93.76      95.81     553281
--! (label_id: 1)                                         19.51       8.30      11.64        482
--# (label_id: 2)                                         80.86      97.02      88.21      58468
--, (label_id: 3)                                         59.65      65.45      62.42      43682
--- (label_id: 4)                                         71.63      64.60      67.94       3534
--. (label_id: 5)                                         65.83      80.49      72.43      35434
--: (label_id: 6)                                         27.91      23.58      25.56        844
--? (label_id: 7)                                         58.10      73.66      64.96       2961
--… (label_id: 8)                                         23.53       3.35       5.86        239
---------------------
--micro avg                                               91.18      91.18      91.18     698925
--macro avg                                               56.11      56.69      54.98     698925
--weighted avg                                            92.03      91.18      91.45     698925
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--   518745.00        56.00      1576.00      6587.00       941.00      1381.00       109.00       167.00        59.00
--       24.00        40.00         0.00        85.00         0.00        39.00         0.00        17.00         0.00
--    13294.00         0.00     56728.00         9.00        94.00        33.00         0.00         0.00         0.00
--    13853.00        57.00        57.00     28590.00       167.00      4762.00       227.00       125.00        91.00
--      852.00         0.00        50.00         2.00      2283.00         0.00         0.00         0.00         0.00
--     5956.00       255.00        57.00      7659.00        49.00     28522.00       300.00       445.00        81.00
--       74.00         0.00         0.00       166.00         0.00       248.00       199.00        26.00         0.00
--      459.00        74.00         0.00       583.00         0.00       448.00         9.00      2181.00         0.00
--       24.00         0.00         0.00         1.00         0.00         1.00         0.00         0.00         8.00
---------------------
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       5707
---------------------
--micro avg                                              100.00     100.00     100.00       5707
--macro avg                                              100.00     100.00     100.00       5707
--weighted avg                                           100.00     100.00     100.00       5707
--
---------------------
--           0
--     5707.00
---------------------
--
--[INFO] - Epoch 2, global step 246: val_loss reached 0.19571 (best 0.19571), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-22_12-24-19/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.20-epoch=2-v0.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          97.58      95.13      96.34     553281
--! (label_id: 1)                                         13.39      13.69      13.54        482
--# (label_id: 2)                                         85.82      95.73      90.50      58468
--, (label_id: 3)                                         66.86      56.55      61.27      43682
--- (label_id: 4)                                         79.43      60.64      68.77       3534
--. (label_id: 5)                                         61.33      87.20      72.01      35434
--: (label_id: 6)                                         34.62      21.33      26.39        844
--? (label_id: 7)                                         65.42      71.63      68.39       2961
--… (label_id: 8)                                          0.00       0.00       0.00        239
---------------------
--micro avg                                               91.92      91.92      91.92     698925
--macro avg                                               56.05      55.77      55.25     698925
--weighted avg                                            92.44      91.92      91.99     698925
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--   526340.00        48.00      2423.00      7506.00      1132.00      1525.00       135.00       207.00        67.00
--       48.00        66.00         0.00       196.00         8.00       140.00         1.00        26.00         8.00
--     9158.00         0.00     55970.00         0.00        49.00        34.00         8.00         0.00         0.00
--     9360.00        41.00        25.00     24702.00       140.00      2386.00       159.00        75.00        59.00
--      546.00         0.00         9.00         0.00      2143.00         0.00         0.00         0.00         0.00
--     7499.00       262.00        41.00     10671.00        62.00     30900.00       344.00       499.00       105.00
--       49.00         0.00         0.00       116.00         0.00       158.00       180.00        17.00         0.00
--      273.00        57.00         0.00       491.00         0.00       291.00         9.00      2121.00         0.00
--        8.00         8.00         0.00         0.00         0.00         0.00         8.00        16.00         0.00
---------------------
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       5707
---------------------
--micro avg                                              100.00     100.00     100.00       5707
--macro avg                                              100.00     100.00     100.00       5707
--weighted avg                                           100.00     100.00     100.00       5707
--
---------------------
--           0
--     5707.00
---------------------
--
--[INFO] - Epoch 3, step 259: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          97.14      95.92      96.53     553281
--! (label_id: 1)                                         47.06       3.32       6.20        482
--# (label_id: 2)                                         87.26      94.15      90.57      58468
--, (label_id: 3)                                         64.67      65.41      65.04      43682
--- (label_id: 4)                                         70.17      66.55      68.31       3534
--. (label_id: 5)                                         71.97      77.73      74.74      35434
--: (label_id: 6)                                         36.02      19.08      24.94        844
--? (label_id: 7)                                         66.89      71.70      69.21       2961
--… (label_id: 8)                                         33.33       3.35       6.08        239
---------------------
--micro avg                                               92.51      92.51      92.51     698925
--macro avg                                               63.83      55.24      55.74     698925
--weighted avg                                            92.61      92.51      92.52     698925
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--   530732.00        88.00      3308.00      8855.00       948.00      1936.00       153.00       271.00        75.00
--        0.00        16.00         0.00        17.00         0.00         1.00         0.00         0.00         0.00
--     7975.00         0.00     55045.00         0.00        41.00        17.00         0.00         0.00         0.00
--     9522.00        90.00        24.00     28572.00       159.00      5373.00       222.00       128.00        91.00
--      932.00         0.00        50.00        18.00      2352.00         0.00         0.00         0.00         0.00
--     3873.00       230.00        41.00      5777.00        34.00     27543.00       291.00       415.00        65.00
--       41.00         0.00         0.00        81.00         0.00       148.00       161.00        16.00         0.00
--      198.00        58.00         0.00       362.00         0.00       416.00        17.00      2123.00         0.00
--        8.00         0.00         0.00         0.00         0.00         0.00         0.00         8.00         8.00
---------------------
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       5707
---------------------
--micro avg                                              100.00     100.00     100.00       5707
--macro avg                                              100.00     100.00     100.00       5707
--weighted avg                                           100.00     100.00     100.00       5707
--
---------------------
--           0
--     5707.00
---------------------
--
--[INFO] - Epoch 4, step 272: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          96.84      96.29      96.56     553281
--! (label_id: 1)                                         29.63       1.66       3.14        482
--# (label_id: 2)                                         87.16      94.45      90.66      58468
--, (label_id: 3)                                         63.29      66.03      64.63      43682
--- (label_id: 4)                                         74.22      65.82      69.77       3534
--. (label_id: 5)                                         75.99      72.19      74.04      35434
--: (label_id: 6)                                         42.41      15.88      23.10        844
--? (label_id: 7)                                         74.04      67.61      70.68       2961
--… (label_id: 8)                                         25.00       3.35       5.90        239
---------------------
--micro avg                                               92.55      92.55      92.55     698925
--macro avg                                               63.17      53.70      55.39     698925
--weighted avg                                            92.53      92.55      92.50     698925
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--   532747.00        89.00      3145.00     10235.00       996.00      2387.00       147.00       287.00        83.00
--        0.00         8.00         0.00        19.00         0.00         0.00         0.00         0.00         0.00
--     8064.00         0.00     55223.00         0.00        41.00        25.00         8.00         0.00         0.00
--     8762.00       157.00         9.00     28842.00       145.00      7026.00       305.00       225.00        99.00
--      756.00         0.00        34.00        18.00      2326.00         0.00         0.00         0.00         0.00
--     2778.00       188.00        57.00      4314.00        26.00     25581.00       241.00       431.00        49.00
--        9.00         0.00         0.00        41.00         0.00       132.00       134.00         0.00         0.00
--      157.00        40.00         0.00       213.00         0.00       283.00         9.00      2002.00         0.00
--        8.00         0.00         0.00         0.00         0.00         0.00         0.00        16.00         8.00
---------------------
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       5707
---------------------
--micro avg                                              100.00     100.00     100.00       5707
--macro avg                                              100.00     100.00     100.00       5707
--weighted avg                                           100.00     100.00     100.00       5707
--
---------------------
--           0
--     5707.00
---------------------
--
--[INFO] - Epoch 5, step 285: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          96.99      96.13      96.56     553281
--! (label_id: 1)                                         29.63       1.66       3.14        482
--# (label_id: 2)                                         86.84      94.79      90.64      58468
--, (label_id: 3)                                         65.74      63.34      64.52      43682
--- (label_id: 4)                                         74.58      65.59      69.80       3534
--. (label_id: 5)                                         71.42      77.89      74.52      35434
--: (label_id: 6)                                         38.97      18.84      25.40        844
--? (label_id: 7)                                         79.01      64.94      71.29       2961
--… (label_id: 8)                                          0.00       0.00       0.00        239
---------------------
--micro avg                                               92.57      92.57      92.57     698925
--macro avg                                               60.35      53.69      55.10     698925
--weighted avg                                            92.55      92.57      92.52     698925
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--   531879.00        96.00      2938.00      9817.00       996.00      2150.00       138.00       295.00        83.00
--        0.00         8.00         0.00        10.00         0.00         1.00         0.00         0.00         8.00
--     8320.00         0.00     55423.00         0.00        41.00        33.00         8.00         0.00         0.00
--     8275.00       140.00         9.00     27668.00       137.00      5351.00       229.00       185.00        91.00
--      755.00         0.00        18.00        17.00      2318.00         0.00         0.00         0.00         0.00
--     3887.00       198.00        80.00      5923.00        42.00     27601.00       309.00       549.00        57.00
--       33.00         0.00         0.00        59.00         0.00       148.00       159.00         9.00         0.00
--      132.00        40.00         0.00       188.00         0.00       150.00         1.00      1923.00         0.00
--        0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
---------------------
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       5707
---------------------
--micro avg                                              100.00     100.00     100.00       5707
--macro avg                                              100.00     100.00     100.00       5707
--weighted avg                                           100.00     100.00     100.00       5707
--
---------------------
--           0
--     5707.00
---------------------
--
--[INFO] - Epoch 6, step 298: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          97.08      96.01      96.54     553281
--! (label_id: 1)                                         29.63       1.66       3.14        482
--# (label_id: 2)                                         86.74      94.89      90.63      58468
--, (label_id: 3)                                         65.44      63.78      64.60      43682
--- (label_id: 4)                                         74.56      65.59      69.79       3534
--. (label_id: 5)                                         70.98      78.47      74.54      35434
--: (label_id: 6)                                         38.22      18.84      25.24        844
--? (label_id: 7)                                         77.95      66.02      71.49       2961
--… (label_id: 8)                                          0.00       0.00       0.00        239
---------------------
--micro avg                                               92.54      92.54      92.54     698925
--macro avg                                               60.07      53.92      55.11     698925
--weighted avg                                            92.57      92.54      92.51     698925
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--   531194.00        96.00      2878.00      9458.00       988.00      2027.00       129.00       295.00        83.00
--        0.00         8.00         0.00        10.00         0.00         1.00         0.00         0.00         8.00
--     8395.00         0.00     55483.00         0.00        49.00        33.00         8.00         0.00         0.00
--     8702.00       124.00         9.00     27859.00       137.00      5245.00       222.00       185.00        91.00
--      756.00         0.00        18.00        17.00      2318.00         0.00         0.00         0.00         0.00
--     4059.00       214.00        80.00      6075.00        42.00     27806.00       325.00       517.00        57.00
--       33.00         0.00         0.00        59.00         0.00       156.00       159.00         9.00         0.00
--      142.00        40.00         0.00       204.00         0.00       166.00         1.00      1955.00         0.00
--        0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
---------------------
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       5707
---------------------
--micro avg                                              100.00     100.00     100.00       5707
--macro avg                                              100.00     100.00     100.00       5707
--weighted avg                                           100.00     100.00     100.00       5707
--
---------------------
--           0
--     5707.00
---------------------
--
--[INFO] - Epoch 7, step 311: val_loss was not in top 3
--[INFO] - GPU available: True, used: True
--[INFO] - TPU available: None, using: 0 TPU cores
--[INFO] - Using environment variable NODE_RANK for node rank (0).
--[INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          97.16      96.24      96.70     684537
--! (label_id: 1)                                         37.50       6.70      11.37        358
--# (label_id: 2)                                         87.06      95.15      90.92      71989
--, (label_id: 3)                                         64.29      64.67      64.48      54369
--- (label_id: 4)                                         64.70      73.91      69.00       3335
--. (label_id: 5)                                         75.61      75.92      75.77      48200
--: (label_id: 6)                                         34.92      24.87      29.05        796
--? (label_id: 7)                                         76.35      68.59      72.26       3887
--… (label_id: 8)                                          0.00       0.00       0.00        247
---------------------
--micro avg                                               92.70      92.70      92.70     867718
--macro avg                                               59.73      56.23      56.62     867718
--weighted avg                                            92.74      92.70      92.70     867718
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--   658791.00        58.00      3443.00     11543.00       641.00      3116.00        68.00       253.00       106.00
--       24.00        24.00         0.00         8.00         0.00         8.00         0.00         0.00         0.00
--    10037.00         0.00     68497.00         0.00        45.00       101.00         0.00         0.00         0.00
--    10752.00        99.00         0.00     35160.00       160.00      7835.00       212.00       370.00        99.00
--     1251.00         0.00         8.00        32.00      2465.00        54.00         0.00         0.00         0.00
--     3460.00       145.00        33.00      7200.00        24.00     36595.00       318.00       590.00        34.00
--       58.00         0.00         0.00       139.00         0.00       164.00       198.00         8.00         0.00
--      164.00        32.00         8.00       287.00         0.00       327.00         0.00      2666.00         8.00
--        0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
---------------------
--
--[INFO] - Chunked Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                          97.43      97.34      97.39     343124
--! (label_id: 1)                                         50.00       4.19       7.73        191
--# (label_id: 2)                                         95.72      96.16      95.94      37423
--, (label_id: 3)                                         65.15      65.34      65.24      27606
--- (label_id: 4)                                         66.96      79.00      72.48       1652
--. (label_id: 5)                                         77.65      78.62      78.13      24612
--: (label_id: 6)                                         27.78      22.16      24.65        361
--? (label_id: 7)                                         75.73      70.60      73.08       2007
--… (label_id: 8)                                          0.00       0.00       0.00        158
---------------------
--micro avg                                               93.83      93.83      93.83     437134
--macro avg                                               61.82      57.05      57.18     437134
--weighted avg                                            93.80      93.83      93.81     437134
--
---------------------
--                       !           #           ,           -           .           :           ?           …
--   334000.00        17.00      1420.00      5636.00       264.00      1255.00         9.00       150.00        57.00
--        8.00         8.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
--     1558.00         0.00     35987.00         0.00        10.00        40.00         0.00         0.00         0.00
--     5553.00        42.00         0.00     18037.00        73.00      3682.00        94.00       131.00        75.00
--      609.00         0.00         0.00        24.00      1305.00        11.00         0.00         0.00         0.00
--     1307.00       100.00         8.00      3650.00         0.00     19350.00       178.00       309.00        18.00
--       32.00         0.00         0.00        81.00         0.00        95.00        80.00         0.00         0.00
--       57.00        24.00         8.00       178.00         0.00       179.00         0.00      1417.00         8.00
--        0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
---------------------
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       7048
---------------------
--micro avg                                              100.00     100.00     100.00       7048
--macro avg                                              100.00     100.00     100.00       7048
--weighted avg                                           100.00     100.00     100.00       7048
--
---------------------
--           0
--     7048.00
---------------------
--
--[INFO] - saving to /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-22_12-24-19/test.txt
--[INFO] - Internal process exited
diff --git a/Punctuation_with_Domain_discriminator/2021-02-22_14-06-14/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-22_14-06-14/hparams.yaml
deleted file mode 100644
index 5dd5437..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-22_14-06-14/hparams.yaml
+++ /dev/null
@@ -1,127 +0,0 @@
-seed: 42
-trainer:
-  gpus: 1
-  num_nodes: 1
-  max_epochs: 8
-  max_steps: null
-  accumulate_grad_batches: 16
-  gradient_clip_val: 0
-  amp_level: O1
-  precision: 16
-  accelerator: ddp
-  checkpoint_callback: false
-  logger: false
-  log_every_n_steps: 1
-  val_check_interval: 1.0
-  resume_from_checkpoint: null
-exp_manager:
-  exp_dir: /home/nxingyu2/project/
-  name: Punctuation_with_Domain_discriminator
-  create_tensorboard_logger: true
-  create_checkpoint_callback: true
-base_path: /home/nxingyu2/data
-tmp_path: /home/nxingyu2/data/tmp
-log_dir: /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-22_14-06-14
-model:
-  nemo_path: null
-  transformer_path: google/electra-base-discriminator
-  unfrozen: 0
-  maximum_unfrozen: 2
-  unfreeze_step: 1
-  punct_label_ids:
-  - ''
-  - ','
-  - .
-  - '?'
-  - '-'
-  - '!'
-  - ':'
-  - …
-  label_map:
-    —: ','
-    ;: .
-  no_space_label: '#'
-  test_chunk_percent: 0.5
-  pad_start_and_end: 0
-  punct_class_weights: false
-  dataset:
-    data_dir: /home/nxingyu2/data
-    labelled:
-    - /home/nxingyu2/data/ted_talks_processed
-    unlabelled: null
-    max_seq_length: 128
-    pad_label: ''
-    ignore_extra_tokens: false
-    ignore_start_end: false
-    use_cache: false
-    num_workers: 8
-    pin_memory: false
-    drop_last: true
-    num_labels: 9
-    num_domains: 2
-    test_unlabelled: true
-    attach_label_to_end: null
-    pad_start: 64
-    train_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 16
-      manual_len: 20000
-    validation_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 16
-  tokenizer:
-    tokenizer_name: google/electra-base-discriminator
-    vocab_file: null
-    tokenizer_model: null
-    special_tokens: null
-  language_model:
-    pretrained_model_name: google/electra-base-discriminator
-    lm_checkpoint: null
-    config_file: null
-    config: null
-  punct_head:
-    punct_num_fc_layers: 0
-    fc_dropout: 0.1
-    activation: gelu
-    log_softmax: false
-    use_transformer_init: true
-    loss: crf
-    bilstm: false
-  domain_head:
-    domain_num_fc_layers: 3
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: cel
-    gamma: 0.01
-    pooling: mean_max
-    idx_conditioned_on: 0
-  dice_loss:
-    epsilon: 0.01
-    alpha: 3
-    macro_average: true
-  focal_loss:
-    gamma: 3
-  frozen_lr:
-  - 0.02
-  - 0.001
-  - 0.0004
-  - 0.0001
-  - 1.0e-05
-  - 1.0e-06
-  - 1.0e-07
-  optim:
-    name: adamw
-    lr: 0.01
-    weight_decay: 0.0
-    sched:
-      name: CosineAnnealing
-      warmup_steps: null
-      warmup_ratio: 0.1
-      min_lr: 1.0e-08
-      last_epoch: -1
-      monitor: val_loss
-      reduce_on_plateau: false
diff --git a/Punctuation_with_Domain_discriminator/2021-02-22_14-06-14/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-22_14-06-14/lightning_logs.txt
deleted file mode 100644
index e0f5452..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-22_14-06-14/lightning_logs.txt
+++ /dev/null
@@ -1,22 +0,0 @@
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
-Using native 16bit precision.
-initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-
-  | Name                       | Type                 | Params
---------------------------------------------------------------------
-0 | transformer                | ElectraModel         | 108 M 
-1 | punct_classifier           | TokenClassifier      | 6.9 K 
-2 | domain_classifier          | SequenceClassifier   | 4.7 M 
-3 | punctuation_loss           | LinearChainCRF       | 99    
-4 | domain_loss                | CrossEntropyLoss     | 0     
-5 | agg_loss                   | AggregatorLoss       | 0     
-6 | punct_class_report         | ClassificationReport | 0     
-7 | chunked_punct_class_report | ClassificationReport | 0     
-8 | domain_class_report        | ClassificationReport | 0     
---------------------------------------------------------------------
-4.7 M     Trainable params
-108 M     Non-trainable params
-113 M     Total params
-Epoch 0, global step 12: val_loss reached 53.28495 (best 53.28495), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-22_14-06-14/checkpoints/Punctuation_with_Domain_discriminator---val_loss=53.28-epoch=0.ckpt" as top 3
diff --git a/Punctuation_with_Domain_discriminator/2021-02-22_14-06-14/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-22_14-06-14/nemo_error_log.txt
deleted file mode 100644
index ffde286..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-22_14-06-14/nemo_error_log.txt
+++ /dev/null
@@ -1,31 +0,0 @@
-[NeMo W 2021-02-22 14:06:12 nemo_logging:349] /home/nxingyu2/project/experiment/models/punctuation_domain_model.py:68: SyntaxWarning: assertion is always true, perhaps remove parentheses?
-      assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
-    
-[NeMo W 2021-02-22 14:06:14 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-22 14:06:18 nemo_logging:349] /home/nxingyu2/project/experiment/models/punctuation_domain_model.py:68: SyntaxWarning: assertion is always true, perhaps remove parentheses?
-      assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
-    
-[NeMo W 2021-02-22 14:06:18 nemo_logging:349] /home/nxingyu2/project/experiment/models/punctuation_domain_model.py:68: SyntaxWarning: assertion is always true, perhaps remove parentheses?
-      assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
-    
-[NeMo W 2021-02-22 14:06:22 nemo_logging:349] /home/nxingyu2/project/experiment/models/punctuation_domain_model.py:68: SyntaxWarning: assertion is always true, perhaps remove parentheses?
-      assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
-    
-[NeMo W 2021-02-22 14:06:22 nemo_logging:349] /home/nxingyu2/project/experiment/models/punctuation_domain_model.py:68: SyntaxWarning: assertion is always true, perhaps remove parentheses?
-      assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
-    
-[NeMo W 2021-02-22 14:06:22 nemo_logging:349] /home/nxingyu2/project/experiment/models/punctuation_domain_model.py:68: SyntaxWarning: assertion is always true, perhaps remove parentheses?
-      assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
-    
-[NeMo W 2021-02-22 14:06:24 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-22 14:06:46 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-    
-[NeMo W 2021-02-22 14:09:49 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7ff6d885dbb0> was reported to be 199 (when accessing len(dataloader)), but 200 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-22 14:10:24 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7ff6bc0e2340> was reported to be 25 (when accessing len(dataloader)), but 26 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-22_14-06-14/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-22_14-06-14/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 993d557..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-22_14-06-14/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,33 +0,0 @@
-[NeMo W 2021-02-22 14:06:12 nemo_logging:349] /home/nxingyu2/project/experiment/models/punctuation_domain_model.py:68: SyntaxWarning: assertion is always true, perhaps remove parentheses?
-      assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
-    
-[NeMo I 2021-02-22 14:06:14 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-22_14-06-14
-[NeMo I 2021-02-22 14:06:14 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-22 14:06:14 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-22 14:06:18 nemo_logging:349] /home/nxingyu2/project/experiment/models/punctuation_domain_model.py:68: SyntaxWarning: assertion is always true, perhaps remove parentheses?
-      assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
-    
-[NeMo W 2021-02-22 14:06:18 nemo_logging:349] /home/nxingyu2/project/experiment/models/punctuation_domain_model.py:68: SyntaxWarning: assertion is always true, perhaps remove parentheses?
-      assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
-    
-[NeMo W 2021-02-22 14:06:22 nemo_logging:349] /home/nxingyu2/project/experiment/models/punctuation_domain_model.py:68: SyntaxWarning: assertion is always true, perhaps remove parentheses?
-      assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
-    
-[NeMo W 2021-02-22 14:06:22 nemo_logging:349] /home/nxingyu2/project/experiment/models/punctuation_domain_model.py:68: SyntaxWarning: assertion is always true, perhaps remove parentheses?
-      assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
-    
-[NeMo W 2021-02-22 14:06:22 nemo_logging:349] /home/nxingyu2/project/experiment/models/punctuation_domain_model.py:68: SyntaxWarning: assertion is always true, perhaps remove parentheses?
-      assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
-    
-[NeMo W 2021-02-22 14:06:24 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-22 14:06:46 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-    
-[NeMo W 2021-02-22 14:09:49 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7ff6d885dbb0> was reported to be 199 (when accessing len(dataloader)), but 200 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-22 14:10:24 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7ff6bc0e2340> was reported to be 25 (when accessing len(dataloader)), but 26 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
diff --git a/experiment/config.yaml b/experiment/config.yaml
index 5f6090f..aecc0bb 100644
--- a/experiment/config.yaml
+++ b/experiment/config.yaml
@@ -4,7 +4,7 @@ trainer:
     num_nodes: 1
     max_epochs: 8
     max_steps: null # precedence over max_epochs
-    accumulate_grad_batches: 16 # accumulates grads every k batches
+    accumulate_grad_batches: 2 # accumulates grads every k batches
     gradient_clip_val: 0
     amp_level: O1 # O1/O2 for mixed precision
     precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
@@ -101,7 +101,7 @@ model:
         train_ds:
             shuffle: true
             num_samples: -1
-            batch_size: 16
+            batch_size: 32
             manual_len: 20000 #default 0 84074
 
         validation_ds:
@@ -110,7 +110,7 @@ model:
             # ds_item: null # expected format: [PATH_TO_DEV1,PATH_TO_DEV2] (Note no space between the paths and square brackets)
             shuffle: true
             num_samples: -1
-            batch_size: 16 #4
+            batch_size: 32 #4
 
     tokenizer:
         tokenizer_name: ${model.language_model.pretrained_model_name} # or sentencepiece
@@ -126,12 +126,12 @@ model:
         # unfrozen_layers: 1
     
     punct_head:
-        punct_num_fc_layers: 0
+        punct_num_fc_layers: 3
         fc_dropout: 0.1
         activation: 'gelu'
         log_softmax: false
         use_transformer_init: true
-        loss: 'crf'
+        loss: 'dice'
         bilstm: false
 
     domain_head:
diff --git a/experiment/info.log b/experiment/info.log
index cbf5b77..104c5dd 100644
--- a/experiment/info.log
+++ b/experiment/info.log
@@ -2,123 +2,3 @@
 [INFO] - TPU available: None, using: 0 TPU cores
 [INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
 [INFO] - Using native 16bit precision.
-[INFO] - shuffling train set
-[INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-[INFO] - Optimizer config = AdamW (
-Parameter Group 0
-    amsgrad: False
-    betas: (0.9, 0.999)
-    eps: 1e-08
-    lr: 0.02
-    weight_decay: 0.0
-)
-[INFO] - Scheduler "<core.optim.lr_scheduler.CosineAnnealing object at 0x7ff6846b3b80>" 
-will be used during training (effective maximum steps = 104) - 
-Parameters : 
-(warmup_steps: null
-warmup_ratio: 0.1
-min_lr: 1.0e-08
-last_epoch: -1
-max_steps: 104
-)
-[INFO] - 
-  | Name                       | Type                 | Params
---------------------------------------------------------------------
-0 | transformer                | ElectraModel         | 108 M 
-1 | punct_classifier           | TokenClassifier      | 6.9 K 
-2 | domain_classifier          | SequenceClassifier   | 4.7 M 
-3 | punctuation_loss           | LinearChainCRF       | 99    
-4 | domain_loss                | CrossEntropyLoss     | 0     
-5 | agg_loss                   | AggregatorLoss       | 0     
-6 | punct_class_report         | ClassificationReport | 0     
-7 | chunked_punct_class_report | ClassificationReport | 0     
-8 | domain_class_report        | ClassificationReport | 0     
---------------------------------------------------------------------
-4.7 M     Trainable params
-108 M     Non-trainable params
-113 M     Total params
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          81.82       0.10       0.21      34866
-! (label_id: 1)                                          0.00       0.00       0.00         34
-# (label_id: 2)                                         14.68       6.63       9.13       3230
-, (label_id: 3)                                          4.08       2.61       3.18       2376
-- (label_id: 4)                                          1.47      16.67       2.69        240
-. (label_id: 5)                                          2.37      31.05       4.40       2074
-: (label_id: 6)                                          0.00       0.00       0.00         56
-? (label_id: 7)                                          0.00       0.00       0.00        170
-… (label_id: 8)                                          0.00       0.00       0.00         10
--------------------
-micro avg                                                2.31       2.31       2.31      43056
-macro avg                                               11.60       6.34       2.18      43056
-weighted avg                                            67.70       2.31       1.25      43056
-
--------------------
-                       !           #           ,           -           .           :           ?           …
-       36.00         0.00         8.00         0.00         0.00         0.00         0.00         0.00         0.00
-      240.00         0.00        22.00        14.00         0.00        34.00         0.00         2.00         0.00
-     1066.00         0.00       214.00        86.00        14.00        70.00         2.00         6.00         0.00
-     1248.00         2.00       124.00        62.00         2.00        78.00         0.00         2.00         2.00
-     1798.00         6.00       230.00       288.00        40.00       328.00         6.00        34.00         0.00
-    23962.00         8.00      1578.00       784.00        90.00       644.00        32.00        62.00         8.00
-      382.00         0.00       570.00         4.00         0.00         2.00         0.00         0.00         0.00
-      332.00         0.00        10.00        26.00         0.00         2.00         0.00         0.00         0.00
-     5802.00        18.00       474.00      1112.00        94.00       916.00        16.00        64.00         0.00
--------------------
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00        368
--------------------
-micro avg                                              100.00     100.00     100.00        368
-macro avg                                              100.00     100.00     100.00        368
-weighted avg                                           100.00     100.00     100.00        368
-
--------------------
-           0
-      368.00
--------------------
-
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          90.44      93.04      91.72     553457
-! (label_id: 1)                                          0.00       0.00       0.00        482
-# (label_id: 2)                                         79.14      75.72      77.39      58468
-, (label_id: 3)                                         37.80      38.59      38.19      43682
-- (label_id: 4)                                          0.00       0.00       0.00       3534
-. (label_id: 5)                                         55.89      12.10      19.90      35434
-: (label_id: 6)                                          0.00       0.00       0.00        844
-? (label_id: 7)                                          0.00       0.00       0.00       2961
-… (label_id: 8)                                          0.07       6.69       0.15        239
--------------------
-micro avg                                               83.02      83.02      83.02     699101
-macro avg                                               29.26      25.13      25.26     699101
-weighted avg                                            83.41      83.02      82.48     699101
-
--------------------
-                       !           #           ,           -           .           :           ?           …
-   514952.00       196.00     12206.00     24278.00      2977.00     13236.00       270.00      1184.00       116.00
-        8.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
-    10961.00         0.00     44271.00       256.00       268.00       170.00        10.00         1.00         0.00
-     8710.00       253.00        77.00     16855.00        99.00     16758.00       497.00      1237.00       105.00
-        8.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
-     1926.00        33.00        42.00       879.00        33.00      4288.00        26.00       443.00         2.00
-        0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
-        0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00         0.00
-    16892.00         0.00      1872.00      1414.00       157.00       982.00        41.00        96.00        16.00
--------------------
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       5883
--------------------
-micro avg                                              100.00     100.00     100.00       5883
-macro avg                                              100.00     100.00     100.00       5883
-weighted avg                                           100.00     100.00     100.00       5883
-
--------------------
-           0
-     5883.00
--------------------
-
-[INFO] - Epoch 0, global step 12: val_loss reached 53.28495 (best 53.28495), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-22_14-06-14/checkpoints/Punctuation_with_Domain_discriminator---val_loss=53.28-epoch=0.ckpt" as top 3
