commit hash: 75754a819d9f2c3acd220c2a6b3c936e4942c8ab
diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/events.out.tfevents.1613548228.intern-instance.32277.0 b/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/events.out.tfevents.1613548228.intern-instance.32277.0
deleted file mode 100644
index 282ddc8..0000000
Binary files a/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/events.out.tfevents.1613548228.intern-instance.32277.0 and /dev/null differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/git-info.log
deleted file mode 100644
index 3f1a26f..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/git-info.log
+++ /dev/null
@@ -1,272 +0,0 @@
-commit hash: 49f437be4dc897ae49942253feec6e44513b972e
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/events.out.tfevents.1613533775.intern-instance.25711.0 b/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/events.out.tfevents.1613533775.intern-instance.25711.0
-index 72a6b47..5665c36 100644
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/events.out.tfevents.1613533775.intern-instance.25711.0 and b/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/events.out.tfevents.1613533775.intern-instance.25711.0 differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/hparams.yaml
-index 8780f5e..80b2cf4 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/hparams.yaml
-+++ b/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/hparams.yaml
-@@ -40,6 +40,7 @@ model:
-     …: .
-     ;: .
-   no_space_label: '#'
-+  test_chunk_percent: 0.5
-   punct_class_weights: false
-   dataset:
-     data_dir: /home/nxingyu2/data
-@@ -54,7 +55,7 @@ model:
-     num_workers: 4
-     pin_memory: false
-     drop_last: true
--    num_labels: 11
-+    num_labels: 9
-     num_domains: 1
-     test_unlabelled: true
-     attach_label_to_end: none
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/lightning_logs.txt
-index 6942473..8ca803c 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/lightning_logs.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/lightning_logs.txt
-@@ -56,3 +56,48 @@ Epoch 1, global step 1682: val_loss reached 0.19387 (best 0.19241), saving model
- Epoch 2, step 1781: val_loss was not in top 3
- Epoch 3, step 1880: val_loss was not in top 3
- Epoch 4, step 1979: val_loss was not in top 3
-+Epoch 5, step 2078: val_loss was not in top 3
-+Epoch 6, step 2177: val_loss was not in top 3
-+Epoch 7, step 2276: val_loss was not in top 3
-+Epoch 8, step 2375: val_loss was not in top 3
-+Epoch 9, step 2474: val_loss was not in top 3
-+Epoch 10, step 2573: val_loss was not in top 3
-+Epoch 11, step 2672: val_loss was not in top 3
-+Epoch 12, step 2771: val_loss was not in top 3
-+Epoch 13, step 2870: val_loss was not in top 3
-+Epoch 14, step 2969: val_loss was not in top 3
-+
-+  | Name                | Type                 | Params
-+-------------------------------------------------------------
-+0 | transformer         | ElectraModel         | 108 M 
-+1 | punct_classifier    | TokenClassifier      | 1.2 M 
-+2 | domain_classifier   | SequenceClassifier   | 1.5 K 
-+3 | punctuation_loss    | FocalDiceLoss        | 0     
-+4 | bilstm              | LSTM                 | 7.1 M 
-+5 | domain_loss         | CrossEntropyLoss     | 0     
-+6 | agg_loss            | AggregatorLoss       | 0     
-+7 | punct_class_report  | ClassificationReport | 0     
-+8 | domain_class_report | ClassificationReport | 0     
-+-------------------------------------------------------------
-+22.5 M    Trainable params
-+94.7 M    Non-trainable params
-+117 M     Total params
-+Epoch 0, step 3068: val_loss was not in top 3
-+Epoch 1, step 3167: val_loss was not in top 3
-+Epoch 2, step 3266: val_loss was not in top 3
-+Epoch 3, step 3365: val_loss was not in top 3
-+Epoch 4, step 3464: val_loss was not in top 3
-+Epoch 5, step 3563: val_loss was not in top 3
-+Epoch 6, step 3662: val_loss was not in top 3
-+Epoch 7, step 3761: val_loss was not in top 3
-+Epoch 8, step 3860: val_loss was not in top 3
-+Epoch 9, step 3959: val_loss was not in top 3
-+Epoch 10, step 4058: val_loss was not in top 3
-+Epoch 11, step 4157: val_loss was not in top 3
-+Epoch 12, step 4256: val_loss was not in top 3
-+Epoch 13, step 4355: val_loss was not in top 3
-+Epoch 14, step 4454: val_loss was not in top 3
-+GPU available: True, used: True
-+TPU available: None, using: 0 TPU cores
-+Using environment variable NODE_RANK for node rank (0).
-+LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/nemo_error_log.txt
-index 01b5cea..950d6a0 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/nemo_error_log.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/nemo_error_log.txt
-@@ -8,3 +8,6 @@
- [NeMo W 2021-02-17 11:50:55 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f83b091f3d0> was reported to be 12 (when accessing len(dataloader)), but 13 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-       warnings.warn(warn_msg)
-     
-+[NeMo W 2021-02-17 13:03:56 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f83b0974a90> was reported to be 12 (when accessing len(dataloader)), but 13 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/nemo_log_globalrank-0_localrank-0.txt
-index 76321e4..2cf0e0c 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/nemo_log_globalrank-0_localrank-0.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-17_11-49-25/nemo_log_globalrank-0_localrank-0.txt
-@@ -10,3 +10,6 @@
- [NeMo W 2021-02-17 11:50:55 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f83b091f3d0> was reported to be 12 (when accessing len(dataloader)), but 13 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-       warnings.warn(warn_msg)
-     
-+[NeMo W 2021-02-17 13:03:56 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f83b0974a90> was reported to be 12 (when accessing len(dataloader)), but 13 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/events.out.tfevents.1613535472.intern-instance.5941.0 b/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/events.out.tfevents.1613535472.intern-instance.5941.0
-index 9b6538f..d09da1e 100644
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/events.out.tfevents.1613535472.intern-instance.5941.0 and b/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/events.out.tfevents.1613535472.intern-instance.5941.0 differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/lightning_logs.txt
-index d56406b..c177ddb 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/lightning_logs.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/lightning_logs.txt
-@@ -19,3 +19,8 @@ initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
- 7.7 M     Trainable params
- 108 M     Non-trainable params
- 116 M     Total params
-+Epoch 0, global step 2626: val_loss reached 0.06840 (best 0.06840), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.07-epoch=0.ckpt" as top 3
-+Epoch 1, global step 5253: val_loss reached 0.01696 (best 0.01696), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.02-epoch=1.ckpt" as top 3
-+Epoch 2, global step 7880: val_loss reached 0.01890 (best 0.01696), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.02-epoch=2.ckpt" as top 3
-+Saving latest checkpoint...
-+Epoch 3, global step 8484: val_loss reached 0.01890 (best 0.01696), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.02-epoch=3.ckpt" as top 3
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/nemo_error_log.txt
-index d8819de..e75da19 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/nemo_error_log.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/nemo_error_log.txt
-@@ -2,3 +2,12 @@
- [NeMo W 2021-02-17 12:17:52 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-       warnings.warn(*args, **kwargs)
-     
-+[NeMo W 2021-02-17 13:01:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fc72c6c1850> was reported to be 2627 (when accessing len(dataloader)), but 2628 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-+[NeMo W 2021-02-17 13:06:36 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fc770032130> was reported to be 328 (when accessing len(dataloader)), but 329 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-+[NeMo W 2021-02-17 15:01:57 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-+      warnings.warn(*args, **kwargs)
-+    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/nemo_log_globalrank-0_localrank-0.txt
-index 8207f2f..e4933a9 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/nemo_log_globalrank-0_localrank-0.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-17_12-17-21/nemo_log_globalrank-0_localrank-0.txt
-@@ -4,3 +4,12 @@
- [NeMo W 2021-02-17 12:17:52 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-       warnings.warn(*args, **kwargs)
-     
-+[NeMo W 2021-02-17 13:01:44 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fc72c6c1850> was reported to be 2627 (when accessing len(dataloader)), but 2628 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-+[NeMo W 2021-02-17 13:06:36 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fc770032130> was reported to be 328 (when accessing len(dataloader)), but 329 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-+[NeMo W 2021-02-17 15:01:57 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-+      warnings.warn(*args, **kwargs)
-+    
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index 9006e36..4e438d7 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -2,9 +2,9 @@ seed: 42
- trainer:
-     gpus: 1 # the number of gpus, 0 for CPU
-     num_nodes: 1
--    max_epochs: 15
-+    max_epochs: 20
-     max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 1 # accumulates grads every k batches
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-     gradient_clip_val: 0
-     amp_level: O1 # O1/O2 for mixed precision
-     precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-@@ -82,7 +82,7 @@ model:
-         ignore_start_end: false
-         use_cache: false
-         # shared among dataloaders
--        num_workers:  4
-+        num_workers:  8
-         pin_memory: false
-         drop_last: true
-         num_labels: 9
-@@ -94,7 +94,7 @@ model:
-             shuffle: true
-             num_samples: -1
-             batch_size: 32
--            manual_len: 4000 #default 0 84074
-+            manual_len: 3000 #default 0 84074
- 
-         validation_ds:
-             # if evaluation data is not in the model.dataset.data_dir as the training data or multiple datasets are used for
-@@ -118,13 +118,13 @@ model:
-         # unfrozen_layers: 1
-     
-     punct_head:
--        punct_num_fc_layers: 2
-+        punct_num_fc_layers: 3
-         fc_dropout: 0.1
-         activation: 'gelu'
-         log_softmax: false
-         use_transformer_init: true
-         loss: 'dice'
--        bilstm: true
-+        bilstm: false
- 
-     domain_head:
-         domain_num_fc_layers: 1
-@@ -147,7 +147,7 @@ model:
- 
-     frozen_lr:
-         - 2e-2
--        - 1e-4
-+        - 5e-4
-         - 5e-6
-         - 5e-7
-         - 1e-7
-diff --git a/experiment/core/utils.py b/experiment/core/utils.py
-index 6226a01..2d3c063 100644
---- a/experiment/core/utils.py
-+++ b/experiment/core/utils.py
-@@ -26,7 +26,7 @@ def position_to_mask(max_seq_length:int,indices:list):
-     try:
-         o[np.array(indices)%(max_seq_length-2)+1]=1
-     except:
--        pp('position_to_mask',np.array(indices)%(max_seq_length-2)+1)
-+        # pp('empty array')
-         o[(np.array(indices)%(max_seq_length-2)+1).astype(int)]=1
-     return o
- 
-diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
-index dda9cf5..cc59795 100644
---- a/experiment/data/punctuation_dataset_multi.py
-+++ b/experiment/data/punctuation_dataset_multi.py
-@@ -114,7 +114,8 @@ class PunctuationDomainDataset(IterableDataset):
-         
- 
-     def __len__(self):
--        return self.len
-+        pp('dataset')
-+        return pp(self.len)
-     
-     def shuffle(self, randomize=True, seed=42):
-         pp(os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)))
-@@ -177,7 +178,10 @@ class PunctuationDomainDatasets(IterableDataset):
-         self.label_map=label_map
-         self.ds_lengths=[]
-         for path in labelled+unlabelled:
--            self.ds_lengths.append(int(subprocess.Popen(['wc', '-l', f'{path}.{split}.csv'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0]))
-+            if manual_len>0:
-+                self.ds_lengths.append(min(manual_len,int(subprocess.Popen(['wc', '-l', f'{path}.{split}.csv'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0])))
-+            else:
-+                self.ds_lengths.append(int(subprocess.Popen(['wc', '-l', f'{path}.{split}.csv'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()[0].split()[0]))
-         self.max_length=max(self.ds_lengths) 
-         self.per_worker=int(self.max_length/self.num_workers)
-         self.len=int(self.per_worker/num_samples) 
-@@ -245,7 +249,7 @@ class PunctuationDomainDatasets(IterableDataset):
-             return {k:torch.cat([d[k] for d in ds], dim=0) for k in ['input_ids','attention_mask','subtoken_mask','labels','domain']}
- 
-     def __len__(self):
--        return self.len
-+        return pp(self.len)
- 
-     def shuffle(self, randomize=True, seed=42):
-         worker_info = get_worker_info()
-diff --git a/experiment/info.log b/experiment/info.log
-index 688df06..dc33065 100644
-Binary files a/experiment/info.log and b/experiment/info.log differ
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index 71700f4..e689302 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -63,6 +63,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-                               for k,v in self.ids_to_labels.items()}
-         self.label_map={k:v for k,v in self._cfg.model.label_map.items()}
-         self.data_id=data_id
-+        assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
-         self.setup_datamodule()
- 
-         if (self.hparams.model.punct_class_weights==True and self.hparams.model.punct_head.loss!='crf'):
diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/hparams.yaml
deleted file mode 100644
index f9dd10a..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/hparams.yaml
+++ /dev/null
@@ -1,122 +0,0 @@
-seed: 42
-trainer:
-  gpus: 1
-  num_nodes: 1
-  max_epochs: 20
-  max_steps: null
-  accumulate_grad_batches: 4
-  gradient_clip_val: 0
-  amp_level: O1
-  precision: 16
-  accelerator: ddp
-  checkpoint_callback: false
-  logger: false
-  log_every_n_steps: 1
-  val_check_interval: 1.0
-  resume_from_checkpoint: null
-exp_manager:
-  exp_dir: /home/nxingyu2/project/
-  name: Punctuation_with_Domain_discriminator
-  create_tensorboard_logger: true
-  create_checkpoint_callback: true
-base_path: /home/nxingyu2/data
-tmp_path: /home/nxingyu2/data/tmp
-model:
-  nemo_path: null
-  transformer_path: google/electra-base-discriminator
-  unfrozen: 0
-  maximum_unfrozen: 2
-  unfreeze_step: 1
-  punct_label_ids:
-  - ''
-  - '!'
-  - ','
-  - '-'
-  - .
-  - ':'
-  - '?'
-  - —
-  label_map:
-    …: .
-    ;: .
-  no_space_label: '#'
-  test_chunk_percent: 0.5
-  punct_class_weights: false
-  dataset:
-    data_dir: /home/nxingyu2/data
-    labelled:
-    - /home/nxingyu2/data/open_subtitles_processed
-    unlabelled: null
-    max_seq_length: 128
-    pad_label: ''
-    ignore_extra_tokens: false
-    ignore_start_end: false
-    use_cache: false
-    num_workers: 8
-    pin_memory: false
-    drop_last: true
-    num_labels: 9
-    num_domains: 1
-    test_unlabelled: true
-    attach_label_to_end: none
-    train_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 32
-      manual_len: 3000
-    validation_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 32
-  tokenizer:
-    tokenizer_name: google/electra-base-discriminator
-    vocab_file: null
-    tokenizer_model: null
-    special_tokens: null
-  language_model:
-    pretrained_model_name: google/electra-base-discriminator
-    lm_checkpoint: null
-    config_file: null
-    config: null
-  punct_head:
-    punct_num_fc_layers: 3
-    fc_dropout: 0.1
-    activation: gelu
-    log_softmax: false
-    use_transformer_init: true
-    loss: dice
-    bilstm: false
-  domain_head:
-    domain_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: cel
-    gamma: 0.1
-    pooling: mean_max
-    idx_conditioned_on: 0
-  dice_loss:
-    epsilon: 0.01
-    alpha: 3
-    macro_average: true
-  focal_loss:
-    gamma: 2
-  frozen_lr:
-  - 0.02
-  - 0.0005
-  - 5.0e-06
-  - 5.0e-07
-  - 1.0e-07
-  optim:
-    name: adamw
-    lr: 0.01
-    weight_decay: 0.0
-    sched:
-      name: CosineAnnealing
-      warmup_steps: null
-      warmup_ratio: 0.1
-      min_lr: 1.0e-08
-      last_epoch: -1
-      monitor: val_loss
-      reduce_on_plateau: false
diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/lightning_logs.txt
deleted file mode 100644
index d990740..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/lightning_logs.txt
+++ /dev/null
@@ -1,108 +0,0 @@
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
-Using native 16bit precision.
-initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-
-  | Name                       | Type                 | Params
---------------------------------------------------------------------
-0 | transformer                | ElectraModel         | 108 M 
-1 | punct_classifier           | TokenClassifier      | 1.2 M 
-2 | domain_classifier          | SequenceClassifier   | 1.5 K 
-3 | punctuation_loss           | FocalDiceLoss        | 0     
-4 | domain_loss                | CrossEntropyLoss     | 0     
-5 | agg_loss                   | AggregatorLoss       | 0     
-6 | punct_class_report         | ClassificationReport | 0     
-7 | chunked_punct_class_report | ClassificationReport | 0     
-8 | domain_class_report        | ClassificationReport | 0     
---------------------------------------------------------------------
-1.2 M     Trainable params
-108 M     Non-trainable params
-110 M     Total params
-Epoch 0, global step 23: val_loss reached 0.20797 (best 0.20797), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.21-epoch=0.ckpt" as top 3
-Epoch 1, global step 47: val_loss reached 0.21217 (best 0.20797), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.21-epoch=1.ckpt" as top 3
-Epoch 2, global step 71: val_loss reached 0.20422 (best 0.20422), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.20-epoch=2.ckpt" as top 3
-Epoch 3, global step 95: val_loss reached 0.12150 (best 0.12150), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.12-epoch=3.ckpt" as top 3
-Epoch 4, global step 119: val_loss reached 0.11584 (best 0.11584), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.12-epoch=4.ckpt" as top 3
-Epoch 5, global step 143: val_loss reached 0.11417 (best 0.11417), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.11-epoch=5.ckpt" as top 3
-Epoch 6, global step 167: val_loss reached 0.11546 (best 0.11417), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.12-epoch=6.ckpt" as top 3
-Epoch 7, global step 191: val_loss reached 0.10206 (best 0.10206), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.10-epoch=7.ckpt" as top 3
-Epoch 8, global step 215: val_loss reached 0.10457 (best 0.10206), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.10-epoch=8.ckpt" as top 3
-Epoch 9, global step 239: val_loss reached 0.10407 (best 0.10206), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.10-epoch=9.ckpt" as top 3
-Epoch 10, step 263: val_loss was not in top 3
-Epoch 11, global step 287: val_loss reached 0.07005 (best 0.07005), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.07-epoch=11.ckpt" as top 3
-Epoch 12, global step 311: val_loss reached 0.02657 (best 0.02657), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.03-epoch=12.ckpt" as top 3
-Epoch 13, global step 335: val_loss reached 0.05531 (best 0.02657), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.06-epoch=13.ckpt" as top 3
-Epoch 14, step 359: val_loss was not in top 3
-Epoch 15, global step 383: val_loss reached 0.02220 (best 0.02220), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.02-epoch=15.ckpt" as top 3
-Epoch 16, global step 407: val_loss reached 0.01779 (best 0.01779), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.02-epoch=16.ckpt" as top 3
-Epoch 17, global step 431: val_loss reached 0.01862 (best 0.01779), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.02-epoch=17.ckpt" as top 3
-Epoch 18, global step 455: val_loss reached 0.01871 (best 0.01779), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.02-epoch=18.ckpt" as top 3
-Epoch 19, step 479: val_loss was not in top 3
-Saving latest checkpoint...
-
-  | Name                       | Type                 | Params
---------------------------------------------------------------------
-0 | transformer                | ElectraModel         | 108 M 
-1 | punct_classifier           | TokenClassifier      | 1.2 M 
-2 | domain_classifier          | SequenceClassifier   | 1.5 K 
-3 | punctuation_loss           | FocalDiceLoss        | 0     
-4 | domain_loss                | CrossEntropyLoss     | 0     
-5 | agg_loss                   | AggregatorLoss       | 0     
-6 | punct_class_report         | ClassificationReport | 0     
-7 | chunked_punct_class_report | ClassificationReport | 0     
-8 | domain_class_report        | ClassificationReport | 0     
---------------------------------------------------------------------
-8.3 M     Trainable params
-101 M     Non-trainable params
-110 M     Total params
-Epoch 0, step 503: val_loss was not in top 3
-Epoch 1, step 527: val_loss was not in top 3
-Epoch 2, step 551: val_loss was not in top 3
-Epoch 3, step 575: val_loss was not in top 3
-Epoch 4, step 599: val_loss was not in top 3
-Epoch 5, step 623: val_loss was not in top 3
-Epoch 6, step 647: val_loss was not in top 3
-Epoch 7, step 671: val_loss was not in top 3
-Epoch 8, step 695: val_loss was not in top 3
-Epoch 9, step 719: val_loss was not in top 3
-Epoch 10, step 743: val_loss was not in top 3
-Epoch 11, step 767: val_loss was not in top 3
-Epoch 12, step 791: val_loss was not in top 3
-Epoch 13, step 815: val_loss was not in top 3
-Epoch 14, step 839: val_loss was not in top 3
-Epoch 15, step 863: val_loss was not in top 3
-Epoch 16, step 887: val_loss was not in top 3
-Epoch 17, step 911: val_loss was not in top 3
-Epoch 18, step 935: val_loss was not in top 3
-Epoch 19, step 959: val_loss was not in top 3
-
-  | Name                       | Type                 | Params
---------------------------------------------------------------------
-0 | transformer                | ElectraModel         | 108 M 
-1 | punct_classifier           | TokenClassifier      | 1.2 M 
-2 | domain_classifier          | SequenceClassifier   | 1.5 K 
-3 | punctuation_loss           | FocalDiceLoss        | 0     
-4 | domain_loss                | CrossEntropyLoss     | 0     
-5 | agg_loss                   | AggregatorLoss       | 0     
-6 | punct_class_report         | ClassificationReport | 0     
-7 | chunked_punct_class_report | ClassificationReport | 0     
-8 | domain_class_report        | ClassificationReport | 0     
---------------------------------------------------------------------
-15.4 M    Trainable params
-94.7 M    Non-trainable params
-110 M     Total params
-Epoch 0, step 983: val_loss was not in top 3
-Epoch 1, step 1007: val_loss was not in top 3
-Epoch 2, step 1031: val_loss was not in top 3
-Epoch 3, step 1055: val_loss was not in top 3
-Epoch 4, step 1079: val_loss was not in top 3
-Epoch 5, step 1103: val_loss was not in top 3
-Epoch 6, step 1127: val_loss was not in top 3
-Epoch 7, step 1151: val_loss was not in top 3
-Epoch 8, step 1175: val_loss was not in top 3
-Epoch 9, step 1199: val_loss was not in top 3
-Epoch 10, step 1223: val_loss was not in top 3
-Epoch 11, step 1247: val_loss was not in top 3
-Epoch 12, step 1271: val_loss was not in top 3
-Epoch 13, step 1295: val_loss was not in top 3
diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/nemo_error_log.txt
deleted file mode 100644
index caf1cb9..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/nemo_error_log.txt
+++ /dev/null
@@ -1,25 +0,0 @@
-[NeMo W 2021-02-17 15:49:58 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-17 15:50:23 nemo_logging:349] /home/nxingyu2/project/experiment/models/punctuation_domain_model.py:66: SyntaxWarning: assertion is always true, perhaps remove parentheses?
-      assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
-    
-[NeMo W 2021-02-17 15:50:23 nemo_logging:349] /home/nxingyu2/project/experiment/models/punctuation_domain_model.py:66: SyntaxWarning: assertion is always true, perhaps remove parentheses?
-      assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
-    
-[NeMo W 2021-02-17 15:50:26 nemo_logging:349] /home/nxingyu2/project/experiment/models/punctuation_domain_model.py:66: SyntaxWarning: assertion is always true, perhaps remove parentheses?
-      assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
-    
-[NeMo W 2021-02-17 15:50:26 nemo_logging:349] /home/nxingyu2/project/experiment/models/punctuation_domain_model.py:66: SyntaxWarning: assertion is always true, perhaps remove parentheses?
-      assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
-    
-[NeMo W 2021-02-17 15:50:26 nemo_logging:349] /home/nxingyu2/project/experiment/models/punctuation_domain_model.py:66: SyntaxWarning: assertion is always true, perhaps remove parentheses?
-      assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
-    
-[NeMo W 2021-02-17 15:50:28 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-17 15:51:56 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7feb635f9670> was reported to be 93 (when accessing len(dataloader)), but 94 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-17 15:56:16 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7feb636c44c0> was reported to be 328 (when accessing len(dataloader)), but 329 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index fe3756a..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,27 +0,0 @@
-[NeMo I 2021-02-17 15:49:58 exp_manager:183] Experiments will be logged at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_15-49-58
-[NeMo I 2021-02-17 15:49:58 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-17 15:49:58 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-17 15:50:23 nemo_logging:349] /home/nxingyu2/project/experiment/models/punctuation_domain_model.py:66: SyntaxWarning: assertion is always true, perhaps remove parentheses?
-      assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
-    
-[NeMo W 2021-02-17 15:50:23 nemo_logging:349] /home/nxingyu2/project/experiment/models/punctuation_domain_model.py:66: SyntaxWarning: assertion is always true, perhaps remove parentheses?
-      assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
-    
-[NeMo W 2021-02-17 15:50:26 nemo_logging:349] /home/nxingyu2/project/experiment/models/punctuation_domain_model.py:66: SyntaxWarning: assertion is always true, perhaps remove parentheses?
-      assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
-    
-[NeMo W 2021-02-17 15:50:26 nemo_logging:349] /home/nxingyu2/project/experiment/models/punctuation_domain_model.py:66: SyntaxWarning: assertion is always true, perhaps remove parentheses?
-      assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
-    
-[NeMo W 2021-02-17 15:50:26 nemo_logging:349] /home/nxingyu2/project/experiment/models/punctuation_domain_model.py:66: SyntaxWarning: assertion is always true, perhaps remove parentheses?
-      assert(len(self._cfg.model.dataset.labelled)>0,'Please include at least 1 labelled dataset')
-    
-[NeMo W 2021-02-17 15:50:28 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-17 15:51:56 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7feb635f9670> was reported to be 93 (when accessing len(dataloader)), but 94 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-17 15:56:16 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7feb636c44c0> was reported to be 328 (when accessing len(dataloader)), but 329 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_21-09-22/events.out.tfevents.1613567392.intern-instance.29489.0 b/Punctuation_with_Domain_discriminator/2021-02-17_21-09-22/events.out.tfevents.1613567392.intern-instance.29489.0
index 27406d2..416bf3f 100644
Binary files a/Punctuation_with_Domain_discriminator/2021-02-17_21-09-22/events.out.tfevents.1613567392.intern-instance.29489.0 and b/Punctuation_with_Domain_discriminator/2021-02-17_21-09-22/events.out.tfevents.1613567392.intern-instance.29489.0 differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_21-09-22/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-17_21-09-22/lightning_logs.txt
index 6ce2f94..2085c98 100644
--- a/Punctuation_with_Domain_discriminator/2021-02-17_21-09-22/lightning_logs.txt
+++ b/Punctuation_with_Domain_discriminator/2021-02-17_21-09-22/lightning_logs.txt
@@ -20,3 +20,27 @@ initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
 10.6 M    Trainable params
 108 M     Non-trainable params
 119 M     Total params
+Epoch 0, global step 62: val_loss reached 1.03330 (best 1.03330), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_21-09-22/checkpoints/Punctuation_with_Domain_discriminator---val_loss=1.03-epoch=0.ckpt" as top 3
+Epoch 0, global step 124: val_loss reached 0.33597 (best 0.33597), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_21-09-22/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.34-epoch=0.ckpt" as top 3
+Epoch 0, global step 187: val_loss reached 0.42837 (best 0.33597), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_21-09-22/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.43-epoch=0.ckpt" as top 3
+Epoch 0, global step 249: val_loss reached 0.87454 (best 0.33597), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-17_21-09-22/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=0.ckpt" as top 3
+Epoch 0, step 312: val_loss was not in top 3
+Saving latest checkpoint...
+Epoch 1, step 346: val_loss was not in top 3
+
+  | Name                       | Type                 | Params
+--------------------------------------------------------------------
+0 | transformer                | ElectraModel         | 108 M 
+1 | punct_classifier           | TokenClassifier      | 1.2 M 
+2 | domain_classifier          | SequenceClassifier   | 2.4 M 
+3 | punctuation_loss           | FocalDiceLoss        | 0     
+4 | bilstm                     | LSTM                 | 7.1 M 
+5 | domain_loss                | CrossEntropyLoss     | 0     
+6 | agg_loss                   | AggregatorLoss       | 0     
+7 | punct_class_report         | ClassificationReport | 0     
+8 | chunked_punct_class_report | ClassificationReport | 0     
+9 | domain_class_report        | ClassificationReport | 0     
+--------------------------------------------------------------------
+17.7 M    Trainable params
+101 M     Non-trainable params
+119 M     Total params
diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_21-09-22/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-17_21-09-22/nemo_error_log.txt
index 1f3f446..33f79b1 100644
--- a/Punctuation_with_Domain_discriminator/2021-02-17_21-09-22/nemo_error_log.txt
+++ b/Punctuation_with_Domain_discriminator/2021-02-17_21-09-22/nemo_error_log.txt
@@ -17,3 +17,12 @@
 [NeMo W 2021-02-17 21:09:52 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
       warnings.warn(*args, **kwargs)
     
+[NeMo W 2021-02-17 21:37:58 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f3b1c0ce100> was reported to be 328 (when accessing len(dataloader)), but 329 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
+[NeMo W 2021-02-17 22:54:16 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f3b3d9c92b0> was reported to be 1250 (when accessing len(dataloader)), but 1251 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
+[NeMo W 2021-02-17 23:17:57 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
+      warnings.warn(*args, **kwargs)
+    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-17_21-09-22/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-17_21-09-22/nemo_log_globalrank-0_localrank-0.txt
index 507c7be..a8ca775 100644
--- a/Punctuation_with_Domain_discriminator/2021-02-17_21-09-22/nemo_log_globalrank-0_localrank-0.txt
+++ b/Punctuation_with_Domain_discriminator/2021-02-17_21-09-22/nemo_log_globalrank-0_localrank-0.txt
@@ -19,3 +19,12 @@
 [NeMo W 2021-02-17 21:09:52 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
       warnings.warn(*args, **kwargs)
     
+[NeMo W 2021-02-17 21:37:58 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f3b1c0ce100> was reported to be 328 (when accessing len(dataloader)), but 329 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
+[NeMo W 2021-02-17 22:54:16 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f3b3d9c92b0> was reported to be 1250 (when accessing len(dataloader)), but 1251 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
+      warnings.warn(warn_msg)
+    
+[NeMo W 2021-02-17 23:17:57 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
+      warnings.warn(*args, **kwargs)
+    
diff --git a/experiment/config.yaml b/experiment/config.yaml
index 234e5a9..a3c6e12 100644
--- a/experiment/config.yaml
+++ b/experiment/config.yaml
@@ -12,7 +12,7 @@ trainer:
     checkpoint_callback: false  # Provided by exp_manager
     logger: false #false  # Provided by exp_manager
     log_every_n_steps: 1  # Interval of logging.
-    val_check_interval: 0.2  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
+    val_check_interval: 0.2 #0.2  # Set to 0.25 to check 4 times per epoch, 1.0 for normal, or an int for number of iterations
     resume_from_checkpoint: null
 
     # gpus: 0 # the number of gpus, 0 for CPU
@@ -48,18 +48,21 @@ model:
     unfreeze_step: 1
     punct_label_ids:
         - ""
-        - "!"
         - ","
         - "-"
         - "."
-        - ":"
         - "?"
-        - "—"
 
+        # - "!"
+        # - ":"
+        # - "—"
         # - ";"
         # - "…"
 
     label_map:
+        ":": ","
+        "—": ","
+        "!": "."
         "…": "."
         ";": "."
 
@@ -130,7 +133,7 @@ model:
     domain_head:
         domain_num_fc_layers: 2
         fc_dropout: 0.1
-        activation: 'gelu'
+        activation: 'relu'
         log_softmax: false
         use_transformer_init: true
         loss: 'cel'
@@ -147,7 +150,7 @@ model:
         gamma: 2
 
     frozen_lr:
-        - 2e-2
+        - 1e-2
         - 5e-4
         - 5e-6
         - 5e-7
diff --git a/experiment/data/punctuation_datamodule.py b/experiment/data/punctuation_datamodule.py
index 4950246..f41d06a 100644
--- a/experiment/data/punctuation_datamodule.py
+++ b/experiment/data/punctuation_datamodule.py
@@ -84,7 +84,8 @@ class PunctuationDataModule(LightningDataModule):
                     data_id=self.data_id,
                     tmp_path=self.tmp_path,
                     attach_label_to_end=self.attach_label_to_end,
-                    manual_len=self.manual_len)
+                    manual_len=self.manual_len,
+                    no_space_label=self.no_space_label)
             self.val_dataset = PunctuationDomainDatasets(split='dev',
                     num_samples=self.val_batch_size,
                     max_seq_length=self.max_seq_length,
@@ -96,7 +97,8 @@ class PunctuationDataModule(LightningDataModule):
                     randomize=self.val_shuffle,
                     data_id=self.data_id,
                     tmp_path=self.tmp_path,
-                    attach_label_to_end=self.attach_label_to_end)
+                    attach_label_to_end=self.attach_label_to_end,
+                    no_space_label=self.no_space_label)
         if stage=='test' or stage is None:
             if (len(self.unlabelled)>0) and self.test_unlabelled:
                 self.test_dataset = PunctuationDomainDatasets(split='test',
@@ -110,7 +112,8 @@ class PunctuationDataModule(LightningDataModule):
                     randomize=self.val_shuffle,
                     data_id=self.data_id,
                     tmp_path=self.tmp_path,
-                    attach_label_to_end=self.attach_label_to_end
+                    attach_label_to_end=self.attach_label_to_end,
+                    no_space_label=self.no_space_label
                     )
             else: self.test_dataset = PunctuationDomainDatasets(split='test',
                     num_samples=self.val_batch_size,
@@ -123,7 +126,8 @@ class PunctuationDataModule(LightningDataModule):
                     randomize=self.val_shuffle,
                     data_id=self.data_id,
                     tmp_path=self.tmp_path,
-                    attach_label_to_end=self.attach_label_to_end
+                    attach_label_to_end=self.attach_label_to_end,
+                    no_space_label=self.no_space_label
                     )
 
         logging.info(f"shuffling train set")
diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
index e2021a3..adae5cd 100644
--- a/experiment/data/punctuation_dataset_multi.py
+++ b/experiment/data/punctuation_dataset_multi.py
@@ -166,7 +166,7 @@ class PunctuationDomainDatasets(IterableDataset):
                  tmp_path='~/data/tmp',
                  attach_label_to_end=None,
                  manual_len:int=0,
-                 no_space_label:int=2,
+                 no_space_label:int=None,
                  ):
         worker_info = get_worker_info()
         self.num_workers=1 if worker_info is None else worker_info.num_workers
diff --git a/experiment/info.log b/experiment/info.log
index 3fb93c0..104c5dd 100644
Binary files a/experiment/info.log and b/experiment/info.log differ
