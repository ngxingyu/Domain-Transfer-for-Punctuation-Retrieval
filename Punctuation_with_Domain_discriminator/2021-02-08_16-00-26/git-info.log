commit hash: d9cdb13829a1dfa2d74afb03fde5acec0f85d2cc
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/lightning_logs.txt
index 2a26724..31870ad 100644
--- a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/lightning_logs.txt
+++ b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/lightning_logs.txt
@@ -20,3 +20,36 @@ initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
 299 K     Trainable params
 13.2 M    Non-trainable params
 13.5 M    Total params
+Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/lr_find_temp_model.ckpt
+Global seed set to 42
+
+  | Name                | Type                 | Params
+-------------------------------------------------------------
+0 | transformer         | ElectraModel         | 13.5 M
+1 | punct_classifier    | TokenClassifier      | 2.6 K 
+2 | domain_classifier   | SequenceClassifier   | 1.0 K 
+3 | punctuation_loss    | FocalDiceLoss        | 0     
+4 | domain_loss         | CrossEntropyLoss     | 0     
+5 | agg_loss            | AggregatorLoss       | 0     
+6 | punct_class_report  | ClassificationReport | 0     
+7 | domain_class_report | ClassificationReport | 0     
+-------------------------------------------------------------
+299 K     Trainable params
+13.2 M    Non-trainable params
+13.5 M    Total params
+Global seed set to 42
+
+  | Name                | Type                 | Params
+-------------------------------------------------------------
+0 | transformer         | ElectraModel         | 13.5 M
+1 | punct_classifier    | TokenClassifier      | 2.6 K 
+2 | domain_classifier   | SequenceClassifier   | 1.0 K 
+3 | punctuation_loss    | FocalDiceLoss        | 0     
+4 | domain_loss         | CrossEntropyLoss     | 0     
+5 | agg_loss            | AggregatorLoss       | 0     
+6 | punct_class_report  | ClassificationReport | 0     
+7 | domain_class_report | ClassificationReport | 0     
+-------------------------------------------------------------
+826 K     Trainable params
+12.7 M    Non-trainable params
+13.5 M    Total params
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_error_log.txt
index 4ddbe1b..90e4c4c 100644
--- a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_error_log.txt
+++ b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_error_log.txt
@@ -2,3 +2,12 @@
 [NeMo W 2021-02-08 15:34:34 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
       warnings.warn(*args, **kwargs)
     
+[NeMo W 2021-02-08 15:46:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
+      warnings.warn(*args, **kwargs)
+    
+[NeMo W 2021-02-08 15:51:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
+      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
+    
+[NeMo W 2021-02-08 15:57:37 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
+      warnings.warn(*args, **kwargs)
+    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_log_globalrank-0_localrank-0.txt
index dea36af..926854f 100644
--- a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_log_globalrank-0_localrank-0.txt
+++ b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_log_globalrank-0_localrank-0.txt
@@ -4,3 +4,12 @@
 [NeMo W 2021-02-08 15:34:34 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
       warnings.warn(*args, **kwargs)
     
+[NeMo W 2021-02-08 15:46:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
+      warnings.warn(*args, **kwargs)
+    
+[NeMo W 2021-02-08 15:51:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
+      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
+    
+[NeMo W 2021-02-08 15:57:37 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
+      warnings.warn(*args, **kwargs)
+    
diff --git a/README.md b/README.md
index beca3ef..ef7d22c 100644
--- a/README.md
+++ b/README.md
@@ -449,3 +449,7 @@ weighted avg                                            69.12      72.03      70
  'punct_recall': 33.78831481933594,
  'test_loss': 0.2638570964336395}
 
+
+
+### domain adversarial dice 3, open l ted ul 
+initial_lr 0.007943282347242822
diff --git a/experiment/config.yaml b/experiment/config.yaml
index 070bc4f..37d105e 100644
--- a/experiment/config.yaml
+++ b/experiment/config.yaml
@@ -85,7 +85,7 @@ model:
         train_ds:
             shuffle: true
             num_samples: -1
-            batch_size: 8
+            batch_size: 4
 
         validation_ds:
             # if evaluation data is not in the model.dataset.data_dir as the training data or multiple datasets are used for
@@ -93,7 +93,7 @@ model:
             # ds_item: null # expected format: [PATH_TO_DEV1,PATH_TO_DEV2] (Note no space between the paths and square brackets)
             shuffle: true
             num_samples: -1
-            batch_size: 8
+            batch_size: 4
 
     tokenizer:
         tokenizer_name: ${model.language_model.pretrained_model_name} # or sentencepiece
@@ -123,7 +123,7 @@ model:
         log_softmax: false
         use_transformer_init: true
         loss: 'cel'
-        gamma: 0 #0.1 # coefficient of gradient reversal
+        gamma: 0.1 #0.1 # coefficient of gradient reversal
         pooling: 'mean_max' # 'mean' mean_max
         idx_conditioned_on: 0
     
diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
index bc844cd..4027bb2 100644
--- a/experiment/data/punctuation_dataset_multi.py
+++ b/experiment/data/punctuation_dataset_multi.py
@@ -63,7 +63,8 @@ class PunctuationDomainDataset(IterableDataset):
         self.randomize=randomize
         self.target_file=target_file
         self.tmp_path=tmp_path
-        os.system(f'cp {self.csv_file} {self.target_file}')
+        if not (os.path.exists(self.target_file)):
+            os.system(f'cp {self.csv_file} {self.target_file}')
 
     def __iter__(self):
         self.dataset=iter(pd.read_csv(
diff --git a/experiment/info.log b/experiment/info.log
index 481b8ff..e69de29 100644
--- a/experiment/info.log
+++ b/experiment/info.log
@@ -1,43 +0,0 @@
-[INFO] - shuffling train set
-[INFO] - Optimizer config = AdamW (
-Parameter Group 0
-    amsgrad: False
-    betas: (0.9, 0.999)
-    eps: 1e-08
-    lr: 0.001
-    weight_decay: 0.0
-)
-[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fd3155e83d0>" 
-will be used during training (effective maximum steps = 80) - 
-Parameters : 
-(warmup_steps: null
-warmup_ratio: 0.1
-last_epoch: -1
-max_steps: 80
-)
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                           0.00       0.00       0.00        168
-! (label_id: 1)                                         14.29       7.69      10.00        104
-, (label_id: 2)                                         23.21      27.23      25.06        584
-- (label_id: 3)                                          4.02      46.67       7.39         45
-. (label_id: 4)                                         54.17       1.19       2.33       1091
-: (label_id: 5)                                          0.00       0.00       0.00          0
-; (label_id: 6)                                          0.00       0.00       0.00          0
-? (label_id: 7)                                          6.42      22.75      10.01        189
-— (label_id: 8)                                          0.00       0.00       0.00          0
-… (label_id: 9)                                          9.09       2.38       3.77         84
--------------------
-micro avg                                               10.86      10.86      10.86       2265
-macro avg                                               15.88      15.42       8.37       2265
-weighted avg                                            33.68      10.86       9.17       2265
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         50.00     100.00      66.67         84
-1 (label_id: 1)                                          0.00       0.00       0.00         84
--------------------
-micro avg                                               50.00      50.00      50.00        168
-macro avg                                               25.00      50.00      33.33        168
-weighted avg                                            25.00      50.00      33.33        168
-
