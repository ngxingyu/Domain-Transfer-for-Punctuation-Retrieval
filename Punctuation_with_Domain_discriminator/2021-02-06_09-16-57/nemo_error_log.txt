[NeMo W 2021-02-06 09:16:57 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.
      warnings.warn(*args, **kwargs)
    
[NeMo W 2021-02-06 09:16:57 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
[NeMo W 2021-02-06 09:17:08 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
      warnings.warn(*args, **kwargs)
    
[NeMo W 2021-02-06 09:17:08 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
      warnings.warn(*args, **kwargs)
    
[NeMo W 2021-02-06 09:17:12 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
      warnings.warn(*args, **kwargs)
    
[NeMo W 2021-02-06 09:17:21 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torchtools/optim/radam.py:62: UserWarning: This overload of addcmul_ is deprecated:
    	addcmul_(Number value, Tensor tensor1, Tensor tensor2)
    Consider using one of the following signatures instead:
    	addcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/torch/csrc/utils/python_arg_parser.cpp:882.)
      exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
    
[NeMo W 2021-02-06 09:26:31 nemo_logging:349] /home/nxingyu2/project/experiment/core/optim/lr_scheduler.py:49: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
      warnings.warn(
    
[NeMo W 2021-02-06 09:36:51 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f56bb2964c0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
      warnings.warn(warn_msg)
    
[NeMo W 2021-02-06 09:37:19 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f56bb296220> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
      warnings.warn(warn_msg)
    
[NeMo W 2021-02-06 09:38:03 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
      warnings.warn(*args, **kwargs)
    
[NeMo W 2021-02-06 09:38:05 nemo_logging:349] /home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
      warnings.warn(*args, **kwargs)
    
Failed to compute suggesting for `lr`. There might not be enough points.
Traceback (most recent call last):
  File "/home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 353, in suggestion
    min_grad = np.gradient(loss).argmin()
  File "<__array_function__ internals>", line 5, in gradient
  File "/home/nxingyu2/miniconda3/envs/NLP/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1052, in gradient
    raise ValueError(
ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
