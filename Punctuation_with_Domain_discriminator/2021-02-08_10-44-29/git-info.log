commit hash: 939a671c8c117db6975316767ced5d95449e2b27
diff --git a/README.md b/README.md
index d52dba5..0b403d3 100644
--- a/README.md
+++ b/README.md
@@ -333,4 +333,71 @@ label                                                precision    recall       f
  'punct_f1': 0.5858508944511414,
  'punct_precision': 0.30176490545272827,
  'punct_recall': 10.0,
- 'test_loss': 0.8140875697135925}
\ No newline at end of file
+ 'test_loss': 0.8140875697135925}
+
+### elsmall dice alpha 3 unweighted ted-l unfrozen 0-2 2 ep 
+
+ (label_id: 0)                                          62.15     100.00      76.66       5154
+! (label_id: 1)                                          0.00       0.00       0.00        108
+, (label_id: 2)                                          0.00       0.00       0.00      18022
+- (label_id: 3)                                          0.00       0.00       0.00       1557
+. (label_id: 4)                                         41.74      94.01      57.81      15164
+: (label_id: 5)                                          0.00       0.00       0.00        319
+; (label_id: 6)                                          0.00       0.00       0.00         88
+? (label_id: 7)                                          0.00       0.00       0.00       1217
+ (label_id: 8)                                          0.00       0.00       0.00        752
+… (label_id: 9)                                          0.00       0.00       0.00         67
+-------------------
+micro avg                                               45.72      45.72      45.72      42448
+macro avg                                               10.39      19.40      13.45      42448
+weighted avg                                            22.46      45.72      29.96      42448
+{ 'punct_f1': 13.446383476257324,
+ 'punct_precision': 10.388500213623047,
+ 'punct_recall': 19.400554656982422,
+ 'test_loss': 0.44148480892181396}
+
+
+ ## Log for 8/2/2021
+
+ I believe Dice loss performs better when unweighted, and the experiments that failed to converge were due to it being weighted.
+ For 3 layers unfreezing, the tuning of the 1st and 2nd layers result in some form of divergence, I believe the training process causes divergence and requires a much smaller learning rate.
+
+For /2021-02-08_07-56-46 crf adam, frozen best lr was 0.01, auto set to 0.007943282347242822.
+End frozen 
+micro avg                                               41.42      41.42      41.42      33406
+macro avg                                               11.01      13.54      11.07      33406
+weighted avg                                            34.88      41.42      34.20      33406
+
+1st layer best lr 1e-10, set to 0.007943282347242822
+micro avg                                               36.65      36.65      36.65      33463
+macro avg                                               10.71       9.91       8.26      33463
+weighted avg                                            34.32      36.65      31.08      33463
+
+2nd layer best lr 1e-10, set to 0.007943282347242822
+micro avg                                               35.72      35.72      35.72      42448
+macro avg                                                3.57      10.00       5.26      42448
+weighted avg                                            12.76      35.72      18.81      42448
+
+{'punct_f1': 5.264181137084961,
+ 'punct_precision': 3.572371006011963,
+ 'punct_recall': 10.0,
+ 'test_loss': 18.49854850769043}
+
+
+For 2021-02-08_08-37-54/ dice adamw, the frozen best lr was at 0.01, auto set to 0.005011872336272725.
+alpha from 3->4 seems to reduce convergence rate.
+
+micro avg                                               50.98      50.98      50.98      33463
+macro avg                                               25.99      25.38      23.38      33463
+weighted avg                                            50.31      50.98      48.27      33463
+
+unfreeze 1 0.0025118864315095825 best lr 1e-10, 
+micro avg                                               58.55      58.55      58.55      39340
+macro avg                                               30.02      29.74      29.52      39340
+weighted avg                                            57.91      58.55      57.51      39340
+
+still increasing?!
+{'punct_f1': 29.523975372314453,
+ 'punct_precision': 30.015613555908203,
+ 'punct_recall': 29.738296508789062,
+ 'test_loss': 0.3690211772918701}
diff --git a/experiment/config.yaml b/experiment/config.yaml
index fe58670..872a5ed 100644
--- a/experiment/config.yaml
+++ b/experiment/config.yaml
@@ -1,49 +1,49 @@
 seed: 42
 trainer:
-    # gpus: 1 # the number of gpus, 0 for CPU
-    # num_nodes: 1
-    # max_epochs: 2
-    # max_steps: null # precedence over max_epochs
-    # accumulate_grad_batches: 4 # accumulates grads every k batches
-    # gradient_clip_val: 0
-    # amp_level: O1 # O1/O2 for mixed precision
-    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-    # accelerator: ddp
-    # checkpoint_callback: false  # Provided by exp_manager
-    # logger: false #false  # Provided by exp_manager
-    # log_every_n_steps: 1  # Interval of logging.
-    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-    # resume_from_checkpoint: null
-
-    gpus: 0 # the number of gpus, 0 for CPU
+    gpus: 1 # the number of gpus, 0 for CPU
     num_nodes: 1
-    max_epochs: 8
+    max_epochs: 10
     max_steps: null # precedence over max_epochs
-    accumulate_grad_batches: 1 # accumulates grads every k batches
+    accumulate_grad_batches: 4 # accumulates grads every k batches
     gradient_clip_val: 0
-    amp_level: O0 # O1/O2 for mixed precision
-    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-    # accelerator: ddp
+    amp_level: O1 # O1/O2 for mixed precision
+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
+    accelerator: ddp
     checkpoint_callback: false  # Provided by exp_manager
     logger: false #false  # Provided by exp_manager
     log_every_n_steps: 1  # Interval of logging.
     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-    reload_dataloaders_every_epoch: true
     resume_from_checkpoint: null
 
+    # gpus: 0 # the number of gpus, 0 for CPU
+    # num_nodes: 1
+    # max_epochs: 8
+    # max_steps: null # precedence over max_epochs
+    # accumulate_grad_batches: 1 # accumulates grads every k batches
+    # gradient_clip_val: 0
+    # amp_level: O0 # O1/O2 for mixed precision
+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
+    # # accelerator: ddp
+    # checkpoint_callback: false  # Provided by exp_manager
+    # logger: false #false  # Provided by exp_manager
+    # log_every_n_steps: 1  # Interval of logging.
+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
+    # reload_dataloaders_every_epoch: true
+    # resume_from_checkpoint: null
+
 exp_manager:
-    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
     name: Punctuation_with_Domain_discriminator  # The name of your model
     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
     create_checkpoint_callback: true 
-base_path: /home/nxingyu2/data # /root/data # 
-tmp_path: /home/nxingyu2/data/tmp # /tmp # 
+base_path: /home/nxingyu/data # /root/data # 
+tmp_path: /home/nxingyu/data/tmp # /tmp # 
 
 model:
     nemo_path: null
-    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
     unfrozen: 0
-    maximum_unfrozen: 2
+    maximum_unfrozen: 1
     unfreeze_step: 1
     # unfreeze_every: 3
     punct_label_ids:
@@ -58,10 +58,10 @@ model:
         - "—"
         - "…"
 
-    punct_class_weights: true
+    punct_class_weights: false
     
     dataset:
-        data_dir: /home/nxingyu2/data # /root/data # 
+        data_dir: /home/nxingyu/data # /root/data # 
         labelled:
             - ${base_path}/ted_talks_processed #
         unlabelled:
@@ -106,6 +106,11 @@ model:
         config: null
         # unfrozen_layers: 1
 
+    mlp:
+        num_fc_layers: 2
+        fc_dropout: 0.1
+        activation: 'relu'
+        
     punct_head:
         punct_num_fc_layers: 1
         fc_dropout: 0.1
@@ -117,22 +122,24 @@ model:
     domain_head:
         domain_num_fc_layers: 1
         fc_dropout: 0.1
-        activation: 'relu'
+        activation: 'gelu'
         log_softmax: false
         use_transformer_init: true
         loss: 'cel'
         gamma: 0.1 # coefficient of gradient reversal
+        pooling: 'token'
+        idx_conditioned_on: 0
     
     dice_loss:
         epsilon: 0.01
-        alpha: 3
+        alpha: 4
         macro_average: true
 
     focal_loss: 
         gamma: 5
 
     optim:
-        name: novograd
+        name: adamw
         lr: 1e-3
         weight_decay: 0.00
         sched:
diff --git a/experiment/core/layers/sequence_classifier.py b/experiment/core/layers/sequence_classifier.py
index d4ff927..b4b92b0 100644
--- a/experiment/core/layers/sequence_classifier.py
+++ b/experiment/core/layers/sequence_classifier.py
@@ -20,7 +20,8 @@ class SequenceClassifier(nn.Module):
         log_softmax: bool = True,
         dropout: float = 0.0,
         use_transformer_init: bool = True,
-        idx_conditioned_on: int = 0,
+        pooling: str = 'mean', # mean, max, mean_max, token
+        idx_conditioned_on: int = None,
     ):
         """
         Initializes the SequenceClassifier module.
@@ -37,6 +38,7 @@ class SequenceClassifier(nn.Module):
         super().__init__()
         self.log_softmax = log_softmax
         self._idx_conditioned_on = idx_conditioned_on
+        self.pooling = pooling
         self.mlp = MultiLayerPerceptron(
             hidden_size=hidden_size,
             num_classes=num_classes,
@@ -48,7 +50,23 @@ class SequenceClassifier(nn.Module):
         if use_transformer_init:
             self.apply(lambda module: transformer_weights_init(module, xavier=False))
 
-    def forward(self, hidden_states):
+    def forward(self, hidden_states, subtoken_mask=None):
         hidden_states = self.dropout(hidden_states)
-        logits = self.mlp(hidden_states[:, self._idx_conditioned_on])
+        if self.pooling=='token':
+            pooled = hidden_states[:, self._idx_conditioned_on]
+        else:
+            if subtoken_mask==None:
+                ct=hidden_states.shape[1] # Seq len
+                hidden_states=hidden_states*subtoken_mask.unsqueeze(2) # remove subtoken or padding contribution.
+            else:
+                ct = torch.sum(subtoken_mask,axis=1)
+            pooled_sum = torch.sum(hidden_states,axis=1)
+            if self.pooling=='mean' or self.pooling == 'mean_max':
+                pooled_mean = pooled_sum//ct
+            if self.pooling=='max' or self.pooling=='mean_max':
+                pooled_max = torch.max(hidden_states,axis=1)[0]
+            pooled=pooled_mean if self.pooling=='mean' else \
+                pooled_max if self.pooling=='max' else \
+                    torch.cat([pooled_mean,pooled_max],axis=-1)
+        logits = self.mlp(pooled)
         return logits
diff --git a/experiment/info.log b/experiment/info.log
index 2471fe9..e69de29 100644
--- a/experiment/info.log
+++ b/experiment/info.log
@@ -1,300 +0,0 @@
-[INFO] - GPU available: True, used: False
-[INFO] - TPU available: None, using: 0 TPU cores
-[INFO] - shuffling train set
-[INFO] - Optimizer config = Novograd (
-Parameter Group 0
-    amsgrad: False
-    betas: (0.95, 0.98)
-    eps: 1e-08
-    grad_averaging: False
-    lr: 0.001
-    weight_decay: 0.0
-)
-[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
-will be used during training (effective maximum steps = 80) - 
-Parameters : 
-(warmup_steps: null
-warmup_ratio: 0.1
-last_epoch: -1
-max_steps: 80
-)
-[INFO] - 
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 108 M 
-1 | punct_classifier    | TokenClassifier      | 7.7 K 
-2 | domain_classifier   | SequenceClassifier   | 769   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-8.5 K     Trainable params
-108 M     Non-trainable params
-108 M     Total params
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                           0.00       0.00       0.00        184
-! (label_id: 1)                                          0.00       0.00       0.00          4
-, (label_id: 2)                                          1.23       0.34       0.53        594
-- (label_id: 3)                                          3.06      25.42       5.46         59
-. (label_id: 4)                                         47.22      12.98      20.36        524
-: (label_id: 5)                                          0.00       0.00       0.00         18
-; (label_id: 6)                                          0.00       0.00       0.00         13
-? (label_id: 7)                                          8.45       6.32       7.23         95
-— (label_id: 8)                                          0.00       0.00       0.00         12
-… (label_id: 9)                                          0.00       0.00       0.00          0
--------------------
-micro avg                                                6.05       6.05       6.05       1503
-macro avg                                                6.66       5.01       3.73       1503
-weighted avg                                            17.61       6.05       7.98       1503
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00         92
--------------------
-micro avg                                              100.00     100.00     100.00         92
-macro avg                                              100.00     100.00     100.00         92
-weighted avg                                           100.00     100.00     100.00         92
-
-[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
-[INFO] - Optimizer config = Novograd (
-Parameter Group 0
-    amsgrad: False
-    betas: (0.95, 0.98)
-    eps: 1e-08
-    grad_averaging: False
-    lr: 1.5848931924611143e-08
-    weight_decay: 0.0
-)
-[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
-will be used during training (effective maximum steps = 3192) - 
-Parameters : 
-(warmup_steps: null
-warmup_ratio: 0.1
-last_epoch: -1
-max_steps: 3192
-)
-[INFO] - 
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 108 M 
-1 | punct_classifier    | TokenClassifier      | 7.7 K 
-2 | domain_classifier   | SequenceClassifier   | 769   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-8.5 K     Trainable params
-108 M     Non-trainable params
-108 M     Total params
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                           0.00       0.00       0.00        202
-! (label_id: 1)                                          0.00       0.00       0.00          4
-, (label_id: 2)                                          1.62       0.45       0.70        669
-- (label_id: 3)                                          3.48      27.27       6.17         66
-. (label_id: 4)                                         45.06      13.01      20.19        561
-: (label_id: 5)                                          1.52       6.67       2.47         15
-; (label_id: 6)                                          0.00       0.00       0.00         15
-? (label_id: 7)                                          8.70       7.32       7.95         82
-— (label_id: 8)                                          0.00       0.00       0.00         13
-… (label_id: 9)                                          0.00       0.00       0.00          1
--------------------
-micro avg                                                6.20       6.20       6.20       1628
-macro avg                                                6.04       5.47       3.75       1628
-weighted avg                                            16.79       6.20       7.92       1628
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00        101
--------------------
-micro avg                                              100.00     100.00     100.00        101
-macro avg                                              100.00     100.00     100.00        101
-weighted avg                                           100.00     100.00     100.00        101
-
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                           0.34       0.73       0.46       4402
-! (label_id: 1)                                          0.42      13.95       0.82        129
-, (label_id: 2)                                          2.53       0.64       1.03      15243
-- (label_id: 3)                                          2.45      21.03       4.38       1322
-. (label_id: 4)                                         44.00      11.40      18.11      12542
-: (label_id: 5)                                          0.43       1.41       0.65        354
-; (label_id: 6)                                          0.00       0.00       0.00        163
-? (label_id: 7)                                          4.16       6.27       5.00       1117
-— (label_id: 8)                                          3.00       0.61       1.02        488
-… (label_id: 9)                                          0.97       6.17       1.68         81
--------------------
-micro avg                                                5.41       5.41       5.41      35841
-macro avg                                                5.83       6.22       3.32      35841
-weighted avg                                            16.78       5.41       7.18      35841
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       2201
--------------------
-micro avg                                              100.00     100.00     100.00       2201
-macro avg                                              100.00     100.00     100.00       2201
-weighted avg                                           100.00     100.00     100.00       2201
-
-[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                           0.20       0.43       0.27       4226
-! (label_id: 1)                                          0.44      14.17       0.86        127
-, (label_id: 2)                                          1.93       0.49       0.78      14611
-- (label_id: 3)                                          2.23      19.56       4.01       1237
-. (label_id: 4)                                         43.37      11.25      17.86      11977
-: (label_id: 5)                                          0.68       2.34       1.05        342
-; (label_id: 6)                                          0.00       0.00       0.00        129
-? (label_id: 7)                                          5.16       7.47       6.10       1058
-— (label_id: 8)                                          2.15       0.49       0.80        409
-… (label_id: 9)                                          0.69       4.23       1.19         71
--------------------
-micro avg                                                5.23       5.23       5.23      34187
-macro avg                                                5.68       6.04       3.29      34187
-weighted avg                                            16.32       5.23       6.98      34187
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       2113
--------------------
-micro avg                                              100.00     100.00     100.00       2113
-macro avg                                              100.00     100.00     100.00       2113
-weighted avg                                           100.00     100.00     100.00       2113
-
-[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                           0.28       0.61       0.39       4228
-! (label_id: 1)                                          0.30       8.28       0.58        145
-, (label_id: 2)                                          2.27       0.58       0.92      14495
-- (label_id: 3)                                          2.64      21.78       4.70       1327
-. (label_id: 4)                                         44.87      11.66      18.51      12193
-: (label_id: 5)                                          0.60       1.93       0.91        362
-; (label_id: 6)                                          0.00       0.00       0.00        164
-? (label_id: 7)                                          4.19       6.40       5.07       1078
-— (label_id: 8)                                          1.16       0.22       0.37        459
-… (label_id: 9)                                          0.85       4.17       1.41         96
--------------------
-micro avg                                                5.54       5.54       5.54      34547
-macro avg                                                5.72       5.56       3.29      34547
-weighted avg                                            17.08       5.54       7.33      34547
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       2114
--------------------
-micro avg                                              100.00     100.00     100.00       2114
-macro avg                                              100.00     100.00     100.00       2114
-weighted avg                                           100.00     100.00     100.00       2114
-
-[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                           0.29       0.63       0.40       4444
-! (label_id: 1)                                          0.38      10.67       0.74        150
-, (label_id: 2)                                          2.32       0.59       0.94      15290
-- (label_id: 3)                                          2.34      20.28       4.19       1292
-. (label_id: 4)                                         43.85      11.68      18.44      12599
-: (label_id: 5)                                          0.41       1.28       0.62        392
-; (label_id: 6)                                          0.00       0.00       0.00        164
-? (label_id: 7)                                          4.24       6.30       5.07       1111
-— (label_id: 8)                                          0.00       0.00       0.00        456
-… (label_id: 9)                                          0.38       2.41       0.65         83
--------------------
-micro avg                                                5.40       5.40       5.40      35981
-macro avg                                                5.42       5.38       3.11      35981
-weighted avg                                            16.59       5.40       7.22      35981
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       2222
--------------------
-micro avg                                              100.00     100.00     100.00       2222
-macro avg                                              100.00     100.00     100.00       2222
-weighted avg                                           100.00     100.00     100.00       2222
-
-[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                           0.35       0.73       0.48       3844
-! (label_id: 1)                                          0.54      14.62       1.04        130
-, (label_id: 2)                                          2.32       0.59       0.94      13056
-- (label_id: 3)                                          2.67      22.28       4.77       1194
-. (label_id: 4)                                         44.45      11.95      18.84      10791
-: (label_id: 5)                                          0.84       3.21       1.33        280
-; (label_id: 6)                                          0.00       0.00       0.00        140
-? (label_id: 7)                                          4.17       6.56       5.10        914
-— (label_id: 8)                                          0.00       0.00       0.00        401
-… (label_id: 9)                                          0.48       2.63       0.81         76
--------------------
-micro avg                                                5.68       5.68       5.68      30826
-macro avg                                                5.58       6.26       3.33      30826
-weighted avg                                            16.82       5.68       7.41      30826
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       1922
--------------------
-micro avg                                              100.00     100.00     100.00       1922
-macro avg                                              100.00     100.00     100.00       1922
-weighted avg                                           100.00     100.00     100.00       1922
-
-[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                           0.28       0.60       0.39       3970
-! (label_id: 1)                                          0.35      10.66       0.68        122
-, (label_id: 2)                                          2.09       0.53       0.85      13469
-- (label_id: 3)                                          2.29      19.32       4.10       1201
-. (label_id: 4)                                         43.43      11.24      17.86      11227
-: (label_id: 5)                                          0.63       2.30       0.99        304
-; (label_id: 6)                                          0.00       0.00       0.00        141
-? (label_id: 7)                                          4.52       6.86       5.45       1006
-— (label_id: 8)                                          1.15       0.23       0.38        444
-… (label_id: 9)                                          0.45       2.67       0.78         75
--------------------
-micro avg                                                5.26       5.26       5.26      31959
-macro avg                                                5.52       5.44       3.15      31959
-weighted avg                                            16.42       5.26       7.02      31959
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       1985
--------------------
-micro avg                                              100.00     100.00     100.00       1985
-macro avg                                              100.00     100.00     100.00       1985
-weighted avg                                           100.00     100.00     100.00       1985
-
-[INFO] - Epoch 5, step 2394: val_loss was not in top 3
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                           0.23       0.48       0.31       4126
-! (label_id: 1)                                          0.29       9.40       0.56        117
-, (label_id: 2)                                          1.91       0.49       0.77      14019
-- (label_id: 3)                                          2.52      22.59       4.53       1164
-. (label_id: 4)                                         44.15      11.65      18.44      11789
-: (label_id: 5)                                          0.72       2.41       1.11        332
-; (label_id: 6)                                          0.56       0.61       0.58        165
-? (label_id: 7)                                          3.89       6.53       4.88        980
-— (label_id: 8)                                          2.30       0.47       0.77        430
-… (label_id: 9)                                          1.18       8.33       2.07         60
--------------------
-micro avg                                                5.47       5.47       5.47      33182
-macro avg                                                5.77       6.30       3.40      33182
-weighted avg                                            16.77       5.47       7.25      33182
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00       2063
--------------------
-micro avg                                              100.00     100.00     100.00       2063
-macro avg                                              100.00     100.00     100.00       2063
-weighted avg                                           100.00     100.00     100.00       2063
-
-[INFO] - Epoch 6, step 2793: val_loss was not in top 3
diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
index fa37b4c..fd77a7f 100644
--- a/experiment/models/punctuation_domain_model.py
+++ b/experiment/models/punctuation_domain_model.py
@@ -62,6 +62,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
         else:
             self.hparams.model.punct_class_weights=None
 
+        self.dropout=nn.Dropout(self.hparams.model.mlp.dropout)
+        self.mlp = MultiLayerPerceptron(
+            self.transformer.config.hidden_size,
+            self.transformer.config.hidden_size,
+            num_layers=self.hparams.model.mlp.num_fc_layers, 
+            activation=self.hparams.model.mlp.activation, 
+            log_softmax=self.hparams.model.mlp.log_softmax
+        )
+
         self.punct_classifier = TokenClassifier(
             hidden_size=self.transformer.config.hidden_size,
             num_classes=len(self.labels_to_ids),
@@ -80,6 +89,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
             log_softmax=self.hparams.model.domain_head.log_softmax,
             dropout=self.hparams.model.domain_head.fc_dropout,
             use_transformer_init=self.hparams.model.domain_head.use_transformer_init,
+            idx_conditioned_on = self.hparams.model.domain_head.idx_conditioned_on,
         )
 
         if not self.hparams.model.punct_head.loss in ['cel', 'dice', 'crf', 'focal']:
@@ -119,14 +129,17 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
         self.grad_reverse.scale = self.hparams.model.domain_head.gamma
         self.freeze()
 
-    def forward(self, input_ids, attention_mask, domain_ids=None):
+    def forward(self, input_ids, attention_mask, subtoken_mask=None, domain_ids=None):
         hidden_states = self.transformer(
             input_ids=input_ids, attention_mask=attention_mask
         )[0]
+        hidden_states = self.dropout(hidden_states)
+        hidden_states = self.mlp(hidden_states)
         punct_logits = self.punct_classifier(hidden_states=hidden_states)
         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
         domain_logits = self.domain_classifier(
-            hidden_states=reverse_grad_hidden_states)
+            hidden_states=reverse_grad_hidden_states,
+            subtoken_mask=subtoken_mask)
         return punct_logits, domain_logits
 
     def _make_step(self, batch):
@@ -136,7 +149,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
         punct_labels = batch['labels']
         domain_labels = batch['domain']
         punct_logits, domain_logits = self(
-            input_ids=input_ids, attention_mask=attention_mask
+            input_ids=input_ids, attention_mask=attention_mask, subtoken_mask=subtoken_mask,
         )
         punctuation_loss = self.punctuation_loss(
             logits=punct_logits[subtoken_mask[:,0]>0], labels=punct_labels[subtoken_mask[:,0]>0], loss_mask=subtoken_mask[subtoken_mask[:,0]>0])
