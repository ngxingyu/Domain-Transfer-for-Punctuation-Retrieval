commit hash: 4cccf1ef224a31f16ab4158a6dbd1472d2705371
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/events.out.tfevents.1612840713.Titan.20119.0 b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/events.out.tfevents.1612840713.Titan.20119.0
deleted file mode 100644
index a4eb1d2..0000000
Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/events.out.tfevents.1612840713.Titan.20119.0 and /dev/null differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/git-info.log
deleted file mode 100644
index 5420ee2..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/git-info.log
+++ /dev/null
@@ -1,645 +0,0 @@
-commit hash: 089ad1caa03e468560d6d322ace7a5164a8178f3
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt
-index 2a26724..5be2535 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt
-@@ -20,3 +20,21 @@ initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
- 299 K     Trainable params
- 13.2 M    Non-trainable params
- 13.5 M    Total params
-+Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lr_find_temp_model.ckpt
-+Global seed set to 42
-+
-+  | Name                | Type                 | Params
-+-------------------------------------------------------------
-+0 | transformer         | ElectraModel         | 13.5 M
-+1 | punct_classifier    | TokenClassifier      | 2.6 K 
-+2 | domain_classifier   | SequenceClassifier   | 1.0 K 
-+3 | punctuation_loss    | FocalDiceLoss        | 0     
-+4 | domain_loss         | CrossEntropyLoss     | 0     
-+5 | agg_loss            | AggregatorLoss       | 0     
-+6 | punct_class_report  | ClassificationReport | 0     
-+7 | domain_class_report | ClassificationReport | 0     
-+-------------------------------------------------------------
-+299 K     Trainable params
-+13.2 M    Non-trainable params
-+13.5 M    Total params
-+Epoch 0, global step 5305: val_loss reached 10.18683 (best 10.18683), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/checkpoints/Punctuation_with_Domain_discriminator---val_loss=10.19-epoch=0.ckpt" as top 3
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt
-index 0f1c742..2f2fa91 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt
-@@ -2,3 +2,18 @@
- [NeMo W 2021-02-09 08:27:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-       warnings.warn(*args, **kwargs)
-     
-+[NeMo W 2021-02-09 08:34:47 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-+      warnings.warn(*args, **kwargs)
-+    
-+[NeMo W 2021-02-09 08:40:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-+      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-+    
-+[NeMo W 2021-02-09 10:21:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fd0> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-+[NeMo W 2021-02-09 10:36:30 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fa0> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-+[NeMo W 2021-02-09 11:07:22 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-+      warnings.warn(*args, **kwargs)
-+    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt
-index e609b5b..2546dc9 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt
-@@ -4,3 +4,18 @@
- [NeMo W 2021-02-09 08:27:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-       warnings.warn(*args, **kwargs)
-     
-+[NeMo W 2021-02-09 08:34:47 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-+      warnings.warn(*args, **kwargs)
-+    
-+[NeMo W 2021-02-09 08:40:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-+      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-+    
-+[NeMo W 2021-02-09 10:21:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fd0> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-+[NeMo W 2021-02-09 10:36:30 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fa0> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-+[NeMo W 2021-02-09 11:07:22 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-+      warnings.warn(*args, **kwargs)
-+    
-diff --git a/experiment/Nemo2Lightning.ipynb b/experiment/Nemo2Lightning.ipynb
-index d2ec988..0dbd499 100644
---- a/experiment/Nemo2Lightning.ipynb
-+++ b/experiment/Nemo2Lightning.ipynb
-@@ -2,7 +2,7 @@
-  "cells": [
-   {
-    "cell_type": "code",
--   "execution_count": 2,
-+   "execution_count": 1,
-    "metadata": {},
-    "outputs": [
-     {
-@@ -11,7 +11,7 @@
-      "text": [
-       "Using device: cuda\n",
-       "\n",
--      "Tesla T4\n",
-+      "GeForce GTX 1080 Ti\n",
-       "Memory Usage:\n",
-       "Allocated: 0.0 GB\n",
-       "Cached:    0.0 GB\n"
-@@ -33,16 +33,16 @@
-   },
-   {
-    "cell_type": "code",
--   "execution_count": 1,
-+   "execution_count": 2,
-    "metadata": {},
-    "outputs": [
-     {
-      "data": {
-       "text/plain": [
--       "{'seed': 42, 'trainer': {'gpus': 1, 'num_nodes': 1, 'max_epochs': 6, 'max_steps': None, 'accumulate_grad_batches': 8, 'gradient_clip_val': 0, 'amp_level': 'O1', 'precision': 16, 'accelerator': 'ddp', 'checkpoint_callback': False, 'logger': False, 'log_every_n_steps': 1, 'val_check_interval': 1.0, 'resume_from_checkpoint': None}, 'exp_manager': {'exp_dir': '/home/nxingyu2/project/', 'name': 'Punctuation_with_Domain_discriminator', 'create_tensorboard_logger': True, 'create_checkpoint_callback': True}, 'base_path': '/home/nxingyu2/data', 'tmp_path': '/home/nxingyu2/data/tmp', 'model': {'nemo_path': None, 'transformer_path': 'google/electra-base-discriminator', 'initial_unfrozen': 1, 'punct_label_ids': ['', '!', ',', '-', '.', ':', ';', '?', '—', '…'], 'dataset': {'data_dir': '/home/nxingyu2/data', 'labelled': ['${base_path}/open_subtitles_processed'], 'unlabelled': ['${base_path}/ted_talks_processed'], 'max_seq_length': 128, 'pad_label': '', 'ignore_extra_tokens': False, 'ignore_start_end': False, 'use_cache': False, 'num_workers': 0, 'pin_memory': True, 'drop_last': False, 'num_labels': 10, 'num_domains': 1, 'test_unlabelled': True, 'train_ds': {'shuffle': True, 'num_samples': -1, 'batch_size': 8}, 'validation_ds': {'shuffle': True, 'num_samples': -1, 'batch_size': 2}}, 'tokenizer': {'tokenizer_name': '${model.language_model.pretrained_model_name}', 'vocab_file': None, 'tokenizer_model': None, 'special_tokens': None}, 'language_model': {'pretrained_model_name': '${model.transformer_path}', 'lm_checkpoint': None, 'config_file': None, 'config': None}, 'punct_head': {'punct_num_fc_layers': 1, 'fc_dropout': 0.1, 'activation': 'relu', 'log_softmax': False, 'use_transformer_init': True, 'loss': 'crf'}, 'domain_head': {'domain_num_fc_layers': 1, 'fc_dropout': 0.1, 'activation': 'relu', 'log_softmax': False, 'use_transformer_init': True, 'loss': 'cel', 'gamma': 0.1}, 'dice_loss': {'epsilon': 0.01, 'alpha': 3, 'macro_average': True}, 'focal_loss': {'gamma': 5}, 'optim': {'name': 'adamw', 'lr': 0.001, 'weight_decay': 0.0, 'sched': {'name': 'WarmupAnnealing', 'warmup_steps': None, 'warmup_ratio': 0.1, 'last_epoch': -1, 'monitor': 'val_loss', 'reduce_on_plateau': False}}}}"
-+       "{'seed': 42, 'trainer': {'gpus': 1, 'num_nodes': 1, 'max_epochs': 10, 'max_steps': None, 'accumulate_grad_batches': 4, 'gradient_clip_val': 0, 'amp_level': 'O1', 'precision': 16, 'accelerator': 'ddp', 'checkpoint_callback': False, 'logger': False, 'log_every_n_steps': 1, 'val_check_interval': 1.0, 'resume_from_checkpoint': None}, 'exp_manager': {'exp_dir': '/home/nxingyu/project/', 'name': 'Punctuation_with_Domain_discriminator', 'create_tensorboard_logger': True, 'create_checkpoint_callback': True}, 'base_path': '/home/nxingyu/data', 'tmp_path': '/home/nxingyu/data/tmp', 'model': {'nemo_path': None, 'transformer_path': 'google/electra-small-discriminator', 'unfrozen': 0, 'maximum_unfrozen': 1, 'unfreeze_step': 1, 'punct_label_ids': ['', '!', ',', '-', '.', ':', ';', '?', '—', '…'], 'punct_class_weights': False, 'dataset': {'data_dir': '/home/nxingyu/data', 'labelled': ['${base_path}/ted_talks_processed'], 'unlabelled': ['${base_path}/open_subtitles_processed'], 'max_seq_length': 128, 'pad_label': '', 'ignore_extra_tokens': False, 'ignore_start_end': False, 'use_cache': False, 'num_workers': 4, 'pin_memory': True, 'drop_last': False, 'num_labels': 10, 'num_domains': 2, 'test_unlabelled': True, 'train_ds': {'shuffle': True, 'num_samples': -1, 'batch_size': 4}, 'validation_ds': {'shuffle': True, 'num_samples': -1, 'batch_size': 4}}, 'tokenizer': {'tokenizer_name': '${model.language_model.pretrained_model_name}', 'vocab_file': None, 'tokenizer_model': None, 'special_tokens': None}, 'language_model': {'pretrained_model_name': '${model.transformer_path}', 'lm_checkpoint': None, 'config_file': None, 'config': None}, 'punct_head': {'punct_num_fc_layers': 1, 'fc_dropout': 0.1, 'activation': 'relu', 'log_softmax': False, 'use_transformer_init': True, 'loss': 'dice'}, 'domain_head': {'domain_num_fc_layers': 1, 'fc_dropout': 0.1, 'activation': 'relu', 'log_softmax': False, 'use_transformer_init': True, 'loss': 'cel', 'gamma': 0, 'pooling': 'mean_max', 'idx_conditioned_on': 0}, 'dice_loss': {'epsilon': 0.01, 'alpha': 3, 'macro_average': True}, 'focal_loss': {'gamma': 1}, 'optim': {'name': 'adamw', 'lr': 0.001, 'weight_decay': 0.0, 'sched': {'name': 'WarmupAnnealing', 'warmup_steps': None, 'warmup_ratio': 0.1, 'last_epoch': -1, 'monitor': 'val_loss', 'reduce_on_plateau': False}}}}"
-       ]
-      },
--     "execution_count": 1,
-+     "execution_count": 2,
-      "metadata": {},
-      "output_type": "execute_result"
-     }
-@@ -74,79 +74,79 @@
-   },
-   {
-    "cell_type": "code",
--   "execution_count": 13,
-+   "execution_count": 3,
-    "metadata": {},
-    "outputs": [
-     {
-      "name": "stderr",
-      "output_type": "stream",
-      "text": [
--      "09:01:12.28 LOG:\n",
--      "09:01:12.30 .... 'cel none' = 'cel none'\n",
--      "09:01:12.31 .... output = tensor([1.5514, 1.5514, 0.5514, 0.5514], grad_fn=<NllLossBackward>)\n",
--      "09:01:12.31 LOG:\n",
--      "09:01:12.31 .... 'cel mean' = 'cel mean'\n",
--      "09:01:12.31 .... output = tensor(1.0514, grad_fn=<NllLossBackward>)\n",
--      "09:01:12.31 LOG:\n",
--      "09:01:12.31 .... 'cel sum' = 'cel sum'\n",
--      "09:01:12.31 .... output = tensor(4.2058, grad_fn=<NllLossBackward>)\n",
--      "09:01:12.31 LOG:\n",
--      "09:01:12.32 .... 'focal sum' = 'focal sum'\n",
--      "09:01:12.32 .... loss(inp, tar) = tensor(6.7352, grad_fn=<SumBackward0>)\n",
--      "09:01:12.32 LOG:\n",
--      "09:01:12.32 .... 'focal mean' = 'focal mean'\n",
--      "09:01:12.32 .... loss(inp, tar) = tensor(0.4210, grad_fn=<MeanBackward0>)\n",
--      "09:01:12.32 LOG:\n",
--      "09:01:12.33 .... 'focal none' = 'focal none'\n",
--      "09:01:12.33 .... loss(inp, tar) = tensor([0.9635, 0.9635, 0.0991, 0.0991], grad_fn=<MulBackward0>)\n",
--      "09:01:12.33 LOG:\n",
--      "09:01:12.33 .... 'focal none' = 'focal none'\n",
--      "09:01:12.33 .... loss(inp, tar) = tensor([0.4716, 0.4716, 0.0075, 0.0075], grad_fn=<MulBackward0>)\n",
--      "09:01:12.33 LOG:\n",
--      "09:01:12.34 .... 'crf,none' = 'crf,none'\n",
--      "09:01:12.34 .... output = tensor([4.2927], grad_fn=<NegBackward>)\n",
--      "09:01:12.34 LOG:\n",
--      "09:01:12.34 .... 'crf,mean' = 'crf,mean'\n",
--      "09:01:12.34 .... output = tensor(4.3138, grad_fn=<NegBackward>)\n",
--      "09:01:12.34 LOG:\n",
--      "09:01:12.35 .... 'crf,sum' = 'crf,sum'\n",
--      "09:01:12.35 .... output = tensor(4.2588, grad_fn=<NegBackward>)\n",
--      "09:01:12.35 LOG:\n",
--      "09:01:12.35 .... 'crf,token_mean' = 'crf,token_mean'\n",
--      "09:01:12.35 .... output = tensor(1.0429, grad_fn=<DivBackward0>)\n",
--      "09:01:12.35 LOG:\n",
--      "09:01:12.35 .... 'dice none,micro' = 'dice none,micro'\n",
--      "09:01:12.36 .... output = tensor(0.1876, grad_fn=<PowBackward0>)\n",
--      "09:01:12.36 LOG:\n",
--      "09:01:12.36 .... 'dice mean,micro' = 'dice mean,micro'\n",
--      "09:01:12.36 .... output = tensor(0.0625, grad_fn=<DivBackward0>)\n",
--      "09:01:12.36 LOG:\n",
--      "09:01:12.36 .... 'dice sum,micro' = 'dice sum,micro'\n",
--      "09:01:12.36 .... output = tensor(0.1876, grad_fn=<SumBackward0>)\n",
--      "09:01:12.37 LOG:\n",
--      "09:01:12.37 .... 'dice sum,micro' = 'dice sum,micro'\n",
--      "09:01:12.37 .... output = tensor(0.0812, grad_fn=<SumBackward0>)\n",
--      "09:01:12.37 LOG:\n",
--      "09:01:12.37 .... 'dice none,macro' = 'dice none,macro'\n",
--      "09:01:12.37 .... loss(inp, tar) = tensor([5.9547, 4.4872, 2.0551], grad_fn=<MulBackward0>)\n",
--      "09:01:12.37 LOG:\n",
--      "09:01:12.38 .... 'dice mean,macro' = 'dice mean,macro'\n",
--      "09:01:12.38 .... loss(inp, tar) = tensor(0.2112, grad_fn=<DivBackward0>)\n",
--      "09:01:12.38 LOG:\n",
--      "09:01:12.38 .... 'dice sum,macro' = 'dice sum,macro'\n",
--      "09:01:12.38 .... loss(inp, tar) = tensor(0.6335, grad_fn=<SumBackward0>)\n",
--      "09:01:12.38 LOG:\n",
--      "09:01:12.38 .... 'dice none,macro' = 'dice none,macro'\n",
--      "09:01:12.39 .... loss(inp, tar) = tensor([0.3340, 0.6546, 0.3057], grad_fn=<MulBackward0>)\n",
--      "09:01:12.39 LOG:\n",
--      "09:01:12.39 .... 'dice none,macro' = 'dice none,macro'\n",
--      "09:01:12.39 .... loss(inp, tar) = tensor([0.0373, 0.2805, 0.0286], grad_fn=<MulBackward0>)\n",
--      "09:01:12.39 LOG:\n",
--      "09:01:12.39 .... 'dice sum,macro' = 'dice sum,macro'\n",
--      "09:01:12.40 .... output = tensor([0.5989, 0.7696, 0.2411], grad_fn=<MulBackward0>)\n",
--      "09:01:12.40 LOG:\n",
--      "09:01:12.40 .... 'dice sum,macro' = 'dice sum,macro'\n",
--      "09:01:12.40 .... output = tensor([0.2148, 0.4559, 0.0140], grad_fn=<MulBackward0>)\n"
-+      "10:11:13.98 LOG:\n",
-+      "10:11:14.02 .... 'cel none' = 'cel none'\n",
-+      "10:11:14.02 .... output = tensor([1.5514, 1.5514, 0.5514, 0.5514], grad_fn=<NllLossBackward>)\n",
-+      "10:11:14.02 LOG:\n",
-+      "10:11:14.02 .... 'cel mean' = 'cel mean'\n",
-+      "10:11:14.02 .... output = tensor(1.0514, grad_fn=<NllLossBackward>)\n",
-+      "10:11:14.03 LOG:\n",
-+      "10:11:14.03 .... 'cel sum' = 'cel sum'\n",
-+      "10:11:14.03 .... output = tensor(4.2058, grad_fn=<NllLossBackward>)\n",
-+      "10:11:14.08 LOG:\n",
-+      "10:11:14.08 .... 'focal sum' = 'focal sum'\n",
-+      "10:11:14.08 .... loss(inp, tar) = tensor(6.7352, grad_fn=<SumBackward0>)\n",
-+      "10:11:14.08 LOG:\n",
-+      "10:11:14.08 .... 'focal mean' = 'focal mean'\n",
-+      "10:11:14.09 .... loss(inp, tar) = tensor(0.4210, grad_fn=<MeanBackward0>)\n",
-+      "10:11:14.09 LOG:\n",
-+      "10:11:14.09 .... 'focal none' = 'focal none'\n",
-+      "10:11:14.10 .... loss(inp, tar) = tensor([0.9635, 0.9635, 0.0991, 0.0991], grad_fn=<MulBackward0>)\n",
-+      "10:11:14.10 LOG:\n",
-+      "10:11:14.10 .... 'focal none' = 'focal none'\n",
-+      "10:11:14.10 .... loss(inp, tar) = tensor([0.4716, 0.4716, 0.0075, 0.0075], grad_fn=<MulBackward0>)\n",
-+      "10:11:14.11 LOG:\n",
-+      "10:11:14.12 .... 'crf,none' = 'crf,none'\n",
-+      "10:11:14.12 .... output = tensor([4.1689], grad_fn=<NegBackward>)\n",
-+      "10:11:14.12 LOG:\n",
-+      "10:11:14.12 .... 'crf,mean' = 'crf,mean'\n",
-+      "10:11:14.12 .... output = tensor(4.0544, grad_fn=<NegBackward>)\n",
-+      "10:11:14.12 LOG:\n",
-+      "10:11:14.13 .... 'crf,sum' = 'crf,sum'\n",
-+      "10:11:14.13 .... output = tensor(4.1272, grad_fn=<NegBackward>)\n",
-+      "10:11:14.13 LOG:\n",
-+      "10:11:14.13 .... 'crf,token_mean' = 'crf,token_mean'\n",
-+      "10:11:14.13 .... output = tensor(1.0815, grad_fn=<DivBackward0>)\n",
-+      "10:11:14.13 LOG:\n",
-+      "10:11:14.14 .... 'dice none,micro' = 'dice none,micro'\n",
-+      "10:11:14.14 .... output = tensor(0.4331, grad_fn=<PowBackward0>)\n",
-+      "10:11:14.14 LOG:\n",
-+      "10:11:14.14 .... 'dice mean,micro' = 'dice mean,micro'\n",
-+      "10:11:14.14 .... output = tensor(0.1444, grad_fn=<DivBackward0>)\n",
-+      "10:11:14.14 LOG:\n",
-+      "10:11:14.14 .... 'dice sum,micro' = 'dice sum,micro'\n",
-+      "10:11:14.15 .... output = tensor(0.4331, grad_fn=<SumBackward0>)\n",
-+      "10:11:14.15 LOG:\n",
-+      "10:11:14.15 .... 'dice sum,micro' = 'dice sum,micro'\n",
-+      "10:11:14.15 .... output = tensor(0.0812, grad_fn=<SumBackward0>)\n",
-+      "10:11:14.15 LOG:\n",
-+      "10:11:14.15 .... 'dice none,macro' = 'dice none,macro'\n",
-+      "10:11:14.16 .... loss(inp, tar) = tensor([5.9547, 4.4872, 2.0551], grad_fn=<MulBackward0>)\n",
-+      "10:11:14.16 LOG:\n",
-+      "10:11:14.16 .... 'dice mean,macro' = 'dice mean,macro'\n",
-+      "10:11:14.16 .... loss(inp, tar) = tensor(0.4314, grad_fn=<DivBackward0>)\n",
-+      "10:11:14.16 LOG:\n",
-+      "10:11:14.16 .... 'dice sum,macro' = 'dice sum,macro'\n",
-+      "10:11:14.16 .... loss(inp, tar) = tensor(1.2943, grad_fn=<SumBackward0>)\n",
-+      "10:11:14.17 LOG:\n",
-+      "10:11:14.17 .... 'dice none,macro' = 'dice none,macro'\n",
-+      "10:11:14.17 .... loss(inp, tar) = tensor([0.3340, 0.6546, 0.3057], grad_fn=<MulBackward0>)\n",
-+      "10:11:14.17 LOG:\n",
-+      "10:11:14.17 .... 'dice none,macro' = 'dice none,macro'\n",
-+      "10:11:14.17 .... loss(inp, tar) = tensor([0.0373, 0.2805, 0.0286], grad_fn=<MulBackward0>)\n",
-+      "10:11:14.18 LOG:\n",
-+      "10:11:14.18 .... 'dice sum,macro' = 'dice sum,macro'\n",
-+      "10:11:14.18 .... output = tensor([0.5989, 0.7696, 0.2411], grad_fn=<MulBackward0>)\n",
-+      "10:11:14.18 LOG:\n",
-+      "10:11:14.18 .... 'dice sum,macro' = 'dice sum,macro'\n",
-+      "10:11:14.18 .... output = tensor([0.2148, 0.4559, 0.0140], grad_fn=<MulBackward0>)\n"
-      ]
-     },
-     {
-@@ -155,7 +155,7 @@
-        "('dice sum,macro', tensor([0.2148, 0.4559, 0.0140], grad_fn=<MulBackward0>))"
-       ]
-      },
--     "execution_count": 13,
-+     "execution_count": 3,
-      "metadata": {},
-      "output_type": "execute_result"
-     }
-@@ -286,32 +286,25 @@
-   },
-   {
-    "cell_type": "code",
--   "execution_count": 2,
-+   "execution_count": 4,
-    "metadata": {},
-    "outputs": [
-     {
--     "name": "stderr",
--     "output_type": "stream",
--     "text": [
--      "10:05:46.40 LOG:\n",
--      "10:05:46.46 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
--      "10:05:46.66 LOG:\n",
--      "10:05:46.66 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
--      "10:06:04.19 LOG:\n",
--      "10:06:04.20 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
--      "10:06:04.34 LOG:\n",
--      "10:06:04.34 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n"
-+     "ename": "KeyboardInterrupt",
-+     "evalue": "",
-+     "output_type": "error",
-+     "traceback": [
-+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
-+      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
-+      "\u001b[0;32m<ipython-input-4-b22a0d7714b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mdata_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-+      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_prepared_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-+      "\u001b[0;32m~/project/experiment/data/punctuation_datamodule.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'fit'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             self.train_dataset = PunctuationDomainDatasets(split='train',\n\u001b[0m\u001b[1;32m     64\u001b[0m                     \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                     \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-+      "\u001b[0;32m~/project/experiment/data/punctuation_dataset_multi.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, split, num_samples, max_seq_length, punct_label_ids, labelled, unlabelled, tokenizer, randomize, data_id, tmp_path)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munlabelled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             dataset=PunctuationDomainDataset(\n\u001b[0m\u001b[1;32m    187\u001b[0m                     \u001b[0mcsv_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'{path}.{split}.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                     \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-+      "\u001b[0;32m~/project/experiment/data/punctuation_dataset_multi.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, csv_file, tokenizer, num_samples, max_seq_length, degree, punct_label_ids, domain, labelled, randomize, target_file, tmp_path, start, end)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv_file\u001b[0m \u001b[0;34m=\u001b[0m   \u001b[0mcsv_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m   \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m=\u001b[0m  \u001b[0mdomain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunct_label_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpunct_label_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-+      "\u001b[0;32m~/project/experiment/data/punctuation_dataset_multi.py\u001b[0m in \u001b[0;36mset_num_samples\u001b[0;34m(self, csv_file, num_samples)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-l'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTDOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_samples\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
-+      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1009\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stdin_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m                 \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-+      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
-      ]
--    },
--    {
--     "data": {
--      "text/plain": [
--       "10609"
--      ]
--     },
--     "execution_count": 2,
--     "metadata": {},
--     "output_type": "execute_result"
-     }
-    ],
-    "source": [
-@@ -343,20 +336,9 @@
-   },
-   {
-    "cell_type": "code",
--   "execution_count": 4,
-+   "execution_count": null,
-    "metadata": {},
--   "outputs": [
--    {
--     "data": {
--      "text/plain": [
--       "10609"
--      ]
--     },
--     "execution_count": 4,
--     "metadata": {},
--     "output_type": "execute_result"
--    }
--   ],
-+   "outputs": [],
-    "source": [
-     "# it=dm.train_dataset\n",
-     "# ni=next(it)\n",
-diff --git a/experiment/core/losses/linear_chain_crf.py b/experiment/core/losses/linear_chain_crf.py
-index ed813a9..8dc59cc 100644
---- a/experiment/core/losses/linear_chain_crf.py
-+++ b/experiment/core/losses/linear_chain_crf.py
-@@ -92,6 +92,17 @@ class LinearChainCRF(torch.nn.Module):
-             mask = logits.new_ones(logits.shape[:2], dtype=torch.bool)
-         return self._viterbi_decode(logits,mask)
- 
-+    @jit.export
-+    def predict(self, logits: Tensor, mask: Optional[Tensor] = None) -> LongTensor:
-+        self._validate(logits, mask=mask)
-+
-+        if mask is None:
-+            mask = logits.new_ones(logits.shape[:2], dtype=torch.bool)
-+        out=[]
-+        for p,m in iter(zip(logits,mask)):
-+            out.append(pad_to_len(logits.shape[1],self._viterbi_decode(p.unsqueeze(0),m.unsqueeze(0))))
-+        return torch.tensor(out)
-+        
-     def _viterbi_decode(self, logits: Tensor, mask: Tensor) -> LongTensor:
-         """
-         decode labels using viterbi algorithm
-diff --git a/experiment/core/utils.py b/experiment/core/utils.py
-index 058cc87..4be7503 100644
---- a/experiment/core/utils.py
-+++ b/experiment/core/utils.py
-@@ -3,6 +3,7 @@ import torch
- from torch import nn
- import regex as re
- import snoop
-+from copy import deepcopy
- 
- __all__ = ['chunk_examples_with_degree', 'chunk_to_len_batch', 'view_aligned']
- 
-@@ -26,14 +27,15 @@ def position_to_mask(max_seq_length:int,indices:list):
-         o[np.array(indices)%(max_seq_length-2)+1]=1
-     except:
-         pp('position_to_mask',np.array(indices)%(max_seq_length-2)+1)
--        # o[(np.array(indices)%(max_seq_length-2)+1).astype(int)]=1
-+        o[(np.array(indices)%(max_seq_length-2)+1).astype(int)]=1
-     return o
- 
- def align_labels_to_mask(mask,labels):
-     '''[0,1,0],[2] -> [0,2,0]'''
-     assert(sum(mask)==len(labels))
--    mask[mask>0]=torch.tensor(labels)
--    return mask.tolist()
-+    m1=mask.copy()
-+    m1[mask>0]=torch.tensor(labels)
-+    return m1.tolist()
- 
- def view_aligned(texts,tags,tokenizer,labels_to_ids):
-         return [re.sub(' ##','',' '.join(
-@@ -101,7 +103,9 @@ def chunk_to_len(max_seq_length,tokenizer,tokens,labels=None):
-     split_token_end_idxs=np.array_split(token_end_idxs,breakpoints)
-     split_subwords=np.array_split(subwords,np.arange(max_seq_length-2,len(subwords),max_seq_length-2))
-     ids=[pad_to_len(max_seq_length,tokenizer.convert_tokens_to_ids(['[CLS]']+list(_)+['[SEP]'])) for _ in split_subwords]
--    masks=[position_to_mask(max_seq_length,_) for _ in split_token_end_idxs]
-+    masks=[]
-+    for _ in split_token_end_idxs:
-+        masks.append(position_to_mask(max_seq_length,_).copy())
-     padded_labels=None
-     if labels!=None:
-         split_labels=np.array_split(labels,breakpoints)
-@@ -121,7 +125,7 @@ def chunk_to_len_batch(max_seq_length,tokenizer,tokens,labels=None,labelled=True
-     output = {'input_ids': torch.as_tensor(batch_ids, dtype=torch.long),
-               'attention_mask': torch.as_tensor(batch_ids, dtype=torch.bool),
-               'subtoken_mask': torch.as_tensor(batch_masks,dtype=torch.bool)}
--    output['subtoken_mask']|=(output['input_ids']==101)|(output['input_ids']==102)
-+    output['subtoken_mask']|=((output['input_ids']==101)|(output['input_ids']==102))
-     output['subtoken_mask']&=labelled
-     output['labels']=torch.as_tensor(batch_labels,dtype=torch.long) if labelled==True else torch.zeros_like(output['input_ids'],dtype=torch.long)
-     return output
-diff --git a/experiment/data/punctuation_dataset.py b/experiment/data/punctuation_dataset.py
-index bfd015c..c3d9fb6 100644
---- a/experiment/data/punctuation_dataset.py
-+++ b/experiment/data/punctuation_dataset.py
-@@ -10,6 +10,7 @@ import torch
- import subprocess
- from time import time
- from itertools import cycle
-+import math
- 
- class PunctuationDomainDataset(IterableDataset):
- 
-@@ -242,18 +243,23 @@ class PunctuationInferenceDataset(Dataset):
-             "labels": NeuralType(('B', 'T'), ChannelType()),
-         }
- 
--    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], degree:int = 0, ):
-+    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], num_samples:int=256, degree:int = 0):
-         """ Initializes BertPunctuationInferDataset. """
-+        self.degree=degree
-+        self.punct_label_ids=punct_label_ids
-         chunked=chunk_examples_with_degree(self.degree, self.punct_label_ids)(queries)
-         features = chunk_to_len_batch(max_seq_length=max_seq_length, tokenizer=tokenizer,tokens=chunked['texts'],labelled=False)
-         self.all_input_ids = features['input_ids']
-         self.all_attention_mask = features['attention_mask']
-         self.all_subtoken_mask = features['subtoken_mask']
-+        self.num_samples=num_samples
- 
-     def __len__(self):
--        return len(self.all_input_ids)
-+        return math.ceil(len(self.all_input_ids)/self.num_samples)
- 
-     def __getitem__(self, idx):
--        return {'input_ids':self.all_input_ids[idx],
--                'attention_mask':self.all_attention_mask[idx],
--                'subtoken_mask':self.all_subtoken_mask[idx]}
-+        lower=idx*self.num_samples
-+        upper=min(len(self.all_input_ids),(idx+1)*self.num_samples+1)
-+        return {'input_ids':self.all_input_ids[lower:upper],
-+                'attention_mask':self.all_attention_mask[lower:upper],
-+                'subtoken_mask':self.all_subtoken_mask[lower:upper]}
-diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
-index 4027bb2..b3fe282 100644
---- a/experiment/data/punctuation_dataset_multi.py
-+++ b/experiment/data/punctuation_dataset_multi.py
-@@ -261,18 +261,23 @@ class PunctuationInferenceDataset(Dataset):
-             "labels": NeuralType(('B', 'T'), ChannelType()),
-         }
- 
--    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], degree:int = 0, ):
-+    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], num_samples:int=256, degree:int = 0):
-         """ Initializes BertPunctuationInferDataset. """
-+        self.degree=degree
-+        self.punct_label_ids=punct_label_ids
-         chunked=chunk_examples_with_degree(self.degree, self.punct_label_ids)(queries)
-         features = chunk_to_len_batch(max_seq_length=max_seq_length, tokenizer=tokenizer,tokens=chunked['texts'],labelled=False)
-         self.all_input_ids = features['input_ids']
-         self.all_attention_mask = features['attention_mask']
-         self.all_subtoken_mask = features['subtoken_mask']
-+        self.num_samples=num_samples
- 
-     def __len__(self):
--        return len(self.all_input_ids)
-+        return math.ceil(len(self.all_input_ids)/self.num_samples)
- 
-     def __getitem__(self, idx):
--        return {'input_ids':self.all_input_ids[idx],
--                'attention_mask':self.all_attention_mask[idx],
--                'subtoken_mask':self.all_subtoken_mask[idx]}
-+        lower=idx*self.num_samples
-+        upper=min(len(self.all_input_ids),(idx+1)*self.num_samples+1)
-+        return {'input_ids':self.all_input_ids[lower:upper],
-+                'attention_mask':self.all_attention_mask[lower:upper],
-+                'subtoken_mask':self.all_subtoken_mask[lower:upper]}
-diff --git a/experiment/info.log b/experiment/info.log
-index d9d501b..e69de29 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,17 +0,0 @@
--[INFO] - shuffling train set
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f412fde0d90>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index c5db7b3..aa05eac 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -15,9 +15,9 @@ from core.losses import (AggregatorLoss, CrossEntropyLoss, FocalDiceLoss, FocalL
- from pytorch_lightning.utilities import rank_zero_only
- from core.optim import get_optimizer, parse_optimizer_args, prepare_lr_scheduler
- from omegaconf import DictConfig, OmegaConf, open_dict
--from transformers import AutoModel
-+from transformers import AutoModel, AutoTokenizer
- import torch.utils.data.dataloader as dataloader
--from data import PunctuationDataModule
-+from data import PunctuationDataModule, PunctuationInferenceDataset
- from os import path
- import tempfile
- from core.common import Serialization, FileIO
-@@ -51,6 +51,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         self._trainer = trainer
- 
-         self.transformer = AutoModel.from_pretrained(self.hparams.model.transformer_path)
-+        self.tokenizer=AutoTokenizer.from_pretrained(self._cfg.model.transformer_path)
-         self.ids_to_labels = {_[0]: _[1]
-                               for _ in enumerate(self.hparams.model.punct_label_ids)}
-         self.labels_to_ids = {_[1]: _[0]
-@@ -707,4 +708,23 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-             if 'PL_TRAINER_GPUS' in os.environ:
-                 os.environ.pop('PL_TRAINER_GPUS')
- 
--        super().teardown(stage)
-\ No newline at end of file
-+        super().teardown(stage)
-+
-+    def add_punctuation(self, queries):
-+        infer_ds=PunctuationInferenceDataset(
-+            tokenizer= self._cfg.model.transformer_path,
-+            queries=queries, 
-+            max_seq_length=self.hparams.model.dataset.max_seq_length,
-+            punct_label_ids=self._cfg.model.punct_label_ids)
-+        attention_mask = batch['attention_mask']
-+        subtoken_mask = batch['subtoken_mask']
-+        punct_labels = batch['labels']
-+        domain_labels = batch['domain']
-+        input_ids = batch['input_ids']
-+
-+        labelled_mask=(subtoken_mask[:,0]>0)
-+        test_loss, punct_logits, domain_logits = self._make_step(batch)
-+        # attention_mask = attention_mask > 0.5
-+        punct_preds = self.punctuation_loss.predict(punct_logits[labelled_mask], subtoken_mask[labelled_mask]) \
-+            if self.hparams.model.punct_head.loss == 'crf' else torch.argmax(punct_logits[labelled_mask], axis=-1)[subtoken_mask[labelled_mask]]
-+        return view_aligned(input_ids,punct_preds, self.tokenizer,self.ids_to_labels)
-\ No newline at end of file
-diff --git a/experiment/utils/__init__.py b/experiment/utils/__init__.py
-deleted file mode 100644
-index 9a292b8..0000000
---- a/experiment/utils/__init__.py
-+++ /dev/null
-@@ -1,2 +0,0 @@
--from utils.logging import Logger as _Logger
--logging = _Logger()
-diff --git a/experiment/utils/logging.py b/experiment/utils/logging.py
-deleted file mode 100644
-index 15511fd..0000000
---- a/experiment/utils/logging.py
-+++ /dev/null
-@@ -1,69 +0,0 @@
--import os.path
--import logging
--import traceback
--
--from logging import DEBUG, WARNING, ERROR, INFO
--__all__ = ['Logger']
--
--class Logger(object):
--
--    show_source_location = True
--    # Formats the message as needed and calls the correct logging method
--    # to actually handle it
--    def _raw_log(self, logfn, message, exc_info):
--        cname = ''
--        loc = ''
--        fn = ''
--        tb = traceback.extract_stack()
--        if len(tb) > 2:
--            if self.show_source_location:
--                loc = '(%s:%d):' % (os.path.basename(tb[-3][0]), tb[-3][1])
--            fn = tb[-3][2]
--            if fn != '<module>':
--                if self.__class__.__name__ != Logger.__name__:
--                    fn = self.__class__.__name__ + '.' + fn
--                fn += '()'
--
--        logfn(loc + cname + fn + ': ' + message, exc_info=exc_info)
--
--    def info(self, message, exc_info=False):
--        """
--        Log a info-level message. If exc_info is True, if an exception
--        was caught, show the exception information (message and stack trace).
--        """
--        self._raw_log(logging.info, message, exc_info)
--
--    def debug(self, message, exc_info=False):
--        """
--        Log a debug-level message. If exc_info is True, if an exception
--        was caught, show the exception information (message and stack trace).
--        """
--        self._raw_log(logging.debug, message, exc_info)
--
--    def warning(self, message, exc_info=False):
--        """
--        Log a warning-level message. If exc_info is True, if an exception
--        was caught, show the exception information (message and stack trace).
--        """
--        self._raw_log(logging.warning, message, exc_info)
--
--    def error(self, message, exc_info=False):
--        """
--        Log an error-level message. If exc_info is True, if an exception
--        was caught, show the exception information (message and stack trace).
--        """
--        self._raw_log(logging.error, message, exc_info)
--
--    @staticmethod
--    def basicConfig(level=DEBUG):
--        """
--        Apply a basic logging configuration which outputs the log to the
--        console (stderr). Optionally, the minimum log level can be set, one
--        of DEBUG, WARNING, ERROR (or any of the levels from the logging
--        module). If not set, DEBUG log level is used as minimum.
--        """
--        logging.basicConfig(level=level,
--                format='%(asctime)s %(levelname)s %(message)s',
--                datefmt='%Y-%m-%d %H:%M:%S')
--
--        logger = Logger()
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/hparams.yaml
deleted file mode 100644
index cbac11e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/hparams.yaml
+++ /dev/null
@@ -1,110 +0,0 @@
-seed: 42
-trainer:
-  gpus: 1
-  num_nodes: 1
-  max_epochs: 10
-  max_steps: null
-  accumulate_grad_batches: 4
-  gradient_clip_val: 0
-  amp_level: O1
-  precision: 16
-  accelerator: ddp
-  checkpoint_callback: false
-  logger: false
-  log_every_n_steps: 1
-  val_check_interval: 1.0
-  resume_from_checkpoint: null
-exp_manager:
-  exp_dir: /home/nxingyu/project/
-  name: Punctuation_with_Domain_discriminator
-  create_tensorboard_logger: true
-  create_checkpoint_callback: true
-base_path: /home/nxingyu/data
-tmp_path: /home/nxingyu/data/tmp
-model:
-  nemo_path: null
-  transformer_path: google/electra-small-discriminator
-  unfrozen: 0
-  maximum_unfrozen: 1
-  unfreeze_step: 1
-  punct_label_ids:
-  - ''
-  - '!'
-  - ','
-  - '-'
-  - .
-  - ':'
-  - ;
-  - '?'
-  - —
-  - …
-  punct_class_weights: false
-  dataset:
-    data_dir: /home/nxingyu/data
-    labelled:
-    - /home/nxingyu/data/ted_talks_processed
-    unlabelled:
-    - /home/nxingyu/data/open_subtitles_processed
-    max_seq_length: 128
-    pad_label: ''
-    ignore_extra_tokens: false
-    ignore_start_end: false
-    use_cache: false
-    num_workers: 4
-    pin_memory: true
-    drop_last: false
-    num_labels: 10
-    num_domains: 2
-    test_unlabelled: true
-    train_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 4
-    validation_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 4
-  tokenizer:
-    tokenizer_name: google/electra-small-discriminator
-    vocab_file: null
-    tokenizer_model: null
-    special_tokens: null
-  language_model:
-    pretrained_model_name: google/electra-small-discriminator
-    lm_checkpoint: null
-    config_file: null
-    config: null
-  punct_head:
-    punct_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: dice
-  domain_head:
-    domain_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: cel
-    gamma: 0
-    pooling: mean_max
-    idx_conditioned_on: 0
-  dice_loss:
-    epsilon: 0.01
-    alpha: 3
-    macro_average: true
-  focal_loss:
-    gamma: 1
-  optim:
-    name: adamw
-    lr: 0.001
-    weight_decay: 0.0
-    sched:
-      name: WarmupAnnealing
-      warmup_steps: null
-      warmup_ratio: 0.1
-      last_epoch: -1
-      monitor: val_loss
-      reduce_on_plateau: false
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/lightning_logs.txt
deleted file mode 100644
index c7d0c2d..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/lightning_logs.txt
+++ /dev/null
@@ -1,39 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-Using native 16bit precision.
-Global seed set to 42
-initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 1.0 K 
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-299 K     Trainable params
-13.2 M    Non-trainable params
-13.5 M    Total params
-Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/lr_find_temp_model.ckpt
-Global seed set to 42
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 1.0 K 
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-299 K     Trainable params
-13.2 M    Non-trainable params
-13.5 M    Total params
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_error_log.txt
deleted file mode 100644
index 0c8b389..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_error_log.txt
+++ /dev/null
@@ -1,10 +0,0 @@
-[NeMo W 2021-02-09 11:10:37 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-09 11:11:08 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-09 11:18:33 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-09 11:24:13 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 84d61e5..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,12 +0,0 @@
-[NeMo I 2021-02-09 11:10:37 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37
-[NeMo I 2021-02-09 11:10:37 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-09 11:10:37 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-09 11:11:08 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-09 11:18:33 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-09 11:24:13 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-    
diff --git a/experiment/Untitled.ipynb b/experiment/Untitled.ipynb
index 1a5526d..cd5247e 100644
--- a/experiment/Untitled.ipynb
+++ b/experiment/Untitled.ipynb
@@ -3,33 +3,33 @@
   {
    "cell_type": "code",
    "execution_count": 1,
-   "id": "dense-meaning",
+   "id": "modern-amplifier",
    "metadata": {},
    "outputs": [
     {
      "name": "stderr",
      "output_type": "stream",
      "text": [
-      "12:16:24.02 LOG:\n"
+      "12:17:39.23 LOG:\n"
      ]
     },
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "shuffling <data.punctuation_dataset_multi.PunctuationDomainDataset object at 0x7f86e5bc2220>\n"
+      "shuffling <data.punctuation_dataset_multi.PunctuationDomainDataset object at 0x7f383a13c0a0>\n"
      ]
     },
     {
      "name": "stderr",
      "output_type": "stream",
      "text": [
-      "12:16:24.11 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
-      "12:16:24.17 LOG:\n",
-      "12:16:24.48 .... f\"1st {n} encoder layers of transformer frozen\" = '1st 11 encoder layers of transformer frozen'\n",
+      "12:17:39.32 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
+      "12:17:39.38 LOG:\n",
+      "12:17:39.75 .... f\"1st {n} encoder layers of transformer frozen\" = '1st 11 encoder layers of transformer frozen'\n",
       "GPU available: True, used: False\n",
       "TPU available: None, using: 0 TPU cores\n",
-      "[NeMo W 2021-02-09 12:16:24 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
+      "[NeMo W 2021-02-09 12:17:39 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
       "      warnings.warn(*args, **kwargs)\n",
       "    \n"
      ]
@@ -74,157 +74,26 @@
   },
   {
    "cell_type": "code",
+   "execution_count": null,
+   "id": "minus-mississippi",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import regex as re\n",
+    "re.sub('(\"[CLS]\"|)','','[CLS] we, bought: four- shirts, one, pen, and, a, mug, from, the, nvidia- gear, store, in, santa- clara, [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]')"
+   ]
+  },
+  {
+   "cell_type": "code",
    "execution_count": 2,
-   "id": "potential-adrian",
+   "id": "hairy-proxy",
    "metadata": {},
    "outputs": [
     {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "12:16:24.62 LOG:\n",
-      "12:16:24.69 .... chunk_to_len_batch(max_seq_length, tokenizer,chunked['texts'],chunked['tags'],True) = {'attention_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
-      "12:16:24.69                                                                                                      True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
-      "12:16:24.69                                                                                                      True, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False],\n",
-      "12:16:24.69                                                                                                    [ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False],\n",
-      "12:16:24.69                                                                                                    [ True,  True,  True,  True,  True, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False]]),\n",
-      "12:16:24.69                                                                                             'input_ids': tensor([[  101,  2057,  4149,  2176, 11344,  2028,  7279,  1998,  1037, 14757,\n",
-      "12:16:24.69                                                                                                      2013,  1996,  1050, 17258,  2401,  6718,  3573,  1999,  4203, 10254,\n",
-      "12:16:24.69                                                                                                       102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0],\n",
-      "12:16:24.69                                                                                                    [  101,  2054,  2064,  1045,  2079,  2005,  2017,  2651,   102,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0],\n",
-      "12:16:24.69                                                                                                    [  101,  2129,  2024,  2017,   102,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0]]),\n",
-      "12:16:24.69                                                                                             'labels': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0],\n",
-      "12:16:24.69                                                                                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0],\n",
-      "12:16:24.69                                                                                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0]]),\n",
-      "12:16:24.69                                                                                             'subtoken_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
-      "12:16:24.69                                                                                                      True,  True, False, False,  True,  True,  True,  True,  True,  True,\n",
-      "12:16:24.69                                                                                                      True, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False],\n",
-      "12:16:24.69                                                                                                    [ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False],\n",
-      "12:16:24.69                                                                                                    [ True,  True,  True,  True,  True, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False]])}\n"
-     ]
-    },
-    {
      "data": {
       "text/plain": [
-       "['[CLS] we, bought, four- shirts: one, pen, and, a, mug, from, the, nvidia- gear, store, in, santa- clara, [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
-       " '[CLS] what? can— i? do? for? you? today? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
+       "['[CLS] we, bought: four- shirts, one, pen, and, a, mug, from, the, nvidia- gear, store, in, santa- clara, [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
+       " '[CLS] what? can, i? do? for? you? today? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
        " '[CLS] how? are? you? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']"
       ]
      },
@@ -246,7 +115,7 @@
   {
    "cell_type": "code",
    "execution_count": null,
-   "id": "amateur-production",
+   "id": "employed-station",
    "metadata": {},
    "outputs": [],
    "source": []
diff --git a/experiment/config.yaml b/experiment/config.yaml
index efa7a5d..4cd1ad0 100644
--- a/experiment/config.yaml
+++ b/experiment/config.yaml
@@ -67,7 +67,7 @@ model:
             # - ${base_path}/open_subtitles_processed #  
         unlabelled:
             # - ${base_path}/ted_talks_processed #
-            - ${base_path}/open_subtitles_processed #  
+            # - ${base_path}/open_subtitles_processed #  
             # parameters for dataset preprocessing
         max_seq_length: 128
         pad_label: ''
@@ -75,11 +75,11 @@ model:
         ignore_start_end: false
         use_cache: false
         # shared among dataloaders
-        num_workers:  4
+        num_workers:  6
         pin_memory: true
         drop_last: false
         num_labels: 10
-        num_domains: 2
+        num_domains: 1
         test_unlabelled: true
 
         train_ds:
@@ -129,7 +129,7 @@ model:
     
     dice_loss:
         epsilon: 0.01
-        alpha: 3
+        alpha: 1
         macro_average: true
 
     focal_loss: 
diff --git a/experiment/info.log b/experiment/info.log
index 69e9a76..e69de29 100644
--- a/experiment/info.log
+++ b/experiment/info.log
@@ -1,85 +0,0 @@
-[INFO] - shuffling train set
-[INFO] - Optimizer config = AdamW (
-Parameter Group 0
-    amsgrad: False
-    betas: (0.9, 0.999)
-    eps: 1e-08
-    lr: 0.001
-    weight_decay: 0.0
-)
-[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f717ffe5b80>" 
-will be used during training (effective maximum steps = 80) - 
-Parameters : 
-(warmup_steps: null
-warmup_ratio: 0.1
-last_epoch: -1
-max_steps: 80
-)
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          89.19      22.74      36.24       3012
-! (label_id: 1)                                          0.00       0.00       0.00          1
-, (label_id: 2)                                          7.31      36.21      12.16        243
-- (label_id: 3)                                          2.27      21.43       4.11         28
-. (label_id: 4)                                          1.68       1.65       1.66        182
-: (label_id: 5)                                          0.00       0.00       0.00          5
-; (label_id: 6)                                          0.00       0.00       0.00          3
-? (label_id: 7)                                          0.24      22.22       0.48          9
-— (label_id: 8)                                          0.00       0.00       0.00         10
-… (label_id: 9)                                          0.00       0.00       0.00          1
--------------------
-micro avg                                               22.44      22.44      22.44       3494
-macro avg                                               10.07      10.43       5.47       3494
-weighted avg                                            77.50      22.44      32.21       3494
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         50.00     100.00      66.67         34
-1 (label_id: 1)                                          0.00       0.00       0.00         34
--------------------
-micro avg                                               50.00      50.00      50.00         68
-macro avg                                               25.00      50.00      33.33         68
-weighted avg                                            25.00      50.00      33.33         68
-
-[INFO] - Optimizer config = AdamW (
-Parameter Group 0
-    amsgrad: False
-    betas: (0.9, 0.999)
-    eps: 1e-08
-    lr: 0.007943282347242822
-    weight_decay: 0.0
-)
-[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f717f560fa0>" 
-will be used during training (effective maximum steps = 53050) - 
-Parameters : 
-(warmup_steps: null
-warmup_ratio: 0.1
-last_epoch: -1
-max_steps: 53050
-)
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          89.19      22.74      36.24       3012
-! (label_id: 1)                                          0.00       0.00       0.00          1
-, (label_id: 2)                                          7.31      36.21      12.16        243
-- (label_id: 3)                                          2.27      21.43       4.11         28
-. (label_id: 4)                                          1.68       1.65       1.66        182
-: (label_id: 5)                                          0.00       0.00       0.00          5
-; (label_id: 6)                                          0.00       0.00       0.00          3
-? (label_id: 7)                                          0.24      22.22       0.48          9
-— (label_id: 8)                                          0.00       0.00       0.00         10
-… (label_id: 9)                                          0.00       0.00       0.00          1
--------------------
-micro avg                                               22.44      22.44      22.44       3494
-macro avg                                               10.07      10.43       5.47       3494
-weighted avg                                            77.50      22.44      32.21       3494
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         50.00     100.00      66.67         34
-1 (label_id: 1)                                          0.00       0.00       0.00         34
--------------------
-micro avg                                               50.00      50.00      50.00         68
-macro avg                                               25.00      50.00      33.33         68
-weighted avg                                            25.00      50.00      33.33         68
-
diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
index dab395e..c409952 100644
--- a/experiment/models/punctuation_domain_model.py
+++ b/experiment/models/punctuation_domain_model.py
@@ -157,6 +157,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
         passed in as `batch`.
         """
         p=(self.current_epoch*self.train_size+batch_idx)/(self.train_size*self.hparams.trainer.max_epochs)
+        if (batch_idx%1000==0):
+            print('gamma:',p)
         self.grad_reverse.scale=2/(1+math.exp(-10*p))-1
         loss, _, _ = self._make_step(batch)
         lr = self._optimizer.param_groups[0]['lr']
