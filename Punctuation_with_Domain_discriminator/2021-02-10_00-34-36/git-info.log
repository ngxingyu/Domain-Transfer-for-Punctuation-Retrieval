commit hash: e0a5c7222a0bb57c8d0a8b1f51a95872d2375c3c
diff --git a/ASR b/ASR
deleted file mode 120000
index 538f614..0000000
--- a/ASR
+++ /dev/null
@@ -1 +0,0 @@
-/content/ASR
\ No newline at end of file
diff --git a/experiment/config.yaml b/experiment/config.yaml
index 6b576c7..e9cfd19 100644
--- a/experiment/config.yaml
+++ b/experiment/config.yaml
@@ -1,43 +1,43 @@
 seed: 42
 trainer:
-    # gpus: 1 # the number of gpus, 0 for CPU
-    # num_nodes: 1
-    # max_epochs: 15
-    # max_steps: null # precedence over max_epochs
-    # accumulate_grad_batches: 4 # accumulates grads every k batches
-    # gradient_clip_val: 0
-    # amp_level: O1 # O1/O2 for mixed precision
-    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-    # accelerator: ddp
-    # checkpoint_callback: false  # Provided by exp_manager
-    # logger: false #false  # Provided by exp_manager
-    # log_every_n_steps: 1  # Interval of logging.
-    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-    # resume_from_checkpoint: null
-
-    gpus: 0 # the number of gpus, 0 for CPU
+    gpus: 1 # the number of gpus, 0 for CPU
     num_nodes: 1
     max_epochs: 15
     max_steps: null # precedence over max_epochs
     accumulate_grad_batches: 4 # accumulates grads every k batches
     gradient_clip_val: 0
-    amp_level: O0 # O1/O2 for mixed precision
-    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-    # accelerator: ddp
+    amp_level: O1 # O1/O2 for mixed precision
+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
+    accelerator: ddp
     checkpoint_callback: false  # Provided by exp_manager
     logger: false #false  # Provided by exp_manager
     log_every_n_steps: 1  # Interval of logging.
     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-    reload_dataloaders_every_epoch: true
     resume_from_checkpoint: null
 
+    # gpus: 0 # the number of gpus, 0 for CPU
+    # num_nodes: 1
+    # max_epochs: 15
+    # max_steps: null # precedence over max_epochs
+    # accumulate_grad_batches: 4 # accumulates grads every k batches
+    # gradient_clip_val: 0
+    # amp_level: O0 # O1/O2 for mixed precision
+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
+    # # accelerator: ddp
+    # checkpoint_callback: false  # Provided by exp_manager
+    # logger: false #false  # Provided by exp_manager
+    # log_every_n_steps: 1  # Interval of logging.
+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
+    # reload_dataloaders_every_epoch: true
+    # resume_from_checkpoint: null
+
 exp_manager:
-    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
+    exp_dir: /root/project # /home/nxingyu2/project/ # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
     name: Punctuation_with_Domain_discriminator  # The name of your model
     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
     create_checkpoint_callback: true 
-base_path: /home/nxingyu2/data # /root/data # 
-tmp_path: /home/nxingyu2/data/tmp # /tmp # 
+base_path: /root/data # /home/nxingyu2/data # 
+tmp_path: /tmp # /home/nxingyu2/data/tmp # 
 
 model:
     nemo_path: null
@@ -61,7 +61,7 @@ model:
     punct_class_weights: false
     
     dataset:
-        data_dir: /home/nxingyu2/data # /root/data # 
+        data_dir: /root/data # /home/nxingyu2/data # 
         labelled:
             # - ${base_path}/ted_talks_processed #
             - ${base_path}/open_subtitles_processed #  
@@ -75,7 +75,7 @@ model:
         ignore_start_end: false
         use_cache: false
         # shared among dataloaders
-        num_workers:  4
+        num_workers:  2
         pin_memory: true
         drop_last: true
         num_labels: 10
diff --git a/experiment/info.log b/experiment/info.log
index 8b94e5c..e69de29 100644
--- a/experiment/info.log
+++ b/experiment/info.log
@@ -1,60 +0,0 @@
-[INFO] - GPU available: True, used: False
-[INFO] - TPU available: None, using: 0 TPU cores
-[INFO] - shuffling train set
-[INFO] - Optimizer config = AdamW (
-Parameter Group 0
-    amsgrad: False
-    betas: (0.9, 0.999)
-    eps: 1e-08
-    lr: 0.01
-    weight_decay: 0.0
-)
-[INFO] - Scheduler "<core.optim.lr_scheduler.CosineAnnealing object at 0x7ff722d4b4c0>" 
-will be used during training (effective maximum steps = 79575) - 
-Parameters : 
-(warmup_steps: null
-warmup_ratio: 0.1
-min_lr: 1.0e-10
-last_epoch: -1
-max_steps: 79575
-)
-[INFO] - 
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 513   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-299 K     Trainable params
-13.2 M    Non-trainable params
-13.5 M    Total params
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          80.99      20.22      32.36       9732
-! (label_id: 1)                                          1.45       2.63       1.87        152
-, (label_id: 2)                                          5.42      27.46       9.05        772
-- (label_id: 3)                                          1.70      53.57       3.30         56
-. (label_id: 4)                                          3.12       0.85       1.34       1642
-: (label_id: 5)                                          0.00       0.00       0.00          0
-; (label_id: 6)                                          0.00       0.00       0.00          0
-? (label_id: 7)                                          1.42      22.94       2.67        218
-— (label_id: 8)                                          0.00       0.00       0.00          0
-… (label_id: 9)                                          0.93       2.04       1.28         98
--------------------
-micro avg                                               18.00      18.00      18.00      12670
-macro avg                                               13.58      18.53       7.41      12670
-weighted avg                                            63.00      18.00      25.68      12670
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                        100.00     100.00     100.00        116
--------------------
-micro avg                                              100.00     100.00     100.00        116
-macro avg                                              100.00     100.00     100.00        116
-weighted avg                                           100.00     100.00     100.00        116
-
diff --git a/setup.sh b/setup.sh
index 6115ed1..411c3e3 100644
--- a/setup.sh
+++ b/setup.sh
@@ -1,8 +1,9 @@
-# wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
-# bash Miniconda3-latest-Linux-x86_64.sh
-# source ~/.bashrc
+sudo apt install screen
+wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
+bash Miniconda3-latest-Linux-x86_64.sh
+source ~/.bashrc
 conda create --name NLP python=3.8
 conda activate NLP
-pip install snoop nemo_toolkit[all]==1.0.0b2 transformers datasets hydra-core
+pip install snoop nemo_toolkit[all]==1.0.0b2 transformers datasets hydra-core git+https://github.com/pabloppp/pytorch-tools -U
 conda install -c conda-forge pytorch-lightning
 conda install pytorch cudatoolkit=10.1 -c pytorch
