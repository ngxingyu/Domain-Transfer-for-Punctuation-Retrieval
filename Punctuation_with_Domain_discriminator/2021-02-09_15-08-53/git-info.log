commit hash: 4cccf1ef224a31f16ab4158a6dbd1472d2705371
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/events.out.tfevents.1612741439.Titan.4686.0 b/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/events.out.tfevents.1612741439.Titan.4686.0
deleted file mode 100644
index dc6761d..0000000
Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/events.out.tfevents.1612741439.Titan.4686.0 and /dev/null differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/git-info.log
deleted file mode 100644
index 83e328a..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/git-info.log
+++ /dev/null
@@ -1,406 +0,0 @@
-commit hash: 939a671c8c117db6975316767ced5d95449e2b27
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index fe58670..1442409 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,47 +1,47 @@
- seed: 42
- trainer:
--    # gpus: 1 # the number of gpus, 0 for CPU
--    # num_nodes: 1
--    # max_epochs: 2
--    # max_steps: null # precedence over max_epochs
--    # accumulate_grad_batches: 4 # accumulates grads every k batches
--    # gradient_clip_val: 0
--    # amp_level: O1 # O1/O2 for mixed precision
--    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
--    # checkpoint_callback: false  # Provided by exp_manager
--    # logger: false #false  # Provided by exp_manager
--    # log_every_n_steps: 1  # Interval of logging.
--    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # resume_from_checkpoint: null
--
--    gpus: 0 # the number of gpus, 0 for CPU
-+    gpus: 1 # the number of gpus, 0 for CPU
-     num_nodes: 1
--    max_epochs: 8
-+    max_epochs: 2
-     max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 1 # accumulates grads every k batches
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-     gradient_clip_val: 0
--    amp_level: O0 # O1/O2 for mixed precision
--    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
-+    amp_level: O1 # O1/O2 for mixed precision
-+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    accelerator: ddp
-     checkpoint_callback: false  # Provided by exp_manager
-     logger: false #false  # Provided by exp_manager
-     log_every_n_steps: 1  # Interval of logging.
-     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    reload_dataloaders_every_epoch: true
-     resume_from_checkpoint: null
- 
-+    # gpus: 0 # the number of gpus, 0 for CPU
-+    # num_nodes: 1
-+    # max_epochs: 8
-+    # max_steps: null # precedence over max_epochs
-+    # accumulate_grad_batches: 1 # accumulates grads every k batches
-+    # gradient_clip_val: 0
-+    # amp_level: O0 # O1/O2 for mixed precision
-+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # # accelerator: ddp
-+    # checkpoint_callback: false  # Provided by exp_manager
-+    # logger: false #false  # Provided by exp_manager
-+    # log_every_n_steps: 1  # Interval of logging.
-+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    # reload_dataloaders_every_epoch: true
-+    # resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /home/nxingyu2/data # /root/data # 
--tmp_path: /home/nxingyu2/data/tmp # /tmp # 
-+base_path: /home/nxingyu/data # /root/data # 
-+tmp_path: /home/nxingyu/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
--    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-     unfrozen: 0
-     maximum_unfrozen: 2
-     unfreeze_step: 1
-@@ -58,10 +58,10 @@ model:
-         - "—"
-         - "…"
- 
--    punct_class_weights: true
-+    punct_class_weights: false
-     
-     dataset:
--        data_dir: /home/nxingyu2/data # /root/data # 
-+        data_dir: /home/nxingyu/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -132,7 +132,7 @@ model:
-         gamma: 5
- 
-     optim:
--        name: novograd
-+        name: adam
-         lr: 1e-3
-         weight_decay: 0.00
-         sched:
-diff --git a/experiment/info.log b/experiment/info.log
-index 2471fe9..e69de29 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,300 +0,0 @@
--[INFO] - GPU available: True, used: False
--[INFO] - TPU available: None, using: 0 TPU cores
--[INFO] - shuffling train set
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        184
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.23       0.34       0.53        594
--- (label_id: 3)                                          3.06      25.42       5.46         59
--. (label_id: 4)                                         47.22      12.98      20.36        524
--: (label_id: 5)                                          0.00       0.00       0.00         18
--; (label_id: 6)                                          0.00       0.00       0.00         13
--? (label_id: 7)                                          8.45       6.32       7.23         95
--— (label_id: 8)                                          0.00       0.00       0.00         12
--… (label_id: 9)                                          0.00       0.00       0.00          0
---------------------
--micro avg                                                6.05       6.05       6.05       1503
--macro avg                                                6.66       5.01       3.73       1503
--weighted avg                                            17.61       6.05       7.98       1503
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00         92
---------------------
--micro avg                                              100.00     100.00     100.00         92
--macro avg                                              100.00     100.00     100.00         92
--weighted avg                                           100.00     100.00     100.00         92
--
--[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 1.5848931924611143e-08
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
--will be used during training (effective maximum steps = 3192) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 3192
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        202
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.62       0.45       0.70        669
--- (label_id: 3)                                          3.48      27.27       6.17         66
--. (label_id: 4)                                         45.06      13.01      20.19        561
--: (label_id: 5)                                          1.52       6.67       2.47         15
--; (label_id: 6)                                          0.00       0.00       0.00         15
--? (label_id: 7)                                          8.70       7.32       7.95         82
--— (label_id: 8)                                          0.00       0.00       0.00         13
--… (label_id: 9)                                          0.00       0.00       0.00          1
---------------------
--micro avg                                                6.20       6.20       6.20       1628
--macro avg                                                6.04       5.47       3.75       1628
--weighted avg                                            16.79       6.20       7.92       1628
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        101
---------------------
--micro avg                                              100.00     100.00     100.00        101
--macro avg                                              100.00     100.00     100.00        101
--weighted avg                                           100.00     100.00     100.00        101
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.34       0.73       0.46       4402
--! (label_id: 1)                                          0.42      13.95       0.82        129
--, (label_id: 2)                                          2.53       0.64       1.03      15243
--- (label_id: 3)                                          2.45      21.03       4.38       1322
--. (label_id: 4)                                         44.00      11.40      18.11      12542
--: (label_id: 5)                                          0.43       1.41       0.65        354
--; (label_id: 6)                                          0.00       0.00       0.00        163
--? (label_id: 7)                                          4.16       6.27       5.00       1117
--— (label_id: 8)                                          3.00       0.61       1.02        488
--… (label_id: 9)                                          0.97       6.17       1.68         81
---------------------
--micro avg                                                5.41       5.41       5.41      35841
--macro avg                                                5.83       6.22       3.32      35841
--weighted avg                                            16.78       5.41       7.18      35841
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2201
---------------------
--micro avg                                              100.00     100.00     100.00       2201
--macro avg                                              100.00     100.00     100.00       2201
--weighted avg                                           100.00     100.00     100.00       2201
--
--[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.20       0.43       0.27       4226
--! (label_id: 1)                                          0.44      14.17       0.86        127
--, (label_id: 2)                                          1.93       0.49       0.78      14611
--- (label_id: 3)                                          2.23      19.56       4.01       1237
--. (label_id: 4)                                         43.37      11.25      17.86      11977
--: (label_id: 5)                                          0.68       2.34       1.05        342
--; (label_id: 6)                                          0.00       0.00       0.00        129
--? (label_id: 7)                                          5.16       7.47       6.10       1058
--— (label_id: 8)                                          2.15       0.49       0.80        409
--… (label_id: 9)                                          0.69       4.23       1.19         71
---------------------
--micro avg                                                5.23       5.23       5.23      34187
--macro avg                                                5.68       6.04       3.29      34187
--weighted avg                                            16.32       5.23       6.98      34187
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2113
---------------------
--micro avg                                              100.00     100.00     100.00       2113
--macro avg                                              100.00     100.00     100.00       2113
--weighted avg                                           100.00     100.00     100.00       2113
--
--[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.61       0.39       4228
--! (label_id: 1)                                          0.30       8.28       0.58        145
--, (label_id: 2)                                          2.27       0.58       0.92      14495
--- (label_id: 3)                                          2.64      21.78       4.70       1327
--. (label_id: 4)                                         44.87      11.66      18.51      12193
--: (label_id: 5)                                          0.60       1.93       0.91        362
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.19       6.40       5.07       1078
--— (label_id: 8)                                          1.16       0.22       0.37        459
--… (label_id: 9)                                          0.85       4.17       1.41         96
---------------------
--micro avg                                                5.54       5.54       5.54      34547
--macro avg                                                5.72       5.56       3.29      34547
--weighted avg                                            17.08       5.54       7.33      34547
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2114
---------------------
--micro avg                                              100.00     100.00     100.00       2114
--macro avg                                              100.00     100.00     100.00       2114
--weighted avg                                           100.00     100.00     100.00       2114
--
--[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.29       0.63       0.40       4444
--! (label_id: 1)                                          0.38      10.67       0.74        150
--, (label_id: 2)                                          2.32       0.59       0.94      15290
--- (label_id: 3)                                          2.34      20.28       4.19       1292
--. (label_id: 4)                                         43.85      11.68      18.44      12599
--: (label_id: 5)                                          0.41       1.28       0.62        392
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.24       6.30       5.07       1111
--— (label_id: 8)                                          0.00       0.00       0.00        456
--… (label_id: 9)                                          0.38       2.41       0.65         83
---------------------
--micro avg                                                5.40       5.40       5.40      35981
--macro avg                                                5.42       5.38       3.11      35981
--weighted avg                                            16.59       5.40       7.22      35981
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2222
---------------------
--micro avg                                              100.00     100.00     100.00       2222
--macro avg                                              100.00     100.00     100.00       2222
--weighted avg                                           100.00     100.00     100.00       2222
--
--[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.35       0.73       0.48       3844
--! (label_id: 1)                                          0.54      14.62       1.04        130
--, (label_id: 2)                                          2.32       0.59       0.94      13056
--- (label_id: 3)                                          2.67      22.28       4.77       1194
--. (label_id: 4)                                         44.45      11.95      18.84      10791
--: (label_id: 5)                                          0.84       3.21       1.33        280
--; (label_id: 6)                                          0.00       0.00       0.00        140
--? (label_id: 7)                                          4.17       6.56       5.10        914
--— (label_id: 8)                                          0.00       0.00       0.00        401
--… (label_id: 9)                                          0.48       2.63       0.81         76
---------------------
--micro avg                                                5.68       5.68       5.68      30826
--macro avg                                                5.58       6.26       3.33      30826
--weighted avg                                            16.82       5.68       7.41      30826
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1922
---------------------
--micro avg                                              100.00     100.00     100.00       1922
--macro avg                                              100.00     100.00     100.00       1922
--weighted avg                                           100.00     100.00     100.00       1922
--
--[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.60       0.39       3970
--! (label_id: 1)                                          0.35      10.66       0.68        122
--, (label_id: 2)                                          2.09       0.53       0.85      13469
--- (label_id: 3)                                          2.29      19.32       4.10       1201
--. (label_id: 4)                                         43.43      11.24      17.86      11227
--: (label_id: 5)                                          0.63       2.30       0.99        304
--; (label_id: 6)                                          0.00       0.00       0.00        141
--? (label_id: 7)                                          4.52       6.86       5.45       1006
--— (label_id: 8)                                          1.15       0.23       0.38        444
--… (label_id: 9)                                          0.45       2.67       0.78         75
---------------------
--micro avg                                                5.26       5.26       5.26      31959
--macro avg                                                5.52       5.44       3.15      31959
--weighted avg                                            16.42       5.26       7.02      31959
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1985
---------------------
--micro avg                                              100.00     100.00     100.00       1985
--macro avg                                              100.00     100.00     100.00       1985
--weighted avg                                           100.00     100.00     100.00       1985
--
--[INFO] - Epoch 5, step 2394: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.23       0.48       0.31       4126
--! (label_id: 1)                                          0.29       9.40       0.56        117
--, (label_id: 2)                                          1.91       0.49       0.77      14019
--- (label_id: 3)                                          2.52      22.59       4.53       1164
--. (label_id: 4)                                         44.15      11.65      18.44      11789
--: (label_id: 5)                                          0.72       2.41       1.11        332
--; (label_id: 6)                                          0.56       0.61       0.58        165
--? (label_id: 7)                                          3.89       6.53       4.88        980
--— (label_id: 8)                                          2.30       0.47       0.77        430
--… (label_id: 9)                                          1.18       8.33       2.07         60
---------------------
--micro avg                                                5.47       5.47       5.47      33182
--macro avg                                                5.77       6.30       3.40      33182
--weighted avg                                            16.77       5.47       7.25      33182
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2063
---------------------
--micro avg                                              100.00     100.00     100.00       2063
--macro avg                                              100.00     100.00     100.00       2063
--weighted avg                                           100.00     100.00     100.00       2063
--
--[INFO] - Epoch 6, step 2793: val_loss was not in top 3
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/hparams.yaml
deleted file mode 100644
index 825d089..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/hparams.yaml
+++ /dev/null
@@ -1,107 +0,0 @@
-seed: 42
-trainer:
-  gpus: 1
-  num_nodes: 1
-  max_epochs: 2
-  max_steps: null
-  accumulate_grad_batches: 4
-  gradient_clip_val: 0
-  amp_level: O1
-  precision: 16
-  accelerator: ddp
-  checkpoint_callback: false
-  logger: false
-  log_every_n_steps: 1
-  val_check_interval: 1.0
-  resume_from_checkpoint: null
-exp_manager:
-  exp_dir: /home/nxingyu/project/
-  name: Punctuation_with_Domain_discriminator
-  create_tensorboard_logger: true
-  create_checkpoint_callback: true
-base_path: /home/nxingyu/data
-tmp_path: /home/nxingyu/data/tmp
-model:
-  nemo_path: null
-  transformer_path: google/electra-small-discriminator
-  unfrozen: 0
-  maximum_unfrozen: 2
-  unfreeze_step: 1
-  punct_label_ids:
-  - ''
-  - '!'
-  - ','
-  - '-'
-  - .
-  - ':'
-  - ;
-  - '?'
-  - —
-  - …
-  punct_class_weights: false
-  dataset:
-    data_dir: /home/nxingyu/data
-    labelled:
-    - /home/nxingyu/data/ted_talks_processed
-    unlabelled: null
-    max_seq_length: 128
-    pad_label: ''
-    ignore_extra_tokens: false
-    ignore_start_end: false
-    use_cache: false
-    num_workers: 0
-    pin_memory: true
-    drop_last: false
-    num_labels: 10
-    num_domains: 1
-    test_unlabelled: true
-    train_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 8
-    validation_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 8
-  tokenizer:
-    tokenizer_name: google/electra-small-discriminator
-    vocab_file: null
-    tokenizer_model: null
-    special_tokens: null
-  language_model:
-    pretrained_model_name: google/electra-small-discriminator
-    lm_checkpoint: null
-    config_file: null
-    config: null
-  punct_head:
-    punct_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: dice
-  domain_head:
-    domain_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: cel
-    gamma: 0.1
-  dice_loss:
-    epsilon: 0.01
-    alpha: 3
-    macro_average: true
-  focal_loss:
-    gamma: 5
-  optim:
-    name: adam
-    lr: 0.001
-    weight_decay: 0.0
-    sched:
-      name: WarmupAnnealing
-      warmup_steps: null
-      warmup_ratio: 0.1
-      last_epoch: -1
-      monitor: val_loss
-      reduce_on_plateau: false
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/lightning_logs.txt
deleted file mode 100644
index 5a813ec..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/lightning_logs.txt
+++ /dev/null
@@ -1,117 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-Using native 16bit precision.
-Global seed set to 42
-initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-35.9 K    Trainable params
-13.4 M    Non-trainable params
-13.5 M    Total params
-Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/lr_find_temp_model.ckpt
-Global seed set to 42
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-35.9 K    Trainable params
-13.4 M    Non-trainable params
-13.5 M    Total params
-Epoch 0, global step 100: val_loss reached 0.49661 (best 0.49661), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.50-epoch=0.ckpt" as top 3
-Epoch 1, global step 200: val_loss reached 0.50627 (best 0.49661), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.51-epoch=1.ckpt" as top 3
-Global seed set to 42
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-825 K     Trainable params
-12.7 M    Non-trainable params
-13.5 M    Total params
-LR finder stopped early due to diverging loss.
-Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/lr_find_temp_model.ckpt
-Global seed set to 42
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-825 K     Trainable params
-12.7 M    Non-trainable params
-13.5 M    Total params
-Epoch 0, global step 301: val_loss reached 0.30201 (best 0.30201), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.30-epoch=0.ckpt" as top 3
-Epoch 1, global step 401: val_loss reached 0.31043 (best 0.30201), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.31-epoch=1.ckpt" as top 3
-Global seed set to 42
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-1.6 M     Trainable params
-11.9 M    Non-trainable params
-13.5 M    Total params
-LR finder stopped early due to diverging loss.
-Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/lr_find_temp_model.ckpt
-Global seed set to 42
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-1.6 M     Trainable params
-11.9 M    Non-trainable params
-13.5 M    Total params
-Epoch 0, global step 502: val_loss reached 0.39765 (best 0.30201), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.40-epoch=0.ckpt" as top 3
-Epoch 1, step 602: val_loss was not in top 3
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-Using environment variable NODE_RANK for node rank (0).
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/nemo_error_log.txt
deleted file mode 100644
index 1eb1af1..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/nemo_error_log.txt
+++ /dev/null
@@ -1,43 +0,0 @@
-[NeMo W 2021-02-08 07:39:29 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 07:39:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 07:39:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 07:39:40 nemo_logging:349] /home/nxingyu/project/experiment/core/classification_report.py:116: UserWarning: This overload of nonzero is deprecated:
-    	nonzero(Tensor input, *, Tensor out)
-    Consider using one of the following signatures instead:
-    	nonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629411241/work/torch/csrc/utils/python_arg_parser.cpp:766.)
-      num_non_empty_classes = torch.nonzero(self.num_examples_per_class).size(0)
-    
-[NeMo W 2021-02-08 07:39:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 07:43:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 07:43:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 07:44:01 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 07:44:05 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:118: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-    
-[NeMo W 2021-02-08 07:48:43 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f3e417748e0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-08 07:49:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f3e4bb7b490> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-08 07:59:34 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 08:28:12 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 08:28:39 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f3e4bb7b5b0> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index e7e35be..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,45 +0,0 @@
-[NeMo I 2021-02-08 07:39:29 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-39-29
-[NeMo I 2021-02-08 07:39:29 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-08 07:39:29 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 07:39:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 07:39:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 07:39:40 nemo_logging:349] /home/nxingyu/project/experiment/core/classification_report.py:116: UserWarning: This overload of nonzero is deprecated:
-    	nonzero(Tensor input, *, Tensor out)
-    Consider using one of the following signatures instead:
-    	nonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629411241/work/torch/csrc/utils/python_arg_parser.cpp:766.)
-      num_non_empty_classes = torch.nonzero(self.num_examples_per_class).size(0)
-    
-[NeMo W 2021-02-08 07:39:40 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 07:43:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 07:43:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 07:44:01 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 07:44:05 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:118: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-    
-[NeMo W 2021-02-08 07:48:43 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f3e417748e0> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-08 07:49:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f3e4bb7b490> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-08 07:59:34 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 08:28:12 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 08:28:39 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f3e4bb7b5b0> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/events.out.tfevents.1612742534.Titan.5792.0 b/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/events.out.tfevents.1612742534.Titan.5792.0
deleted file mode 100644
index 51c6937..0000000
Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/events.out.tfevents.1612742534.Titan.5792.0 and /dev/null differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/git-info.log
deleted file mode 100644
index 4f1e878..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/git-info.log
+++ /dev/null
@@ -1,415 +0,0 @@
-commit hash: 939a671c8c117db6975316767ced5d95449e2b27
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index fe58670..2306d25 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -1,47 +1,47 @@
- seed: 42
- trainer:
--    # gpus: 1 # the number of gpus, 0 for CPU
--    # num_nodes: 1
--    # max_epochs: 2
--    # max_steps: null # precedence over max_epochs
--    # accumulate_grad_batches: 4 # accumulates grads every k batches
--    # gradient_clip_val: 0
--    # amp_level: O1 # O1/O2 for mixed precision
--    # precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
--    # checkpoint_callback: false  # Provided by exp_manager
--    # logger: false #false  # Provided by exp_manager
--    # log_every_n_steps: 1  # Interval of logging.
--    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    # resume_from_checkpoint: null
--
--    gpus: 0 # the number of gpus, 0 for CPU
-+    gpus: 1 # the number of gpus, 0 for CPU
-     num_nodes: 1
--    max_epochs: 8
-+    max_epochs: 2
-     max_steps: null # precedence over max_epochs
--    accumulate_grad_batches: 1 # accumulates grads every k batches
-+    accumulate_grad_batches: 4 # accumulates grads every k batches
-     gradient_clip_val: 0
--    amp_level: O0 # O1/O2 for mixed precision
--    precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
--    # accelerator: ddp
-+    amp_level: O1 # O1/O2 for mixed precision
-+    precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    accelerator: ddp
-     checkpoint_callback: false  # Provided by exp_manager
-     logger: false #false  # Provided by exp_manager
-     log_every_n_steps: 1  # Interval of logging.
-     val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
--    reload_dataloaders_every_epoch: true
-     resume_from_checkpoint: null
- 
-+    # gpus: 0 # the number of gpus, 0 for CPU
-+    # num_nodes: 1
-+    # max_epochs: 8
-+    # max_steps: null # precedence over max_epochs
-+    # accumulate_grad_batches: 1 # accumulates grads every k batches
-+    # gradient_clip_val: 0
-+    # amp_level: O0 # O1/O2 for mixed precision
-+    # precision: 32 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
-+    # # accelerator: ddp
-+    # checkpoint_callback: false  # Provided by exp_manager
-+    # logger: false #false  # Provided by exp_manager
-+    # log_every_n_steps: 1  # Interval of logging.
-+    # val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
-+    # reload_dataloaders_every_epoch: true
-+    # resume_from_checkpoint: null
-+
- exp_manager:
--    exp_dir: /home/nxingyu2/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-+    exp_dir: /home/nxingyu/project/ # /root/experiments # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
-     name: Punctuation_with_Domain_discriminator  # The name of your model
-     create_tensorboard_logger: true  # Whether you want exp_manger to create a tb logger
-     create_checkpoint_callback: true 
--base_path: /home/nxingyu2/data # /root/data # 
--tmp_path: /home/nxingyu2/data/tmp # /tmp # 
-+base_path: /home/nxingyu/data # /root/data # 
-+tmp_path: /home/nxingyu/data/tmp # /tmp # 
- 
- model:
-     nemo_path: null
--    transformer_path: google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-+    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
-     unfrozen: 0
-     maximum_unfrozen: 2
-     unfreeze_step: 1
-@@ -58,10 +58,10 @@ model:
-         - "—"
-         - "…"
- 
--    punct_class_weights: true
-+    punct_class_weights: false
-     
-     dataset:
--        data_dir: /home/nxingyu2/data # /root/data # 
-+        data_dir: /home/nxingyu/data # /root/data # 
-         labelled:
-             - ${base_path}/ted_talks_processed #
-         unlabelled:
-@@ -112,7 +112,7 @@ model:
-         activation: 'relu'
-         log_softmax: false
-         use_transformer_init: true
--        loss: 'dice'
-+        loss: 'crf'
- 
-     domain_head:
-         domain_num_fc_layers: 1
-@@ -132,7 +132,7 @@ model:
-         gamma: 5
- 
-     optim:
--        name: novograd
-+        name: adam
-         lr: 1e-3
-         weight_decay: 0.00
-         sched:
-diff --git a/experiment/info.log b/experiment/info.log
-index 2471fe9..e69de29 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,300 +0,0 @@
--[INFO] - GPU available: True, used: False
--[INFO] - TPU available: None, using: 0 TPU cores
--[INFO] - shuffling train set
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f21fc67b0d0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        184
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.23       0.34       0.53        594
--- (label_id: 3)                                          3.06      25.42       5.46         59
--. (label_id: 4)                                         47.22      12.98      20.36        524
--: (label_id: 5)                                          0.00       0.00       0.00         18
--; (label_id: 6)                                          0.00       0.00       0.00         13
--? (label_id: 7)                                          8.45       6.32       7.23         95
--— (label_id: 8)                                          0.00       0.00       0.00         12
--… (label_id: 9)                                          0.00       0.00       0.00          0
---------------------
--micro avg                                                6.05       6.05       6.05       1503
--macro avg                                                6.66       5.01       3.73       1503
--weighted avg                                            17.61       6.05       7.98       1503
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00         92
---------------------
--micro avg                                              100.00     100.00     100.00         92
--macro avg                                              100.00     100.00     100.00         92
--weighted avg                                           100.00     100.00     100.00         92
--
--[INFO] - Restored states from the checkpoint file at /home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/lr_find_temp_model.ckpt
--[INFO] - Optimizer config = Novograd (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.95, 0.98)
--    eps: 1e-08
--    grad_averaging: False
--    lr: 1.5848931924611143e-08
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2216343640>" 
--will be used during training (effective maximum steps = 3192) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 3192
--)
--[INFO] - 
--  | Name                | Type                 | Params
---------------------------------------------------------------
--0 | transformer         | ElectraModel         | 108 M 
--1 | punct_classifier    | TokenClassifier      | 7.7 K 
--2 | domain_classifier   | SequenceClassifier   | 769   
--3 | punctuation_loss    | FocalDiceLoss        | 0     
--4 | domain_loss         | CrossEntropyLoss     | 0     
--5 | agg_loss            | AggregatorLoss       | 0     
--6 | punct_class_report  | ClassificationReport | 0     
--7 | domain_class_report | ClassificationReport | 0     
---------------------------------------------------------------
--8.5 K     Trainable params
--108 M     Non-trainable params
--108 M     Total params
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        202
--! (label_id: 1)                                          0.00       0.00       0.00          4
--, (label_id: 2)                                          1.62       0.45       0.70        669
--- (label_id: 3)                                          3.48      27.27       6.17         66
--. (label_id: 4)                                         45.06      13.01      20.19        561
--: (label_id: 5)                                          1.52       6.67       2.47         15
--; (label_id: 6)                                          0.00       0.00       0.00         15
--? (label_id: 7)                                          8.70       7.32       7.95         82
--— (label_id: 8)                                          0.00       0.00       0.00         13
--… (label_id: 9)                                          0.00       0.00       0.00          1
---------------------
--micro avg                                                6.20       6.20       6.20       1628
--macro avg                                                6.04       5.47       3.75       1628
--weighted avg                                            16.79       6.20       7.92       1628
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        101
---------------------
--micro avg                                              100.00     100.00     100.00        101
--macro avg                                              100.00     100.00     100.00        101
--weighted avg                                           100.00     100.00     100.00        101
--
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.34       0.73       0.46       4402
--! (label_id: 1)                                          0.42      13.95       0.82        129
--, (label_id: 2)                                          2.53       0.64       1.03      15243
--- (label_id: 3)                                          2.45      21.03       4.38       1322
--. (label_id: 4)                                         44.00      11.40      18.11      12542
--: (label_id: 5)                                          0.43       1.41       0.65        354
--; (label_id: 6)                                          0.00       0.00       0.00        163
--? (label_id: 7)                                          4.16       6.27       5.00       1117
--— (label_id: 8)                                          3.00       0.61       1.02        488
--… (label_id: 9)                                          0.97       6.17       1.68         81
---------------------
--micro avg                                                5.41       5.41       5.41      35841
--macro avg                                                5.83       6.22       3.32      35841
--weighted avg                                            16.78       5.41       7.18      35841
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2201
---------------------
--micro avg                                              100.00     100.00     100.00       2201
--macro avg                                              100.00     100.00     100.00       2201
--weighted avg                                           100.00     100.00     100.00       2201
--
--[INFO] - Epoch 0, global step 399: val_loss reached 0.86333 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=0.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.20       0.43       0.27       4226
--! (label_id: 1)                                          0.44      14.17       0.86        127
--, (label_id: 2)                                          1.93       0.49       0.78      14611
--- (label_id: 3)                                          2.23      19.56       4.01       1237
--. (label_id: 4)                                         43.37      11.25      17.86      11977
--: (label_id: 5)                                          0.68       2.34       1.05        342
--; (label_id: 6)                                          0.00       0.00       0.00        129
--? (label_id: 7)                                          5.16       7.47       6.10       1058
--— (label_id: 8)                                          2.15       0.49       0.80        409
--… (label_id: 9)                                          0.69       4.23       1.19         71
---------------------
--micro avg                                                5.23       5.23       5.23      34187
--macro avg                                                5.68       6.04       3.29      34187
--weighted avg                                            16.32       5.23       6.98      34187
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2113
---------------------
--micro avg                                              100.00     100.00     100.00       2113
--macro avg                                              100.00     100.00     100.00       2113
--weighted avg                                           100.00     100.00     100.00       2113
--
--[INFO] - Epoch 1, global step 798: val_loss reached 0.87094 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.87-epoch=1.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.61       0.39       4228
--! (label_id: 1)                                          0.30       8.28       0.58        145
--, (label_id: 2)                                          2.27       0.58       0.92      14495
--- (label_id: 3)                                          2.64      21.78       4.70       1327
--. (label_id: 4)                                         44.87      11.66      18.51      12193
--: (label_id: 5)                                          0.60       1.93       0.91        362
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.19       6.40       5.07       1078
--— (label_id: 8)                                          1.16       0.22       0.37        459
--… (label_id: 9)                                          0.85       4.17       1.41         96
---------------------
--micro avg                                                5.54       5.54       5.54      34547
--macro avg                                                5.72       5.56       3.29      34547
--weighted avg                                            17.08       5.54       7.33      34547
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2114
---------------------
--micro avg                                              100.00     100.00     100.00       2114
--macro avg                                              100.00     100.00     100.00       2114
--weighted avg                                           100.00     100.00     100.00       2114
--
--[INFO] - Epoch 2, global step 1197: val_loss reached 0.86368 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=2.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.29       0.63       0.40       4444
--! (label_id: 1)                                          0.38      10.67       0.74        150
--, (label_id: 2)                                          2.32       0.59       0.94      15290
--- (label_id: 3)                                          2.34      20.28       4.19       1292
--. (label_id: 4)                                         43.85      11.68      18.44      12599
--: (label_id: 5)                                          0.41       1.28       0.62        392
--; (label_id: 6)                                          0.00       0.00       0.00        164
--? (label_id: 7)                                          4.24       6.30       5.07       1111
--— (label_id: 8)                                          0.00       0.00       0.00        456
--… (label_id: 9)                                          0.38       2.41       0.65         83
---------------------
--micro avg                                                5.40       5.40       5.40      35981
--macro avg                                                5.42       5.38       3.11      35981
--weighted avg                                            16.59       5.40       7.22      35981
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2222
---------------------
--micro avg                                              100.00     100.00     100.00       2222
--macro avg                                              100.00     100.00     100.00       2222
--weighted avg                                           100.00     100.00     100.00       2222
--
--[INFO] - Epoch 3, global step 1596: val_loss reached 0.86456 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=3.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.35       0.73       0.48       3844
--! (label_id: 1)                                          0.54      14.62       1.04        130
--, (label_id: 2)                                          2.32       0.59       0.94      13056
--- (label_id: 3)                                          2.67      22.28       4.77       1194
--. (label_id: 4)                                         44.45      11.95      18.84      10791
--: (label_id: 5)                                          0.84       3.21       1.33        280
--; (label_id: 6)                                          0.00       0.00       0.00        140
--? (label_id: 7)                                          4.17       6.56       5.10        914
--— (label_id: 8)                                          0.00       0.00       0.00        401
--… (label_id: 9)                                          0.48       2.63       0.81         76
---------------------
--micro avg                                                5.68       5.68       5.68      30826
--macro avg                                                5.58       6.26       3.33      30826
--weighted avg                                            16.82       5.68       7.41      30826
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1922
---------------------
--micro avg                                              100.00     100.00     100.00       1922
--macro avg                                              100.00     100.00     100.00       1922
--weighted avg                                           100.00     100.00     100.00       1922
--
--[INFO] - Epoch 4, global step 1995: val_loss reached 0.86369 (best 0.86333), saving model to "/home/nxingyu2/project/Punctuation_with_Domain_discriminator/2021-02-06_17-15-23/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.86-epoch=4.ckpt" as top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.28       0.60       0.39       3970
--! (label_id: 1)                                          0.35      10.66       0.68        122
--, (label_id: 2)                                          2.09       0.53       0.85      13469
--- (label_id: 3)                                          2.29      19.32       4.10       1201
--. (label_id: 4)                                         43.43      11.24      17.86      11227
--: (label_id: 5)                                          0.63       2.30       0.99        304
--; (label_id: 6)                                          0.00       0.00       0.00        141
--? (label_id: 7)                                          4.52       6.86       5.45       1006
--— (label_id: 8)                                          1.15       0.23       0.38        444
--… (label_id: 9)                                          0.45       2.67       0.78         75
---------------------
--micro avg                                                5.26       5.26       5.26      31959
--macro avg                                                5.52       5.44       3.15      31959
--weighted avg                                            16.42       5.26       7.02      31959
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       1985
---------------------
--micro avg                                              100.00     100.00     100.00       1985
--macro avg                                              100.00     100.00     100.00       1985
--weighted avg                                           100.00     100.00     100.00       1985
--
--[INFO] - Epoch 5, step 2394: val_loss was not in top 3
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.23       0.48       0.31       4126
--! (label_id: 1)                                          0.29       9.40       0.56        117
--, (label_id: 2)                                          1.91       0.49       0.77      14019
--- (label_id: 3)                                          2.52      22.59       4.53       1164
--. (label_id: 4)                                         44.15      11.65      18.44      11789
--: (label_id: 5)                                          0.72       2.41       1.11        332
--; (label_id: 6)                                          0.56       0.61       0.58        165
--? (label_id: 7)                                          3.89       6.53       4.88        980
--— (label_id: 8)                                          2.30       0.47       0.77        430
--… (label_id: 9)                                          1.18       8.33       2.07         60
---------------------
--micro avg                                                5.47       5.47       5.47      33182
--macro avg                                                5.77       6.30       3.40      33182
--weighted avg                                            16.77       5.47       7.25      33182
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00       2063
---------------------
--micro avg                                              100.00     100.00     100.00       2063
--macro avg                                              100.00     100.00     100.00       2063
--weighted avg                                           100.00     100.00     100.00       2063
--
--[INFO] - Epoch 6, step 2793: val_loss was not in top 3
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/hparams.yaml
deleted file mode 100644
index 081bb10..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/hparams.yaml
+++ /dev/null
@@ -1,107 +0,0 @@
-seed: 42
-trainer:
-  gpus: 1
-  num_nodes: 1
-  max_epochs: 2
-  max_steps: null
-  accumulate_grad_batches: 4
-  gradient_clip_val: 0
-  amp_level: O1
-  precision: 16
-  accelerator: ddp
-  checkpoint_callback: false
-  logger: false
-  log_every_n_steps: 1
-  val_check_interval: 1.0
-  resume_from_checkpoint: null
-exp_manager:
-  exp_dir: /home/nxingyu/project/
-  name: Punctuation_with_Domain_discriminator
-  create_tensorboard_logger: true
-  create_checkpoint_callback: true
-base_path: /home/nxingyu/data
-tmp_path: /home/nxingyu/data/tmp
-model:
-  nemo_path: null
-  transformer_path: google/electra-small-discriminator
-  unfrozen: 0
-  maximum_unfrozen: 2
-  unfreeze_step: 1
-  punct_label_ids:
-  - ''
-  - '!'
-  - ','
-  - '-'
-  - .
-  - ':'
-  - ;
-  - '?'
-  - —
-  - …
-  punct_class_weights: false
-  dataset:
-    data_dir: /home/nxingyu/data
-    labelled:
-    - /home/nxingyu/data/ted_talks_processed
-    unlabelled: null
-    max_seq_length: 128
-    pad_label: ''
-    ignore_extra_tokens: false
-    ignore_start_end: false
-    use_cache: false
-    num_workers: 0
-    pin_memory: true
-    drop_last: false
-    num_labels: 10
-    num_domains: 1
-    test_unlabelled: true
-    train_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 8
-    validation_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 8
-  tokenizer:
-    tokenizer_name: google/electra-small-discriminator
-    vocab_file: null
-    tokenizer_model: null
-    special_tokens: null
-  language_model:
-    pretrained_model_name: google/electra-small-discriminator
-    lm_checkpoint: null
-    config_file: null
-    config: null
-  punct_head:
-    punct_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: crf
-  domain_head:
-    domain_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: cel
-    gamma: 0.1
-  dice_loss:
-    epsilon: 0.01
-    alpha: 3
-    macro_average: true
-  focal_loss:
-    gamma: 5
-  optim:
-    name: adam
-    lr: 0.001
-    weight_decay: 0.0
-    sched:
-      name: WarmupAnnealing
-      warmup_steps: null
-      warmup_ratio: 0.1
-      last_epoch: -1
-      monitor: val_loss
-      reduce_on_plateau: false
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/lightning_logs.txt
deleted file mode 100644
index c01498c..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/lightning_logs.txt
+++ /dev/null
@@ -1,117 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
-Using native 16bit precision.
-Global seed set to 42
-initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | LinearChainCRF       | 120   
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-36.0 K    Trainable params
-13.4 M    Non-trainable params
-13.5 M    Total params
-Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/lr_find_temp_model.ckpt
-Global seed set to 42
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | LinearChainCRF       | 120   
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-36.0 K    Trainable params
-13.4 M    Non-trainable params
-13.5 M    Total params
-Epoch 0, global step 100: val_loss reached 28.76837 (best 28.76837), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/checkpoints/Punctuation_with_Domain_discriminator---val_loss=28.77-epoch=0.ckpt" as top 3
-Epoch 1, global step 200: val_loss reached 18.16606 (best 18.16606), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/checkpoints/Punctuation_with_Domain_discriminator---val_loss=18.17-epoch=1.ckpt" as top 3
-Global seed set to 42
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | LinearChainCRF       | 120   
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-825 K     Trainable params
-12.7 M    Non-trainable params
-13.5 M    Total params
-LR finder stopped early due to diverging loss.
-Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/lr_find_temp_model.ckpt
-Global seed set to 42
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | LinearChainCRF       | 120   
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-825 K     Trainable params
-12.7 M    Non-trainable params
-13.5 M    Total params
-Epoch 0, global step 301: val_loss reached 14.52603 (best 14.52603), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/checkpoints/Punctuation_with_Domain_discriminator---val_loss=14.53-epoch=0.ckpt" as top 3
-Epoch 1, global step 401: val_loss reached 15.01920 (best 14.52603), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/checkpoints/Punctuation_with_Domain_discriminator---val_loss=15.02-epoch=1.ckpt" as top 3
-Global seed set to 42
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | LinearChainCRF       | 120   
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-1.6 M     Trainable params
-11.9 M    Non-trainable params
-13.5 M    Total params
-LR finder stopped early due to diverging loss.
-Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/lr_find_temp_model.ckpt
-Global seed set to 42
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 257   
-3 | punctuation_loss    | LinearChainCRF       | 120   
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-1.6 M     Trainable params
-11.9 M    Non-trainable params
-13.5 M    Total params
-Epoch 0, global step 502: val_loss reached 13.53691 (best 13.53691), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/checkpoints/Punctuation_with_Domain_discriminator---val_loss=13.54-epoch=0.ckpt" as top 3
-Epoch 1, step 602: val_loss was not in top 3
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-Using environment variable NODE_RANK for node rank (0).
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/nemo_error_log.txt
deleted file mode 100644
index 1be0620..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/nemo_error_log.txt
+++ /dev/null
@@ -1,46 +0,0 @@
-[NeMo W 2021-02-08 07:56:46 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 07:56:55 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 07:56:55 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 07:56:57 nemo_logging:349] /home/nxingyu/project/experiment/core/classification_report.py:116: UserWarning: This overload of nonzero is deprecated:
-    	nonzero(Tensor input, *, Tensor out)
-    Consider using one of the following signatures instead:
-    	nonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629411241/work/torch/csrc/utils/python_arg_parser.cpp:766.)
-      num_non_empty_classes = torch.nonzero(self.num_examples_per_class).size(0)
-    
-[NeMo W 2021-02-08 07:56:57 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 07:57:02 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:118: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-    
-[NeMo W 2021-02-08 08:02:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 08:02:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 08:02:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 08:02:21 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:118: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-    
-[NeMo W 2021-02-08 08:08:50 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f0210aec880> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-08 08:09:19 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f02190dca90> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-08 08:23:36 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 09:11:17 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 09:11:50 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f0219122550> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 2ca2442..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,48 +0,0 @@
-[NeMo I 2021-02-08 07:56:46 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_07-56-46
-[NeMo I 2021-02-08 07:56:46 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-08 07:56:46 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 07:56:55 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 07:56:55 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 07:56:57 nemo_logging:349] /home/nxingyu/project/experiment/core/classification_report.py:116: UserWarning: This overload of nonzero is deprecated:
-    	nonzero(Tensor input, *, Tensor out)
-    Consider using one of the following signatures instead:
-    	nonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629411241/work/torch/csrc/utils/python_arg_parser.cpp:766.)
-      num_non_empty_classes = torch.nonzero(self.num_examples_per_class).size(0)
-    
-[NeMo W 2021-02-08 07:56:57 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 07:57:02 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:118: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-    
-[NeMo W 2021-02-08 08:02:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 08:02:14 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 08:02:15 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 08:02:21 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:118: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-    
-[NeMo W 2021-02-08 08:08:50 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f0210aec880> was reported to be 399 (when accessing len(dataloader)), but 400 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-08 08:09:19 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f02190dca90> was reported to be 49 (when accessing len(dataloader)), but 50 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-08 08:23:36 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 09:11:17 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 09:11:50 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:375: UserWarning: Length of IterableDataset <data.punctuation_dataset.PunctuationDomainDatasets object at 0x7f0219122550> was reported to be 50 (when accessing len(dataloader)), but 51 samples have been fetched. 
-      warnings.warn(warn_msg)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/events.out.tfevents.1612771686.Titan.22251.0 b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/events.out.tfevents.1612771686.Titan.22251.0
deleted file mode 100644
index 6b14ff7..0000000
Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/events.out.tfevents.1612771686.Titan.22251.0 and /dev/null differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/git-info.log
deleted file mode 100644
index e93204c..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/git-info.log
+++ /dev/null
@@ -1,181 +0,0 @@
-commit hash: d9cdb13829a1dfa2d74afb03fde5acec0f85d2cc
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/lightning_logs.txt
-index 2a26724..31870ad 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/lightning_logs.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/lightning_logs.txt
-@@ -20,3 +20,36 @@ initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
- 299 K     Trainable params
- 13.2 M    Non-trainable params
- 13.5 M    Total params
-+Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/lr_find_temp_model.ckpt
-+Global seed set to 42
-+
-+  | Name                | Type                 | Params
-+-------------------------------------------------------------
-+0 | transformer         | ElectraModel         | 13.5 M
-+1 | punct_classifier    | TokenClassifier      | 2.6 K 
-+2 | domain_classifier   | SequenceClassifier   | 1.0 K 
-+3 | punctuation_loss    | FocalDiceLoss        | 0     
-+4 | domain_loss         | CrossEntropyLoss     | 0     
-+5 | agg_loss            | AggregatorLoss       | 0     
-+6 | punct_class_report  | ClassificationReport | 0     
-+7 | domain_class_report | ClassificationReport | 0     
-+-------------------------------------------------------------
-+299 K     Trainable params
-+13.2 M    Non-trainable params
-+13.5 M    Total params
-+Global seed set to 42
-+
-+  | Name                | Type                 | Params
-+-------------------------------------------------------------
-+0 | transformer         | ElectraModel         | 13.5 M
-+1 | punct_classifier    | TokenClassifier      | 2.6 K 
-+2 | domain_classifier   | SequenceClassifier   | 1.0 K 
-+3 | punctuation_loss    | FocalDiceLoss        | 0     
-+4 | domain_loss         | CrossEntropyLoss     | 0     
-+5 | agg_loss            | AggregatorLoss       | 0     
-+6 | punct_class_report  | ClassificationReport | 0     
-+7 | domain_class_report | ClassificationReport | 0     
-+-------------------------------------------------------------
-+826 K     Trainable params
-+12.7 M    Non-trainable params
-+13.5 M    Total params
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_error_log.txt
-index 4ddbe1b..90e4c4c 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_error_log.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_error_log.txt
-@@ -2,3 +2,12 @@
- [NeMo W 2021-02-08 15:34:34 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-       warnings.warn(*args, **kwargs)
-     
-+[NeMo W 2021-02-08 15:46:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-+      warnings.warn(*args, **kwargs)
-+    
-+[NeMo W 2021-02-08 15:51:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-+      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-+    
-+[NeMo W 2021-02-08 15:57:37 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-+      warnings.warn(*args, **kwargs)
-+    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_log_globalrank-0_localrank-0.txt
-index dea36af..926854f 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_log_globalrank-0_localrank-0.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-08_15-34-03/nemo_log_globalrank-0_localrank-0.txt
-@@ -4,3 +4,12 @@
- [NeMo W 2021-02-08 15:34:34 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-       warnings.warn(*args, **kwargs)
-     
-+[NeMo W 2021-02-08 15:46:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-+      warnings.warn(*args, **kwargs)
-+    
-+[NeMo W 2021-02-08 15:51:35 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-+      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-+    
-+[NeMo W 2021-02-08 15:57:37 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-+      warnings.warn(*args, **kwargs)
-+    
-diff --git a/README.md b/README.md
-index beca3ef..ef7d22c 100644
---- a/README.md
-+++ b/README.md
-@@ -449,3 +449,7 @@ weighted avg                                            69.12      72.03      70
-  'punct_recall': 33.78831481933594,
-  'test_loss': 0.2638570964336395}
- 
-+
-+
-+### domain adversarial dice 3, open l ted ul 
-+initial_lr 0.007943282347242822
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index 070bc4f..37d105e 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -85,7 +85,7 @@ model:
-         train_ds:
-             shuffle: true
-             num_samples: -1
--            batch_size: 8
-+            batch_size: 4
- 
-         validation_ds:
-             # if evaluation data is not in the model.dataset.data_dir as the training data or multiple datasets are used for
-@@ -93,7 +93,7 @@ model:
-             # ds_item: null # expected format: [PATH_TO_DEV1,PATH_TO_DEV2] (Note no space between the paths and square brackets)
-             shuffle: true
-             num_samples: -1
--            batch_size: 8
-+            batch_size: 4
- 
-     tokenizer:
-         tokenizer_name: ${model.language_model.pretrained_model_name} # or sentencepiece
-@@ -123,7 +123,7 @@ model:
-         log_softmax: false
-         use_transformer_init: true
-         loss: 'cel'
--        gamma: 0 #0.1 # coefficient of gradient reversal
-+        gamma: 0.1 #0.1 # coefficient of gradient reversal
-         pooling: 'mean_max' # 'mean' mean_max
-         idx_conditioned_on: 0
-     
-diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
-index bc844cd..4027bb2 100644
---- a/experiment/data/punctuation_dataset_multi.py
-+++ b/experiment/data/punctuation_dataset_multi.py
-@@ -63,7 +63,8 @@ class PunctuationDomainDataset(IterableDataset):
-         self.randomize=randomize
-         self.target_file=target_file
-         self.tmp_path=tmp_path
--        os.system(f'cp {self.csv_file} {self.target_file}')
-+        if not (os.path.exists(self.target_file)):
-+            os.system(f'cp {self.csv_file} {self.target_file}')
- 
-     def __iter__(self):
-         self.dataset=iter(pd.read_csv(
-diff --git a/experiment/info.log b/experiment/info.log
-index 481b8ff..e69de29 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,43 +0,0 @@
--[INFO] - shuffling train set
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fd3155e83d0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        168
--! (label_id: 1)                                         14.29       7.69      10.00        104
--, (label_id: 2)                                         23.21      27.23      25.06        584
--- (label_id: 3)                                          4.02      46.67       7.39         45
--. (label_id: 4)                                         54.17       1.19       2.33       1091
--: (label_id: 5)                                          0.00       0.00       0.00          0
--; (label_id: 6)                                          0.00       0.00       0.00          0
--? (label_id: 7)                                          6.42      22.75      10.01        189
--— (label_id: 8)                                          0.00       0.00       0.00          0
--… (label_id: 9)                                          9.09       2.38       3.77         84
---------------------
--micro avg                                               10.86      10.86      10.86       2265
--macro avg                                               15.88      15.42       8.37       2265
--weighted avg                                            33.68      10.86       9.17       2265
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                         50.00     100.00      66.67         84
--1 (label_id: 1)                                          0.00       0.00       0.00         84
---------------------
--micro avg                                               50.00      50.00      50.00        168
--macro avg                                               25.00      50.00      33.33        168
--weighted avg                                            25.00      50.00      33.33        168
--
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/hparams.yaml
deleted file mode 100644
index ea7d4f7..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/hparams.yaml
+++ /dev/null
@@ -1,110 +0,0 @@
-seed: 42
-trainer:
-  gpus: 1
-  num_nodes: 1
-  max_epochs: 10
-  max_steps: null
-  accumulate_grad_batches: 4
-  gradient_clip_val: 0
-  amp_level: O1
-  precision: 16
-  accelerator: ddp
-  checkpoint_callback: false
-  logger: false
-  log_every_n_steps: 1
-  val_check_interval: 1.0
-  resume_from_checkpoint: null
-exp_manager:
-  exp_dir: /home/nxingyu/project/
-  name: Punctuation_with_Domain_discriminator
-  create_tensorboard_logger: true
-  create_checkpoint_callback: true
-base_path: /home/nxingyu/data
-tmp_path: /home/nxingyu/data/tmp
-model:
-  nemo_path: null
-  transformer_path: google/electra-small-discriminator
-  unfrozen: 0
-  maximum_unfrozen: 1
-  unfreeze_step: 1
-  punct_label_ids:
-  - ''
-  - '!'
-  - ','
-  - '-'
-  - .
-  - ':'
-  - ;
-  - '?'
-  - —
-  - …
-  punct_class_weights: false
-  dataset:
-    data_dir: /home/nxingyu/data
-    labelled:
-    - /home/nxingyu/data/open_subtitles_processed
-    unlabelled:
-    - /home/nxingyu/data/ted_talks_processed
-    max_seq_length: 128
-    pad_label: ''
-    ignore_extra_tokens: false
-    ignore_start_end: false
-    use_cache: false
-    num_workers: 4
-    pin_memory: true
-    drop_last: false
-    num_labels: 10
-    num_domains: 2
-    test_unlabelled: true
-    train_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 4
-    validation_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 4
-  tokenizer:
-    tokenizer_name: google/electra-small-discriminator
-    vocab_file: null
-    tokenizer_model: null
-    special_tokens: null
-  language_model:
-    pretrained_model_name: google/electra-small-discriminator
-    lm_checkpoint: null
-    config_file: null
-    config: null
-  punct_head:
-    punct_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: dice
-  domain_head:
-    domain_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: cel
-    gamma: 0.1
-    pooling: mean_max
-    idx_conditioned_on: 0
-  dice_loss:
-    epsilon: 0.01
-    alpha: 3
-    macro_average: true
-  focal_loss:
-    gamma: 1
-  optim:
-    name: adamw
-    lr: 0.001
-    weight_decay: 0.0
-    sched:
-      name: WarmupAnnealing
-      warmup_steps: null
-      warmup_ratio: 0.1
-      last_epoch: -1
-      monitor: val_loss
-      reduce_on_plateau: false
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/lightning_logs.txt
deleted file mode 100644
index 846b33e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/lightning_logs.txt
+++ /dev/null
@@ -1,40 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-Using native 16bit precision.
-Global seed set to 42
-initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 1.0 K 
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-299 K     Trainable params
-13.2 M    Non-trainable params
-13.5 M    Total params
-Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/lr_find_temp_model.ckpt
-Global seed set to 42
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 1.0 K 
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-299 K     Trainable params
-13.2 M    Non-trainable params
-13.5 M    Total params
-Epoch 0, global step 5305: val_loss reached 0.69762 (best 0.69762), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.70-epoch=0.ckpt" as top 3
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_error_log.txt
deleted file mode 100644
index 55977aa..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_error_log.txt
+++ /dev/null
@@ -1,16 +0,0 @@
-[NeMo W 2021-02-08 16:00:26 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 16:00:55 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 16:08:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 16:13:48 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-    
-[NeMo W 2021-02-08 17:54:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fcf47c80f70> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-08 18:08:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fcf47c80550> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 5270e5c..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,18 +0,0 @@
-[NeMo I 2021-02-08 16:00:26 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26
-[NeMo I 2021-02-08 16:00:26 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-08 16:00:26 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-08 16:00:55 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 16:08:07 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-08 16:13:48 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-    
-[NeMo W 2021-02-08 17:54:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fcf47c80f70> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-08 18:08:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fcf47c80550> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/events.out.tfevents.1612830887.Titan.6887.0 b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/events.out.tfevents.1612830887.Titan.6887.0
deleted file mode 100644
index cf34bb6..0000000
Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/events.out.tfevents.1612830887.Titan.6887.0 and /dev/null differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/git-info.log
deleted file mode 100644
index b6da706..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/git-info.log
+++ /dev/null
@@ -1,273 +0,0 @@
-commit hash: 08007e7bd84203d450e193af808686ac2c929dce
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/events.out.tfevents.1612771686.Titan.22251.0 b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/events.out.tfevents.1612771686.Titan.22251.0
-index 2a40109..6b14ff7 100644
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/events.out.tfevents.1612771686.Titan.22251.0 and b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/events.out.tfevents.1612771686.Titan.22251.0 differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/lightning_logs.txt
-index 439dccb..846b33e 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/lightning_logs.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/lightning_logs.txt
-@@ -37,3 +37,4 @@ Global seed set to 42
- 299 K     Trainable params
- 13.2 M    Non-trainable params
- 13.5 M    Total params
-+Epoch 0, global step 5305: val_loss reached 0.69762 (best 0.69762), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.70-epoch=0.ckpt" as top 3
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_error_log.txt
-index c85c2a3..55977aa 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_error_log.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_error_log.txt
-@@ -8,3 +8,9 @@
- [NeMo W 2021-02-08 16:13:48 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-     
-+[NeMo W 2021-02-08 17:54:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fcf47c80f70> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-+[NeMo W 2021-02-08 18:08:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fcf47c80550> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_log_globalrank-0_localrank-0.txt
-index b01f19c..5270e5c 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_log_globalrank-0_localrank-0.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-08_16-00-26/nemo_log_globalrank-0_localrank-0.txt
-@@ -10,3 +10,9 @@
- [NeMo W 2021-02-08 16:13:48 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-     
-+[NeMo W 2021-02-08 17:54:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fcf47c80f70> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-+[NeMo W 2021-02-08 18:08:38 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fcf47c80550> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/events.out.tfevents.1612772700.Titan.28938.0 b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/events.out.tfevents.1612772700.Titan.28938.0
-index ca85da6..04e8367 100644
-Binary files a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/events.out.tfevents.1612772700.Titan.28938.0 and b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/events.out.tfevents.1612772700.Titan.28938.0 differ
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/lightning_logs.txt
-index 50c4caa..0f48e2a 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/lightning_logs.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/lightning_logs.txt
-@@ -37,3 +37,29 @@ Global seed set to 42
- 299 K     Trainable params
- 13.2 M    Non-trainable params
- 13.5 M    Total params
-+Epoch 0, global step 5305: val_loss reached 0.20905 (best 0.20905), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.21-epoch=0.ckpt" as top 3
-+Epoch 1, global step 10610: val_loss reached 0.17265 (best 0.17265), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.17-epoch=1.ckpt" as top 3
-+Epoch 2, global step 15915: val_loss reached 0.05470 (best 0.05470), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.05-epoch=2.ckpt" as top 3
-+Epoch 3, global step 21220: val_loss reached 0.02875 (best 0.02875), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=0.03-epoch=3.ckpt" as top 3
-+Epoch 4, global step 26525: val_loss reached -0.03932 (best -0.03932), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.04-epoch=4.ckpt" as top 3
-+Epoch 5, global step 31830: val_loss reached -0.04410 (best -0.04410), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.04-epoch=5.ckpt" as top 3
-+Epoch 6, global step 37135: val_loss reached -0.04524 (best -0.04524), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.05-epoch=6.ckpt" as top 3
-+Epoch 7, global step 42440: val_loss reached -0.04689 (best -0.04689), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.05-epoch=7.ckpt" as top 3
-+Epoch 8, global step 47745: val_loss reached -0.04978 (best -0.04978), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.05-epoch=8.ckpt" as top 3
-+Epoch 9, global step 53050: val_loss reached -0.04850 (best -0.04978), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/checkpoints/Punctuation_with_Domain_discriminator---val_loss=-0.05-epoch=9.ckpt" as top 3
-+Global seed set to 42
-+
-+  | Name                | Type                 | Params
-+-------------------------------------------------------------
-+0 | transformer         | ElectraModel         | 13.5 M
-+1 | punct_classifier    | TokenClassifier      | 2.6 K 
-+2 | domain_classifier   | SequenceClassifier   | 513   
-+3 | punctuation_loss    | FocalDiceLoss        | 0     
-+4 | domain_loss         | CrossEntropyLoss     | 0     
-+5 | agg_loss            | AggregatorLoss       | 0     
-+6 | punct_class_report  | ClassificationReport | 0     
-+7 | domain_class_report | ClassificationReport | 0     
-+-------------------------------------------------------------
-+825 K     Trainable params
-+12.7 M    Non-trainable params
-+13.5 M    Total params
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_error_log.txt
-index 2503857..ce38185 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_error_log.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_error_log.txt
-@@ -8,3 +8,9 @@
- [NeMo W 2021-02-08 16:25:22 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-     
-+[NeMo W 2021-02-08 17:13:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fbd320e2b50> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-+[NeMo W 2021-02-08 17:20:28 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fbd3c4dd8e0> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_log_globalrank-0_localrank-0.txt
-index 48278bd..26f3ea7 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_log_globalrank-0_localrank-0.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-08_16-23-25/nemo_log_globalrank-0_localrank-0.txt
-@@ -10,3 +10,9 @@
- [NeMo W 2021-02-08 16:25:22 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-       warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-     
-+[NeMo W 2021-02-08 17:13:54 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fbd320e2b50> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-+[NeMo W 2021-02-08 17:20:28 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7fbd3c4dd8e0> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-diff --git a/experiment/config.yaml b/experiment/config.yaml
-index e492246..e7f6783 100644
---- a/experiment/config.yaml
-+++ b/experiment/config.yaml
-@@ -63,11 +63,11 @@ model:
-     dataset:
-         data_dir: /home/nxingyu/data # /root/data # 
-         labelled:
--            # - ${base_path}/ted_talks_processed #
--            - ${base_path}/open_subtitles_processed #  
-+            - ${base_path}/ted_talks_processed #
-+            # - ${base_path}/open_subtitles_processed #  
-         unlabelled:
-             # - ${base_path}/ted_talks_processed #
--            # - ${base_path}/open_subtitles_processed #  
-+            - ${base_path}/open_subtitles_processed #  
-             # parameters for dataset preprocessing
-         max_seq_length: 128
-         pad_label: ''
-@@ -79,7 +79,7 @@ model:
-         pin_memory: true
-         drop_last: false
-         num_labels: 10
--        num_domains: 1
-+        num_domains: 2
-         test_unlabelled: true
- 
-         train_ds:
-@@ -123,7 +123,7 @@ model:
-         log_softmax: false
-         use_transformer_init: true
-         loss: 'cel'
--        gamma: 0.1 #0.1 # coefficient of gradient reversal
-+        gamma: 0 #0.1 # coefficient of gradient reversal
-         pooling: 'mean_max' # 'mean' mean_max
-         idx_conditioned_on: 0
-     
-diff --git a/experiment/core/utils.py b/experiment/core/utils.py
-index 5b0efe3..058cc87 100644
---- a/experiment/core/utils.py
-+++ b/experiment/core/utils.py
-@@ -26,7 +26,7 @@ def position_to_mask(max_seq_length:int,indices:list):
-         o[np.array(indices)%(max_seq_length-2)+1]=1
-     except:
-         pp('position_to_mask',np.array(indices)%(max_seq_length-2)+1)
--        o[(np.array(indices)%(max_seq_length-2)+1).astype(int)]=1
-+        # o[(np.array(indices)%(max_seq_length-2)+1).astype(int)]=1
-     return o
- 
- def align_labels_to_mask(mask,labels):
-diff --git a/experiment/info.log b/experiment/info.log
-index 9e4b4d4..e69de29 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,83 +0,0 @@
--[INFO] - shuffling train set
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fbd32b00be0>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        232
--! (label_id: 1)                                          5.41       2.63       3.54        152
--, (label_id: 2)                                         24.37      27.46      25.82        772
--- (label_id: 3)                                          4.66      53.57       8.57         56
--. (label_id: 4)                                         43.75       0.85       1.67       1642
--: (label_id: 5)                                          0.00       0.00       0.00          0
--; (label_id: 6)                                          0.00       0.00       0.00          0
--? (label_id: 7)                                          4.76      22.94       7.89        218
--— (label_id: 8)                                          0.00       0.00       0.00          0
--… (label_id: 9)                                          8.33       2.04       3.28         98
---------------------
--micro avg                                                9.84       9.84       9.84       3170
--macro avg                                               13.04      15.64       7.25       3170
--weighted avg                                            29.52       9.84       8.12       3170
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        116
---------------------
--micro avg                                              100.00     100.00     100.00        116
--macro avg                                              100.00     100.00     100.00        116
--weighted avg                                           100.00     100.00     100.00        116
--
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.007943282347242822
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7fbd3350f790>" 
--will be used during training (effective maximum steps = 53050) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 53050
--)
--[INFO] - Punctuation report: 
--label                                                precision    recall       f1           support   
-- (label_id: 0)                                           0.00       0.00       0.00        232
--! (label_id: 1)                                          5.41       2.63       3.54        152
--, (label_id: 2)                                         24.37      27.46      25.82        772
--- (label_id: 3)                                          4.66      53.57       8.57         56
--. (label_id: 4)                                         43.75       0.85       1.67       1642
--: (label_id: 5)                                          0.00       0.00       0.00          0
--; (label_id: 6)                                          0.00       0.00       0.00          0
--? (label_id: 7)                                          4.76      22.94       7.89        218
--— (label_id: 8)                                          0.00       0.00       0.00          0
--… (label_id: 9)                                          8.33       2.04       3.28         98
---------------------
--micro avg                                                9.84       9.84       9.84       3170
--macro avg                                               13.04      15.64       7.25       3170
--weighted avg                                            29.52       9.84       8.12       3170
--
--[INFO] - Domain report: 
--label                                                precision    recall       f1           support   
--0 (label_id: 0)                                        100.00     100.00     100.00        116
---------------------
--micro avg                                              100.00     100.00     100.00        116
--macro avg                                              100.00     100.00     100.00        116
--weighted avg                                           100.00     100.00     100.00        116
--
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index 4ac54f4..c5db7b3 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -1,5 +1,6 @@
- # %%
- import copy
-+import math
- import logging
- import os
- from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union
-@@ -153,14 +154,15 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         Lightning calls this inside the training loop with the data from the training dataloader
-         passed in as `batch`.
-         """
--        p=(self.current_epoch*self.train_size+batch_idx)/(self.train_size*self.hparams.model.max_epochs)
--        self.grad_reverse.scale=2/(1+exp(-10*p))-1
-+        p=(self.current_epoch*self.train_size+batch_idx)/(self.train_size*self.hparams.trainer.max_epochs)
-+        self.grad_reverse.scale=2/(1+math.exp(-10*p))-1
-         loss, _, _ = self._make_step(batch)
-         lr = self._optimizer.param_groups[0]['lr']
- 
- 
-         self.log('lr', lr, prog_bar=True)
-         self.log('train_loss', loss)
-+        self.log('gamma', self.grad_reverse.scale)
- 
-         return {'loss': loss, 'lr': lr}
- 
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/hparams.yaml
deleted file mode 100644
index cbac11e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/hparams.yaml
+++ /dev/null
@@ -1,110 +0,0 @@
-seed: 42
-trainer:
-  gpus: 1
-  num_nodes: 1
-  max_epochs: 10
-  max_steps: null
-  accumulate_grad_batches: 4
-  gradient_clip_val: 0
-  amp_level: O1
-  precision: 16
-  accelerator: ddp
-  checkpoint_callback: false
-  logger: false
-  log_every_n_steps: 1
-  val_check_interval: 1.0
-  resume_from_checkpoint: null
-exp_manager:
-  exp_dir: /home/nxingyu/project/
-  name: Punctuation_with_Domain_discriminator
-  create_tensorboard_logger: true
-  create_checkpoint_callback: true
-base_path: /home/nxingyu/data
-tmp_path: /home/nxingyu/data/tmp
-model:
-  nemo_path: null
-  transformer_path: google/electra-small-discriminator
-  unfrozen: 0
-  maximum_unfrozen: 1
-  unfreeze_step: 1
-  punct_label_ids:
-  - ''
-  - '!'
-  - ','
-  - '-'
-  - .
-  - ':'
-  - ;
-  - '?'
-  - —
-  - …
-  punct_class_weights: false
-  dataset:
-    data_dir: /home/nxingyu/data
-    labelled:
-    - /home/nxingyu/data/ted_talks_processed
-    unlabelled:
-    - /home/nxingyu/data/open_subtitles_processed
-    max_seq_length: 128
-    pad_label: ''
-    ignore_extra_tokens: false
-    ignore_start_end: false
-    use_cache: false
-    num_workers: 4
-    pin_memory: true
-    drop_last: false
-    num_labels: 10
-    num_domains: 2
-    test_unlabelled: true
-    train_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 4
-    validation_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 4
-  tokenizer:
-    tokenizer_name: google/electra-small-discriminator
-    vocab_file: null
-    tokenizer_model: null
-    special_tokens: null
-  language_model:
-    pretrained_model_name: google/electra-small-discriminator
-    lm_checkpoint: null
-    config_file: null
-    config: null
-  punct_head:
-    punct_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: dice
-  domain_head:
-    domain_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: cel
-    gamma: 0
-    pooling: mean_max
-    idx_conditioned_on: 0
-  dice_loss:
-    epsilon: 0.01
-    alpha: 3
-    macro_average: true
-  focal_loss:
-    gamma: 1
-  optim:
-    name: adamw
-    lr: 0.001
-    weight_decay: 0.0
-    sched:
-      name: WarmupAnnealing
-      warmup_steps: null
-      warmup_ratio: 0.1
-      last_epoch: -1
-      monitor: val_loss
-      reduce_on_plateau: false
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt
deleted file mode 100644
index 5be2535..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt
+++ /dev/null
@@ -1,40 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-Using native 16bit precision.
-Global seed set to 42
-initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 1.0 K 
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-299 K     Trainable params
-13.2 M    Non-trainable params
-13.5 M    Total params
-Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lr_find_temp_model.ckpt
-Global seed set to 42
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 1.0 K 
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-299 K     Trainable params
-13.2 M    Non-trainable params
-13.5 M    Total params
-Epoch 0, global step 5305: val_loss reached 10.18683 (best 10.18683), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/checkpoints/Punctuation_with_Domain_discriminator---val_loss=10.19-epoch=0.ckpt" as top 3
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt
deleted file mode 100644
index 2f2fa91..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt
+++ /dev/null
@@ -1,19 +0,0 @@
-[NeMo W 2021-02-09 08:26:56 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-09 08:27:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-09 08:34:47 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-09 08:40:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-    
-[NeMo W 2021-02-09 10:21:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fd0> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-09 10:36:30 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fa0> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-09 11:07:22 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 2546dc9..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,21 +0,0 @@
-[NeMo I 2021-02-09 08:26:56 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56
-[NeMo I 2021-02-09 08:26:56 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-09 08:26:56 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-09 08:27:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-09 08:34:47 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-09 08:40:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-    
-[NeMo W 2021-02-09 10:21:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fd0> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-09 10:36:30 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fa0> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-      warnings.warn(warn_msg)
-    
-[NeMo W 2021-02-09 11:07:22 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-      warnings.warn(*args, **kwargs)
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/cmd-args.log b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/cmd-args.log
deleted file mode 100644
index 11a5d8e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/cmd-args.log
+++ /dev/null
@@ -1 +0,0 @@
-main.py
\ No newline at end of file
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/events.out.tfevents.1612840713.Titan.20119.0 b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/events.out.tfevents.1612840713.Titan.20119.0
deleted file mode 100644
index a4eb1d2..0000000
Binary files a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/events.out.tfevents.1612840713.Titan.20119.0 and /dev/null differ
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/git-info.log b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/git-info.log
deleted file mode 100644
index 5420ee2..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/git-info.log
+++ /dev/null
@@ -1,645 +0,0 @@
-commit hash: 089ad1caa03e468560d6d322ace7a5164a8178f3
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt
-index 2a26724..5be2535 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lightning_logs.txt
-@@ -20,3 +20,21 @@ initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
- 299 K     Trainable params
- 13.2 M    Non-trainable params
- 13.5 M    Total params
-+Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/lr_find_temp_model.ckpt
-+Global seed set to 42
-+
-+  | Name                | Type                 | Params
-+-------------------------------------------------------------
-+0 | transformer         | ElectraModel         | 13.5 M
-+1 | punct_classifier    | TokenClassifier      | 2.6 K 
-+2 | domain_classifier   | SequenceClassifier   | 1.0 K 
-+3 | punctuation_loss    | FocalDiceLoss        | 0     
-+4 | domain_loss         | CrossEntropyLoss     | 0     
-+5 | agg_loss            | AggregatorLoss       | 0     
-+6 | punct_class_report  | ClassificationReport | 0     
-+7 | domain_class_report | ClassificationReport | 0     
-+-------------------------------------------------------------
-+299 K     Trainable params
-+13.2 M    Non-trainable params
-+13.5 M    Total params
-+Epoch 0, global step 5305: val_loss reached 10.18683 (best 10.18683), saving model to "/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/checkpoints/Punctuation_with_Domain_discriminator---val_loss=10.19-epoch=0.ckpt" as top 3
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt
-index 0f1c742..2f2fa91 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_error_log.txt
-@@ -2,3 +2,18 @@
- [NeMo W 2021-02-09 08:27:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-       warnings.warn(*args, **kwargs)
-     
-+[NeMo W 2021-02-09 08:34:47 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-+      warnings.warn(*args, **kwargs)
-+    
-+[NeMo W 2021-02-09 08:40:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-+      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-+    
-+[NeMo W 2021-02-09 10:21:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fd0> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-+[NeMo W 2021-02-09 10:36:30 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fa0> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-+[NeMo W 2021-02-09 11:07:22 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-+      warnings.warn(*args, **kwargs)
-+    
-diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt
-index e609b5b..2546dc9 100644
---- a/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt
-+++ b/Punctuation_with_Domain_discriminator/2021-02-09_08-26-56/nemo_log_globalrank-0_localrank-0.txt
-@@ -4,3 +4,18 @@
- [NeMo W 2021-02-09 08:27:41 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-       warnings.warn(*args, **kwargs)
-     
-+[NeMo W 2021-02-09 08:34:47 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-+      warnings.warn(*args, **kwargs)
-+    
-+[NeMo W 2021-02-09 08:40:20 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-+      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-+    
-+[NeMo W 2021-02-09 10:21:59 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fd0> was reported to be 21219 (when accessing len(dataloader)), but 21220 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-+[NeMo W 2021-02-09 10:36:30 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/utils/data/dataloader.py:447: UserWarning: Length of IterableDataset <data.punctuation_dataset_multi.PunctuationDomainDatasets object at 0x7f4140b06fa0> was reported to be 2652 (when accessing len(dataloader)), but 2653 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.
-+      warnings.warn(warn_msg)
-+    
-+[NeMo W 2021-02-09 11:07:22 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
-+      warnings.warn(*args, **kwargs)
-+    
-diff --git a/experiment/Nemo2Lightning.ipynb b/experiment/Nemo2Lightning.ipynb
-index d2ec988..0dbd499 100644
---- a/experiment/Nemo2Lightning.ipynb
-+++ b/experiment/Nemo2Lightning.ipynb
-@@ -2,7 +2,7 @@
-  "cells": [
-   {
-    "cell_type": "code",
--   "execution_count": 2,
-+   "execution_count": 1,
-    "metadata": {},
-    "outputs": [
-     {
-@@ -11,7 +11,7 @@
-      "text": [
-       "Using device: cuda\n",
-       "\n",
--      "Tesla T4\n",
-+      "GeForce GTX 1080 Ti\n",
-       "Memory Usage:\n",
-       "Allocated: 0.0 GB\n",
-       "Cached:    0.0 GB\n"
-@@ -33,16 +33,16 @@
-   },
-   {
-    "cell_type": "code",
--   "execution_count": 1,
-+   "execution_count": 2,
-    "metadata": {},
-    "outputs": [
-     {
-      "data": {
-       "text/plain": [
--       "{'seed': 42, 'trainer': {'gpus': 1, 'num_nodes': 1, 'max_epochs': 6, 'max_steps': None, 'accumulate_grad_batches': 8, 'gradient_clip_val': 0, 'amp_level': 'O1', 'precision': 16, 'accelerator': 'ddp', 'checkpoint_callback': False, 'logger': False, 'log_every_n_steps': 1, 'val_check_interval': 1.0, 'resume_from_checkpoint': None}, 'exp_manager': {'exp_dir': '/home/nxingyu2/project/', 'name': 'Punctuation_with_Domain_discriminator', 'create_tensorboard_logger': True, 'create_checkpoint_callback': True}, 'base_path': '/home/nxingyu2/data', 'tmp_path': '/home/nxingyu2/data/tmp', 'model': {'nemo_path': None, 'transformer_path': 'google/electra-base-discriminator', 'initial_unfrozen': 1, 'punct_label_ids': ['', '!', ',', '-', '.', ':', ';', '?', '—', '…'], 'dataset': {'data_dir': '/home/nxingyu2/data', 'labelled': ['${base_path}/open_subtitles_processed'], 'unlabelled': ['${base_path}/ted_talks_processed'], 'max_seq_length': 128, 'pad_label': '', 'ignore_extra_tokens': False, 'ignore_start_end': False, 'use_cache': False, 'num_workers': 0, 'pin_memory': True, 'drop_last': False, 'num_labels': 10, 'num_domains': 1, 'test_unlabelled': True, 'train_ds': {'shuffle': True, 'num_samples': -1, 'batch_size': 8}, 'validation_ds': {'shuffle': True, 'num_samples': -1, 'batch_size': 2}}, 'tokenizer': {'tokenizer_name': '${model.language_model.pretrained_model_name}', 'vocab_file': None, 'tokenizer_model': None, 'special_tokens': None}, 'language_model': {'pretrained_model_name': '${model.transformer_path}', 'lm_checkpoint': None, 'config_file': None, 'config': None}, 'punct_head': {'punct_num_fc_layers': 1, 'fc_dropout': 0.1, 'activation': 'relu', 'log_softmax': False, 'use_transformer_init': True, 'loss': 'crf'}, 'domain_head': {'domain_num_fc_layers': 1, 'fc_dropout': 0.1, 'activation': 'relu', 'log_softmax': False, 'use_transformer_init': True, 'loss': 'cel', 'gamma': 0.1}, 'dice_loss': {'epsilon': 0.01, 'alpha': 3, 'macro_average': True}, 'focal_loss': {'gamma': 5}, 'optim': {'name': 'adamw', 'lr': 0.001, 'weight_decay': 0.0, 'sched': {'name': 'WarmupAnnealing', 'warmup_steps': None, 'warmup_ratio': 0.1, 'last_epoch': -1, 'monitor': 'val_loss', 'reduce_on_plateau': False}}}}"
-+       "{'seed': 42, 'trainer': {'gpus': 1, 'num_nodes': 1, 'max_epochs': 10, 'max_steps': None, 'accumulate_grad_batches': 4, 'gradient_clip_val': 0, 'amp_level': 'O1', 'precision': 16, 'accelerator': 'ddp', 'checkpoint_callback': False, 'logger': False, 'log_every_n_steps': 1, 'val_check_interval': 1.0, 'resume_from_checkpoint': None}, 'exp_manager': {'exp_dir': '/home/nxingyu/project/', 'name': 'Punctuation_with_Domain_discriminator', 'create_tensorboard_logger': True, 'create_checkpoint_callback': True}, 'base_path': '/home/nxingyu/data', 'tmp_path': '/home/nxingyu/data/tmp', 'model': {'nemo_path': None, 'transformer_path': 'google/electra-small-discriminator', 'unfrozen': 0, 'maximum_unfrozen': 1, 'unfreeze_step': 1, 'punct_label_ids': ['', '!', ',', '-', '.', ':', ';', '?', '—', '…'], 'punct_class_weights': False, 'dataset': {'data_dir': '/home/nxingyu/data', 'labelled': ['${base_path}/ted_talks_processed'], 'unlabelled': ['${base_path}/open_subtitles_processed'], 'max_seq_length': 128, 'pad_label': '', 'ignore_extra_tokens': False, 'ignore_start_end': False, 'use_cache': False, 'num_workers': 4, 'pin_memory': True, 'drop_last': False, 'num_labels': 10, 'num_domains': 2, 'test_unlabelled': True, 'train_ds': {'shuffle': True, 'num_samples': -1, 'batch_size': 4}, 'validation_ds': {'shuffle': True, 'num_samples': -1, 'batch_size': 4}}, 'tokenizer': {'tokenizer_name': '${model.language_model.pretrained_model_name}', 'vocab_file': None, 'tokenizer_model': None, 'special_tokens': None}, 'language_model': {'pretrained_model_name': '${model.transformer_path}', 'lm_checkpoint': None, 'config_file': None, 'config': None}, 'punct_head': {'punct_num_fc_layers': 1, 'fc_dropout': 0.1, 'activation': 'relu', 'log_softmax': False, 'use_transformer_init': True, 'loss': 'dice'}, 'domain_head': {'domain_num_fc_layers': 1, 'fc_dropout': 0.1, 'activation': 'relu', 'log_softmax': False, 'use_transformer_init': True, 'loss': 'cel', 'gamma': 0, 'pooling': 'mean_max', 'idx_conditioned_on': 0}, 'dice_loss': {'epsilon': 0.01, 'alpha': 3, 'macro_average': True}, 'focal_loss': {'gamma': 1}, 'optim': {'name': 'adamw', 'lr': 0.001, 'weight_decay': 0.0, 'sched': {'name': 'WarmupAnnealing', 'warmup_steps': None, 'warmup_ratio': 0.1, 'last_epoch': -1, 'monitor': 'val_loss', 'reduce_on_plateau': False}}}}"
-       ]
-      },
--     "execution_count": 1,
-+     "execution_count": 2,
-      "metadata": {},
-      "output_type": "execute_result"
-     }
-@@ -74,79 +74,79 @@
-   },
-   {
-    "cell_type": "code",
--   "execution_count": 13,
-+   "execution_count": 3,
-    "metadata": {},
-    "outputs": [
-     {
-      "name": "stderr",
-      "output_type": "stream",
-      "text": [
--      "09:01:12.28 LOG:\n",
--      "09:01:12.30 .... 'cel none' = 'cel none'\n",
--      "09:01:12.31 .... output = tensor([1.5514, 1.5514, 0.5514, 0.5514], grad_fn=<NllLossBackward>)\n",
--      "09:01:12.31 LOG:\n",
--      "09:01:12.31 .... 'cel mean' = 'cel mean'\n",
--      "09:01:12.31 .... output = tensor(1.0514, grad_fn=<NllLossBackward>)\n",
--      "09:01:12.31 LOG:\n",
--      "09:01:12.31 .... 'cel sum' = 'cel sum'\n",
--      "09:01:12.31 .... output = tensor(4.2058, grad_fn=<NllLossBackward>)\n",
--      "09:01:12.31 LOG:\n",
--      "09:01:12.32 .... 'focal sum' = 'focal sum'\n",
--      "09:01:12.32 .... loss(inp, tar) = tensor(6.7352, grad_fn=<SumBackward0>)\n",
--      "09:01:12.32 LOG:\n",
--      "09:01:12.32 .... 'focal mean' = 'focal mean'\n",
--      "09:01:12.32 .... loss(inp, tar) = tensor(0.4210, grad_fn=<MeanBackward0>)\n",
--      "09:01:12.32 LOG:\n",
--      "09:01:12.33 .... 'focal none' = 'focal none'\n",
--      "09:01:12.33 .... loss(inp, tar) = tensor([0.9635, 0.9635, 0.0991, 0.0991], grad_fn=<MulBackward0>)\n",
--      "09:01:12.33 LOG:\n",
--      "09:01:12.33 .... 'focal none' = 'focal none'\n",
--      "09:01:12.33 .... loss(inp, tar) = tensor([0.4716, 0.4716, 0.0075, 0.0075], grad_fn=<MulBackward0>)\n",
--      "09:01:12.33 LOG:\n",
--      "09:01:12.34 .... 'crf,none' = 'crf,none'\n",
--      "09:01:12.34 .... output = tensor([4.2927], grad_fn=<NegBackward>)\n",
--      "09:01:12.34 LOG:\n",
--      "09:01:12.34 .... 'crf,mean' = 'crf,mean'\n",
--      "09:01:12.34 .... output = tensor(4.3138, grad_fn=<NegBackward>)\n",
--      "09:01:12.34 LOG:\n",
--      "09:01:12.35 .... 'crf,sum' = 'crf,sum'\n",
--      "09:01:12.35 .... output = tensor(4.2588, grad_fn=<NegBackward>)\n",
--      "09:01:12.35 LOG:\n",
--      "09:01:12.35 .... 'crf,token_mean' = 'crf,token_mean'\n",
--      "09:01:12.35 .... output = tensor(1.0429, grad_fn=<DivBackward0>)\n",
--      "09:01:12.35 LOG:\n",
--      "09:01:12.35 .... 'dice none,micro' = 'dice none,micro'\n",
--      "09:01:12.36 .... output = tensor(0.1876, grad_fn=<PowBackward0>)\n",
--      "09:01:12.36 LOG:\n",
--      "09:01:12.36 .... 'dice mean,micro' = 'dice mean,micro'\n",
--      "09:01:12.36 .... output = tensor(0.0625, grad_fn=<DivBackward0>)\n",
--      "09:01:12.36 LOG:\n",
--      "09:01:12.36 .... 'dice sum,micro' = 'dice sum,micro'\n",
--      "09:01:12.36 .... output = tensor(0.1876, grad_fn=<SumBackward0>)\n",
--      "09:01:12.37 LOG:\n",
--      "09:01:12.37 .... 'dice sum,micro' = 'dice sum,micro'\n",
--      "09:01:12.37 .... output = tensor(0.0812, grad_fn=<SumBackward0>)\n",
--      "09:01:12.37 LOG:\n",
--      "09:01:12.37 .... 'dice none,macro' = 'dice none,macro'\n",
--      "09:01:12.37 .... loss(inp, tar) = tensor([5.9547, 4.4872, 2.0551], grad_fn=<MulBackward0>)\n",
--      "09:01:12.37 LOG:\n",
--      "09:01:12.38 .... 'dice mean,macro' = 'dice mean,macro'\n",
--      "09:01:12.38 .... loss(inp, tar) = tensor(0.2112, grad_fn=<DivBackward0>)\n",
--      "09:01:12.38 LOG:\n",
--      "09:01:12.38 .... 'dice sum,macro' = 'dice sum,macro'\n",
--      "09:01:12.38 .... loss(inp, tar) = tensor(0.6335, grad_fn=<SumBackward0>)\n",
--      "09:01:12.38 LOG:\n",
--      "09:01:12.38 .... 'dice none,macro' = 'dice none,macro'\n",
--      "09:01:12.39 .... loss(inp, tar) = tensor([0.3340, 0.6546, 0.3057], grad_fn=<MulBackward0>)\n",
--      "09:01:12.39 LOG:\n",
--      "09:01:12.39 .... 'dice none,macro' = 'dice none,macro'\n",
--      "09:01:12.39 .... loss(inp, tar) = tensor([0.0373, 0.2805, 0.0286], grad_fn=<MulBackward0>)\n",
--      "09:01:12.39 LOG:\n",
--      "09:01:12.39 .... 'dice sum,macro' = 'dice sum,macro'\n",
--      "09:01:12.40 .... output = tensor([0.5989, 0.7696, 0.2411], grad_fn=<MulBackward0>)\n",
--      "09:01:12.40 LOG:\n",
--      "09:01:12.40 .... 'dice sum,macro' = 'dice sum,macro'\n",
--      "09:01:12.40 .... output = tensor([0.2148, 0.4559, 0.0140], grad_fn=<MulBackward0>)\n"
-+      "10:11:13.98 LOG:\n",
-+      "10:11:14.02 .... 'cel none' = 'cel none'\n",
-+      "10:11:14.02 .... output = tensor([1.5514, 1.5514, 0.5514, 0.5514], grad_fn=<NllLossBackward>)\n",
-+      "10:11:14.02 LOG:\n",
-+      "10:11:14.02 .... 'cel mean' = 'cel mean'\n",
-+      "10:11:14.02 .... output = tensor(1.0514, grad_fn=<NllLossBackward>)\n",
-+      "10:11:14.03 LOG:\n",
-+      "10:11:14.03 .... 'cel sum' = 'cel sum'\n",
-+      "10:11:14.03 .... output = tensor(4.2058, grad_fn=<NllLossBackward>)\n",
-+      "10:11:14.08 LOG:\n",
-+      "10:11:14.08 .... 'focal sum' = 'focal sum'\n",
-+      "10:11:14.08 .... loss(inp, tar) = tensor(6.7352, grad_fn=<SumBackward0>)\n",
-+      "10:11:14.08 LOG:\n",
-+      "10:11:14.08 .... 'focal mean' = 'focal mean'\n",
-+      "10:11:14.09 .... loss(inp, tar) = tensor(0.4210, grad_fn=<MeanBackward0>)\n",
-+      "10:11:14.09 LOG:\n",
-+      "10:11:14.09 .... 'focal none' = 'focal none'\n",
-+      "10:11:14.10 .... loss(inp, tar) = tensor([0.9635, 0.9635, 0.0991, 0.0991], grad_fn=<MulBackward0>)\n",
-+      "10:11:14.10 LOG:\n",
-+      "10:11:14.10 .... 'focal none' = 'focal none'\n",
-+      "10:11:14.10 .... loss(inp, tar) = tensor([0.4716, 0.4716, 0.0075, 0.0075], grad_fn=<MulBackward0>)\n",
-+      "10:11:14.11 LOG:\n",
-+      "10:11:14.12 .... 'crf,none' = 'crf,none'\n",
-+      "10:11:14.12 .... output = tensor([4.1689], grad_fn=<NegBackward>)\n",
-+      "10:11:14.12 LOG:\n",
-+      "10:11:14.12 .... 'crf,mean' = 'crf,mean'\n",
-+      "10:11:14.12 .... output = tensor(4.0544, grad_fn=<NegBackward>)\n",
-+      "10:11:14.12 LOG:\n",
-+      "10:11:14.13 .... 'crf,sum' = 'crf,sum'\n",
-+      "10:11:14.13 .... output = tensor(4.1272, grad_fn=<NegBackward>)\n",
-+      "10:11:14.13 LOG:\n",
-+      "10:11:14.13 .... 'crf,token_mean' = 'crf,token_mean'\n",
-+      "10:11:14.13 .... output = tensor(1.0815, grad_fn=<DivBackward0>)\n",
-+      "10:11:14.13 LOG:\n",
-+      "10:11:14.14 .... 'dice none,micro' = 'dice none,micro'\n",
-+      "10:11:14.14 .... output = tensor(0.4331, grad_fn=<PowBackward0>)\n",
-+      "10:11:14.14 LOG:\n",
-+      "10:11:14.14 .... 'dice mean,micro' = 'dice mean,micro'\n",
-+      "10:11:14.14 .... output = tensor(0.1444, grad_fn=<DivBackward0>)\n",
-+      "10:11:14.14 LOG:\n",
-+      "10:11:14.14 .... 'dice sum,micro' = 'dice sum,micro'\n",
-+      "10:11:14.15 .... output = tensor(0.4331, grad_fn=<SumBackward0>)\n",
-+      "10:11:14.15 LOG:\n",
-+      "10:11:14.15 .... 'dice sum,micro' = 'dice sum,micro'\n",
-+      "10:11:14.15 .... output = tensor(0.0812, grad_fn=<SumBackward0>)\n",
-+      "10:11:14.15 LOG:\n",
-+      "10:11:14.15 .... 'dice none,macro' = 'dice none,macro'\n",
-+      "10:11:14.16 .... loss(inp, tar) = tensor([5.9547, 4.4872, 2.0551], grad_fn=<MulBackward0>)\n",
-+      "10:11:14.16 LOG:\n",
-+      "10:11:14.16 .... 'dice mean,macro' = 'dice mean,macro'\n",
-+      "10:11:14.16 .... loss(inp, tar) = tensor(0.4314, grad_fn=<DivBackward0>)\n",
-+      "10:11:14.16 LOG:\n",
-+      "10:11:14.16 .... 'dice sum,macro' = 'dice sum,macro'\n",
-+      "10:11:14.16 .... loss(inp, tar) = tensor(1.2943, grad_fn=<SumBackward0>)\n",
-+      "10:11:14.17 LOG:\n",
-+      "10:11:14.17 .... 'dice none,macro' = 'dice none,macro'\n",
-+      "10:11:14.17 .... loss(inp, tar) = tensor([0.3340, 0.6546, 0.3057], grad_fn=<MulBackward0>)\n",
-+      "10:11:14.17 LOG:\n",
-+      "10:11:14.17 .... 'dice none,macro' = 'dice none,macro'\n",
-+      "10:11:14.17 .... loss(inp, tar) = tensor([0.0373, 0.2805, 0.0286], grad_fn=<MulBackward0>)\n",
-+      "10:11:14.18 LOG:\n",
-+      "10:11:14.18 .... 'dice sum,macro' = 'dice sum,macro'\n",
-+      "10:11:14.18 .... output = tensor([0.5989, 0.7696, 0.2411], grad_fn=<MulBackward0>)\n",
-+      "10:11:14.18 LOG:\n",
-+      "10:11:14.18 .... 'dice sum,macro' = 'dice sum,macro'\n",
-+      "10:11:14.18 .... output = tensor([0.2148, 0.4559, 0.0140], grad_fn=<MulBackward0>)\n"
-      ]
-     },
-     {
-@@ -155,7 +155,7 @@
-        "('dice sum,macro', tensor([0.2148, 0.4559, 0.0140], grad_fn=<MulBackward0>))"
-       ]
-      },
--     "execution_count": 13,
-+     "execution_count": 3,
-      "metadata": {},
-      "output_type": "execute_result"
-     }
-@@ -286,32 +286,25 @@
-   },
-   {
-    "cell_type": "code",
--   "execution_count": 2,
-+   "execution_count": 4,
-    "metadata": {},
-    "outputs": [
-     {
--     "name": "stderr",
--     "output_type": "stream",
--     "text": [
--      "10:05:46.40 LOG:\n",
--      "10:05:46.46 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
--      "10:05:46.66 LOG:\n",
--      "10:05:46.66 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
--      "10:06:04.19 LOG:\n",
--      "10:06:04.20 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
--      "10:06:04.34 LOG:\n",
--      "10:06:04.34 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n"
-+     "ename": "KeyboardInterrupt",
-+     "evalue": "",
-+     "output_type": "error",
-+     "traceback": [
-+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
-+      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
-+      "\u001b[0;32m<ipython-input-4-b22a0d7714b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mdata_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-+      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_prepared_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-+      "\u001b[0;32m~/project/experiment/data/punctuation_datamodule.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'fit'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             self.train_dataset = PunctuationDomainDatasets(split='train',\n\u001b[0m\u001b[1;32m     64\u001b[0m                     \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                     \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-+      "\u001b[0;32m~/project/experiment/data/punctuation_dataset_multi.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, split, num_samples, max_seq_length, punct_label_ids, labelled, unlabelled, tokenizer, randomize, data_id, tmp_path)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munlabelled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             dataset=PunctuationDomainDataset(\n\u001b[0m\u001b[1;32m    187\u001b[0m                     \u001b[0mcsv_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'{path}.{split}.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                     \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-+      "\u001b[0;32m~/project/experiment/data/punctuation_dataset_multi.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, csv_file, tokenizer, num_samples, max_seq_length, degree, punct_label_ids, domain, labelled, randomize, target_file, tmp_path, start, end)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv_file\u001b[0m \u001b[0;34m=\u001b[0m   \u001b[0mcsv_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m   \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m=\u001b[0m  \u001b[0mdomain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunct_label_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpunct_label_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-+      "\u001b[0;32m~/project/experiment/data/punctuation_dataset_multi.py\u001b[0m in \u001b[0;36mset_num_samples\u001b[0;34m(self, csv_file, num_samples)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-l'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTDOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_samples\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
-+      "\u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1009\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stdin_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m                 \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-+      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
-      ]
--    },
--    {
--     "data": {
--      "text/plain": [
--       "10609"
--      ]
--     },
--     "execution_count": 2,
--     "metadata": {},
--     "output_type": "execute_result"
-     }
-    ],
-    "source": [
-@@ -343,20 +336,9 @@
-   },
-   {
-    "cell_type": "code",
--   "execution_count": 4,
-+   "execution_count": null,
-    "metadata": {},
--   "outputs": [
--    {
--     "data": {
--      "text/plain": [
--       "10609"
--      ]
--     },
--     "execution_count": 4,
--     "metadata": {},
--     "output_type": "execute_result"
--    }
--   ],
-+   "outputs": [],
-    "source": [
-     "# it=dm.train_dataset\n",
-     "# ni=next(it)\n",
-diff --git a/experiment/core/losses/linear_chain_crf.py b/experiment/core/losses/linear_chain_crf.py
-index ed813a9..8dc59cc 100644
---- a/experiment/core/losses/linear_chain_crf.py
-+++ b/experiment/core/losses/linear_chain_crf.py
-@@ -92,6 +92,17 @@ class LinearChainCRF(torch.nn.Module):
-             mask = logits.new_ones(logits.shape[:2], dtype=torch.bool)
-         return self._viterbi_decode(logits,mask)
- 
-+    @jit.export
-+    def predict(self, logits: Tensor, mask: Optional[Tensor] = None) -> LongTensor:
-+        self._validate(logits, mask=mask)
-+
-+        if mask is None:
-+            mask = logits.new_ones(logits.shape[:2], dtype=torch.bool)
-+        out=[]
-+        for p,m in iter(zip(logits,mask)):
-+            out.append(pad_to_len(logits.shape[1],self._viterbi_decode(p.unsqueeze(0),m.unsqueeze(0))))
-+        return torch.tensor(out)
-+        
-     def _viterbi_decode(self, logits: Tensor, mask: Tensor) -> LongTensor:
-         """
-         decode labels using viterbi algorithm
-diff --git a/experiment/core/utils.py b/experiment/core/utils.py
-index 058cc87..4be7503 100644
---- a/experiment/core/utils.py
-+++ b/experiment/core/utils.py
-@@ -3,6 +3,7 @@ import torch
- from torch import nn
- import regex as re
- import snoop
-+from copy import deepcopy
- 
- __all__ = ['chunk_examples_with_degree', 'chunk_to_len_batch', 'view_aligned']
- 
-@@ -26,14 +27,15 @@ def position_to_mask(max_seq_length:int,indices:list):
-         o[np.array(indices)%(max_seq_length-2)+1]=1
-     except:
-         pp('position_to_mask',np.array(indices)%(max_seq_length-2)+1)
--        # o[(np.array(indices)%(max_seq_length-2)+1).astype(int)]=1
-+        o[(np.array(indices)%(max_seq_length-2)+1).astype(int)]=1
-     return o
- 
- def align_labels_to_mask(mask,labels):
-     '''[0,1,0],[2] -> [0,2,0]'''
-     assert(sum(mask)==len(labels))
--    mask[mask>0]=torch.tensor(labels)
--    return mask.tolist()
-+    m1=mask.copy()
-+    m1[mask>0]=torch.tensor(labels)
-+    return m1.tolist()
- 
- def view_aligned(texts,tags,tokenizer,labels_to_ids):
-         return [re.sub(' ##','',' '.join(
-@@ -101,7 +103,9 @@ def chunk_to_len(max_seq_length,tokenizer,tokens,labels=None):
-     split_token_end_idxs=np.array_split(token_end_idxs,breakpoints)
-     split_subwords=np.array_split(subwords,np.arange(max_seq_length-2,len(subwords),max_seq_length-2))
-     ids=[pad_to_len(max_seq_length,tokenizer.convert_tokens_to_ids(['[CLS]']+list(_)+['[SEP]'])) for _ in split_subwords]
--    masks=[position_to_mask(max_seq_length,_) for _ in split_token_end_idxs]
-+    masks=[]
-+    for _ in split_token_end_idxs:
-+        masks.append(position_to_mask(max_seq_length,_).copy())
-     padded_labels=None
-     if labels!=None:
-         split_labels=np.array_split(labels,breakpoints)
-@@ -121,7 +125,7 @@ def chunk_to_len_batch(max_seq_length,tokenizer,tokens,labels=None,labelled=True
-     output = {'input_ids': torch.as_tensor(batch_ids, dtype=torch.long),
-               'attention_mask': torch.as_tensor(batch_ids, dtype=torch.bool),
-               'subtoken_mask': torch.as_tensor(batch_masks,dtype=torch.bool)}
--    output['subtoken_mask']|=(output['input_ids']==101)|(output['input_ids']==102)
-+    output['subtoken_mask']|=((output['input_ids']==101)|(output['input_ids']==102))
-     output['subtoken_mask']&=labelled
-     output['labels']=torch.as_tensor(batch_labels,dtype=torch.long) if labelled==True else torch.zeros_like(output['input_ids'],dtype=torch.long)
-     return output
-diff --git a/experiment/data/punctuation_dataset.py b/experiment/data/punctuation_dataset.py
-index bfd015c..c3d9fb6 100644
---- a/experiment/data/punctuation_dataset.py
-+++ b/experiment/data/punctuation_dataset.py
-@@ -10,6 +10,7 @@ import torch
- import subprocess
- from time import time
- from itertools import cycle
-+import math
- 
- class PunctuationDomainDataset(IterableDataset):
- 
-@@ -242,18 +243,23 @@ class PunctuationInferenceDataset(Dataset):
-             "labels": NeuralType(('B', 'T'), ChannelType()),
-         }
- 
--    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], degree:int = 0, ):
-+    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], num_samples:int=256, degree:int = 0):
-         """ Initializes BertPunctuationInferDataset. """
-+        self.degree=degree
-+        self.punct_label_ids=punct_label_ids
-         chunked=chunk_examples_with_degree(self.degree, self.punct_label_ids)(queries)
-         features = chunk_to_len_batch(max_seq_length=max_seq_length, tokenizer=tokenizer,tokens=chunked['texts'],labelled=False)
-         self.all_input_ids = features['input_ids']
-         self.all_attention_mask = features['attention_mask']
-         self.all_subtoken_mask = features['subtoken_mask']
-+        self.num_samples=num_samples
- 
-     def __len__(self):
--        return len(self.all_input_ids)
-+        return math.ceil(len(self.all_input_ids)/self.num_samples)
- 
-     def __getitem__(self, idx):
--        return {'input_ids':self.all_input_ids[idx],
--                'attention_mask':self.all_attention_mask[idx],
--                'subtoken_mask':self.all_subtoken_mask[idx]}
-+        lower=idx*self.num_samples
-+        upper=min(len(self.all_input_ids),(idx+1)*self.num_samples+1)
-+        return {'input_ids':self.all_input_ids[lower:upper],
-+                'attention_mask':self.all_attention_mask[lower:upper],
-+                'subtoken_mask':self.all_subtoken_mask[lower:upper]}
-diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
-index 4027bb2..b3fe282 100644
---- a/experiment/data/punctuation_dataset_multi.py
-+++ b/experiment/data/punctuation_dataset_multi.py
-@@ -261,18 +261,23 @@ class PunctuationInferenceDataset(Dataset):
-             "labels": NeuralType(('B', 'T'), ChannelType()),
-         }
- 
--    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], degree:int = 0, ):
-+    def __init__(self, tokenizer, queries: List[str], max_seq_length: int, punct_label_ids:Dict[str,int], num_samples:int=256, degree:int = 0):
-         """ Initializes BertPunctuationInferDataset. """
-+        self.degree=degree
-+        self.punct_label_ids=punct_label_ids
-         chunked=chunk_examples_with_degree(self.degree, self.punct_label_ids)(queries)
-         features = chunk_to_len_batch(max_seq_length=max_seq_length, tokenizer=tokenizer,tokens=chunked['texts'],labelled=False)
-         self.all_input_ids = features['input_ids']
-         self.all_attention_mask = features['attention_mask']
-         self.all_subtoken_mask = features['subtoken_mask']
-+        self.num_samples=num_samples
- 
-     def __len__(self):
--        return len(self.all_input_ids)
-+        return math.ceil(len(self.all_input_ids)/self.num_samples)
- 
-     def __getitem__(self, idx):
--        return {'input_ids':self.all_input_ids[idx],
--                'attention_mask':self.all_attention_mask[idx],
--                'subtoken_mask':self.all_subtoken_mask[idx]}
-+        lower=idx*self.num_samples
-+        upper=min(len(self.all_input_ids),(idx+1)*self.num_samples+1)
-+        return {'input_ids':self.all_input_ids[lower:upper],
-+                'attention_mask':self.all_attention_mask[lower:upper],
-+                'subtoken_mask':self.all_subtoken_mask[lower:upper]}
-diff --git a/experiment/info.log b/experiment/info.log
-index d9d501b..e69de29 100644
---- a/experiment/info.log
-+++ b/experiment/info.log
-@@ -1,17 +0,0 @@
--[INFO] - shuffling train set
--[INFO] - Optimizer config = AdamW (
--Parameter Group 0
--    amsgrad: False
--    betas: (0.9, 0.999)
--    eps: 1e-08
--    lr: 0.001
--    weight_decay: 0.0
--)
--[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f412fde0d90>" 
--will be used during training (effective maximum steps = 80) - 
--Parameters : 
--(warmup_steps: null
--warmup_ratio: 0.1
--last_epoch: -1
--max_steps: 80
--)
-diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
-index c5db7b3..aa05eac 100644
---- a/experiment/models/punctuation_domain_model.py
-+++ b/experiment/models/punctuation_domain_model.py
-@@ -15,9 +15,9 @@ from core.losses import (AggregatorLoss, CrossEntropyLoss, FocalDiceLoss, FocalL
- from pytorch_lightning.utilities import rank_zero_only
- from core.optim import get_optimizer, parse_optimizer_args, prepare_lr_scheduler
- from omegaconf import DictConfig, OmegaConf, open_dict
--from transformers import AutoModel
-+from transformers import AutoModel, AutoTokenizer
- import torch.utils.data.dataloader as dataloader
--from data import PunctuationDataModule
-+from data import PunctuationDataModule, PunctuationInferenceDataset
- from os import path
- import tempfile
- from core.common import Serialization, FileIO
-@@ -51,6 +51,7 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-         self._trainer = trainer
- 
-         self.transformer = AutoModel.from_pretrained(self.hparams.model.transformer_path)
-+        self.tokenizer=AutoTokenizer.from_pretrained(self._cfg.model.transformer_path)
-         self.ids_to_labels = {_[0]: _[1]
-                               for _ in enumerate(self.hparams.model.punct_label_ids)}
-         self.labels_to_ids = {_[1]: _[0]
-@@ -707,4 +708,23 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
-             if 'PL_TRAINER_GPUS' in os.environ:
-                 os.environ.pop('PL_TRAINER_GPUS')
- 
--        super().teardown(stage)
-\ No newline at end of file
-+        super().teardown(stage)
-+
-+    def add_punctuation(self, queries):
-+        infer_ds=PunctuationInferenceDataset(
-+            tokenizer= self._cfg.model.transformer_path,
-+            queries=queries, 
-+            max_seq_length=self.hparams.model.dataset.max_seq_length,
-+            punct_label_ids=self._cfg.model.punct_label_ids)
-+        attention_mask = batch['attention_mask']
-+        subtoken_mask = batch['subtoken_mask']
-+        punct_labels = batch['labels']
-+        domain_labels = batch['domain']
-+        input_ids = batch['input_ids']
-+
-+        labelled_mask=(subtoken_mask[:,0]>0)
-+        test_loss, punct_logits, domain_logits = self._make_step(batch)
-+        # attention_mask = attention_mask > 0.5
-+        punct_preds = self.punctuation_loss.predict(punct_logits[labelled_mask], subtoken_mask[labelled_mask]) \
-+            if self.hparams.model.punct_head.loss == 'crf' else torch.argmax(punct_logits[labelled_mask], axis=-1)[subtoken_mask[labelled_mask]]
-+        return view_aligned(input_ids,punct_preds, self.tokenizer,self.ids_to_labels)
-\ No newline at end of file
-diff --git a/experiment/utils/__init__.py b/experiment/utils/__init__.py
-deleted file mode 100644
-index 9a292b8..0000000
---- a/experiment/utils/__init__.py
-+++ /dev/null
-@@ -1,2 +0,0 @@
--from utils.logging import Logger as _Logger
--logging = _Logger()
-diff --git a/experiment/utils/logging.py b/experiment/utils/logging.py
-deleted file mode 100644
-index 15511fd..0000000
---- a/experiment/utils/logging.py
-+++ /dev/null
-@@ -1,69 +0,0 @@
--import os.path
--import logging
--import traceback
--
--from logging import DEBUG, WARNING, ERROR, INFO
--__all__ = ['Logger']
--
--class Logger(object):
--
--    show_source_location = True
--    # Formats the message as needed and calls the correct logging method
--    # to actually handle it
--    def _raw_log(self, logfn, message, exc_info):
--        cname = ''
--        loc = ''
--        fn = ''
--        tb = traceback.extract_stack()
--        if len(tb) > 2:
--            if self.show_source_location:
--                loc = '(%s:%d):' % (os.path.basename(tb[-3][0]), tb[-3][1])
--            fn = tb[-3][2]
--            if fn != '<module>':
--                if self.__class__.__name__ != Logger.__name__:
--                    fn = self.__class__.__name__ + '.' + fn
--                fn += '()'
--
--        logfn(loc + cname + fn + ': ' + message, exc_info=exc_info)
--
--    def info(self, message, exc_info=False):
--        """
--        Log a info-level message. If exc_info is True, if an exception
--        was caught, show the exception information (message and stack trace).
--        """
--        self._raw_log(logging.info, message, exc_info)
--
--    def debug(self, message, exc_info=False):
--        """
--        Log a debug-level message. If exc_info is True, if an exception
--        was caught, show the exception information (message and stack trace).
--        """
--        self._raw_log(logging.debug, message, exc_info)
--
--    def warning(self, message, exc_info=False):
--        """
--        Log a warning-level message. If exc_info is True, if an exception
--        was caught, show the exception information (message and stack trace).
--        """
--        self._raw_log(logging.warning, message, exc_info)
--
--    def error(self, message, exc_info=False):
--        """
--        Log an error-level message. If exc_info is True, if an exception
--        was caught, show the exception information (message and stack trace).
--        """
--        self._raw_log(logging.error, message, exc_info)
--
--    @staticmethod
--    def basicConfig(level=DEBUG):
--        """
--        Apply a basic logging configuration which outputs the log to the
--        console (stderr). Optionally, the minimum log level can be set, one
--        of DEBUG, WARNING, ERROR (or any of the levels from the logging
--        module). If not set, DEBUG log level is used as minimum.
--        """
--        logging.basicConfig(level=level,
--                format='%(asctime)s %(levelname)s %(message)s',
--                datefmt='%Y-%m-%d %H:%M:%S')
--
--        logger = Logger()
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/hparams.yaml b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/hparams.yaml
deleted file mode 100644
index cbac11e..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/hparams.yaml
+++ /dev/null
@@ -1,110 +0,0 @@
-seed: 42
-trainer:
-  gpus: 1
-  num_nodes: 1
-  max_epochs: 10
-  max_steps: null
-  accumulate_grad_batches: 4
-  gradient_clip_val: 0
-  amp_level: O1
-  precision: 16
-  accelerator: ddp
-  checkpoint_callback: false
-  logger: false
-  log_every_n_steps: 1
-  val_check_interval: 1.0
-  resume_from_checkpoint: null
-exp_manager:
-  exp_dir: /home/nxingyu/project/
-  name: Punctuation_with_Domain_discriminator
-  create_tensorboard_logger: true
-  create_checkpoint_callback: true
-base_path: /home/nxingyu/data
-tmp_path: /home/nxingyu/data/tmp
-model:
-  nemo_path: null
-  transformer_path: google/electra-small-discriminator
-  unfrozen: 0
-  maximum_unfrozen: 1
-  unfreeze_step: 1
-  punct_label_ids:
-  - ''
-  - '!'
-  - ','
-  - '-'
-  - .
-  - ':'
-  - ;
-  - '?'
-  - —
-  - …
-  punct_class_weights: false
-  dataset:
-    data_dir: /home/nxingyu/data
-    labelled:
-    - /home/nxingyu/data/ted_talks_processed
-    unlabelled:
-    - /home/nxingyu/data/open_subtitles_processed
-    max_seq_length: 128
-    pad_label: ''
-    ignore_extra_tokens: false
-    ignore_start_end: false
-    use_cache: false
-    num_workers: 4
-    pin_memory: true
-    drop_last: false
-    num_labels: 10
-    num_domains: 2
-    test_unlabelled: true
-    train_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 4
-    validation_ds:
-      shuffle: true
-      num_samples: -1
-      batch_size: 4
-  tokenizer:
-    tokenizer_name: google/electra-small-discriminator
-    vocab_file: null
-    tokenizer_model: null
-    special_tokens: null
-  language_model:
-    pretrained_model_name: google/electra-small-discriminator
-    lm_checkpoint: null
-    config_file: null
-    config: null
-  punct_head:
-    punct_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: dice
-  domain_head:
-    domain_num_fc_layers: 1
-    fc_dropout: 0.1
-    activation: relu
-    log_softmax: false
-    use_transformer_init: true
-    loss: cel
-    gamma: 0
-    pooling: mean_max
-    idx_conditioned_on: 0
-  dice_loss:
-    epsilon: 0.01
-    alpha: 3
-    macro_average: true
-  focal_loss:
-    gamma: 1
-  optim:
-    name: adamw
-    lr: 0.001
-    weight_decay: 0.0
-    sched:
-      name: WarmupAnnealing
-      warmup_steps: null
-      warmup_ratio: 0.1
-      last_epoch: -1
-      monitor: val_loss
-      reduce_on_plateau: false
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/lightning_logs.txt b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/lightning_logs.txt
deleted file mode 100644
index c7d0c2d..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/lightning_logs.txt
+++ /dev/null
@@ -1,39 +0,0 @@
-Global seed set to 42
-GPU available: True, used: True
-TPU available: None, using: 0 TPU cores
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
-Using native 16bit precision.
-Global seed set to 42
-initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 1.0 K 
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-299 K     Trainable params
-13.2 M    Non-trainable params
-13.5 M    Total params
-Restored states from the checkpoint file at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/lr_find_temp_model.ckpt
-Global seed set to 42
-
-  | Name                | Type                 | Params
--------------------------------------------------------------
-0 | transformer         | ElectraModel         | 13.5 M
-1 | punct_classifier    | TokenClassifier      | 2.6 K 
-2 | domain_classifier   | SequenceClassifier   | 1.0 K 
-3 | punctuation_loss    | FocalDiceLoss        | 0     
-4 | domain_loss         | CrossEntropyLoss     | 0     
-5 | agg_loss            | AggregatorLoss       | 0     
-6 | punct_class_report  | ClassificationReport | 0     
-7 | domain_class_report | ClassificationReport | 0     
--------------------------------------------------------------
-299 K     Trainable params
-13.2 M    Non-trainable params
-13.5 M    Total params
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_error_log.txt b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_error_log.txt
deleted file mode 100644
index 0c8b389..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_error_log.txt
+++ /dev/null
@@ -1,10 +0,0 @@
-[NeMo W 2021-02-09 11:10:37 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-09 11:11:08 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-09 11:18:33 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-09 11:24:13 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-    
diff --git a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_log_globalrank-0_localrank-0.txt b/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_log_globalrank-0_localrank-0.txt
deleted file mode 100644
index 84d61e5..0000000
--- a/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37/nemo_log_globalrank-0_localrank-0.txt
+++ /dev/null
@@ -1,12 +0,0 @@
-[NeMo I 2021-02-09 11:10:37 exp_manager:183] Experiments will be logged at /home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_11-10-37
-[NeMo I 2021-02-09 11:10:37 exp_manager:519] TensorboardLogger has been set up
-[NeMo W 2021-02-09 11:10:37 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
-[NeMo W 2021-02-09 11:11:08 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-09 11:18:33 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.
-      warnings.warn(*args, **kwargs)
-    
-[NeMo W 2021-02-09 11:24:13 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
-      warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
-    
diff --git a/experiment/Untitled.ipynb b/experiment/Untitled.ipynb
index 1a5526d..f5bb089 100644
--- a/experiment/Untitled.ipynb
+++ b/experiment/Untitled.ipynb
@@ -3,33 +3,33 @@
   {
    "cell_type": "code",
    "execution_count": 1,
-   "id": "dense-meaning",
+   "id": "modern-amplifier",
    "metadata": {},
    "outputs": [
     {
      "name": "stderr",
      "output_type": "stream",
      "text": [
-      "12:16:24.02 LOG:\n"
+      "14:59:48.20 LOG:\n"
      ]
     },
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "shuffling <data.punctuation_dataset_multi.PunctuationDomainDataset object at 0x7f86e5bc2220>\n"
+      "shuffling <data.punctuation_dataset_multi.PunctuationDomainDataset object at 0x7f44ea05e5b0>\n"
      ]
     },
     {
      "name": "stderr",
      "output_type": "stream",
      "text": [
-      "12:16:24.11 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
-      "12:16:24.17 LOG:\n",
-      "12:16:24.48 .... f\"1st {n} encoder layers of transformer frozen\" = '1st 11 encoder layers of transformer frozen'\n",
+      "14:59:48.28 .... os.system('bash data/shuffle.sh -i {} -o {} -a {} -s {} -m {} -t {}'.format(self.target_file, self.target_file, ['true','false'][randomize], seed, '100M',self.tmp_path)) = 0\n",
+      "14:59:48.34 LOG:\n",
+      "14:59:48.63 .... f\"1st {n} encoder layers of transformer frozen\" = '1st 11 encoder layers of transformer frozen'\n",
       "GPU available: True, used: False\n",
       "TPU available: None, using: 0 TPU cores\n",
-      "[NeMo W 2021-02-09 12:16:24 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
+      "[NeMo W 2021-02-09 14:59:48 nemo_logging:349] /home/nxingyu/miniconda3/envs/NLP/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
       "      warnings.warn(*args, **kwargs)\n",
       "    \n"
      ]
@@ -67,168 +67,27 @@
     "ids_to_labels = {_[0]:_[1] for _ in enumerate(cfg.model.punct_label_ids)}\n",
     "\n",
     "model = PunctuationDomainModel.load_from_checkpoint(\n",
-    "    checkpoint_path=\"/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-08_11-57-58/checkpoints/Punctuation_with_Domain_discriminator-last.ckpt\")\n",
+    "    checkpoint_path=\"/home/nxingyu/project/Punctuation_with_Domain_discriminator/2021-02-09_14-05-14/checkpoints/Punctuation_with_Domain_discriminator-last.ckpt\")\n",
     "\n",
     "trainer = pl.Trainer(**cfg.trainer)"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 2,
-   "id": "potential-adrian",
+   "execution_count": 8,
+   "id": "hairy-proxy",
    "metadata": {},
    "outputs": [
     {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "12:16:24.62 LOG:\n",
-      "12:16:24.69 .... chunk_to_len_batch(max_seq_length, tokenizer,chunked['texts'],chunked['tags'],True) = {'attention_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
-      "12:16:24.69                                                                                                      True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
-      "12:16:24.69                                                                                                      True, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False],\n",
-      "12:16:24.69                                                                                                    [ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False],\n",
-      "12:16:24.69                                                                                                    [ True,  True,  True,  True,  True, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False]]),\n",
-      "12:16:24.69                                                                                             'input_ids': tensor([[  101,  2057,  4149,  2176, 11344,  2028,  7279,  1998,  1037, 14757,\n",
-      "12:16:24.69                                                                                                      2013,  1996,  1050, 17258,  2401,  6718,  3573,  1999,  4203, 10254,\n",
-      "12:16:24.69                                                                                                       102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0],\n",
-      "12:16:24.69                                                                                                    [  101,  2054,  2064,  1045,  2079,  2005,  2017,  2651,   102,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0],\n",
-      "12:16:24.69                                                                                                    [  101,  2129,  2024,  2017,   102,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
-      "12:16:24.69                                                                                                         0,     0,     0,     0,     0,     0,     0,     0]]),\n",
-      "12:16:24.69                                                                                             'labels': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0],\n",
-      "12:16:24.69                                                                                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0],\n",
-      "12:16:24.69                                                                                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-      "12:16:24.69                                                                                                     0, 0, 0, 0, 0, 0, 0, 0]]),\n",
-      "12:16:24.69                                                                                             'subtoken_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
-      "12:16:24.69                                                                                                      True,  True, False, False,  True,  True,  True,  True,  True,  True,\n",
-      "12:16:24.69                                                                                                      True, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False],\n",
-      "12:16:24.69                                                                                                    [ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False],\n",
-      "12:16:24.69                                                                                                    [ True,  True,  True,  True,  True, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False, False, False,\n",
-      "12:16:24.69                                                                                                     False, False, False, False, False, False, False, False]])}\n"
-     ]
-    },
-    {
      "data": {
       "text/plain": [
-       "['[CLS] we, bought, four- shirts: one, pen, and, a, mug, from, the, nvidia- gear, store, in, santa- clara, [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
-       " '[CLS] what? can— i? do? for? you? today? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
-       " '[CLS] how? are? you? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']"
+       "[' we bought four shirts, one pen, and a mug from the nvidia gear store in santa clara                                                                                                            ',\n",
+       " ' what can i do for you today?                                                                                                                        ',\n",
+       " ' , how are you? ,                                                                                                                           ',\n",
+       " ' good morning. everyone? how have your weekends been? its a really great day?                                                                                                                  ']"
       ]
      },
-     "execution_count": 2,
+     "execution_count": 8,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -238,6 +97,7 @@
     "    'we bought four shirts one pen and a mug from the nvidia gear store in santa clara',\n",
     "    'what can i do for you today',\n",
     "    'how are you',\n",
+    "    'good morning everyone how have your weekends been its a really great day'\n",
     "]\n",
     "inference_results = model.add_punctuation(queries)\n",
     "inference_results"
@@ -246,7 +106,7 @@
   {
    "cell_type": "code",
    "execution_count": null,
-   "id": "amateur-production",
+   "id": "employed-station",
    "metadata": {},
    "outputs": [],
    "source": []
diff --git a/experiment/config.yaml b/experiment/config.yaml
index efa7a5d..ba6412d 100644
--- a/experiment/config.yaml
+++ b/experiment/config.yaml
@@ -41,7 +41,7 @@ tmp_path: /home/nxingyu/data/tmp # /tmp #
 
 model:
     nemo_path: null
-    transformer_path: google/electra-small-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
+    transformer_path: distilbert-base-uncased #google/electra-base-discriminator # distilbert-base-uncased #  filename to save the model and associated artifacts to .nemo file
     unfrozen: 0
     maximum_unfrozen: 1
     unfreeze_step: 1
@@ -67,7 +67,7 @@ model:
             # - ${base_path}/open_subtitles_processed #  
         unlabelled:
             # - ${base_path}/ted_talks_processed #
-            - ${base_path}/open_subtitles_processed #  
+            # - ${base_path}/open_subtitles_processed #  
             # parameters for dataset preprocessing
         max_seq_length: 128
         pad_label: ''
@@ -77,9 +77,9 @@ model:
         # shared among dataloaders
         num_workers:  4
         pin_memory: true
-        drop_last: false
+        drop_last: true
         num_labels: 10
-        num_domains: 2
+        num_domains: 1
         test_unlabelled: true
 
         train_ds:
@@ -137,7 +137,7 @@ model:
 
     optim:
         name: adamw
-        lr: 1e-3
+        lr: 0.009261935523740748 #1e-3
         weight_decay: 0.00
         sched:
             name: WarmupAnnealing #CyclicLR
diff --git a/experiment/core/utils.py b/experiment/core/utils.py
index 4be7503..ce7436b 100644
--- a/experiment/core/utils.py
+++ b/experiment/core/utils.py
@@ -38,12 +38,12 @@ def align_labels_to_mask(mask,labels):
     return m1.tolist()
 
 def view_aligned(texts,tags,tokenizer,labels_to_ids):
-        return [re.sub(' ##','',' '.join(
+        return [re.sub(r'( ?\[((PAD)|(CLS)|(SEP))\] ?)',' ',re.sub(' ##','',' '.join(
             [_[0]+_[1] for _ in list(
                 zip(tokenizer.convert_ids_to_tokens(_[0]),
                     [labels_to_ids[id] for id in _[1].tolist()])
             )]
-        )) for _ in zip(texts,tags)]
+        ))) for _ in zip(texts,tags)]
 
 def text2masks(n, labels_to_ids):
     def text2masks(text):
diff --git a/experiment/data/punctuation_datamodule.py b/experiment/data/punctuation_datamodule.py
index 6978318..20a4093 100644
--- a/experiment/data/punctuation_datamodule.py
+++ b/experiment/data/punctuation_datamodule.py
@@ -54,9 +54,10 @@ class PunctuationDataModule(LightningDataModule):
         self.test_unlabelled=test_unlabelled
     
     def reset(self):
-        self.train_dataset.__iter__()
-        self.val_dataset.__iter__()
-        self.test_dataset.__iter__()
+        # self.setup('fit')
+        self.train_dataset=iter(self.train_dataset)
+        self.val_dataset=iter(self.val_dataset)
+        self.test_dataset=iter(self.test_dataset)
 
     def setup(self, stage=None):
         if stage=='fit' or stage is None:
diff --git a/experiment/data/punctuation_dataset_multi.py b/experiment/data/punctuation_dataset_multi.py
index 03d661c..88a268f 100644
--- a/experiment/data/punctuation_dataset_multi.py
+++ b/experiment/data/punctuation_dataset_multi.py
@@ -167,7 +167,7 @@ class PunctuationDomainDatasets(IterableDataset):
         self.max_length=max(self.ds_lengths)
         self.len=int(self.max_length/num_samples)
         self.per_worker=int(self.max_length/self.num_workers)
-
+        self.class_weights=None
 
         for i,path in enumerate(labelled):
             target=os.path.join(tmp_path,os.path.split(path)[1])
diff --git a/experiment/info.log b/experiment/info.log
index 69e9a76..e69de29 100644
--- a/experiment/info.log
+++ b/experiment/info.log
@@ -1,85 +0,0 @@
-[INFO] - shuffling train set
-[INFO] - Optimizer config = AdamW (
-Parameter Group 0
-    amsgrad: False
-    betas: (0.9, 0.999)
-    eps: 1e-08
-    lr: 0.001
-    weight_decay: 0.0
-)
-[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f717ffe5b80>" 
-will be used during training (effective maximum steps = 80) - 
-Parameters : 
-(warmup_steps: null
-warmup_ratio: 0.1
-last_epoch: -1
-max_steps: 80
-)
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          89.19      22.74      36.24       3012
-! (label_id: 1)                                          0.00       0.00       0.00          1
-, (label_id: 2)                                          7.31      36.21      12.16        243
-- (label_id: 3)                                          2.27      21.43       4.11         28
-. (label_id: 4)                                          1.68       1.65       1.66        182
-: (label_id: 5)                                          0.00       0.00       0.00          5
-; (label_id: 6)                                          0.00       0.00       0.00          3
-? (label_id: 7)                                          0.24      22.22       0.48          9
-— (label_id: 8)                                          0.00       0.00       0.00         10
-… (label_id: 9)                                          0.00       0.00       0.00          1
--------------------
-micro avg                                               22.44      22.44      22.44       3494
-macro avg                                               10.07      10.43       5.47       3494
-weighted avg                                            77.50      22.44      32.21       3494
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         50.00     100.00      66.67         34
-1 (label_id: 1)                                          0.00       0.00       0.00         34
--------------------
-micro avg                                               50.00      50.00      50.00         68
-macro avg                                               25.00      50.00      33.33         68
-weighted avg                                            25.00      50.00      33.33         68
-
-[INFO] - Optimizer config = AdamW (
-Parameter Group 0
-    amsgrad: False
-    betas: (0.9, 0.999)
-    eps: 1e-08
-    lr: 0.007943282347242822
-    weight_decay: 0.0
-)
-[INFO] - Scheduler "<core.optim.lr_scheduler.WarmupAnnealing object at 0x7f717f560fa0>" 
-will be used during training (effective maximum steps = 53050) - 
-Parameters : 
-(warmup_steps: null
-warmup_ratio: 0.1
-last_epoch: -1
-max_steps: 53050
-)
-[INFO] - Punctuation report: 
-label                                                precision    recall       f1           support   
- (label_id: 0)                                          89.19      22.74      36.24       3012
-! (label_id: 1)                                          0.00       0.00       0.00          1
-, (label_id: 2)                                          7.31      36.21      12.16        243
-- (label_id: 3)                                          2.27      21.43       4.11         28
-. (label_id: 4)                                          1.68       1.65       1.66        182
-: (label_id: 5)                                          0.00       0.00       0.00          5
-; (label_id: 6)                                          0.00       0.00       0.00          3
-? (label_id: 7)                                          0.24      22.22       0.48          9
-— (label_id: 8)                                          0.00       0.00       0.00         10
-… (label_id: 9)                                          0.00       0.00       0.00          1
--------------------
-micro avg                                               22.44      22.44      22.44       3494
-macro avg                                               10.07      10.43       5.47       3494
-weighted avg                                            77.50      22.44      32.21       3494
-
-[INFO] - Domain report: 
-label                                                precision    recall       f1           support   
-0 (label_id: 0)                                         50.00     100.00      66.67         34
-1 (label_id: 1)                                          0.00       0.00       0.00         34
--------------------
-micro avg                                               50.00      50.00      50.00         68
-macro avg                                               25.00      50.00      33.33         68
-weighted avg                                            25.00      50.00      33.33         68
-
diff --git a/experiment/main.py b/experiment/main.py
index 6f0a8ea..6b15e25 100644
--- a/experiment/main.py
+++ b/experiment/main.py
@@ -39,7 +39,7 @@ def main(cfg: DictConfig)->None:
     
     while(model.hparams.model.unfrozen<=cfg.model.maximum_unfrozen and model.hparams.model.unfrozen>=0):
         trainer.current_epoch=0
-        lr_finder = trainer.tuner.lr_find(model,min_lr=1e-10, max_lr=1e-02, num_training=80, early_stop_threshold=None)
+        lr_finder = trainer.tuner.lr_find(model,min_lr=1e-8, max_lr=0.5, num_training=80) #, early_stop_threshold=None
         # Results can be found in
         pp(lr_finder.results)
         new_lr = lr_finder.suggestion()
diff --git a/experiment/models/punctuation_domain_model.py b/experiment/models/punctuation_domain_model.py
index dab395e..999ae0d 100644
--- a/experiment/models/punctuation_domain_model.py
+++ b/experiment/models/punctuation_domain_model.py
@@ -130,9 +130,13 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
         )[0]
         punct_logits = self.punct_classifier(hidden_states=hidden_states)
         reverse_grad_hidden_states = self.grad_reverse.apply(hidden_states)
+        assert not torch.isnan(input_ids).any(), (input_ids,'inputid')
+        assert not torch.isnan(attention_mask).any(), ('amask',attention_mask)
+        assert not torch.isnan(hidden_states).any(), (hidden_states,attention_mask.sum(1),'hiddenstate')
         domain_logits = self.domain_classifier(
             hidden_states=reverse_grad_hidden_states,
             attention_mask=attention_mask)
+        # print(attention_mask.sum(axis=1),domain_logits)
         return punct_logits, domain_logits
 
     def _make_step(self, batch):
@@ -157,6 +161,8 @@ class PunctuationDomainModel(pl.LightningModule, Serialization, FileIO):
         passed in as `batch`.
         """
         p=(self.current_epoch*self.train_size+batch_idx)/(self.train_size*self.hparams.trainer.max_epochs)
+        if (batch_idx%1000==0):
+            print('gamma:',p)
         self.grad_reverse.scale=2/(1+math.exp(-10*p))-1
         loss, _, _ = self._make_step(batch)
         lr = self._optimizer.param_groups[0]['lr']
